{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ee5074-bc96-4fc0-b5be-04a5dd07ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "import fastcoref\n",
    "from fastcoref import FCoref\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe84c00b-659f-471b-a36c-0e20bf8c4682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffb780c052d48efb9a5ca2d80a37d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 16:48:24 INFO: Downloaded file to C:\\Users\\lbeln\\stanza_resources\\resources.json\n",
      "03/14/2025 16:48:24 - INFO - \t Downloaded file to C:\\Users\\lbeln\\stanza_resources\\resources.json\n",
      "2025-03-14 16:48:24 INFO: Downloading default packages for language: en (English) ...\n",
      "03/14/2025 16:48:24 - INFO - \t Downloading default packages for language: en (English) ...\n",
      "2025-03-14 16:48:28 INFO: File exists: C:\\Users\\lbeln\\stanza_resources\\en\\default.zip\n",
      "03/14/2025 16:48:28 - INFO - \t File exists: C:\\Users\\lbeln\\stanza_resources\\en\\default.zip\n",
      "2025-03-14 16:48:37 INFO: Finished downloading models and saved to C:\\Users\\lbeln\\stanza_resources\n",
      "03/14/2025 16:48:37 - INFO - \t Finished downloading models and saved to C:\\Users\\lbeln\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "stanza.download(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb0d9cd-c0d9-4236-b49c-63a73b24dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 16:50:22 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "03/14/2025 16:50:22 - INFO - \t Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8bc9711918424ba62b0867c6738a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 16:50:23 INFO: Downloaded file to C:\\Users\\lbeln\\stanza_resources\\resources.json\n",
      "03/14/2025 16:50:23 - INFO - \t Downloaded file to C:\\Users\\lbeln\\stanza_resources\\resources.json\n",
      "2025-03-14 16:50:26 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "03/14/2025 16:50:26 - INFO - \t Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2025-03-14 16:50:26 INFO: Using device: cpu\n",
      "03/14/2025 16:50:26 - INFO - \t Using device: cpu\n",
      "2025-03-14 16:50:26 INFO: Loading: tokenize\n",
      "03/14/2025 16:50:26 - INFO - \t Loading: tokenize\n",
      "2025-03-14 16:50:27 INFO: Loading: mwt\n",
      "03/14/2025 16:50:27 - INFO - \t Loading: mwt\n",
      "2025-03-14 16:50:27 INFO: Loading: pos\n",
      "03/14/2025 16:50:27 - INFO - \t Loading: pos\n",
      "2025-03-14 16:50:33 INFO: Loading: lemma\n",
      "03/14/2025 16:50:33 - INFO - \t Loading: lemma\n",
      "2025-03-14 16:50:35 INFO: Loading: constituency\n",
      "03/14/2025 16:50:35 - INFO - \t Loading: constituency\n",
      "2025-03-14 16:50:37 INFO: Loading: depparse\n",
      "03/14/2025 16:50:37 - INFO - \t Loading: depparse\n",
      "2025-03-14 16:50:38 INFO: Loading: sentiment\n",
      "03/14/2025 16:50:38 - INFO - \t Loading: sentiment\n",
      "2025-03-14 16:50:39 INFO: Loading: ner\n",
      "03/14/2025 16:50:39 - INFO - \t Loading: ner\n",
      "2025-03-14 16:50:45 INFO: Done loading processors!\n",
      "03/14/2025 16:50:45 - INFO - \t Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b386c50c-4412-47c0-8a6a-ebc5e7f20c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dependents(id, doc):\n",
    "    dependents = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.head == id:\n",
    "                dependents.append(word)\n",
    "    return dependents\n",
    "\n",
    "def find_start_verb(doc):\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.upos == \"VERB\":\n",
    "                return word\n",
    "    return None\n",
    "\n",
    "def find_object(doc, verb):\n",
    "    dependents = find_dependents(verb.id, doc)\n",
    "    for word in dependents:\n",
    "        if word.deprel == \"obj\" or word.deprel == \"nsubj:pass\":\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def find_subject(doc, verb):\n",
    "    dependents = find_dependents(verb.id, doc)\n",
    "    for word in dependents:\n",
    "        if word.deprel == \"nsubj\" or word.deprel == \"obl:agent\":\n",
    "            return word\n",
    "    \n",
    "    for word in dependents:\n",
    "        if word.deprel == \"advcl\":\n",
    "            subject = find_subject(doc, word)\n",
    "            if subject:\n",
    "                return subject\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8e1a5af-8f9a-4ee0-9097-e01f83892dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_noun(doc, word):\n",
    "    dependents = find_dependents(word.id, doc)\n",
    "    for dependent in dependents:\n",
    "        if dependent.deprel == \"nmod:poss\":\n",
    "            return find_noun(doc, dependent)\n",
    "    if word.upos == \"NOUN\" or word.upos == \"PRON\":\n",
    "        return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1845811-dd25-40de-ac86-656afae5df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_reference(sentence, a, b, doc=None):\n",
    "    if not doc:\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "    model = FCoref(enable_progress_bar=False)\n",
    "    \n",
    "    noun_a = find_noun(doc, a)\n",
    "    noun_b = find_noun(doc, b)\n",
    "\n",
    "    if not noun_a or not noun_b:\n",
    "        return False\n",
    "\n",
    "    a_pos = (noun_a.start_char, noun_a.end_char)\n",
    "    b_pos = (noun_b.start_char, noun_b.end_char)\n",
    "\n",
    "    a_found = False\n",
    "    b_found = False\n",
    "    \n",
    "    clusters = model.predict(texts=[sentence])[0].get_clusters(as_strings=False)\n",
    "    for cluster in clusters:\n",
    "        for c in cluster:\n",
    "            if a_pos[0] >= c[0] and a_pos[0] <= c[1] and a_pos[1] >= c[0] and a_pos[1] <= c[1]:\n",
    "                a_found = True\n",
    "            if b_pos[0] >= c[0] and b_pos[0] <= c[1] and b_pos[1] >= c[0] and b_pos[1] <= c[1]:\n",
    "                b_found = True\n",
    "            if a_found and b_found:\n",
    "                return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43f5626e-a108-496a-8aee-3a3652bbb46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relations(sentence):\n",
    "    relations = []\n",
    "    doc = nlp(sentence)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.upos != \"VERB\":\n",
    "                continue\n",
    "            verb = word\n",
    "            object = find_object(doc, verb)\n",
    "            subject = find_subject(doc, verb)\n",
    "            if verb and object and subject and not same_reference(sentence, object, subject, doc=doc) and is_species_or_trait(sentence, object) and is_species_or_trait(sentence, subject):\n",
    "                relations.append({\"verb\": verb, \"object\": object, \"subject\": subject})\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e0ae5b8-b93c-44cb-a248-1d8b7280699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context(sentence, stanza_word):\n",
    "    snlp = spacy.load(\"en_core_web_sm\")\n",
    "    matcher = Matcher(snlp.vocab)\n",
    "    \n",
    "    pattern = [\n",
    "        {\"POS\": {\"IN\": [\"DET\", \"ADJ\", \"NOUN\", \"PROPN\"]}, \"OP\": \"+\"},\n",
    "        {\"POS\": \"ADP\", \"OP\": \"?\"},\n",
    "        {\"POS\": {\"IN\": [\"DET\", \"ADJ\", \"NOUN\", \"PROPN\"]}, \"OP\": \"+\"}\n",
    "    ]\n",
    "    matcher.add(\"NOUN_PHRASE\", [pattern])\n",
    "\n",
    "    doc = snlp(sentence)\n",
    "    spans = [doc[start:end] for _, start, end in matcher(doc)]\n",
    "    for span in spacy.util.filter_spans(spans):\n",
    "        for word in span:\n",
    "            if stanza_word.start_char == word.idx and stanza_word.end_char == (word.idx + len(word)):\n",
    "                return span\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c17505e9-e46f-46fb-8c39-3a3167607fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_species_or_trait(context_span):\n",
    "    contains_traits = \"\"\n",
    "    contains_species = \"\"\n",
    "\n",
    "    # There is no current way to determine whether\n",
    "    # a species is included or not, I'd have to fine-tune\n",
    "    # a model for that. So, for now, I'm hard-coding it.\n",
    "    for word in context_span:\n",
    "        if word.text[0:2] == \"TR\":\n",
    "            contains_traits = \"TR\"\n",
    "        if word.text[0:2] == \"SP\":\n",
    "            contains_species = \"SP\"\n",
    "\n",
    "    return [contains_traits, contains_species]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83dd22b4-3f22-4969-bbbb-bb495b4aeb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_species_or_trait(sentence, stanza_word):\n",
    "    context = find_context(sentence, stanza_word)\n",
    "    if not context:\n",
    "        return False\n",
    "    \n",
    "    st_context = find_species_or_trait(context)\n",
    "    return st_context[0] != \"\" or st_context[1] != \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a77d961b-b6a9-41db-a30d-b58a29b6bf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/14/2025 18:20:44 - INFO - \t missing_keys: []\n",
      "03/14/2025 18:20:44 - INFO - \t unexpected_keys: []\n",
      "03/14/2025 18:20:44 - INFO - \t mismatched_keys: []\n",
      "03/14/2025 18:20:44 - INFO - \t error_msgs: []\n",
      "03/14/2025 18:20:44 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "03/14/2025 18:20:44 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c0355a4d9d4e92991ea7f89af961f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/14/2025 18:20:44 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb: reduced, Object: TRAIT, Subject: Presence\n",
      "Object Context: \"the TRAIT of SPECIES2\"\n",
      "Subject Context: \"Presence of SPECIES1\"\n"
     ]
    }
   ],
   "source": [
    "sentence_basic = \"Presence of SPECIES1 reduced the TRAIT of SPECIES2 on SPECIES3\"\n",
    "relations = find_relations(sentence_basic)\n",
    "\n",
    "if not relations:\n",
    "    print(\"No Relations\")\n",
    "for r in relations:\n",
    "    print(f\"Verb: {r['verb'].text}, Object: {r['object'].text}, Subject: {r['subject'].text}\")\n",
    "    print(f\"Object Context: \\\"{find_context(sentence_basic, r['object'])}\\\"\")\n",
    "    print(f\"Subject Context: \\\"{find_context(sentence_basic, r['subject'])}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbcbc37-0f0a-4ae3-b36e-beae5c69f69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
