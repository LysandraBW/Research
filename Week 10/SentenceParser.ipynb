{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "448ab48e-4188-49b4-af53-af4b47d00abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import stanza\n",
    "import textacy\n",
    "from fastcoref import FCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0aa3bba9-2983-4f25-a3c1-792a1d084600",
   "metadata": {},
   "outputs": [],
   "source": [
    "class References:\n",
    "    fcoref = None\n",
    "    \n",
    "    def __init__(self, corpus, token_map=None):\n",
    "        if not References.fcoref:\n",
    "            References.fcoref = FCoref(enable_progress_bar=False)\n",
    "        if isinstance(corpus, list):\n",
    "            corpus = [corpus]        \n",
    "        self.predictions = References.fcoref.predict(texts=corpus)\n",
    "        self.token_map = token_map\n",
    "        self.cluster_map = self.index_to_cluster(self.predictions)\n",
    "\n",
    "    def index_to_cluster(self, predictions):\n",
    "        print(predictions)\n",
    "        index_to_cluster_map = {}\n",
    "        for prediction in predictions:\n",
    "            clusters = prediction.get_clusters(as_strings=False)\n",
    "            for cluster in clusters:\n",
    "                for token in cluster:\n",
    "                    index = token[0]\n",
    "                    if char_token_map:\n",
    "                        if index not in self.token_map:\n",
    "                            raise Exception(\"Invalid Token\")\n",
    "                        index = self.token_map[index]\n",
    "                    index_to_cluster_map[index] = list(filter(lambda t: t[0] != index, cluster))\n",
    "        return index_to_cluster_map\n",
    "\n",
    "    def get_references(self, tokens):\n",
    "        refs = []\n",
    "        for token in tokens:\n",
    "            index = token.idx if not self.char_token_map else token.i\n",
    "            if index in self.cluster_map:\n",
    "                refs += self.cluster_map[index]\n",
    "        return refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "669a1024-4171-4bf9-ab59-2d24095039ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species:\n",
    "    tn_nlp = None\n",
    "    \n",
    "    def __init__(self, text, token_map=None):\n",
    "        if not Species.tn_nlp:\n",
    "            Species.tn_nlp = TaxoNERD().load(model=\"en_ner_eco_biobert\")\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        self.token_map = token_map\n",
    "        self.species_list = self.species_indices(self.tn_doc)\n",
    "\n",
    "    def species_indices(self, tn_doc):\n",
    "        indices = []\n",
    "        for species_span in self.tn_doc.ents:\n",
    "            for species in species_span:\n",
    "                index = species.idx\n",
    "                if self.token_map:\n",
    "                    if index not in self.token_map:\n",
    "                        raise Exception(\"Invalid Token\")\n",
    "                    index = self.token_map[index]\n",
    "                indices.append(index)\n",
    "        return indices\n",
    "\n",
    "    def is_species(self, token):\n",
    "        index = token.idx if not self.token_map else token.i\n",
    "        return index in self.species_list\n",
    "        \n",
    "    def contains_species(self, tokens):\n",
    "        for token in tokens:\n",
    "            index = token.idx if not self.token_map else token.i\n",
    "            if index in self.species_list:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "059ac509-5604-49c2-9bad-761a5edfec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit:\n",
    "    def __init__(self, *, species=None, trait=None, change=None, cause=None):\n",
    "        self.species = species\n",
    "        self.trait = trait\n",
    "        self.cause = cause\n",
    "        self.change = change\n",
    "\n",
    "    def empty(self):\n",
    "        if not self.species and not self.trait and not self.cause and not self.change:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def not_empty(self):\n",
    "        return not empty()\n",
    "\n",
    "    def can_merge(self, unit):\n",
    "        # Two units can merge if there's no\n",
    "        # overlap.\n",
    "        if self.species and unit.species:\n",
    "            return False\n",
    "        if self.trait and unit.trait:\n",
    "            return False\n",
    "        if self.cause and unit.cause:\n",
    "            return False\n",
    "        if self.change and unit.change:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def merge(self, unit):\n",
    "        # We take the parts that the\n",
    "        # other unit has; assuming that\n",
    "        # there's no overlap, there's\n",
    "        # no loss of information.\n",
    "        if unit.species:\n",
    "            self.species = unit.species\n",
    "        if unit.trait:\n",
    "            self.trait = unit.trait\n",
    "        if unit.cause:\n",
    "            self.cause = unit.cause\n",
    "        if unit.change:\n",
    "            self.change = unit.change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b3cb30a-a3da-4a74-8730-1615b1f84f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Possession:\n",
    "    matcher = None\n",
    "    \n",
    "    # There's no definite names for these patterns as I do not know what\n",
    "    # to call them. These patterns are used to extract possessive\n",
    "    # relationships from a sentence. I also could not find better names for\n",
    "    # the two variables below.\n",
    "    OWNER = \"owner\"\n",
    "    OWNED = \"owned\"\n",
    "    \n",
    "    patterns = {\n",
    "        \"Pattern1\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": OWNED,\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"poss\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern2\": [\n",
    "             {\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": OWNED,\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": \"adp\",\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"prep\",\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"ADP\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"adp\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"pobj\",\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern3\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": \"verb\",\n",
    "                \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"nsubj\",\n",
    "                    \"POS\": {\"IN\": [\"PRON\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"dobj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\"]}\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern4\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": \"verb\",\n",
    "                \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"nsubj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": \"adp\",\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"prep\",\n",
    "                    \"POS\": {\"IN\": [\"ADP\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"adp\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"pobj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\"]}\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    def __init__(self, sp_nlp, sp_doc=None):\n",
    "        self.sp_nlp = sp_nlp\n",
    "        print(f\"SP_NLP: {self.sp_nlp}\")\n",
    "        self.sp_doc = sp_doc\n",
    "        print(f\"SP_DOC: {self.sp_doc}\")\n",
    "\n",
    "        if not Possession.matcher:\n",
    "            Possession.matcher = DependencyMatcher(self.sp_nlp.vocab)\n",
    "            for pattern_id, pattern in Possession.patterns.items():\n",
    "                Possession.matcher.add(pattern_id, [pattern])\n",
    "\n",
    "        if sp_doc:\n",
    "            # Processed Output of Matcher\n",
    "            matches = Possession.matcher(sp_doc)\n",
    "            print(f\"Matches: {matches}\")\n",
    "            owner_map, owned_map = self.index_to_what(matches)\n",
    "            self.owner_map = owner_map # Maps Owner to Owned\n",
    "            self.owned_map = owned_map # Maps Owned to Owner\n",
    "        \n",
    "    def index_to_what(self, matches):\n",
    "        owner_map = {}\n",
    "        owned_map = {}\n",
    "\n",
    "        \n",
    "        print(f\"SP_NLP: {self.sp_nlp}\")\n",
    "        for match_id, token_ids in matches:\n",
    "            pattern_id = self.sp_nlp.vocab.strings[match_id]\n",
    "            # print(pattern_id)\n",
    "            owner = None\n",
    "            owned = None\n",
    "            for i in range(len(token_ids)):\n",
    "                right_id = Possession.patterns[pattern_id][i][\"RIGHT_ID\"]\n",
    "                if right_id == Possession.OWNER:\n",
    "                    owner = self.sp_doc[token_ids[i]]\n",
    "                if right_id == Possession.OWNED:\n",
    "                    owned = self.sp_doc[token_ids[i]]\n",
    "\n",
    "            # Owner to Owned\n",
    "            if owner.i not in owner_map:\n",
    "                owner_map[owner.i] = []\n",
    "            owner_map[owner.i].append(owned)\n",
    "\n",
    "            # Owned to Owner\n",
    "            if owned.i not in owned_map:\n",
    "                owned_map[owned.i] = []\n",
    "            owned_map[owned.i].append(owner)\n",
    "            \n",
    "        return (owner_map, owned_map)\n",
    "\n",
    "    def get_owner(self, tokens):\n",
    "        owners = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.owned_map:\n",
    "                owners += self.owned_map[index]\n",
    "        return owners\n",
    "\n",
    "    def get_owned(self, tokens):\n",
    "        owned = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.owner_map:\n",
    "                owned += self.owner_map[index]\n",
    "        return owned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3fe11ad7-581c-44a7-843b-740e706609c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x00000275ADA25480>\n",
      "SP_NLP: <spacy.lang.en.English object at 0x00000275ADA25480>\n",
      "SP_DOC: Grasshoppers exhibited significant diet shifts from grass to herbs (Kruskal-Wallis test, P 0.01, df 3) when they were in the presence of the comparatively sedentary species (the smaller Pisaurina and the larger Hogna) compared to controls without spiders (Fig. 2).\n",
      "Matches: [(14621589392117008497, [4, 5, 6]), (14621589392117008497, [26, 27, 31]), (14621589392117008497, [43, 44, 45]), (17237321022846380202, [41, 15, 42, 43])]\n",
      "SP_NLP: <spacy.lang.en.English object at 0x00000275ADA25480>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/29/2025 13:27:33 - INFO - \t missing_keys: []\n",
      "04/29/2025 13:27:33 - INFO - \t unexpected_keys: []\n",
      "04/29/2025 13:27:33 - INFO - \t mismatched_keys: []\n",
      "04/29/2025 13:27:33 - INFO - \t error_msgs: []\n",
      "04/29/2025 13:27:33 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/29/2025 13:27:33 - INFO - \t Tokenize 1 inputs...\n",
      "04/29/2025 13:27:33 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CorefResult(text=\"Grasshoppers exhibited significant diet shifts fro...\", clusters=[['Grasshoppers', 'they']])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'CorefResult' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 132\u001b[0m\n\u001b[0;32m    129\u001b[0m             units\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_sentence(sent\u001b[38;5;241m.\u001b[39mstart, sent\u001b[38;5;241m.\u001b[39mend))\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m units\n\u001b[1;32m--> 132\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mParser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGrasshoppers exhibited significant diet shifts from grass to herbs (Kruskal-Wallis test, P 0.01, df 3) when they were in the presence of the comparatively sedentary species (the smaller Pisaurina and the larger Hogna) compared to controls without spiders (Fig. 2).\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[74], line 21\u001b[0m, in \u001b[0;36mParser.__init__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpossession \u001b[38;5;241m=\u001b[39m Possession(Parser\u001b[38;5;241m.\u001b[39msp_nlp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_doc)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecies \u001b[38;5;241m=\u001b[39m Species(text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_map)\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreferences \u001b[38;5;241m=\u001b[39m \u001b[43mReferences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 11\u001b[0m, in \u001b[0;36mReferences.__init__\u001b[1;34m(self, corpus, token_map)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions \u001b[38;5;241m=\u001b[39m References\u001b[38;5;241m.\u001b[39mfcoref\u001b[38;5;241m.\u001b[39mpredict(texts\u001b[38;5;241m=\u001b[39mcorpus)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_map \u001b[38;5;241m=\u001b[39m token_map\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_to_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 16\u001b[0m, in \u001b[0;36mReferences.index_to_cluster\u001b[1;34m(self, predictions)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n\u001b[0;32m     15\u001b[0m index_to_cluster_map \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m predictions:\n\u001b[0;32m     17\u001b[0m     clusters \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mget_clusters(as_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m clusters:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'CorefResult' object is not iterable"
     ]
    }
   ],
   "source": [
    "class Parser:\n",
    "    sp_nlp = None\n",
    "\n",
    "    def __init__(self, text):\n",
    "        if not Parser.sp_nlp:\n",
    "            Parser.sp_nlp = spacy.load(\"en_core_web_sm\")\n",
    "        print(Parser.sp_nlp)\n",
    "        \n",
    "        # Clean Text\n",
    "        self.text = text\n",
    "        self.text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", self.text)\n",
    "        self.text = re.sub(\"\\s+\", \" \", self.text)\n",
    "        \n",
    "        # Process Text\n",
    "        self.sp_doc = Parser.sp_nlp(text)\n",
    "        self.token_map = self.index_to_token(self.sp_doc)\n",
    "        \n",
    "        # Tools\n",
    "        self.possession = Possession(Parser.sp_nlp, self.sp_doc)\n",
    "        self.species = Species(text, self.token_map)\n",
    "        self.references = References(text, self.token_map)\n",
    "\n",
    "    def index_to_token(self, sp_doc):\n",
    "        index_to_token_map = {}\n",
    "        for token in sp_doc:\n",
    "            index_to_token_map[token.idx] = token.i\n",
    "        return index_to_token_map\n",
    "\n",
    "    def parse_segment(self, l_i, r_i):\n",
    "        used = []\n",
    "        \n",
    "        # Find Species\n",
    "        species = None\n",
    "        for token in self.sp_doc[l_i:r_i+1]:\n",
    "            if self.species.is_species(token):\n",
    "                species = token\n",
    "                used.append(species)\n",
    "                break\n",
    "        \n",
    "        # Find Change\n",
    "        change_keywords = {\"increase\", \"decrease\", \"change\"}\n",
    "        change = []\n",
    "        for token in self.sp_doc[l_i:r_i+1]:\n",
    "            if token not in used and not token.is_oov:\n",
    "                for keyword in change_keywords:\n",
    "                    if keyword.similarity(token) > 0.7:\n",
    "                        change.append(token)\n",
    "                        used.append(token)\n",
    "                        break\n",
    "            # I only want one word that represents\n",
    "            # the change for simplicity\n",
    "            if change:\n",
    "                break\n",
    "\n",
    "        # Find Trait\n",
    "        trait = []\n",
    "        if change:\n",
    "            # The trait is listed before the change (i.e. \"diet shifts from ...\")\n",
    "            prev_i = change[0].i - 1\n",
    "            prev_token = None if prev_i < 0 else self.sp_doc[prev_i]\n",
    "            if prev_token and prev_token not in used and prev_token.pos_ == \"NOUN\":\n",
    "                used.append(prev_token)\n",
    "                trait.append(prev_token)\n",
    "            else:\n",
    "                # Look for \"in\" (i.e. \"increase in ...\")\n",
    "                for child in change[0].children:\n",
    "                    if child in used:\n",
    "                        continue\n",
    "                    if child.pos_ == \"ADP\" and child.children:\n",
    "                        if child.children[0] not in used:\n",
    "                            used.append(child.children[0])\n",
    "                            trait.append(child.children[0])\n",
    "\n",
    "        # Find Cause\n",
    "        cause = []\n",
    "        for token in self.sp_doc[l_i:r_i+1]:\n",
    "            if token not in used and token.pos_ == \"SCONJ\":\n",
    "                start_i = token.i + 1\n",
    "                end_i = start_i\n",
    "                while end_i <= r_i + 1 and self.sp_doc[end_i].pos_ in [\"ADP\", \"DET\", \"NOUN\", \"PROPN\", \"AUX\"]:\n",
    "                    used.append(self.sp_doc[end_i])\n",
    "                    end_i += 1\n",
    "                used.append(token)\n",
    "                break\n",
    "\n",
    "        unit = Unit(species, trait, change, cause)\n",
    "        return unit\n",
    "\n",
    "    def parse_sentence(self, l_i, r_i):\n",
    "        # Find Verb\n",
    "        # The verb is used to divide\n",
    "        # the \"parsing\" space, which\n",
    "        # makes the work simpler.\n",
    "        verb = None\n",
    "        for token in self.sp_doc:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                verb = token\n",
    "                break\n",
    "\n",
    "        # Base Case\n",
    "        # If there is no verb, we have\n",
    "        # reached the simplest case and\n",
    "        # can extract information.\n",
    "        if verb == None:\n",
    "            return parse_segment(l_i, r_i)\n",
    "        # Merge\n",
    "        # Given two units of information,\n",
    "        # we need to merge them.\n",
    "        else:\n",
    "            l_unit = self.parse_sentence(l_i, verb.i - 1)\n",
    "            r_unit = self.parse_sentence(verb.i + 1, r_i)\n",
    "\n",
    "            if l_unit.not_empty() and not r_unit.empty():\n",
    "                l_unit.change.append(verb)\n",
    "                return l_unit\n",
    "            elif r_unit.not_empty() and not l_unit.empty():\n",
    "                r_unit.change.append(verb)\n",
    "                return r_unit    \n",
    "            elif l_unit.can_merge(r_unit):\n",
    "                l_unit.merge(r_unit)\n",
    "                return l_unit\n",
    "            else:\n",
    "                r_unit.cause = l_unit\n",
    "                return r_unit\n",
    "\n",
    "    def parse(self):\n",
    "        units = []\n",
    "        for sent in doc.sents:\n",
    "            units.append(self.parse_sentence(sent.start, sent.end))\n",
    "        return units\n",
    "\n",
    "parser = Parser(\"Grasshoppers exhibited significant diet shifts from grass to herbs (Kruskal-Wallis test, P 0.01, df 3) when they were in the presence of the comparatively sedentary species (the smaller Pisaurina and the larger Hogna) compared to controls without spiders (Fig. 2).\")\n",
    "# units = parser.parse()\n",
    "# pprint(units, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c2664-1d57-4038-94be-a9fbb6b7783f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63462818-6826-4927-a9eb-b532062fff3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c905a10c-1381-4de0-b740-418882f921ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
