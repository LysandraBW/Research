{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbad3bc0-44ed-4f26-a445-ad4a2e027b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import stanza\n",
    "import textacy\n",
    "from fastcoref import FCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from pprint import pprint\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "960be1f2-9233-459d-8217-1d9848776472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tools:\n",
    "    def __init__(self, *, text=None):\n",
    "        # Tools\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.sp_doc = None\n",
    "        self.tn_nlp = TaxoNERD().load(model=\"en_ner_eco_biobert\")\n",
    "        self.tn_doc = None\n",
    "        self.fcoref = FCoref(enable_progress_bar=False)\n",
    "        self.token_map = None\n",
    "        if text:\n",
    "            self.update(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        cleaned_text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "        cleaned_text = re.sub(\"\\s+\", \" \", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"\\s+([?.!,])\", r\"\\1\", cleaned_text)\n",
    "        return cleaned_text\n",
    "        \n",
    "    def update(self, text):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        # Map Tokens to Index\n",
    "        self.token_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            self.token_map[token.idx] = token.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d6711e0-ee21-47d0-ade1-e15515725ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class References:\n",
    "    def __init__(self, tools, texts=None):\n",
    "        self.tools = tools\n",
    "        self.predictions = None\n",
    "        self.cluster_map = None\n",
    "        if texts:\n",
    "            self.update(texts)\n",
    "\n",
    "    def update(self, texts):\n",
    "        self.predictions = self.tools.fcoref.predict(texts=texts)\n",
    "        self.cluster_map = self.get_cluster_map(self.predictions)\n",
    "        \n",
    "    def get_cluster_map(self, predictions):\n",
    "        cluster_map = {}\n",
    "        for prediction in predictions:\n",
    "            clusters = prediction.get_clusters(as_strings=False)\n",
    "            for cluster in clusters:\n",
    "                # Converting the spans in a cluster to tokens.\n",
    "                # This makes it easier when using it later.\n",
    "                token_cluster = []\n",
    "                for span in cluster:\n",
    "                    if span[0] not in self.tools.token_map:\n",
    "                        raise Exception(\"Invalid Token\")\n",
    "                    index = self.tools.token_map[span[0]]\n",
    "                    token_cluster.append(self.tools.sp_doc[index])\n",
    "                # Mapping\n",
    "                for token in token_cluster:\n",
    "                    cluster_map[token.i] = list(filter(lambda t: t != token, token_cluster))\n",
    "        return cluster_map\n",
    "            \n",
    "    def get_references(self, tokens):\n",
    "        refs = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.cluster_map:\n",
    "                refs += self.cluster_map[index]\n",
    "        return refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d07f01e-584a-453c-8dca-153d5c6556cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Possession:\n",
    "    # There's no definite names for these patterns as I do not know what\n",
    "    # to call them. These patterns are used to extract possessive\n",
    "    # relationships from a sentence. I also could not find better names for\n",
    "    # the two variables below.\n",
    "    OWNER = \"owner\"\n",
    "    OWNED = \"owned\"\n",
    "    \n",
    "    patterns = {\n",
    "        \"Pattern1\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": OWNED,\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"poss\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern2\": [\n",
    "             {\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": OWNED,\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": \"adp\",\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"prep\",\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"ADP\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"adp\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"pobj\",\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern3\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": \"verb\",\n",
    "                \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"nsubj\",\n",
    "                    \"POS\": {\"IN\": [\"PRON\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"dobj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern4\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": \"verb\",\n",
    "                \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"nsubj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": \"adp\",\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"prep\",\n",
    "                    \"POS\": {\"IN\": [\"ADP\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"adp\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"pobj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        self.matcher = DependencyMatcher(self.tools.sp_nlp.vocab)\n",
    "        for pattern_id, pattern in Possession.patterns.items():\n",
    "            self.matcher.add(pattern_id, [pattern])\n",
    "        self.update()\n",
    "    \n",
    "    def update(self):\n",
    "        matches = self.matcher(self.tools.sp_doc)\n",
    "        owner_map, owned_map = self.get_ownership_map(matches)\n",
    "        self.owner_map = owner_map # Maps Owner to Owned\n",
    "        self.owned_map = owned_map # Maps Owned to Owner\n",
    "        \n",
    "    def get_ownership_map(self, matches):\n",
    "        owner_map = {}\n",
    "        owned_map = {}\n",
    "\n",
    "        for match_id, token_ids in matches:\n",
    "            pattern_id = self.tools.sp_nlp.vocab.strings[match_id]\n",
    "            # print(pattern_id)\n",
    "            owner = None\n",
    "            owned = None\n",
    "            for i in range(len(token_ids)):\n",
    "                right_id = Possession.patterns[pattern_id][i][\"RIGHT_ID\"]\n",
    "                if right_id == Possession.OWNER:\n",
    "                    owner = self.tools.sp_doc[token_ids[i]]\n",
    "                if right_id == Possession.OWNED:\n",
    "                    owned = self.tools.sp_doc[token_ids[i]]\n",
    "\n",
    "            # Owner to Owned\n",
    "            if owner.i not in owner_map:\n",
    "                owner_map[owner.i] = []\n",
    "            owner_map[owner.i].append(owned)\n",
    "\n",
    "            # Owned to Owner\n",
    "            if owned.i not in owned_map:\n",
    "                owned_map[owned.i] = []\n",
    "            owned_map[owned.i].append(owner)\n",
    "            \n",
    "        return (owner_map, owned_map)\n",
    "\n",
    "    def get_owner(self, tokens):\n",
    "        owners = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.owned_map:\n",
    "                owners += self.owned_map[index]\n",
    "        return owners\n",
    "\n",
    "    def get_owned(self, tokens):\n",
    "        owned = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.owner_map:\n",
    "                owned += self.owner_map[index]\n",
    "        return owned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0d752ef-11de-4c19-a0cc-b5c32a199dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit:\n",
    "    def __init__(self, *, species=None, trait=None, change=None, cause=None):\n",
    "        self.species = species\n",
    "        self.trait = trait\n",
    "        self.cause = cause\n",
    "        self.change = change\n",
    "        # Flag\n",
    "        self.is_cause = False\n",
    "\n",
    "    def empty(self):\n",
    "        if not self.species and not self.trait and not self.cause and not self.change:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def not_empty(self):\n",
    "        return not self.empty()\n",
    "\n",
    "    def can_merge(self, unit):\n",
    "        # Two units can merge if there's no\n",
    "        # overlap.\n",
    "        if self.species and unit.species:\n",
    "            return False\n",
    "        if self.trait and unit.trait:\n",
    "            return False\n",
    "        if self.cause and unit.cause:\n",
    "            return False\n",
    "        if self.change and unit.change:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def merge(self, unit):\n",
    "        # We take the parts that the\n",
    "        # other unit has; assuming that\n",
    "        # there's no overlap, there's\n",
    "        # no loss of information.\n",
    "        if unit.species:\n",
    "            self.species = unit.species\n",
    "        if unit.trait:\n",
    "            self.trait = unit.trait\n",
    "        if unit.cause:\n",
    "            self.cause = unit.cause\n",
    "        if unit.change:\n",
    "            self.change = unit.change\n",
    "\n",
    "    def get_score(self):\n",
    "        score = 0\n",
    "        if self.species:\n",
    "            score += 1\n",
    "        if self.trait:\n",
    "            score += 1\n",
    "        if self.cause:\n",
    "            score += 1\n",
    "        if self.change:\n",
    "            score += 1    \n",
    "        return score\n",
    "    def __str__(self):\n",
    "        return f\"Species: {self.species}, Trait: {self.trait}, Cause: ({self.cause}), Change: {self.change}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47b3361d-db39-43bb-bc44-aa4edb4d97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        self.species_indices = None\n",
    "        self.update()\n",
    "\n",
    "    def update(self):\n",
    "        self.species_indices = self.get_species_indices()\n",
    "        \n",
    "    def get_species_indices(self):\n",
    "        indices = []\n",
    "\n",
    "        lowered_text = self.tools.sp_doc.text.lower()\n",
    "        for token in self.tools.sp_doc:\n",
    "            if token.pos_ not in [\"NOUN\", \"PROPN\"]:\n",
    "                continue\n",
    "            try:\n",
    "                results = requests.get(f\"https://api.inaturalist.org/v1/search?q={token.lemma_}&sources=taxa&include_taxon_ancestors=false\")\n",
    "                results = results.json()\n",
    "                results = results[\"results\"]\n",
    "                for result in results:\n",
    "                    if \"record\" not in result or \"name\" not in result[\"record\"]:\n",
    "                        continue\n",
    "                    if lowered_text.find(result[\"record\"][\"name\"].lower()) == -1:\n",
    "                        continue\n",
    "                    indices.append(token.i)\n",
    "            except Exception as e:\n",
    "                print(\"Network Error\")\n",
    "                \n",
    "        for species_span in self.tools.tn_doc.ents:\n",
    "            for species in species_span:\n",
    "                if species.idx not in self.tools.token_map:\n",
    "                    raise Exception(\"Invalid Token\")\n",
    "                index = self.tools.token_map[species.idx]\n",
    "                if index in indices:\n",
    "                    continue\n",
    "                indices.append(index)\n",
    "        return indices\n",
    "\n",
    "    def is_species(self, token):\n",
    "        index = token.i\n",
    "        return index in self.species_indices\n",
    "        \n",
    "    def contains_species(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.species_indices:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b95190af-07ec-46f0-ad6d-6b0b784408a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:    \n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        # References\n",
    "        self.unit_keywords = [self.tools.sp_nlp(keyword) for keyword in {\"unit\", \"%\", \"percent\"}]\n",
    "        self.change_keywords = [self.tools.sp_nlp(keyword) for keyword in {\"increase\", \"decrease\", \"change\", \"weaken\", \"shift\"}]\n",
    "        self.quantity_keywords = [self.tools.sp_nlp(keyword) for keyword in {\"tenfold\", \"half\", \"double\", \"triple\", \"quadruple\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"}]\n",
    "        # Instances\n",
    "        self.unit_indices = []\n",
    "        self.change_indices = []\n",
    "        self.quantity_indices = []\n",
    "        self.cause_indices = []\n",
    "        self.update()\n",
    "        \n",
    "    def update(self):\n",
    "        self.unit_indices = self.load_unit_indices()\n",
    "        self.change_indices = self.load_change_indices()\n",
    "        self.quantity_indices = self.load_quantity_indices()\n",
    "        self.cause_indices = self.load_cause_indices()\n",
    "        return\n",
    "\n",
    "    def is_unit(self, token):\n",
    "        return token.i in self.unit_indices\n",
    "\n",
    "    def has_unit(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.unit_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_unit_indices(self):\n",
    "        indices = []\n",
    "        for token in self.tools.sp_doc:\n",
    "            if token.pos_ not in [\"NOUN\"]:\n",
    "                continue\n",
    "            lemma = self.tools.sp_nlp(token.lemma_)\n",
    "            for keyword in self.unit_keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                # print(f\"{lemma} and {keyword} Similarity: {similarity}\")\n",
    "                if similarity > 0.7:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def is_change(self, token):\n",
    "        return token.i in self.change_indices\n",
    "\n",
    "    def has_change(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.change_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_change_indices(self):\n",
    "        indices = []\n",
    "        for token in self.tools.sp_doc:\n",
    "            if token.lower_ == \"to\" and token.head and token.head.lower_ == \"from\":\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            if token.pos_ not in [\"NOUN\", \"VERB\"]:\n",
    "                continue\n",
    "            lemma = self.tools.sp_nlp(token.lemma_)\n",
    "            for keyword in self.change_keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                # print(f\"{lemma} and {keyword} Similarity: {similarity}\")\n",
    "                if similarity > 0.7:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def is_quantity(self, token):\n",
    "        return token.i in self.quantity_indices\n",
    "\n",
    "    def has_quantity(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.quantity_indices:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def load_quantity_indices(self):\n",
    "        indices = []\n",
    "        for token in self.tools.sp_doc:\n",
    "            if token.pos_ not in [\"NOUN\", \"NUM\"]:\n",
    "                continue\n",
    "            lemma = self.tools.sp_nlp(token.lemma_)\n",
    "            for keyword in self.quantity_keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                # print(f\"{lemma} and {keyword} Similarity: {similarity}\")\n",
    "                if similarity > 0.7:\n",
    "                    # Make sure that if there is a noun the quantity\n",
    "                    # modifies, that it is a unit.\n",
    "                    if token.head and token.head.pos_ == \"NOUN\" and not self.is_unit(token.head):\n",
    "                        continue\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def is_cause(self, token):\n",
    "        return token.i in self.cause_indices\n",
    "\n",
    "    def has_cause(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.cause_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_cause_indices(self):\n",
    "        indices = []\n",
    "        for token in self.tools.sp_doc:\n",
    "            if token.pos_ not in [\"ADP\", \"SCONJ\", \"PART\", \"PRON\"]:\n",
    "                continue\n",
    "            print(token, token.pos_)\n",
    "            if token.pos_ == \"SCONJ\":\n",
    "                # print(\"It's a SCONJ\")\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            elif token.pos_ == \"PART\":\n",
    "                if token.head and token.head.pos_ == \"VERB\":\n",
    "                    # print(\"It's a PART + VERB\")\n",
    "                    indices.append(token.i)\n",
    "                    continue\n",
    "            elif token.pos_ == \"ADP\":\n",
    "                if token.lower_ == \"due\" and self.tools.sp_doc[token.i + 1] and self.tools.sp_doc[token.i + 1].lower_ == \"to\":\n",
    "                    # print(\"It's a DUE TO\")\n",
    "                    indices.append(token.i)\n",
    "                    continue\n",
    "                elif token.head:\n",
    "                    if token.head.pos_ == \"AUX\":\n",
    "                        # print(\"The head is an AUX\")\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "                    elif token.head.pos_ == \"VERB\" and token.head.i < token.i and self.is_change(token.head):\n",
    "                        # print(\"The head is a change-VERB\")\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "                    elif token.lower_ != \"to\" and \"AUX\" in [child.pos_ for child in list(filter(lambda t: t.i < token.i,token.head.children))]:\n",
    "                        # print(\"There's an AUX in the head's children\")\n",
    "                        for child in token.head.children:\n",
    "                            print(f\"\\t\\t{child}, {child.pos_}\")\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "                elif token.ancestors:\n",
    "                    # print(\"There's an AUX ancestor\")\n",
    "                    if \"AUX\" in [ancestor.pos_ for ancestor in token.ancestors]:\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "            elif token.pos_ == \"PRON\":\n",
    "                if token.head and token.head.pos_ == \"VERB\" and self.is_change(token.head):\n",
    "                    indices.append(token.i)\n",
    "                    continue\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc5cc71f-45c1-4cd7-b446-1e25b4468ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self, text, *, tools=None, species=None, possession=None, references=None, keywords=None):\n",
    "        self.tools = tools if tools else Tools(text=text)\n",
    "        self.species = species if species else Species(self.tools)\n",
    "        self.possession = possession if possession else Possession(self.tools)\n",
    "        self.references = references if references else References(self.tools, texts=[text])\n",
    "        self.keywords = keywords if keywords else Keywords(self.tools)\n",
    "            \n",
    "    def update(self, text):\n",
    "        self.tools.update(text)\n",
    "        self.species.update()\n",
    "        self.possession.update()\n",
    "        self.references.update(texts=[text])\n",
    "        self.keywords.update()\n",
    "                \n",
    "    def parse_segment(self, l_i, r_i):\n",
    "        # print(f\"\\nPARSING SEGMENT\\n\")\n",
    "        # print(f\"Text: {self.tools.sp_doc[l_i:r_i+1].text}\")\n",
    "        used = []\n",
    "\n",
    "        # Find Cause\n",
    "        cause = []\n",
    "        for token in self.tools.sp_doc[l_i:r_i+1]:\n",
    "            if token not in used and token.pos_ in [\"SCONJ\"] and self.keywords.is_cause(token):\n",
    "                start_i = token.i + 1\n",
    "                end_i = start_i\n",
    "                while end_i <= r_i and self.tools.sp_doc[end_i] not in used and self.tools.sp_doc[end_i].pos_ in [\"ADP\", \"DET\", \"NOUN\", \"PROPN\", \"AUX\", \"ADV\", \"PRON\", \"ADJ\"]:\n",
    "                    used.append(self.tools.sp_doc[end_i])\n",
    "                    cause.append(self.tools.sp_doc[end_i])\n",
    "                    end_i += 1\n",
    "                used.append(token)\n",
    "        # print(f\"Cause 1: {cause}\")\n",
    "        \n",
    "        # Find Species\n",
    "        species = []\n",
    "        for token in self.tools.sp_doc[l_i:r_i+1]:\n",
    "            if self.species.is_species(token) and token not in used and (token.head and (token.head.pos_ not in [\"SCONJ\", \"ADP\"] or token.head.lower_ == \"of\")):\n",
    "                species.append(token)\n",
    "                used.append(species)\n",
    "                break\n",
    "        # print(f\"Species: {species}\")\n",
    "        \n",
    "        # Find Change\n",
    "        change = []\n",
    "        for token in self.tools.sp_doc[l_i:r_i+1]:\n",
    "            if token not in used and not token.is_oov:\n",
    "                if self.keywords.is_change(token):\n",
    "                    change.append(token)\n",
    "                    used.append(token)\n",
    "            # I only want one word that represents\n",
    "            # the change for simplicity\n",
    "            if change:\n",
    "                break\n",
    "        # print(f\"Change 1: {change}\")\n",
    "\n",
    "        # Next Method to Find Change\n",
    "        for token in self.tools.sp_doc[l_i:r_i+1]:\n",
    "            if token not in used and token.pos_ == \"ADP\" and token.lower_ != \"of\":\n",
    "                # print(\"In Next Method...\", token, token not in used, token.pos_)\n",
    "                start_i = token.i + 1\n",
    "                end_i = start_i\n",
    "                possible_changes = []\n",
    "                while end_i <= r_i and self.tools.sp_doc[end_i].pos_ in [\"NUM\", \"SYM\", \"NOUN\", \"ADP\", \"DET\"]:\n",
    "                    possible_changes.append(self.tools.sp_doc[end_i])\n",
    "                    end_i += 1\n",
    "                if not self.keywords.has_quantity(possible_changes):\n",
    "                    continue\n",
    "                # print(f\"Actual Changes: {possible_changes}\")\n",
    "                for possible_change in possible_changes:\n",
    "                    used.append(possible_change)\n",
    "                    change.append(possible_change)\n",
    "                used.append(token)\n",
    "                break\n",
    "        # print(f\"Change 2: {change}\")\n",
    "\n",
    "        # Find Trait\n",
    "        trait = []\n",
    "        if species:\n",
    "            possible_traits = list(filter(lambda t: t.i >= l_i and t.i <= r_i, self.possession.get_owned(species)))\n",
    "            valid_trait = False\n",
    "            for possible_trait in possible_traits:\n",
    "                for ancestor in possible_trait.ancestors:\n",
    "                    if self.keywords.is_change(ancestor):\n",
    "                        valid_trait = True\n",
    "                        break\n",
    "            if valid_trait:\n",
    "                trait = possible_traits\n",
    "        elif change:\n",
    "            # The trait is listed before the change (i.e. \"diet shifts from ...\")\n",
    "            prev_i = change[0].i - 1\n",
    "            prev_token = None if prev_i < 0 else self.tools.sp_doc[prev_i]\n",
    "            if prev_token and prev_token not in used and prev_token.pos_ == \"NOUN\":\n",
    "                used.append(prev_token)\n",
    "                trait.append(prev_token)\n",
    "            else:\n",
    "                # Look for \"in\" (i.e. \"increase in ...\")\n",
    "                for child in change[0].children:\n",
    "                    # print(child, child.pos_, child.children)\n",
    "                    if child in used:\n",
    "                        continue\n",
    "                    if child.pos_ == \"ADP\" and child.children:\n",
    "                        children = list(child.children)\n",
    "                        if children[0] not in used:\n",
    "                            used.append(children[0])\n",
    "                            trait.append(children[0])\n",
    "        else:\n",
    "            for token in self.tools.sp_doc[l_i:r_i+1]:\n",
    "                if token.head and self.keywords.is_change(token.head):\n",
    "                    trait.append(token)\n",
    "                    used.append(token)\n",
    "\n",
    "                    possible_species = self.possession.get_owner(trait)\n",
    "                    if self.species.contains_species(possible_species):\n",
    "                        for sp in possible_species:\n",
    "                            if token in sp.ancestors:\n",
    "                                species.append(sp)\n",
    "                                used.append(sp)\n",
    "                    break\n",
    "        # print(f\"Trait: {trait}\")\n",
    "        \n",
    "        # Find Cause\n",
    "        is_cause = False\n",
    "        for token in self.tools.sp_doc[l_i:r_i+1]:\n",
    "            if token not in used and token.pos_ in [\"PRON\"] and self.keywords.is_cause(token):\n",
    "                is_cause = True\n",
    "                used.append(token)\n",
    "            elif token not in used and token.pos_ in [\"ADP\"] and self.keywords.is_cause(token):\n",
    "                start_i = token.i + 1\n",
    "                end_i = start_i\n",
    "                buffer = []\n",
    "                noun_found = False\n",
    "                while end_i <= r_i and self.tools.sp_doc[end_i] not in used and self.tools.sp_doc[end_i].pos_ in [\"ADP\", \"DET\", \"NOUN\", \"PROPN\", \"AUX\", \"ADV\", \"PRON\", \"ADJ\"]:\n",
    "                    if self.tools.sp_doc[end_i].pos_ in [\"NOUN\", \"PROPN\", \"PRON\"]:\n",
    "                        noun_found = True\n",
    "                    buffer.append(self.tools.sp_doc[end_i])\n",
    "                    end_i += 1\n",
    "                if noun_found:\n",
    "                    for token in buffer:\n",
    "                        used.append(token)\n",
    "                        cause.append(token)\n",
    "                    used.append(token)\n",
    "        # print(f\"Cause 2: {cause}\")\n",
    "        \n",
    "        unit = Unit(species=species, trait=trait, change=change, cause=cause)\n",
    "        unit.is_cause = is_cause\n",
    "        return unit\n",
    "\n",
    "    def parse_sentence(self, l_i, r_i):\n",
    "        # Find Verb\n",
    "        # The verb is used to divide\n",
    "        # the \"parsing\" space, which\n",
    "        # makes the work simpler.\n",
    "        verb = None\n",
    "        for token in self.tools.sp_doc[l_i:r_i+1]:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                verb = token\n",
    "                break\n",
    "\n",
    "        # Base Case\n",
    "        # If there is no verb, we have\n",
    "        # reached the simplest case and\n",
    "        # can extract information.\n",
    "        if verb == None:\n",
    "            return self.parse_segment(l_i, r_i)\n",
    "        # Merge\n",
    "        # Given two units of information,\n",
    "        # we need to merge them.\n",
    "        else:\n",
    "            verb_is_change = self.keywords.is_change(verb)\n",
    "            # print(f\"Text: {self.tools.sp_doc[l_i:r_i+1].text}\")\n",
    "            l_unit = self.parse_sentence(l_i, verb.i - 1)\n",
    "            r_unit = self.parse_sentence(verb.i + 1, r_i)\n",
    "            m_unit = None\n",
    "            print(f\"Verb: {verb}\")\n",
    "            print(f\"\\tText: {self.tools.sp_doc[l_i:verb.i].text}\\n\\t\\tL Unit: {l_unit}\")\n",
    "            print(f\"\\tText: {self.tools.sp_doc[verb.i + 1:r_i].text}\\n\\t\\tR Unit: {r_unit}\")\n",
    "            if l_unit.empty() and r_unit.empty():\n",
    "                m_unit = Unit()\n",
    "            elif l_unit.not_empty() and r_unit.empty():\n",
    "                print(1)\n",
    "                if verb_is_change:\n",
    "                    l_unit.change.append(verb)\n",
    "                m_unit = l_unit\n",
    "            elif r_unit.not_empty() and l_unit.empty():\n",
    "                print(2)\n",
    "                if verb_is_change:\n",
    "                    r_unit.change.append(verb)\n",
    "                m_unit = r_unit    \n",
    "            elif l_unit.can_merge(r_unit):\n",
    "                print(3)\n",
    "                l_unit.merge(r_unit)\n",
    "                if verb_is_change:\n",
    "                    l_unit.change.append(verb)\n",
    "                m_unit = l_unit\n",
    "            elif verb_is_change:\n",
    "                print(4)\n",
    "                r_unit.cause = l_unit\n",
    "                if verb_is_change:\n",
    "                    r_unit.change.append(verb)\n",
    "                m_unit = r_unit\n",
    "            else:\n",
    "                print(5)\n",
    "                if l_unit.get_score() >= r_unit.get_score():\n",
    "                    m_unit = l_unit\n",
    "                else:\n",
    "                    m_unit = r_unit\n",
    "            print(f\"\\tM Unit: {m_unit}\")\n",
    "            return m_unit\n",
    "\n",
    "    def parse(self):\n",
    "        units = []\n",
    "        for sent in self.tools.sp_doc.sents:\n",
    "            units.append(self.parse_sentence(sent.start, sent.end - 1))\n",
    "        return units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9903d921-8cd0-4038-bfef-4d711540d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "text00 = Tools.clean_text(\"Acridoidea and Selachii exhibited significant diet shifts from grass to herbs (Kruskal-Wallis test, P 0.01, df 3) when they were in the presence of the comparatively sedentary species (the smaller Pisaurina and the larger Hogna) compared to controls without spiders (Fig. 2).\")\n",
    "text01 = Tools.clean_text(\"Our results show that phototrophs can indirectly decrease the population density of heterotrophic bacteria by modification of the nature of bacterial interactions with predators.\")\n",
    "text02 = Tools.clean_text(\"Our results show that Selachii can indirectly decrease the population density of Selachimorpha by modification of the nature of bacterial interactions with predators.\")\n",
    "text03 = Tools.clean_text(\"All predators inflicted significant mortality on the prey at each prey density compared to the predator-free control for that density\")\n",
    "text04 = Tools.clean_text(\"Our results show that an increase in sediment organic matter content is associated to a decline in the abundance of Loripes lucinalis (lucinid bivalve) in the Cymodocea nodosa meadows studied, which potentially may weaken the mutualism between the two species.\")\n",
    "text05 = Tools.clean_text(\"The abundance of lucinids showed a negative correlation with the organic matter content in vegetated sediments (Fig. 3a), but showed no correlation in bare ones (Fig. 3b).\")\n",
    "text06 = Tools.clean_text(\"The MANOVA on the cattle tank experiment showed that the presence of Tramea, nonlethal Anax, and large bullfrog tadpoles all had significant effects on both small tadpole species (Table 1).\")\n",
    "text07 = Tools.clean_text(\"Thus the presence of predators, both nonlethal Anax and lethal Tramea, modified the tank environment in a way that facilitated invasion by midges, but only in the absence of large bullfrogs.\")\n",
    "text08 = Tools.clean_text(\"We hypothesized that the presence of Anax would decrease foraging activity of small tadpoles, which in turn would decrease predation by Tramea on the small tadpoles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe7afe49-383b-41b2-b0a3-52f6faa16b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/01/2025 14:24:38 - INFO - \t missing_keys: []\n",
      "05/01/2025 14:24:38 - INFO - \t unexpected_keys: []\n",
      "05/01/2025 14:24:38 - INFO - \t mismatched_keys: []\n",
      "05/01/2025 14:24:38 - INFO - \t error_msgs: []\n",
      "05/01/2025 14:24:38 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "05/01/2025 14:24:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 43.86 examples/s]\n",
      "05/01/2025 14:24:44 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We PRON\n",
      "that SCONJ\n",
      "of ADP\n",
      "of ADP\n",
      "which PRON\n",
      "in ADP\n",
      "by ADP\n",
      "on ADP\n"
     ]
    }
   ],
   "source": [
    "text = text08\n",
    "tools = Tools(text=text)\n",
    "species = Species(tools)\n",
    "possession = Possession(tools)\n",
    "references = References(tools, texts=[text])\n",
    "keywords = Keywords(tools)\n",
    "# for index in keywords.cause_indices:\n",
    "#     print(tools.sp_doc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b54981fc-2d82-4ea6-942b-b37a15023624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/01/2025 14:36:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 69.75 examples/s]\n",
      "05/01/2025 14:36:38 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We PRON\n",
      "that SCONJ\n",
      "of ADP\n",
      "of ADP\n",
      "which PRON\n",
      "in ADP\n",
      "by ADP\n",
      "on ADP\n",
      "Verb: decrease\n",
      "\tText: activity of small tadpoles, which in turn would\n",
      "\t\tL Unit: Species: [tadpoles], Trait: [activity], Cause: ([]), Change: []\n",
      "\tText: predation by Tramea on the small\n",
      "\t\tR Unit: Species: [Tramea, tadpoles], Trait: [predation], Cause: ([]), Change: []\n",
      "4\n",
      "\tM Unit: Species: [Tramea, tadpoles], Trait: [predation], Cause: (Species: [tadpoles], Trait: [activity], Cause: ([]), Change: []), Change: [decrease]\n",
      "Verb: foraging\n",
      "\tText: \n",
      "\t\tL Unit: Species: [], Trait: [], Cause: ([]), Change: []\n",
      "\tText: activity of small tadpoles, which in turn would decrease predation by Tramea on the small\n",
      "\t\tR Unit: Species: [Tramea, tadpoles], Trait: [predation], Cause: (Species: [tadpoles], Trait: [activity], Cause: ([]), Change: []), Change: [decrease]\n",
      "2\n",
      "\tM Unit: Species: [Tramea, tadpoles], Trait: [predation], Cause: (Species: [tadpoles], Trait: [activity], Cause: ([]), Change: []), Change: [decrease]\n",
      "Verb: decrease\n",
      "\tText: that the presence of Anax would\n",
      "\t\tL Unit: Species: [], Trait: [that], Cause: ([the, presence, of, Anax, would]), Change: []\n",
      "\tText: foraging activity of small tadpoles, which in turn would decrease predation by Tramea on the small\n",
      "\t\tR Unit: Species: [Tramea, tadpoles], Trait: [predation], Cause: (Species: [tadpoles], Trait: [activity], Cause: ([]), Change: []), Change: [decrease]\n",
      "4\n",
      "\tM Unit: Species: [Tramea, tadpoles], Trait: [predation], Cause: (Species: [], Trait: [that], Cause: ([the, presence, of, Anax, would]), Change: []), Change: [decrease, decrease]\n",
      "Verb: hypothesized\n",
      "\tText: We\n",
      "\t\tL Unit: Species: [], Trait: [], Cause: ([]), Change: []\n",
      "\tText: that the presence of Anax would decrease foraging activity of small tadpoles, which in turn would decrease predation by Tramea on the small\n",
      "\t\tR Unit: Species: [Tramea, tadpoles], Trait: [predation], Cause: (Species: [], Trait: [that], Cause: ([the, presence, of, Anax, would]), Change: []), Change: [decrease, decrease]\n",
      "2\n",
      "\tM Unit: Species: [Tramea, tadpoles], Trait: [predation], Cause: (Species: [], Trait: [that], Cause: ([the, presence, of, Anax, would]), Change: []), Change: [decrease, decrease]\n",
      "Species: [Tramea, tadpoles], Trait: [predation], Cause: (Species: [], Trait: [that], Cause: ([the, presence, of, Anax, would]), Change: []), Change: [decrease, decrease]\n"
     ]
    }
   ],
   "source": [
    "parser = Parser(text, tools=tools, species=species, possession=possession, references=references, keywords=keywords)\n",
    "parser.update(text)\n",
    "units = parser.parse()\n",
    "for unit in units:\n",
    "    print(unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032bfd2-02f9-45e8-8586-e2e2ae7118cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
