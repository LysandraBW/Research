{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbad3bc0-44ed-4f26-a445-ad4a2e027b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import stanza\n",
    "import textacy\n",
    "from fastcoref import FCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from pprint import pprint\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960be1f2-9233-459d-8217-1d9848776472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tools:\n",
    "    def __init__(self, *, text=None):\n",
    "        # Tools\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.sp_doc = None\n",
    "        self.tn_nlp = TaxoNERD().load(model=\"en_ner_eco_biobert\")\n",
    "        self.tn_doc = None\n",
    "        self.fcoref = FCoref(enable_progress_bar=False)\n",
    "        self.token_map = None\n",
    "        if text:\n",
    "            self.update(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        cleaned_text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "        cleaned_text = re.sub(\"\\s+\", \" \", cleaned_text)\n",
    "        return cleaned_text\n",
    "        \n",
    "    def update(self, text):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        # Map Tokens to Index\n",
    "        self.token_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            self.token_map[token.idx] = token.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d6711e0-ee21-47d0-ade1-e15515725ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class References:\n",
    "    def __init__(self, tools, texts=None):\n",
    "        self.tools = tools\n",
    "        self.predictions = None\n",
    "        self.cluster_map = None\n",
    "        if texts:\n",
    "            self.update(texts)\n",
    "\n",
    "    def update(self, texts):\n",
    "        self.predictions = self.tools.fcoref.predict(texts=texts)\n",
    "        self.cluster_map = self.get_cluster_map(self.predictions)\n",
    "        \n",
    "    def get_cluster_map(self, predictions):\n",
    "        cluster_map = {}\n",
    "        for prediction in predictions:\n",
    "            clusters = prediction.get_clusters(as_strings=False)\n",
    "            for cluster in clusters:\n",
    "                # Converting the spans in a cluster to tokens.\n",
    "                # This makes it easier when using it later.\n",
    "                token_cluster = []\n",
    "                for span in cluster:\n",
    "                    if span[0] not in self.tools.token_map:\n",
    "                        raise Exception(\"Invalid Token\")\n",
    "                    index = self.tools.token_map[span[0]]\n",
    "                    token_cluster.append(self.tools.sp_doc[index])\n",
    "                # Mapping\n",
    "                for token in token_cluster:\n",
    "                    cluster_map[token.i] = list(filter(lambda t: t != token, token_cluster))\n",
    "        return cluster_map\n",
    "            \n",
    "    def get_references(self, tokens):\n",
    "        refs = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.cluster_map:\n",
    "                refs += self.cluster_map[index]\n",
    "        return refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d07f01e-584a-453c-8dca-153d5c6556cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Possession:\n",
    "    # There's no definite names for these patterns as I do not know what\n",
    "    # to call them. These patterns are used to extract possessive\n",
    "    # relationships from a sentence. I also could not find better names for\n",
    "    # the two variables below.\n",
    "    OWNER = \"owner\"\n",
    "    OWNED = \"owned\"\n",
    "    \n",
    "    patterns = {\n",
    "        \"Pattern1\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": OWNED,\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"poss\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern2\": [\n",
    "             {\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": OWNED,\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": \"adp\",\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"prep\",\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"ADP\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"adp\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"pobj\",\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern3\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": \"verb\",\n",
    "                \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"nsubj\",\n",
    "                    \"POS\": {\"IN\": [\"PRON\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"dobj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern4\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": \"verb\",\n",
    "                \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"nsubj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": \"adp\",\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"prep\",\n",
    "                    \"POS\": {\"IN\": [\"ADP\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"adp\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"pobj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        self.matcher = DependencyMatcher(self.tools.sp_nlp.vocab)\n",
    "        for pattern_id, pattern in Possession.patterns.items():\n",
    "            self.matcher.add(pattern_id, [pattern])\n",
    "        self.update()\n",
    "    \n",
    "    def update(self):\n",
    "        matches = self.matcher(self.tools.sp_doc)\n",
    "        owner_map, owned_map = self.get_ownership_map(matches)\n",
    "        self.owner_map = owner_map # Maps Owner to Owned\n",
    "        self.owned_map = owned_map # Maps Owned to Owner\n",
    "        \n",
    "    def get_ownership_map(self, matches):\n",
    "        owner_map = {}\n",
    "        owned_map = {}\n",
    "\n",
    "        for match_id, token_ids in matches:\n",
    "            pattern_id = self.tools.sp_nlp.vocab.strings[match_id]\n",
    "            # print(pattern_id)\n",
    "            owner = None\n",
    "            owned = None\n",
    "            for i in range(len(token_ids)):\n",
    "                right_id = Possession.patterns[pattern_id][i][\"RIGHT_ID\"]\n",
    "                if right_id == Possession.OWNER:\n",
    "                    owner = self.tools.sp_doc[token_ids[i]]\n",
    "                if right_id == Possession.OWNED:\n",
    "                    owned = self.tools.sp_doc[token_ids[i]]\n",
    "\n",
    "            # Owner to Owned\n",
    "            if owner.i not in owner_map:\n",
    "                owner_map[owner.i] = []\n",
    "            owner_map[owner.i].append(owned)\n",
    "\n",
    "            # Owned to Owner\n",
    "            if owned.i not in owned_map:\n",
    "                owned_map[owned.i] = []\n",
    "            owned_map[owned.i].append(owner)\n",
    "            \n",
    "        return (owner_map, owned_map)\n",
    "\n",
    "    def get_owner(self, tokens):\n",
    "        owners = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.owned_map:\n",
    "                owners += self.owned_map[index]\n",
    "        return owners\n",
    "\n",
    "    def get_owned(self, tokens):\n",
    "        owned = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.owner_map:\n",
    "                owned += self.owner_map[index]\n",
    "        return owned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d752ef-11de-4c19-a0cc-b5c32a199dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit:\n",
    "    def __init__(self, *, species=None, trait=None, change=None, cause=None):\n",
    "        self.species = species\n",
    "        self.trait = trait\n",
    "        self.cause = cause\n",
    "        self.change = change\n",
    "\n",
    "    def empty(self):\n",
    "        if not self.species and not self.trait and not self.cause and not self.change:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def not_empty(self):\n",
    "        return not self.empty()\n",
    "\n",
    "    def can_merge(self, unit):\n",
    "        # Two units can merge if there's no\n",
    "        # overlap.\n",
    "        if self.species and unit.species:\n",
    "            return False\n",
    "        if self.trait and unit.trait:\n",
    "            return False\n",
    "        if self.cause and unit.cause:\n",
    "            return False\n",
    "        if self.change and unit.change:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def merge(self, unit):\n",
    "        # We take the parts that the\n",
    "        # other unit has; assuming that\n",
    "        # there's no overlap, there's\n",
    "        # no loss of information.\n",
    "        if unit.species:\n",
    "            self.species = unit.species\n",
    "        if unit.trait:\n",
    "            self.trait = unit.trait\n",
    "        if unit.cause:\n",
    "            self.cause = unit.cause\n",
    "        if unit.change:\n",
    "            self.change = unit.change\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Species: {self.species}, Trait: {self.trait}, Cause: ({self.cause}), Change: {self.change}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b3361d-db39-43bb-bc44-aa4edb4d97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        self.species_indices = None\n",
    "        self.update()\n",
    "\n",
    "    def update(self):\n",
    "        self.species_indices = self.get_species_indices()\n",
    "        \n",
    "    def get_species_indices(self):\n",
    "        indices = []\n",
    "\n",
    "        lowered_text = self.tools.sp_doc.text.lower()\n",
    "        for token in self.tools.sp_doc:\n",
    "            if token.pos_ not in [\"NOUN\", \"PROPN\"]:\n",
    "                continue\n",
    "            results = requests.get(f\"https://api.inaturalist.org/v1/search?q={token.lemma_}&sources=taxa&include_taxon_ancestors=false\")\n",
    "            results = results.json()\n",
    "            results = results[\"results\"]\n",
    "            for result in results:\n",
    "                if \"record\" not in result or \"name\" not in result[\"record\"]:\n",
    "                    continue\n",
    "                if lowered_text.find(result[\"record\"][\"name\"].lower()) == -1:\n",
    "                    continue\n",
    "                indices.append(token.i)\n",
    "                \n",
    "        for species_span in self.tools.tn_doc.ents:\n",
    "            for species in species_span:\n",
    "                if species.idx not in self.tools.token_map:\n",
    "                    raise Exception(\"Invalid Token\")\n",
    "                index = self.tools.token_map[species.idx]\n",
    "                if index in indices:\n",
    "                    continue\n",
    "                indices.append(index)\n",
    "        return indices\n",
    "\n",
    "    def is_species(self, token):\n",
    "        index = token.i\n",
    "        return index in self.species_indices\n",
    "        \n",
    "    def contains_species(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.species_indices:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdc9da78-4f21-4561-bd98-9118ae4150c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self, text, *, tools=None, species=None, possession=None, references=None):\n",
    "        self.tools = tools if tools else Tools(text=text)\n",
    "        self.species = species if species else Species(self.tools)\n",
    "        self.possession = possession if possession else Possession(self.tools)\n",
    "        self.references = references if references else References(self.tools, texts=[text])\n",
    "        self.change_keywords = []\n",
    "        for keyword in {\"increase\", \"decrease\", \"change\", \"weaken\"}:\n",
    "            self.change_keywords.append(self.tools.sp_nlp(keyword))\n",
    "        self.change_quantity_keywords = []\n",
    "        for keyword in {\"tenfold\", \"half\", \"double\", \"triple\", \"quadruple\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"}:\n",
    "            self.change_quantity_keywords.append(self.tools.sp_nlp(keyword))\n",
    "            \n",
    "    def update(self, text):\n",
    "        self.tools.update(text)\n",
    "        self.species.update()\n",
    "        self.possession.update()\n",
    "        self.references.update(texts=[text])\n",
    "\n",
    "    def is_change_keyword(self, token):\n",
    "        # print(f\"Lemma of {token}: {token.lemma_}\")\n",
    "        token_lemma = self.tools.sp_nlp(token.lemma_)\n",
    "        for keyword in self.change_keywords:\n",
    "            if keyword.similarity(token_lemma) > 0.9:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def has_change_quantity(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.pos_ == \"NUM\":\n",
    "                return True\n",
    "            if token.lower_ == \"to\":\n",
    "                return True\n",
    "            if token.pos_ != \"NOUN\":\n",
    "                continue\n",
    "            token_lemma = self.tools.sp_nlp(token.lemma_)\n",
    "            for keyword in self.change_quantity_keywords:\n",
    "                if keyword.similarity(token_lemma) > 0.9:\n",
    "                    return True\n",
    "        return False\n",
    "                \n",
    "    def parse_segment(self, l_i, r_i):\n",
    "        # print(f\"\\n\\nPARSING SEGMENT\\n\")\n",
    "        # print(f\"Text: {self.tools.sp_doc[l_i:r_i+1].text}\")\n",
    "        used = []\n",
    "\n",
    "        # Find Cause\n",
    "        cause = []\n",
    "        for token in self.tools.sp_doc[l_i:r_i+1]:\n",
    "            if token not in used and token.pos_ in [\"SCONJ\"]:\n",
    "                start_i = token.i + 1\n",
    "                end_i = start_i\n",
    "                while end_i <= r_i and self.tools.sp_doc[end_i] not in used and self.tools.sp_doc[end_i].pos_ in [\"ADP\", \"DET\", \"NOUN\", \"PROPN\", \"AUX\", \"ADV\", \"PRON\", \"ADJ\"]:\n",
    "                    used.append(self.tools.sp_doc[end_i])\n",
    "                    cause.append(self.tools.sp_doc[end_i])\n",
    "                    end_i += 1\n",
    "                used.append(token)\n",
    "        # print(f\"Cause: {cause}\")\n",
    "        \n",
    "        # Find Species\n",
    "        species = None\n",
    "        for token in self.tools.sp_doc[l_i:r_i+1]:\n",
    "            if self.species.is_species(token) and token not in used and (token.head and (token.head.pos_ not in [\"SCONJ\", \"ADP\"] or token.head.lower_ == \"of\")):\n",
    "                species = token\n",
    "                used.append(species)\n",
    "                break\n",
    "        # print(f\"Species: {species}\")\n",
    "        \n",
    "        # Find Change\n",
    "        change_keywords = {\"increase\", \"decrease\", \"change\"}\n",
    "        change = []\n",
    "        for token in self.tools.sp_doc[l_i:r_i+1]:\n",
    "            if token not in used and not token.is_oov:\n",
    "                if self.is_change_keyword(token):\n",
    "                    change.append(token)\n",
    "                    used.append(token)\n",
    "            # I only want one word that represents\n",
    "            # the change for simplicity\n",
    "            if change:\n",
    "                break\n",
    "        # print(f\"Change 1: {change}\")\n",
    "\n",
    "        # Next Method to Find Change\n",
    "        for token in self.tools.sp_doc[l_i:r_i+1]:\n",
    "            if token not in used and token.pos_ == \"ADP\" and token.lower_ != \"of\":\n",
    "                # print(\"In Next Method...\", token, token not in used, token.pos_)\n",
    "                start_i = token.i + 1\n",
    "                end_i = start_i\n",
    "                possible_changes = []\n",
    "                while end_i <= r_i and self.tools.sp_doc[end_i].pos_ in [\"NUM\", \"SYM\", \"NOUN\", \"ADP\", \"DET\"]:\n",
    "                    possible_changes.append(self.tools.sp_doc[end_i])\n",
    "                    end_i += 1\n",
    "                if not self.has_change_quantity(possible_changes):\n",
    "                    continue\n",
    "                # print(f\"Actual Changes: {possible_changes}\")\n",
    "                for possible_change in possible_changes:\n",
    "                    used.append(possible_change)\n",
    "                    change.append(possible_change)\n",
    "                used.append(token)\n",
    "                break\n",
    "        # print(f\"Change 2: {change}\")\n",
    "\n",
    "        # Find Trait\n",
    "        trait = []\n",
    "        if species:\n",
    "            possible_traits = self.possession.get_owned([species])\n",
    "            trait = possible_traits\n",
    "        if change:\n",
    "            # The trait is listed before the change (i.e. \"diet shifts from ...\")\n",
    "            prev_i = change[0].i - 1\n",
    "            prev_token = None if prev_i < 0 else self.tools.sp_doc[prev_i]\n",
    "            if prev_token and prev_token not in used and prev_token.pos_ == \"NOUN\":\n",
    "                used.append(prev_token)\n",
    "                trait.append(prev_token)\n",
    "            else:\n",
    "                # Look for \"in\" (i.e. \"increase in ...\")\n",
    "                for child in change[0].children:\n",
    "                    # print(child, child.pos_, child.children)\n",
    "                    if child in used:\n",
    "                        continue\n",
    "                    if child.pos_ == \"ADP\" and child.children:\n",
    "                        children = list(child.children)\n",
    "                        if children[0] not in used:\n",
    "                            used.append(children[0])\n",
    "                            trait.append(children[0])\n",
    "        # print(f\"Trait: {trait}\")\n",
    "        \n",
    "        # Find Cause\n",
    "        for token in self.tools.sp_doc[l_i:r_i+1]:\n",
    "            if token not in used and token.pos_ in [\"ADP\"]:\n",
    "                start_i = token.i + 1\n",
    "                end_i = start_i\n",
    "                buffer = []\n",
    "                noun_found = False\n",
    "                while end_i <= r_i and self.tools.sp_doc[end_i] not in used and self.tools.sp_doc[end_i].pos_ in [\"ADP\", \"DET\", \"NOUN\", \"PROPN\", \"AUX\", \"ADV\", \"PRON\", \"ADJ\"]:\n",
    "                    if self.tools.sp_doc[end_i].pos_ in [\"NOUN\", \"PROPN\", \"PRON\"]:\n",
    "                        noun_found = True\n",
    "                    buffer.append(self.tools.sp_doc[end_i])\n",
    "                    end_i += 1\n",
    "                if noun_found:\n",
    "                    for token in buffer:\n",
    "                        used.append(token)\n",
    "                        cause.append(token)\n",
    "                    used.append(token)\n",
    "        # print(f\"Cause: {cause}\")\n",
    "        \n",
    "        unit = Unit(species=species, trait=trait, change=change, cause=cause)\n",
    "        return unit\n",
    "\n",
    "    def parse_sentence(self, l_i, r_i):\n",
    "        # Find Verb\n",
    "        # The verb is used to divide\n",
    "        # the \"parsing\" space, which\n",
    "        # makes the work simpler.\n",
    "        verb = None\n",
    "        for token in self.tools.sp_doc[l_i:r_i+1]:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                verb = token\n",
    "                break\n",
    "\n",
    "        # Base Case\n",
    "        # If there is no verb, we have\n",
    "        # reached the simplest case and\n",
    "        # can extract information.\n",
    "        if verb == None:\n",
    "            return self.parse_segment(l_i, r_i)\n",
    "        # Merge\n",
    "        # Given two units of information,\n",
    "        # we need to merge them.\n",
    "        else:\n",
    "            verb_is_change = self.is_change_keyword(verb)\n",
    "            print(f\"Verb: {verb}\")\n",
    "            print(f\"Text: {self.tools.sp_doc[l_i:r_i+1].text}\")\n",
    "            l_unit = self.parse_sentence(l_i, verb.i - 1)\n",
    "            r_unit = self.parse_sentence(verb.i + 1, r_i)\n",
    "            m_unit = None\n",
    "            print(f\"\\tL Unit: {l_unit}\")\n",
    "            print(f\"\\tR Unit: {r_unit}\")\n",
    "            if l_unit.not_empty() and r_unit.empty():\n",
    "                print(1)\n",
    "                if verb_is_change:\n",
    "                    l_unit.change.append(verb)\n",
    "                m_unit = l_unit\n",
    "            elif r_unit.not_empty() and l_unit.empty():\n",
    "                print(2)\n",
    "                if verb_is_change:\n",
    "                    r_unit.change.append(verb)\n",
    "                m_unit = r_unit    \n",
    "            elif l_unit.can_merge(r_unit):\n",
    "                print(3)\n",
    "                l_unit.merge(r_unit)\n",
    "                if verb_is_change:\n",
    "                    l_unit.change.append(verb)\n",
    "                m_unit = l_unit\n",
    "            elif self.is_change_keyword(verb):\n",
    "                print(4)\n",
    "                r_unit.cause = l_unit\n",
    "                if verb_is_change:\n",
    "                    r_unit.change.append(verb)\n",
    "                m_unit = r_unit\n",
    "            else:\n",
    "                print(5)\n",
    "                if verb.i - 1 > r_i - (verb.i + 1):\n",
    "                    m_unit = l_unit\n",
    "                else:\n",
    "                    m_unit = r_unit\n",
    "            print(f\"M Unit: {m_unit}\")\n",
    "            return m_unit\n",
    "\n",
    "    def parse(self):\n",
    "        units = []\n",
    "        for sent in self.tools.sp_doc.sents:\n",
    "            units.append(self.parse_sentence(sent.start, sent.end - 1))\n",
    "        return units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55fa17d7-220c-45b3-9259-d3fd6b19ac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our results show that an increase in sediment organic matter content is associated to a decline in the abundance of Loripes lucinalis in the Cymodocea nodosa meadows studied, which potentially may weaken the mutualism between the two species.\n"
     ]
    }
   ],
   "source": [
    "# text = Tools.clean_text(\"Acridoidea and Selachii exhibited significant diet shifts from grass to herbs (Kruskal-Wallis test, P 0.01, df 3) when they were in the presence of the comparatively sedentary species (the smaller Pisaurina and the larger Hogna) compared to controls without spiders (Fig. 2).\")\n",
    "# text = Tools.clean_text(\"Our results show that phototrophs can indirectly decrease the population density of heterotrophic bacteria by modification of the nature of bacterial interactions with predators.\")\n",
    "# text = Tools.clean_text(\"Our results show that Selachii can indirectly decrease the population density of Selachimorpha by modification of the nature of bacterial interactions with predators.\")\n",
    "# text = Tools.clean_text(\"All predators inflicted significant mortality on the prey at each prey density compared to the predator-free control for that density\")\n",
    "text = Tools.clean_text(\"Our results show that an increase in sediment organic matter content is associated to a decline in the abundance of Loripes lucinalis (lucinid bivalve) in the Cymodocea nodosa meadows studied, which potentially may weaken the mutualism between the two species.\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe7afe49-383b-41b2-b0a3-52f6faa16b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_lg' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "05/01/2025 00:16:53 - INFO - \t missing_keys: []\n",
      "05/01/2025 00:16:53 - INFO - \t unexpected_keys: []\n",
      "05/01/2025 00:16:53 - INFO - \t mismatched_keys: []\n",
      "05/01/2025 00:16:53 - INFO - \t error_msgs: []\n",
      "05/01/2025 00:16:53 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "05/01/2025 00:17:02 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 36.95 examples/s]\n",
      "05/01/2025 00:17:02 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    }
   ],
   "source": [
    "tools = Tools(text=text)\n",
    "species = Species(tools)\n",
    "possession = Possession(tools)\n",
    "references = References(tools, texts=[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54981fc-2d82-4ea6-942b-b37a15023624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/01/2025 00:17:10 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 46.81 examples/s]\n",
      "05/01/2025 00:17:11 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb: show\n",
      "Text: Our results show that an increase in sediment organic matter content is associated to a decline in the abundance of Loripes lucinalis in the Cymodocea nodosa meadows studied, which potentially may weaken the mutualism between the two species.\n",
      "Verb: associated\n",
      "Text: that an increase in sediment organic matter content is associated to a decline in the abundance of Loripes lucinalis in the Cymodocea nodosa meadows studied, which potentially may weaken the mutualism between the two species.\n",
      "Verb: lucinalis\n",
      "Text: to a decline in the abundance of Loripes lucinalis in the Cymodocea nodosa meadows studied, which potentially may weaken the mutualism between the two species.\n",
      "Verb: studied\n",
      "Text: in the Cymodocea nodosa meadows studied, which potentially may weaken the mutualism between the two species.\n",
      "Verb: weaken\n",
      "Text: , which potentially may weaken the mutualism between the two species.\n",
      "\tL Unit: Species: None, Trait: [], Cause: ([]), Change: []\n",
      "\tR Unit: Species: None, Trait: [], Cause: ([]), Change: [the, two, species]\n",
      "2\n",
      "M Unit: Species: None, Trait: [], Cause: ([]), Change: [the, two, species, weaken]\n",
      "\tL Unit: Species: Cymodocea, Trait: [], Cause: ([]), Change: []\n",
      "\tR Unit: Species: None, Trait: [], Cause: ([]), Change: [the, two, species, weaken]\n",
      "3\n",
      "M Unit: Species: Cymodocea, Trait: [], Cause: ([]), Change: [the, two, species, weaken]\n",
      "\tL Unit: Species: Loripes, Trait: [abundance], Cause: ([a, decline, in, the, abundance, of]), Change: []\n",
      "\tR Unit: Species: Cymodocea, Trait: [], Cause: ([]), Change: [the, two, species, weaken]\n",
      "5\n",
      "M Unit: Species: Loripes, Trait: [abundance], Cause: ([a, decline, in, the, abundance, of]), Change: []\n",
      "\tL Unit: Species: None, Trait: [], Cause: ([an, increase, in, sediment, organic, matter, content, is]), Change: []\n",
      "\tR Unit: Species: Loripes, Trait: [abundance], Cause: ([a, decline, in, the, abundance, of]), Change: []\n",
      "5\n",
      "M Unit: Species: Loripes, Trait: [abundance], Cause: ([a, decline, in, the, abundance, of]), Change: []\n",
      "\tL Unit: Species: None, Trait: [], Cause: ([]), Change: []\n",
      "\tR Unit: Species: Loripes, Trait: [abundance], Cause: ([a, decline, in, the, abundance, of]), Change: []\n",
      "2\n",
      "M Unit: Species: Loripes, Trait: [abundance], Cause: ([a, decline, in, the, abundance, of]), Change: []\n",
      "Species: Loripes, Trait: [abundance], Cause: ([a, decline, in, the, abundance, of]), Change: []\n"
     ]
    }
   ],
   "source": [
    "parser = Parser(text, tools=tools, species=species, possession=possession, references=references)\n",
    "parser.update(text)\n",
    "units = parser.parse()\n",
    "for unit in units:\n",
    "    print(unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d36e0555-44cb-493a-b904-637d586b33e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 24, 24, 24, 25, 21, 38]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species.species_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649240e-d940-4e9d-8af5-ef813b6af566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
