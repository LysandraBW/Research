{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab525f4-ad5d-4e5e-a5e4-f8a951261e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import stanza\n",
    "import textacy\n",
    "from fastcoref import FCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db21e96d-b343-45a9-a784-3cc99b11eec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_md' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "2025-04-29 12:26:29 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "04/29/2025 12:26:29 - INFO - \t Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 8.24MB/s]                    \n",
      "2025-04-29 12:26:30 INFO: Downloaded file to C:\\Users\\lbeln\\stanza_resources\\resources.json\n",
      "04/29/2025 12:26:30 - INFO - \t Downloaded file to C:\\Users\\lbeln\\stanza_resources\\resources.json\n",
      "2025-04-29 12:26:30 WARNING: Language en package default expects mwt, which has been added\n",
      "04/29/2025 12:26:30 - WARNING - \t Language en package default expects mwt, which has been added\n",
      "2025-04-29 12:26:30 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "04/29/2025 12:26:30 - INFO - \t Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2025-04-29 12:26:30 INFO: Using device: cpu\n",
      "04/29/2025 12:26:30 - INFO - \t Using device: cpu\n",
      "2025-04-29 12:26:30 INFO: Loading: tokenize\n",
      "04/29/2025 12:26:30 - INFO - \t Loading: tokenize\n",
      "2025-04-29 12:26:30 INFO: Loading: mwt\n",
      "04/29/2025 12:26:30 - INFO - \t Loading: mwt\n",
      "2025-04-29 12:26:30 INFO: Done loading processors!\n",
      "04/29/2025 12:26:30 - INFO - \t Done loading processors!\n",
      "04/29/2025 12:26:53 - INFO - \t missing_keys: []\n",
      "04/29/2025 12:26:53 - INFO - \t unexpected_keys: []\n",
      "04/29/2025 12:26:53 - INFO - \t mismatched_keys: []\n",
      "04/29/2025 12:26:53 - INFO - \t error_msgs: []\n",
      "04/29/2025 12:26:53 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    }
   ],
   "source": [
    "sp_nlp = spacy.load(\"en_core_web_md\")\n",
    "st_nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "tn_nlp = TaxoNERD().load(model=\"en_ner_eco_biobert\")\n",
    "fcoref = FCoref(enable_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb2b7c5-bd89-4dd3-9ed4-1008b5174c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "OWNER = \"owner\"\n",
    "OWNED = \"owned\"\n",
    "\n",
    "# Pattern 1\n",
    "pattern_1 = [\n",
    "    {\n",
    "        \"RIGHT_ID\": OWNED,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"POS\": {\n",
    "                \"IN\": [\"NOUN\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": OWNED,\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": OWNER,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"poss\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Pattern 2\n",
    "pattern_2 = [\n",
    "     {\n",
    "        \"RIGHT_ID\": OWNED,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"POS\": {\n",
    "                \"IN\": [\"NOUN\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": OWNED,\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"adp\",\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"prep\",\n",
    "            \"POS\": {\n",
    "                \"IN\": [\"ADP\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"adp\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": OWNER,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"pobj\",\n",
    "            \"POS\": {\n",
    "                \"IN\": [\"NOUN\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Pattern 3\n",
    "pattern_3 = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"verb\",\n",
    "        \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"verb\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": OWNER,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"nsubj\",\n",
    "            \"POS\": {\"IN\": [\"PRON\"]}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"verb\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": OWNED,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"dobj\",\n",
    "            \"POS\": {\"IN\": [\"NOUN\"]}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Pattern 4:\n",
    "pattern_4 = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"verb\",\n",
    "        \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"verb\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": OWNED,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"nsubj\",\n",
    "            \"POS\": {\"IN\": [\"NOUN\"]}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"verb\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"adp\",\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"prep\",\n",
    "            \"POS\": {\"IN\": [\"ADP\"]}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"adp\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": OWNER,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"pobj\",\n",
    "            \"POS\": {\"IN\": [\"NOUN\"]}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "patterns = {\n",
    "    \"Pattern1\": pattern_1,\n",
    "    \"Pattern2\": pattern_2,\n",
    "    \"Pattern3\": pattern_3,\n",
    "    \"Pattern4\": pattern_4\n",
    "}\n",
    "\n",
    "def what_matcher(sp_nlp):\n",
    "    matcher = DependencyMatcher(sp_nlp.vocab)\n",
    "    for pattern_id, pattern in patterns.items():\n",
    "        matcher.add(pattern_id, [pattern])\n",
    "    return matcher\n",
    "\n",
    "def what_mapping(sp_nlp, sp_doc):\n",
    "    matcher = what_matcher(sp_nlp)\n",
    "    what_matches = matcher(sp_doc)\n",
    "    \n",
    "    index_to_what_map = {}\n",
    "    for match_id, token_ids in what_matches:\n",
    "        pattern_id = sp_nlp.vocab.strings[match_id]\n",
    "        # print(pattern_id)\n",
    "        owner = None\n",
    "        owned = None\n",
    "        for i in range(len(token_ids)):\n",
    "            right_id = patterns[pattern_id][i][\"RIGHT_ID\"]\n",
    "            if right_id == OWNER:\n",
    "                owner = sp_doc[token_ids[i]]\n",
    "            if right_id == OWNED:\n",
    "                owned = sp_doc[token_ids[i]]\n",
    "        if owner.i not in index_to_what_map:\n",
    "            index_to_what_map[owner.i] = []\n",
    "        index_to_what_map[owner.i].append(owned)\n",
    "        if owned.i not in index_to_what_map:\n",
    "            index_to_what_map[owned.i] = []\n",
    "        index_to_what_map[owned.i].append(owner)\n",
    "\n",
    "    return index_to_what_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b75ca757-c2af-4f37-a710-ee096b8bad1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'left': {'object': Grasshoppers, 'characteristic': None, 'cause': None},\n",
       " 'right': {'left': {'object': shifts,\n",
       "   'characteristic': [grass],\n",
       "   'cause': when they were in the presence of the comparatively sedentary species},\n",
       "  'right': {'object': controls, 'characteristic': [spiders], 'cause': None}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Grasshoppers exhibited significant diet shifts from grass to herbs (Kruskal-Wallis test, P 0.01, df 3) when they were in the presence of the comparatively sedentary species (the smaller Pisaurina and the larger Hogna) compared to controls without spiders (Fig. 2).\"\n",
    "sent = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", sent)\n",
    "sent = re.sub(\"\\s+\", \" \", sent)\n",
    "# print(f\"Sentence: {sent}\")\n",
    "\n",
    "sp_doc = sp_nlp(sent)\n",
    "\n",
    "def process(text):\n",
    "    sp_doc = sp_nlp(text)\n",
    "    # print(f\"\\tSentence: {text}\")\n",
    "\n",
    "    # Find Object\n",
    "    root = list(sp_doc.sents)[0].root\n",
    "    if root.pos_ not in [\"NOUN\", \"PROPN\"]:\n",
    "        for child in root.children:\n",
    "            if child.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                root = child\n",
    "                break\n",
    "    # print(f\"\\t\\tRoot: {root}\")\n",
    "\n",
    "    # Find Characteristic\n",
    "    what_map = what_mapping(sp_nlp, sp_doc)\n",
    "    # print(what_map)\n",
    "\n",
    "    characteristic = None\n",
    "    if root.i in what_map:\n",
    "        characteristic = what_map[root.i]\n",
    "    # print(f\"Characteristic: {characteristic}\")\n",
    "\n",
    "    # Find Cause\n",
    "    cause = None\n",
    "    cause_li = 0\n",
    "    cause_ri = 0\n",
    "    for token in sp_doc:\n",
    "        if token.pos_ != \"SCONJ\":\n",
    "            continue\n",
    "        cause_li = token.i\n",
    "        cause_ri = cause_li + 1\n",
    "        while cause_ri < len(sp_doc) and sp_doc[cause_ri].pos_ in [\"DET\", \"NOUN\", \"PRON\", \"PROPN\", \"ADP\", \"ADV\", \"ADJ\", \"AUX\"]:\n",
    "            cause_ri += 1\n",
    "        cause = sp_doc[cause_li:cause_ri]\n",
    "    # print(f\"Cause: {cause}\")\n",
    "\n",
    "    data = {\n",
    "        \"object\": root,\n",
    "        \"characteristic\": characteristic,\n",
    "        \"cause\": cause\n",
    "    }\n",
    "    # print(data)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def process_sentence(sp_doc, l_i, r_i):\n",
    "    verb = None\n",
    "    for token in sp_doc[l_i:r_i]:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verb = token\n",
    "            break\n",
    "\n",
    "    # print(f\"Verb: {verb}\")\n",
    "    \n",
    "    if verb == None:\n",
    "        return process(sp_doc[l_i:r_i+1].text)\n",
    "    else:\n",
    "        l_data = process_sentence(sp_doc, l_i, verb.i - 1)\n",
    "        r_data = process_sentence(sp_doc, verb.i + 1, r_i)\n",
    "        \n",
    "        return {\n",
    "            \"left\": l_data,\n",
    "            \"right\": r_data\n",
    "        }\n",
    "\n",
    "process_sentence(sp_doc, 0, len(sp_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9ea661-75e4-4102-a5d7-dc7d9ff6d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceParser:\n",
    "    def __init__(self, name, breed):\n",
    "        self.what_map = {}\n",
    "        self.refs_map = {}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fccc004-5eaa-4151-9f8c-e9e4529175af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'left': {'cause': None, 'characteristic': None, 'object': Grasshoppers},\n",
      "    'right': {   'left': {   'cause': when they were in the presence of the comparatively sedentary species,\n",
      "                             'characteristic': [grass],\n",
      "                             'object': shifts},\n",
      "                 'right': {   'cause': None,\n",
      "                              'characteristic': [spiders],\n",
      "                              'object': controls}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(process_sentence(sp_doc, 0, len(sp_doc)), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd252ec1-983a-452d-9290-d69fc3549d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a706f0-f7b7-4506-9d7b-477a272df617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
