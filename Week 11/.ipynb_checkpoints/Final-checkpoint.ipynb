{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ed3266e-0188-4f68-b4cf-9aa939d7637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import glob\n",
    "import spacy\n",
    "import pymupdf\n",
    "import textacy\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from pyalex import Works\n",
    "from IPython.display import clear_output\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d405ffd-43d0-48d1-81b4-6d99f8f34d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller:\n",
    "    def __init__(self):\n",
    "        # print(\"Loading SP_NLP\")\n",
    "        t0 = time.time()\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        t1 = time.time()\n",
    "        # print(f\"SP_NLP: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Loading TN_NLP\")\n",
    "        t0 = time.time()\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "        t1 = time.time()\n",
    "        # print(f\"TN_NLP: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Loading FCOREF\")\n",
    "        t0 = time.time()\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        t1 = time.time()\n",
    "        # print(f\"FCOREF: {t1-t0}s\")\n",
    "        \n",
    "        self.sp_doc = None\n",
    "        self.tn_doc = None\n",
    "        self.tk_map = None\n",
    "    \n",
    "    def update(self, doc):\n",
    "        self.sp_doc = doc\n",
    "        # print(\"Updating TN_DOC\")\n",
    "        t0 = time.time()\n",
    "        self.tn_doc = self.tn_nlp(doc.text)\n",
    "        t1 = time.time()\n",
    "        # print(f\"TN_DOC: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Updating TK_MAP\")\n",
    "        t0 = time.time()\n",
    "        self.tk_map = self.load_token_map()\n",
    "        t1 = time.time()\n",
    "        # print(f\"TK_MAP: {t1-t0}s\")\n",
    "\n",
    "    def load_token_map(self):\n",
    "        tk_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            tk_map[token.idx] = token.i\n",
    "        return tk_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437196c6-d392-406a-9504-ed76e7608963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species:\n",
    "    def __init__(self, controller):\n",
    "        self.controller = controller\n",
    "        self.species_spans = None\n",
    "        self.species_indices = None\n",
    "\n",
    "    def update(self):\n",
    "        if not self.controller.sp_doc or not self.controller.tn_doc:\n",
    "            return\n",
    "        # print(\"Updating Species Indices and Spans\")\n",
    "        t0 = time.time()\n",
    "        self.species_spans, self.species_indices = self.load_species_spans()\n",
    "        t1 = time.time()\n",
    "        # print(f\"Load Species Indices and Span: {t1-t0}s\")\n",
    "        \n",
    "    def load_species_spans(self):\n",
    "        spans = []\n",
    "        indices = []\n",
    "        for species_span in self.controller.tn_doc.ents:\n",
    "            l_species_idx = species_span[0].idx\n",
    "            r_species_idx = species_span[-1].idx\n",
    "            \n",
    "            if l_species_idx not in self.controller.tk_map or r_species_idx not in self.controller.tk_map:\n",
    "                raise Exception(\"Invalid Token\")\n",
    "                \n",
    "            l_species_i = self.controller.tk_map[l_species_idx]\n",
    "            r_species_i = self.controller.tk_map[r_species_idx]\n",
    "\n",
    "            span = self.controller.sp_doc[l_species_i:r_species_i+1]\n",
    "            spans.append(span)\n",
    "            indices += [token.i for token in span]\n",
    "        return (spans, indices)\n",
    "\n",
    "    def is_species(self, token):\n",
    "        return token.i in self.species_indices\n",
    "        \n",
    "    def has_species(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.species_indices:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0db5f2-46e6-49ec-a00f-39f783e711a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    def __init__(self, controller, literals, pos_types, threshold=0.7):\n",
    "        self.controller = controller\n",
    "        self.literals = literals\n",
    "        self.threshold = threshold\n",
    "        self.pos_types = pos_types\n",
    "        self.keywords = [self.controller.sp_nlp(literal) for literal in self.literals]\n",
    "        self.keyword_indices = []\n",
    "\n",
    "    def update(self):\n",
    "        if not self.controller.sp_doc or not self.controller.sp_nlp:\n",
    "            return\n",
    "        # print(\"Updating Keyword Indices\")\n",
    "        t0 = time.time()\n",
    "        self.keyword_indices = self.load_keyword_indices()\n",
    "        t1 = time.time()\n",
    "        # print(f\"Keyword Indices: {t1-t0}s\")\n",
    "        \n",
    "    def is_keyword(self, token):\n",
    "        return token.i in self.keyword_indices\n",
    "\n",
    "    def has_keyword(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.keyword_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_keyword_indices(self):\n",
    "        indices = []\n",
    "        for token in self.controller.sp_doc:\n",
    "            if token.pos_ not in self.pos_types or self.do_not_check(token):\n",
    "                continue\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.literals:\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.controller.sp_nlp(token.lemma_)\n",
    "            for keyword in self.keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > self.threshold:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def find_keyword_indices(self, tokens):\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token.pos_ not in self.pos_types or self.do_not_check(token):\n",
    "                continue\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.literals:\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.controller.sp_nlp(token.lemma_)\n",
    "            for keyword in self.keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > self.threshold:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def do_not_check(self, token):\n",
    "        return len(token) <= 5 or re.match('^[\\w]+$', token.text) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f0487e-a2c4-4a39-b9c7-53d33229d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, controller):\n",
    "        super().__init__(controller, {\"increase\", \"decrease\", \"change\", \"weaken\", \"shift\", \"cause\", \"produce\", \"invade\", \"modify\", \"affect\"}, [\"NOUN\", \"VERB\"], 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e3dba64-0bf8-470b-aa56-c3a6ca616b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class References:\n",
    "    def __init__(self, controller, texts=None):\n",
    "        self.controller = controller\n",
    "        self.predictions = None\n",
    "        self.cluster_map = None\n",
    "        self.text_size_in_tokens = 100\n",
    "        if texts:\n",
    "            self.update(texts)\n",
    "\n",
    "    def update(self, text):\n",
    "        if not self.controller.sp_doc:\n",
    "            return\n",
    "        # print(\"Updating Predictions\")\n",
    "        t0 = time.time()\n",
    "        texts = []\n",
    "        offsets = []\n",
    "        for i in range(0, len(self.controller.sp_doc), self.text_size_in_tokens):\n",
    "            texts.append(self.controller.sp_doc[i:i+self.text_size_in_tokens].text)\n",
    "            offsets.append(self.controller.sp_doc[i].idx)\n",
    "        self.predictions = self.controller.fcoref.predict(texts=texts)\n",
    "        t1 = time.time()\n",
    "        # print(f\"Predictions: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Updating Cluster Map\")\n",
    "        t0 = time.time()\n",
    "        self.cluster_map = self.load_cluster_map(self.predictions, offsets)\n",
    "        t1 = time.time()\n",
    "        # print(f\"Cluster Map: {t1-t0}s\")\n",
    "\n",
    "    def load_cluster_map(self, predictions, offsets):\n",
    "        cluster_map = {}\n",
    "        for prediction, offset in zip(predictions, offsets):\n",
    "            clusters = prediction.get_clusters(as_strings=False)\n",
    "            print(f\"Clusters: {clusters}\")\n",
    "            for cluster in clusters:\n",
    "                print(f\"\\tCluster: {cluster}\")\n",
    "                # Converting Spans to Tokens\n",
    "                token_cluster = []\n",
    "                for span in cluster:\n",
    "                    print(f\"\\t\\tSpan: {span}\")\n",
    "                    index = span[0] + offset\n",
    "                    if index not in self.controller.tk_map:\n",
    "                        raise Exception(\"Invalid Token\")\n",
    "                    index = self.controller.tk_map[index]\n",
    "                    token_cluster.append(self.controller.sp_doc[index])\n",
    "                # Mapping\n",
    "                for token in token_cluster:\n",
    "                    cluster_map[token.i] = list(filter(lambda t: t != token, token_cluster))\n",
    "        return cluster_map\n",
    "            \n",
    "    def get_references(self, tokens):\n",
    "        refs = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.cluster_map:\n",
    "                refs += self.cluster_map[index]\n",
    "        return refs\n",
    "\n",
    "    def same_reference(self, token_a, token_b):\n",
    "        if token_a.lemma_ == token_b.lemma_:\n",
    "            return True\n",
    "        if token_a.i in self.cluster_map and token_b.i in self.cluster_map[token_a.i]:\n",
    "            return True\n",
    "        if token_b.i in self.cluster_map and token_a.i in self.cluster_map[token_b.i]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def same_reference_span(self, span_a, span_b):\n",
    "        if span_a.lemma_ == span_b.lemma_:\n",
    "            return True\n",
    "        for token_a in span_a:\n",
    "            for token_b in span_b:\n",
    "                if self.same_reference(token_a, token_b):\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e62c837-24a5-419c-b539-37be0b68f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scanner():\n",
    "    def __init__(self, controller):\n",
    "        # Helpers\n",
    "        self.controller = controller\n",
    "        self.species = Species(self.controller)\n",
    "        self.references = References(self.controller)\n",
    "        self.changes = ChangeKeywords(self.controller)\n",
    "\n",
    "        # Used to Evaluate Points\n",
    "        self.level1 = {\"ecological\", \"community\", \"interaction\", \"trait\", \"ecosystem\", \"ecology\"}\n",
    "        self.level2 = {\"model\"}\n",
    "        self.level3 = {\"predator\", \"prey\", \"competitor\", \"resource\", \"predation\", \"specie\", \"result\", \"effect\", \"population\", \"species\", \"invade\", \"presence\"}\n",
    "\n",
    "    def update(self, doc):\n",
    "        self.controller.update(doc)\n",
    "        self.references.update(doc.text)\n",
    "        self.species.update()\n",
    "\n",
    "    def get_full_species(self):\n",
    "        full_species = [*self.species.species_spans]\n",
    "        full_indices = [*self.species.species_indices]\n",
    "        \n",
    "        for k, v in self.references.cluster_map.items():\n",
    "            if k in self.species.species_indices:\n",
    "                for token in v:\n",
    "                    if token.i not in full_indices:\n",
    "                        token_span = self.controller.sp_doc[token.i:token.i+1]\n",
    "                        full_species.append(token_span)\n",
    "                        full_indices.append(token.i)\n",
    "            if self.species.has_species(v):\n",
    "                if k not in full_indices:\n",
    "                    token_span = self.controller.sp_doc[k:k+1]\n",
    "                    full_species.append(token_span)\n",
    "                    full_indices.append(k)\n",
    "        \n",
    "        return (full_species, full_indices)\n",
    "\n",
    "    def get_points(self, verbose=True):\n",
    "        points = 0\n",
    "\n",
    "        change_found = False\n",
    "        other_species_found = False\n",
    "        \n",
    "        # Species Work\n",
    "        if verbose:\n",
    "            print(f\"Species Points Before: {points}\")\n",
    "        visited_species_spans = {}\n",
    "        species_spans, species_indices = self.get_full_species()\n",
    "        \n",
    "        species_sent = None\n",
    "        for species_span in species_spans:\n",
    "            if species_span[0].sent.start == species_sent:\n",
    "                continue\n",
    "\n",
    "            # Repeating Species\n",
    "            past_visits = 0\n",
    "            for sp in visited_species_spans.keys():\n",
    "                if self.references.same_reference_span(species_span, self.controller.sp_doc[sp[0]:sp[1]+1]):\n",
    "                    past_visits = visited_species_spans[sp]\n",
    "                    visited_species_spans[sp] += 1\n",
    "            if past_visits == 0:\n",
    "                visited_species_spans[(species_span[0].i, species_span[-1].i)] = 1\n",
    "                    \n",
    "            li = species_span[0].sent.start\n",
    "            ri = species_span[-1].sent.end\n",
    "\n",
    "            l_token_indices = set([token.i for token in self.controller.sp_doc[li:i]])\n",
    "            r_token_indices = set([token.i for token in self.controller.sp_doc[i+1:ri]])\n",
    "\n",
    "            # Nearby Actions (Modification)\n",
    "            change_indices = set(self.changes.find_keyword_indices(self.controller.sp_doc[li:ri]))\n",
    "            l_changes = l_token_indices.intersection(change_indices)\n",
    "            r_changes = r_token_indices.intersection(change_indices)\n",
    "\n",
    "            # There must be a change.\n",
    "            if not l_changes and not r_changes:\n",
    "                continue\n",
    "                \n",
    "            # Nearby Species (Interaction)\n",
    "            l_species = l_token_indices.intersection(species_indices)\n",
    "            r_species = r_token_indices.intersection(species_indices)\n",
    "\n",
    "            if l_changes or r_changes:\n",
    "                points += 10 * (past_visits + 1)\n",
    "                change_found = True\n",
    "            if l_species or r_species:\n",
    "                points += 10 * (past_visits + 1)\n",
    "                other_species_found = True\n",
    "            points += min(past_visits * 10, 1000)\n",
    "            \n",
    "            species_sent = li\n",
    "        if verbose:\n",
    "            print(f\"Species Points After: {points}\")\n",
    "        \n",
    "        # Keyword Work\n",
    "        if verbose:\n",
    "            print(f\"Keyword Points Before: {points}\")\n",
    "        for token in self.controller.sp_doc:\n",
    "            if token.pos_ not in [\"NOUN\", \"VERB\"]:\n",
    "                continue\n",
    "            \n",
    "            lemma = token.lemma_\n",
    "            if lemma in self.level1:\n",
    "                points += 100\n",
    "            elif lemma in self.level2:\n",
    "                points += 25\n",
    "            elif lemma in self.level3:\n",
    "                points += 1\n",
    "        if verbose:\n",
    "            print(f\"Keyword Points After: {points}\")\n",
    "        \n",
    "        # Fairness\n",
    "        if verbose:\n",
    "            print(f\"Fairness Points Before: {points}\")\n",
    "        points //= len(list(self.controller.sp_doc.sents))\n",
    "        if verbose:\n",
    "            print(f\"Fairness Points After: {points}\")\n",
    "\n",
    "        if not change_found or len(visited_species_spans) < 3:\n",
    "            return 0\n",
    "        return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e800e44-a00b-47b7-84a5-a7504237d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = text\n",
    "    cleaned_text = re.sub(r'http\\S+', '', cleaned_text) # Remove URLs\n",
    "    cleaned_text = re.sub(r'-\\n', '', cleaned_text) # Remove Hyphenations\n",
    "    cleaned_text = re.sub(\"\\s+\", \" \", cleaned_text) # Remove Duplicate Spaces\n",
    "    cleaned_text = re.sub(r\"\\s+([?.!,])\", r\"\\1\", cleaned_text) # Remove Spaces Before Punctuation\n",
    "    return cleaned_text\n",
    "\n",
    "def load_documents(group=\"Cleared\"):\n",
    "    documents = []\n",
    "    filenames = glob.glob(f\"../Week 7/Examples/{group}/*.pdf\")\n",
    "    for filename in filenames:\n",
    "        full_text = \"\"\n",
    "        doc = pymupdf.open(filename)\n",
    "        for page in doc:\n",
    "            text = page.get_text()\n",
    "            full_text += \" \" + text\n",
    "        if full_text:\n",
    "            documents.append(clean_text(full_text))\n",
    "    return documents\n",
    "\n",
    "def pdf_to_text(url):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        f = pdf_bytes(url)\n",
    "        doc = pymupdf.open(stream=f)\n",
    "        for d in doc:\n",
    "            text += d.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def load_documents_from_api():\n",
    "    keywords = [\"higher-order interactions\", \"trait-mediated interaction modification\", \"trait-mediated interaction\", \"polymorphism\", \"apparent competition\", \"resource competition\", \"keystone predation\", \"intraguild predation\", \"intransitive competition\", \"trophic chains\", \"competition chains\", \"mutual competition\"]\n",
    "    number_keywords = len(keywords)\n",
    "    all_keywords = [*keywords]\n",
    "    for i in range(4):\n",
    "        for j in range(4, number_keywords, 1):\n",
    "            all_keywords.append(f\"{keywords[i]} {keywords[j]}\")\n",
    "\n",
    "    # Loading Texts\n",
    "    texts = []\n",
    "    number_works = 0\n",
    "    number_unfiltered_works = 0\n",
    "    number_keywords = len(all_keywords)\n",
    "    k = 0\n",
    "    for keyword in all_keywords:\n",
    "        print(f\"({k + 1}/{number_keywords}) Searching Keyword '{keyword}'\")\n",
    "        pager = Works().search_filter(title=keyword).paginate(per_page=200)\n",
    "        for page in pager:\n",
    "            for work in page:\n",
    "                number_unfiltered_works += 1\n",
    "                \n",
    "                title = work['title']\n",
    "                abstract = work['abstract']\n",
    "                doi = work['doi']\n",
    "                \n",
    "                # Find Full Text\n",
    "                url = None\n",
    "                if work[\"primary_location\"]:\n",
    "                    url = work[\"primary_location\"][\"pdf_url\"]\n",
    "                full_text = \"\" if not url else pdf_to_text(url)\n",
    "                \n",
    "                if not abstract and not full_text:\n",
    "                    continue\n",
    "                texts.append((k, title, doi, abstract if abstract and not full_text else full_text))\n",
    "                number_works += 1\n",
    "        k += 1\n",
    "        clear_output(wait=True)        \n",
    "\n",
    "    assert len(texts) == number_works\n",
    "    print(f\"Number Documents: {len(texts)}, Number Unfiltered Documents: {number_unfiltered_works}\")\n",
    "    return (texts, [text[-1] for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df9bce0-5f71-4387-b6d8-5be36177e372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2777/10356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/09/2025 20:28:45 - INFO - \t Tokenize 2 inputs...\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 31.43 examples/s]\n",
      "05/09/2025 20:28:45 - INFO - \t ***** Running Inference on 2 texts *****\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 9\u001b[0m \u001b[43mscanner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# print(f\"Total Time: {t1-t0}\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m, in \u001b[0;36mScanner.update\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontroller\u001b[38;5;241m.\u001b[39mupdate(doc)\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecies\u001b[38;5;241m.\u001b[39mupdate()\n",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36mReferences.update\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# print(f\"Predictions: {t1-t0}s\")\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# print(\"Updating Cluster Map\")\u001b[39;00m\n\u001b[0;32m     25\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_cluster_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m, in \u001b[0;36mReferences.load_cluster_map\u001b[1;34m(self, predictions, offsets)\u001b[0m\n\u001b[0;32m     36\u001b[0m token_cluster \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m span \u001b[38;5;129;01min\u001b[39;00m cluster:\n\u001b[1;32m---> 38\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mspan\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m offset\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontroller\u001b[38;5;241m.\u001b[39mtk_map:\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Token\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "labeled_documents, documents = load_documents_from_api()\n",
    "labeled_points = []\n",
    "\n",
    "scanner = Scanner(Controller())\n",
    "i = 0\n",
    "for doc in scanner.controller.sp_nlp.pipe(documents):\n",
    "    print(f\"{i+1}/{len(documents)}\")\n",
    "    t0 = time.time()\n",
    "    scanner.update(doc)\n",
    "    t1 = time.time()\n",
    "    # print(f\"Total Time: {t1-t0}\")\n",
    "    \n",
    "    points = scanner.get_points(verbose=False)\n",
    "    labeled_points.append((labeled_documents[i][0], labeled_documents[i][1], labeled_documents[i][2], points))\n",
    "    # print(f\"'{labeled_documents[i][0]}' Points: {points}\\n\")\n",
    "    i += 1\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca0eb52e-af15-424a-8be4-f9e5c1154c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "labeled_points.sort(key=lambda tup: tup[-1], reverse=True)\n",
    "\n",
    "data = [\n",
    "    [\"Index\", \"Title\", \"DOI\", \"Points\"],\n",
    "    *labeled_points\n",
    "]\n",
    "\n",
    "file_path = 'output.csv'\n",
    "with open(file_path, 'w', encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27ed67c-1379-428b-b3b4-9af805dc224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "781522ef-4a8a-4653-a3bf-750e5b9cf316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2777/10356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/09/2025 21:29:51 - INFO - \t Tokenize 2 inputs...\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 21.23 examples/s]\n",
      "05/09/2025 21:29:51 - INFO - \t ***** Running Inference on 2 texts *****\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m \u001b[43mscanner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m, in \u001b[0;36mScanner.update\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontroller\u001b[38;5;241m.\u001b[39mupdate(doc)\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecies\u001b[38;5;241m.\u001b[39mupdate()\n",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36mReferences.update\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# print(f\"Predictions: {t1-t0}s\")\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# print(\"Updating Cluster Map\")\u001b[39;00m\n\u001b[0;32m     25\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_cluster_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m, in \u001b[0;36mReferences.load_cluster_map\u001b[1;34m(self, predictions, offsets)\u001b[0m\n\u001b[0;32m     36\u001b[0m token_cluster \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m span \u001b[38;5;129;01min\u001b[39;00m cluster:\n\u001b[1;32m---> 38\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mspan\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m offset\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontroller\u001b[38;5;241m.\u001b[39mtk_map:\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Token\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "labeled_documents[2776]\n",
    "\n",
    "for doc in scanner.controller.sp_nlp.pipe([documents[2776]]):\n",
    "    print(f\"{i+1}/{len(documents)}\")\n",
    "    t0 = time.time()\n",
    "    scanner.update(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670edeb4-b4cc-440b-bb62-00960edfa8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
