{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d888a6-3df7-4dfd-a573-811ad167fbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import textacy\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from fastcoref import FCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from fastcoref import spacy_component\n",
    "from spacy.matcher import Matcher, DependencyMatcher, PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d3a028-c928-4c47-9be9-e34b313f6efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5013e7f6-f39b-4671-98eb-0c46e494316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tools:\n",
    "    def __init__(self):\n",
    "        self.taxonerd = TaxoNERD(prefer_gpu=True)\n",
    "        self.nlp = self.taxonerd.load(model=\"en_ner_eco_biobert\")\n",
    "        self.nlp.add_pipe(\"fastcoref\")\n",
    "        print(self.nlp.pipe_names)\n",
    "        self.doc = None\n",
    "        self.token_map = None\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        cleaned_text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "        cleaned_text = re.sub(\"\\s+\", \" \", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"\\s+([?.!,])\", r\"\\1\", cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def update(self, doc):\n",
    "        self.doc = doc\n",
    "        # Map Tokens to Index\n",
    "        self.token_map = {}\n",
    "        for token in self.doc:\n",
    "            self.token_map[token.idx] = token.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c96853-5df0-4680-8d1e-2f9cf3c189fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class References:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        self.cluster_map = None\n",
    "\n",
    "    def update(self):\n",
    "        if not self.tools.doc:\n",
    "            return\n",
    "        self.cluster_map = self.get_cluster_map(self.tools.doc._.coref_clusters)\n",
    "        \n",
    "    def get_cluster_map(self, clusters):\n",
    "        cluster_map = {}\n",
    "        for cluster in clusters:\n",
    "            token_cluster = []\n",
    "            for span in cluster:\n",
    "                if span[0] not in self.tools.token_map:\n",
    "                    raise Exception(\"Invalid Token\")\n",
    "                index = self.tools.token_map[span[0]]\n",
    "                token_cluster.append(self.tools.doc[index])\n",
    "            # Mapping\n",
    "            for token in token_cluster:\n",
    "                cluster_map[token.i] = list(filter(lambda t: t != token, token_cluster))\n",
    "        return cluster_map\n",
    "            \n",
    "    def get_references(self, tokens):\n",
    "        refs = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.cluster_map:\n",
    "                refs += self.cluster_map[index]\n",
    "        return refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63830b96-590e-491b-bf60-90833967cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Possession:\n",
    "    # There's no definite names for these patterns as I do not know what\n",
    "    # to call them. These patterns are used to extract possessive\n",
    "    # relationships from a sentence. I also could not find better names for\n",
    "    # the two variables below.\n",
    "    OWNER = \"owner\"\n",
    "    OWNED = \"owned\"\n",
    "    \n",
    "    patterns = {\n",
    "        \"Pattern1\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": OWNED,\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"poss\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern2\": [\n",
    "             {\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": OWNED,\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": \"adp\",\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"prep\",\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"ADP\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"adp\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"pobj\",\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern3\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": \"verb\",\n",
    "                \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"nsubj\",\n",
    "                    \"POS\": {\"IN\": [\"PRON\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"dobj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern4\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": \"verb\",\n",
    "                \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"nsubj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": \"adp\",\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"prep\",\n",
    "                    \"POS\": {\"IN\": [\"ADP\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"adp\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"pobj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        self.matcher = DependencyMatcher(self.tools.nlp.vocab)\n",
    "        for pattern_id, pattern in Possession.patterns.items():\n",
    "            self.matcher.add(pattern_id, [pattern])\n",
    "        self.owner_map = None\n",
    "        self.owned_map = None\n",
    "        self.update()\n",
    "    \n",
    "    def update(self):\n",
    "        if not self.tools.doc:\n",
    "            return\n",
    "        matches = self.matcher(self.tools.doc)\n",
    "        owner_map, owned_map = self.get_ownership_map(matches)\n",
    "        self.owner_map = owner_map # Maps Owner to Owned\n",
    "        self.owned_map = owned_map # Maps Owned to Owner\n",
    "        \n",
    "    def get_ownership_map(self, matches):\n",
    "        owner_map = {}\n",
    "        owned_map = {}\n",
    "\n",
    "        for match_id, token_ids in matches:\n",
    "            pattern_id = self.tools.nlp.vocab.strings[match_id]\n",
    "            # print(pattern_id)\n",
    "            owner = None\n",
    "            owned = None\n",
    "            for i in range(len(token_ids)):\n",
    "                right_id = Possession.patterns[pattern_id][i][\"RIGHT_ID\"]\n",
    "                if right_id == Possession.OWNER:\n",
    "                    owner = self.tools.doc[token_ids[i]]\n",
    "                if right_id == Possession.OWNED:\n",
    "                    owned = self.tools.doc[token_ids[i]]\n",
    "\n",
    "            # Owner to Owned\n",
    "            if owner.i not in owner_map:\n",
    "                owner_map[owner.i] = []\n",
    "            owner_map[owner.i].append(owned)\n",
    "\n",
    "            # Owned to Owner\n",
    "            if owned.i not in owned_map:\n",
    "                owned_map[owned.i] = []\n",
    "            owned_map[owned.i].append(owner)\n",
    "            \n",
    "        return (owner_map, owned_map)\n",
    "\n",
    "    def get_owner(self, tokens):\n",
    "        owners = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.owned_map:\n",
    "                owners += self.owned_map[index]\n",
    "        return owners\n",
    "\n",
    "    def get_owned(self, tokens):\n",
    "        owned = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.owner_map:\n",
    "                owned += self.owner_map[index]\n",
    "        return owned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11b24e95-8df1-4438-aaa5-7d8ba0759ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        self.species_indices = None\n",
    "        self.update()\n",
    "\n",
    "    def update(self):\n",
    "        if not self.tools.doc:\n",
    "            return\n",
    "        self.species_indices = self.get_species_indices()\n",
    "        \n",
    "    def get_species_indices(self):\n",
    "        indices = []\n",
    "\n",
    "        # lowered_text = self.tools.doc.text.lower()\n",
    "        # for token in self.tools.doc:\n",
    "        #     if token.pos_ not in [\"NOUN\", \"PROPN\"]:\n",
    "        #         continue\n",
    "        #     try:\n",
    "        #         results = requests.get(f\"https://api.inaturalist.org/v1/search?q={token.lemma_}&sources=taxa&include_taxon_ancestors=false\")\n",
    "        #         results = results.json()\n",
    "        #         results = results[\"results\"]\n",
    "        #         for result in results:\n",
    "        #             if \"record\" not in result or \"name\" not in result[\"record\"]:\n",
    "        #                 continue\n",
    "        #             if lowered_text.find(result[\"record\"][\"name\"].lower()) == -1:\n",
    "        #                 continue\n",
    "        #             indices.append(token.i)\n",
    "        #     except Exception as e:\n",
    "        #         print(\"Network Error\")\n",
    "                \n",
    "        for species_span in self.tools.doc.ents:\n",
    "            for species in species_span:\n",
    "                if species.idx not in self.tools.token_map:\n",
    "                    raise Exception(\"Invalid Token\")\n",
    "                index = self.tools.token_map[species.idx]\n",
    "                if index in indices:\n",
    "                    continue\n",
    "                indices.append(index)\n",
    "        return indices\n",
    "\n",
    "    def is_species(self, token):\n",
    "        index = token.i\n",
    "        return index in self.species_indices\n",
    "        \n",
    "    def contains_species(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.species_indices:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d802666c-0146-4d8b-80bd-4bd8fa689aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit:\n",
    "    def __init__(self, *, species=None, trait=None, change=None, cause=None):\n",
    "        self.species = species\n",
    "        self.trait = trait\n",
    "        self.cause = cause\n",
    "        self.change = change\n",
    "        # Flag\n",
    "        self.is_cause = False\n",
    "\n",
    "    def empty(self):\n",
    "        if not self.species and not self.trait and not self.cause and not self.change:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def not_empty(self):\n",
    "        return not self.empty()\n",
    "\n",
    "    def can_merge(self, unit):\n",
    "        # Two units can merge if there's no\n",
    "        # overlap.\n",
    "        if self.species and unit.species:\n",
    "            return False\n",
    "        if self.trait and unit.trait:\n",
    "            return False\n",
    "        if self.cause and unit.cause:\n",
    "            return False\n",
    "        if self.change and unit.change:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def merge(self, unit):\n",
    "        # We take the parts that the\n",
    "        # other unit has; assuming that\n",
    "        # there's no overlap, there's\n",
    "        # no loss of information.\n",
    "        if unit.species:\n",
    "            self.species = unit.species\n",
    "        if unit.trait:\n",
    "            self.trait = unit.trait\n",
    "        if unit.cause:\n",
    "            self.cause = unit.cause\n",
    "        if unit.change:\n",
    "            self.change = unit.change\n",
    "\n",
    "    def get_score(self):\n",
    "        score = 0\n",
    "        if self.species:\n",
    "            score += 1\n",
    "        if self.trait:\n",
    "            score += 1\n",
    "        if self.cause:\n",
    "            score += 1\n",
    "        if self.change:\n",
    "            score += 1    \n",
    "        return score\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Species: {self.species}, Trait: {self.trait}, Cause: ({self.cause}), Change: {self.change}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869e90fe-7ecb-4840-89f8-5ad104a7f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:    \n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        # References\n",
    "        self.unit_keywords = [self.tools.nlp(keyword) for keyword in {\"unit\", \"%\", \"percent\"}]\n",
    "        self.change_keywords = [self.tools.nlp(keyword) for keyword in {\"increase\", \"decrease\", \"change\", \"weaken\", \"shift\", \"cause\"}]\n",
    "        self.quantity_keywords = [self.tools.nlp(keyword) for keyword in {\"tenfold\", \"half\", \"double\", \"triple\", \"quadruple\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"}]\n",
    "        # Instances\n",
    "        self.unit_indices = []\n",
    "        self.change_indices = []\n",
    "        self.quantity_indices = []\n",
    "        self.cause_indices = []\n",
    "        self.update()\n",
    "        \n",
    "    def update(self):\n",
    "        if not self.tools.doc:\n",
    "            return\n",
    "        self.unit_indices = self.load_unit_indices()\n",
    "        self.change_indices = self.load_change_indices()\n",
    "        self.quantity_indices = self.load_quantity_indices()\n",
    "        self.cause_indices = self.load_cause_indices()\n",
    "        return\n",
    "\n",
    "    def is_unit(self, token):\n",
    "        return token.i in self.unit_indices\n",
    "\n",
    "    def has_unit(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.unit_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_unit_indices(self):\n",
    "        indices = []\n",
    "        for token in self.tools.doc:\n",
    "            if token.pos_ not in [\"NOUN\"]:\n",
    "                continue\n",
    "            lemma = self.tools.nlp(token.lemma_)\n",
    "            for keyword in self.unit_keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > 0.7:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def is_change(self, token):\n",
    "        return token.i in self.change_indices\n",
    "\n",
    "    def has_change(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.change_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_change_indices(self):\n",
    "        indices = []\n",
    "        for token in self.tools.doc:\n",
    "            if token.lower_ == \"to\" and token.head and token.head.lower_ == \"from\":\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            if token.pos_ not in [\"NOUN\", \"VERB\"]:\n",
    "                continue\n",
    "            lemma = self.tools.nlp(token.lemma_)\n",
    "            for keyword in self.change_keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > 0.7:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def is_quantity(self, token):\n",
    "        return token.i in self.quantity_indices\n",
    "\n",
    "    def has_quantity(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.quantity_indices:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def load_quantity_indices(self):\n",
    "        indices = []\n",
    "        for token in self.tools.doc:\n",
    "            if token.pos_ not in [\"NOUN\", \"NUM\"]:\n",
    "                continue\n",
    "            lemma = self.tools.nlp(token.lemma_)\n",
    "            for keyword in self.quantity_keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > 0.7:\n",
    "                    # Make sure that if there is a noun the quantity\n",
    "                    # modifies, and that it is a unit.\n",
    "                    if token.head and token.head.pos_ == \"NOUN\" and not self.is_unit(token.head):\n",
    "                        continue\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def is_cause(self, token):\n",
    "        return token.i in self.cause_indices\n",
    "\n",
    "    def has_cause(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.cause_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_cause_indices(self):\n",
    "        indices = []\n",
    "        for token in self.tools.doc:\n",
    "            if token.pos_ not in [\"ADP\", \"SCONJ\", \"PART\", \"PRON\"]:\n",
    "                continue\n",
    "            if token.pos_ == \"SCONJ\":\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            elif token.pos_ == \"PART\":\n",
    "                if token.head and token.head.pos_ == \"VERB\":\n",
    "                    indices.append(token.i)\n",
    "                    continue\n",
    "            elif token.pos_ == \"ADP\":\n",
    "                if token.lower_ == \"due\" and self.tools.doc[token.i + 1] and self.tools.doc[token.i + 1].lower_ == \"to\":\n",
    "                    indices.append(token.i)\n",
    "                    continue\n",
    "                elif token.head:\n",
    "                    if token.head.pos_ == \"AUX\":\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "                    elif token.head.pos_ == \"VERB\" and token.head.i < token.i and self.is_change(token.head):\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "                    elif token.lower_ != \"to\" and \"AUX\" in [child.pos_ for child in list(filter(lambda t: t.i < token.i,token.head.children))]:\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "                elif token.ancestors:\n",
    "                    if \"AUX\" in [ancestor.pos_ for ancestor in token.ancestors]:\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "            elif token.pos_ == \"PRON\":\n",
    "                if token.head and token.head.pos_ == \"VERB\" and self.is_change(token.head):\n",
    "                    indices.append(token.i)\n",
    "                    continue\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95c46620-832d-4d6c-8495-155fbcb68442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.tools = Tools()\n",
    "        self.species = Species(self.tools)\n",
    "        self.possession = Possession(self.tools)\n",
    "        self.references = References(self.tools)\n",
    "        self.keywords = Keywords(self.tools)\n",
    "            \n",
    "    def update(self, doc):\n",
    "        self.tools.update(doc)\n",
    "        self.species.update()\n",
    "        self.possession.update()\n",
    "        self.references.update()\n",
    "        self.keywords.update()\n",
    "                \n",
    "    def parse_segment(self, l_i, r_i):\n",
    "        # print(f\"\\nPARSING SEGMENT\\n\")\n",
    "        # print(f\"Text: {self.tools.doc[l_i:r_i+1].text}\")\n",
    "        used = []\n",
    "\n",
    "        # Find Cause\n",
    "        cause = []\n",
    "        for token in self.tools.doc[l_i:r_i+1]:\n",
    "            if token not in used and token.pos_ in [\"SCONJ\"] and self.keywords.is_cause(token):\n",
    "                start_i = token.i + 1\n",
    "                end_i = start_i\n",
    "                while end_i <= r_i and self.tools.doc[end_i] not in used and self.tools.doc[end_i].pos_ in [\"ADP\", \"DET\", \"NOUN\", \"PROPN\", \"AUX\", \"ADV\", \"PRON\", \"ADJ\"]:\n",
    "                    used.append(self.tools.doc[end_i])\n",
    "                    cause.append(self.tools.doc[end_i])\n",
    "                    end_i += 1\n",
    "                used.append(token)\n",
    "        # print(f\"Cause 1: {cause}\")\n",
    "        \n",
    "        # Find Species\n",
    "        species = []\n",
    "        for token in self.tools.doc[l_i:r_i+1]:\n",
    "            if self.species.is_species(token) and token not in used and (token.head and (token.head.pos_ not in [\"SCONJ\", \"ADP\"] or token.head.lower_ == \"of\")):\n",
    "                species.append(token)\n",
    "                used.append(species)\n",
    "                break\n",
    "        # print(f\"Species: {species}\")\n",
    "        \n",
    "        # Find Change\n",
    "        change = []\n",
    "        for token in self.tools.doc[l_i:r_i+1]:\n",
    "            if token not in used and not token.is_oov:\n",
    "                if self.keywords.is_change(token):\n",
    "                    change.append(token)\n",
    "                    used.append(token)\n",
    "            # I only want one word that represents\n",
    "            # the change for simplicity\n",
    "            if change:\n",
    "                break\n",
    "        # print(f\"Change 1: {change}\")\n",
    "\n",
    "        # Next Method to Find Change\n",
    "        for token in self.tools.doc[l_i:r_i+1]:\n",
    "            if token not in used and token.pos_ == \"ADP\" and token.lower_ != \"of\":\n",
    "                # print(\"In Next Method...\", token, token not in used, token.pos_)\n",
    "                start_i = token.i + 1\n",
    "                end_i = start_i\n",
    "                possible_changes = []\n",
    "                while end_i <= r_i and self.tools.doc[end_i].pos_ in [\"NUM\", \"SYM\", \"NOUN\", \"ADP\", \"DET\"]:\n",
    "                    possible_changes.append(self.tools.doc[end_i])\n",
    "                    end_i += 1\n",
    "                if not self.keywords.has_quantity(possible_changes):\n",
    "                    continue\n",
    "                # print(f\"Actual Changes: {possible_changes}\")\n",
    "                for possible_change in possible_changes:\n",
    "                    used.append(possible_change)\n",
    "                    change.append(possible_change)\n",
    "                used.append(token)\n",
    "                break\n",
    "        # print(f\"Change 2: {change}\")\n",
    "\n",
    "        # Find Trait\n",
    "        trait = []\n",
    "        if species:\n",
    "            possible_traits = list(filter(lambda t: t.i >= l_i and t.i <= r_i, self.possession.get_owned(species)))\n",
    "            valid_trait = False\n",
    "            for possible_trait in possible_traits:\n",
    "                for ancestor in possible_trait.ancestors:\n",
    "                    if self.keywords.is_change(ancestor):\n",
    "                        valid_trait = True\n",
    "                        break\n",
    "            if valid_trait:\n",
    "                trait = possible_traits\n",
    "        elif change:\n",
    "            # The trait is listed before the change (i.e. \"diet shifts from ...\")\n",
    "            prev_i = change[0].i - 1\n",
    "            prev_token = None if prev_i < 0 else self.tools.doc[prev_i]\n",
    "            if prev_token and prev_token not in used and prev_token.pos_ == \"NOUN\":\n",
    "                used.append(prev_token)\n",
    "                trait.append(prev_token)\n",
    "            else:\n",
    "                # Look for \"in\" (i.e. \"increase in ...\")\n",
    "                for child in change[0].children:\n",
    "                    # print(child, child.pos_, child.children)\n",
    "                    if child in used:\n",
    "                        continue\n",
    "                    if child.pos_ == \"ADP\" and child.children:\n",
    "                        children = list(child.children)\n",
    "                        if children[0] not in used:\n",
    "                            used.append(children[0])\n",
    "                            trait.append(children[0])\n",
    "        else:\n",
    "            for token in self.tools.doc[l_i:r_i+1]:\n",
    "                if token.head and self.keywords.is_change(token.head):\n",
    "                    trait.append(token)\n",
    "                    used.append(token)\n",
    "\n",
    "                    possible_species = self.possession.get_owner(trait)\n",
    "                    if self.species.contains_species(possible_species):\n",
    "                        for sp in possible_species:\n",
    "                            if token in sp.ancestors:\n",
    "                                species.append(sp)\n",
    "                                used.append(sp)\n",
    "                                break\n",
    "                    break\n",
    "        # print(f\"Trait: {trait}\")\n",
    "        \n",
    "        # Find Cause\n",
    "        is_cause = False\n",
    "        for token in self.tools.doc[l_i:r_i+1]:\n",
    "            if token not in used and token.pos_ in [\"PRON\"] and self.keywords.is_cause(token):\n",
    "                is_cause = True\n",
    "                used.append(token)\n",
    "            elif token not in used and token.pos_ in [\"ADP\"] and self.keywords.is_cause(token):\n",
    "                start_i = token.i + 1\n",
    "                end_i = start_i\n",
    "                buffer = []\n",
    "                noun_found = False\n",
    "                while end_i <= r_i and self.tools.doc[end_i] not in used and self.tools.doc[end_i].pos_ in [\"ADP\", \"DET\", \"NOUN\", \"PROPN\", \"AUX\", \"ADV\", \"PRON\", \"ADJ\"]:\n",
    "                    if self.tools.doc[end_i].pos_ in [\"NOUN\", \"PROPN\", \"PRON\"]:\n",
    "                        noun_found = True\n",
    "                    buffer.append(self.tools.doc[end_i])\n",
    "                    end_i += 1\n",
    "                if noun_found:\n",
    "                    for token in buffer:\n",
    "                        used.append(token)\n",
    "                        cause.append(token)\n",
    "                    used.append(token)\n",
    "        # print(f\"Cause 2: {cause}\")\n",
    "        \n",
    "        unit = Unit(species=species, trait=trait, change=change, cause=cause)\n",
    "        unit.is_cause = is_cause\n",
    "        return unit\n",
    "\n",
    "    def parse_sentence(self, l_i, r_i):\n",
    "        units = []\n",
    "        \n",
    "        # Recursive Split\n",
    "        # We're extracting the core information\n",
    "        # in the sentence into units.\n",
    "        def recursive_split(r_l_i, r_r_i):\n",
    "            nonlocal units\n",
    "            # Find Verb\n",
    "            # The verb is used to divide\n",
    "            # the \"parsing\" space, which\n",
    "            # makes the work simpler.\n",
    "            verb = None\n",
    "            for token in self.tools.doc[r_l_i:r_r_i+1]:\n",
    "                if token.pos_ == \"VERB\":\n",
    "                    verb = token\n",
    "                    break\n",
    "    \n",
    "            # Base Case\n",
    "            # If there is no verb, we have\n",
    "            # reached the simplest case and\n",
    "            # can extract information.\n",
    "            if verb == None:\n",
    "                units.append(self.parse_segment(r_l_i, r_r_i))\n",
    "            else:\n",
    "                recursive_split(r_l_i, verb.i - 1)\n",
    "                units.append(verb)\n",
    "                recursive_split(verb.i + 1, r_r_i)\n",
    "            return\n",
    "        recursive_split(l_i, r_i)\n",
    "\n",
    "        # Recursive Merge\n",
    "        # We are putting the pieces back together,\n",
    "        # so that we, the computer, can understand\n",
    "        # what's going on.\n",
    "        def recursive_merge():\n",
    "            nonlocal units\n",
    "            if len(units) < 3:\n",
    "                return\n",
    "            \n",
    "            l_unit = units[0]\n",
    "            verb = units[1]\n",
    "            r_unit = units[2]\n",
    "            verb_is_change = self.keywords.is_change(verb)\n",
    "            \n",
    "            if l_unit.empty() and r_unit.empty():\n",
    "                m_unit = Unit()  \n",
    "            elif l_unit.not_empty() and r_unit.empty():\n",
    "                # print(1)\n",
    "                if verb_is_change:\n",
    "                    l_unit.change.append(verb)\n",
    "                m_unit = l_unit\n",
    "            elif r_unit.not_empty() and l_unit.empty():\n",
    "                # print(2)\n",
    "                if verb_is_change:\n",
    "                    r_unit.change.append(verb)\n",
    "                m_unit = r_unit    \n",
    "            elif l_unit.can_merge(r_unit):\n",
    "                # print(3)\n",
    "                l_unit.merge(r_unit)\n",
    "                if verb_is_change:\n",
    "                    l_unit.change.append(verb)\n",
    "                m_unit = l_unit\n",
    "            elif verb_is_change:\n",
    "                # print(4)\n",
    "                r_unit.cause = l_unit\n",
    "                if verb_is_change:\n",
    "                    r_unit.change.append(verb)\n",
    "                m_unit = r_unit\n",
    "            else:\n",
    "                # print(5)\n",
    "                if l_unit.get_score() >= r_unit.get_score():\n",
    "                    m_unit = l_unit\n",
    "                else:\n",
    "                    m_unit = r_unit\n",
    "            units = [m_unit] + units[3:]\n",
    "            recursive_merge()\n",
    "            return\n",
    "        recursive_merge()\n",
    "\n",
    "        assert len(units) == 1\n",
    "        return units[0]\n",
    "\n",
    "    def parse(self):\n",
    "        units = []\n",
    "        for sent in self.tools.doc.sents:\n",
    "            print(f\"Sentence: {self.tools.doc[sent.start:sent.end].text}\")\n",
    "            unit = self.parse_sentence(sent.start, sent.end - 1)\n",
    "            print(unit)\n",
    "            units.append(unit)\n",
    "            print()\n",
    "        return units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a1dfd80-ba55-4a46-ab79-bb59f398844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2025 12:00:07 - INFO - \t GPU is available\n",
      "05/04/2025 12:00:07 - INFO - \t TaxoNERD will use GPU\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E002] Can't find factory for 'fastcoref' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, entity_ruler, tagger, morphologizer, ner, beam_ner, senter, sentencizer, spancat, spancat_singlelabel, span_finder, future_entity_ruler, span_ruler, textcat, textcat_multilabel, en.lemmatizer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 2\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization Took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m, in \u001b[0;36mParser.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;241m=\u001b[39m \u001b[43mTools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecies \u001b[38;5;241m=\u001b[39m Species(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpossession \u001b[38;5;241m=\u001b[39m Possession(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools)\n",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m, in \u001b[0;36mTools.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtaxonerd \u001b[38;5;241m=\u001b[39m TaxoNERD(prefer_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtaxonerd\u001b[38;5;241m.\u001b[39mload(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_ner_eco_biobert\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfastcoref\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp\u001b[38;5;241m.\u001b[39mpipe_names)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\spacy\\language.py:821\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    817\u001b[0m     pipe_component, factory_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_pipe_from_source(\n\u001b[0;32m    818\u001b[0m         factory_name, source, name\u001b[38;5;241m=\u001b[39mname\n\u001b[0;32m    819\u001b[0m     )\n\u001b[0;32m    820\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 821\u001b[0m     pipe_component \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_pipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfactory_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    828\u001b[0m pipe_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pipe_index(before, after, first, last)\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_meta[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_factory_meta(factory_name)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\spacy\\language.py:690\u001b[0m, in \u001b[0;36mLanguage.create_pipe\u001b[1;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_factory(factory_name):\n\u001b[0;32m    683\u001b[0m     err \u001b[38;5;241m=\u001b[39m Errors\u001b[38;5;241m.\u001b[39mE002\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    684\u001b[0m         name\u001b[38;5;241m=\u001b[39mfactory_name,\n\u001b[0;32m    685\u001b[0m         opts\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactory_names),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    688\u001b[0m         lang_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang,\n\u001b[0;32m    689\u001b[0m     )\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[0;32m    691\u001b[0m pipe_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_factory_meta(factory_name)\n\u001b[0;32m    692\u001b[0m \u001b[38;5;66;03m# This is unideal, but the alternative would mean you always need to\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;66;03m# specify the full config settings, which is not really viable.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: [E002] Can't find factory for 'fastcoref' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, entity_ruler, tagger, morphologizer, ner, beam_ner, senter, sentencizer, spancat, spancat_singlelabel, span_finder, future_entity_ruler, span_ruler, textcat, textcat_multilabel, en.lemmatizer"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "parser = Parser()\n",
    "t1 = time.time()\n",
    "print(f\"Initialization Took {t1 - t0}s\")\n",
    "\n",
    "for doc in parser.tools.nlp.pipe([\"\"], n_process=2):    \n",
    "    # Update\n",
    "    t0 = time.time()\n",
    "    parser.update(doc)\n",
    "    t1 = time.time()\n",
    "    print(f\"Update Took {t1 - t0}s\")\n",
    "    \n",
    "    # Parse\n",
    "    t0 = time.time()\n",
    "    units = parser.parse()\n",
    "    t1 = time.time()\n",
    "    print(f\"Parsing Took {t1 - t0}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
