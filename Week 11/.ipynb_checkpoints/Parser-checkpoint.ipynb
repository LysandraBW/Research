{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbad3bc0-44ed-4f26-a445-ad4a2e027b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import textacy\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from fastcoref import FCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a08af5-045d-4b57-8c9f-a543f5115832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960be1f2-9233-459d-8217-1d9848776472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main:\n",
    "    def __init__(self, *, text=None):\n",
    "        self.lg_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_trf\")        \n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "        self.fcoref = FCoref(enable_progress_bar=False)\n",
    "        self.sp_doc = None\n",
    "        self.tn_doc = None\n",
    "        self.tk_map = None\n",
    "        if text:\n",
    "            self.update(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        cleaned_text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "        cleaned_text = re.sub(\"\\s+\", \" \", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"\\s+([?.!,])\", r\"\\1\", cleaned_text)\n",
    "        return cleaned_text\n",
    "        \n",
    "    def update(self, text):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        self.tk_map = self.load_token_map()\n",
    "\n",
    "    def load_token_map(self):\n",
    "        tk_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            tk_map[token.idx] = token.i\n",
    "        return tk_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6711e0-ee21-47d0-ade1-e15515725ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class References:\n",
    "    def __init__(self, main, texts=None):\n",
    "        self.main = main\n",
    "        self.predictions = None\n",
    "        self.cluster_map = None\n",
    "        if texts:\n",
    "            self.update(texts)\n",
    "\n",
    "    def update(self, texts):\n",
    "        if not self.main.sp_doc:\n",
    "            return\n",
    "        self.predictions = self.main.fcoref.predict(texts=texts)\n",
    "        self.cluster_map = self.load_cluster_map(self.predictions)\n",
    "        \n",
    "    def load_cluster_map(self, predictions):\n",
    "        cluster_map = {}\n",
    "        for prediction in predictions:\n",
    "            clusters = prediction.get_clusters(as_strings=False)\n",
    "            for cluster in clusters:\n",
    "                # Converting Spans to Tokens\n",
    "                token_cluster = []\n",
    "                for span in cluster:\n",
    "                    if span[0] not in self.main.tk_map:\n",
    "                        raise Exception(\"Invalid Token\")\n",
    "                    index = self.main.tk_map[span[0]]\n",
    "                    token_cluster.append(self.main.sp_doc[index])\n",
    "                # Mapping\n",
    "                for token in token_cluster:\n",
    "                    cluster_map[token.i] = list(filter(lambda t: t != token, token_cluster))\n",
    "        return cluster_map\n",
    "            \n",
    "    def get_references(self, tokens):\n",
    "        refs = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.cluster_map:\n",
    "                refs += self.cluster_map[index]\n",
    "        return refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d07f01e-584a-453c-8dca-153d5c6556cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Possession:\n",
    "    # There's no definite names for these patterns as I do not know what\n",
    "    # to call them. These patterns are used to extract possessive\n",
    "    # relationships from a sentence. I also could not find better names for\n",
    "    # the two variables below.\n",
    "    OWNER = \"owner\"\n",
    "    OWNED = \"owned\"\n",
    "    \n",
    "    patterns = {\n",
    "        \"Pattern1\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": OWNED,\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"poss\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern2\": [\n",
    "             {\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": OWNED,\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": \"adp\",\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"prep\",\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"ADP\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"adp\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"pobj\",\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern3\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": \"verb\",\n",
    "                \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"nsubj\",\n",
    "                    \"POS\": {\"IN\": [\"PRON\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"dobj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern4\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": \"verb\",\n",
    "                \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"nsubj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": \"adp\",\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"prep\",\n",
    "                    \"POS\": {\"IN\": [\"ADP\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"adp\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"pobj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.matcher = DependencyMatcher(self.main.sp_nlp.vocab)\n",
    "        for pattern_id, pattern in Possession.patterns.items():\n",
    "            self.matcher.add(pattern_id, [pattern])\n",
    "        self.owner_map = None\n",
    "        self.owned_map = None\n",
    "        self.update()\n",
    "    \n",
    "    def update(self):\n",
    "        if not self.main.sp_doc:\n",
    "            return\n",
    "        matches = self.matcher(self.main.sp_doc)\n",
    "        owner_map, owned_map = self.load_ownership_maps(matches)\n",
    "        self.owner_map = owner_map # Maps Owner to Owned\n",
    "        self.owned_map = owned_map # Maps Owned to Owner\n",
    "        \n",
    "    def load_ownership_maps(self, matches):\n",
    "        owner_map = {}\n",
    "        owned_map = {}\n",
    "\n",
    "        for match_id, token_ids in matches:\n",
    "            pattern_id = self.main.sp_nlp.vocab.strings[match_id]\n",
    "            # print(pattern_id)\n",
    "            owner = None\n",
    "            owned = None\n",
    "            for i in range(len(token_ids)):\n",
    "                right_id = Possession.patterns[pattern_id][i][\"RIGHT_ID\"]\n",
    "                if right_id == Possession.OWNER:\n",
    "                    owner = self.main.sp_doc[token_ids[i]]\n",
    "                if right_id == Possession.OWNED:\n",
    "                    owned = self.main.sp_doc[token_ids[i]]\n",
    "\n",
    "            # Owner to Owned\n",
    "            if owner.i not in owner_map:\n",
    "                owner_map[owner.i] = []\n",
    "            owner_map[owner.i].append(owned)\n",
    "\n",
    "            # Owned to Owner\n",
    "            if owned.i not in owned_map:\n",
    "                owned_map[owned.i] = []\n",
    "            owned_map[owned.i].append(owner)\n",
    "            \n",
    "        return (owner_map, owned_map)\n",
    "\n",
    "    def get_owner(self, tokens):\n",
    "        owners = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.owned_map:\n",
    "                owners += self.owned_map[index]\n",
    "        return owners\n",
    "\n",
    "    def get_owned(self, tokens):\n",
    "        owned = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.owner_map:\n",
    "                owned += self.owner_map[index]\n",
    "        return owned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b3361d-db39-43bb-bc44-aa4edb4d97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.species_indices = None\n",
    "        self.update()\n",
    "\n",
    "    def update(self):\n",
    "        if not self.main.sp_doc:\n",
    "            return\n",
    "        self.species_indices = self.load_species_indices()\n",
    "        \n",
    "    def load_species_indices(self):\n",
    "        indices = []\n",
    "\n",
    "        # Try TaxoNERD\n",
    "        for species_span in self.main.tn_doc.ents:\n",
    "            for species in species_span:\n",
    "                if species.idx not in self.main.tk_map:\n",
    "                    raise Exception(\"Invalid Token\")\n",
    "                index = self.main.tk_map[species.idx]\n",
    "                if index in indices:\n",
    "                    continue\n",
    "                indices.append(index)\n",
    "\n",
    "        # Try API Call\n",
    "        lowered_text = self.main.sp_doc.text.lower()\n",
    "        for token in self.main.sp_doc:\n",
    "            if token.pos_ not in [\"NOUN\", \"PROPN\"] or token.i in indices:\n",
    "                continue\n",
    "            try:\n",
    "                results = requests.get(f\"https://api.inaturalist.org/v1/search?q={token.lemma_}&sources=taxa&include_taxon_ancestors=false\")\n",
    "                results = results.json()\n",
    "                results = results[\"results\"]\n",
    "                for result in results:\n",
    "                    if \"record\" not in result or \"name\" not in result[\"record\"]:\n",
    "                        continue\n",
    "                    if lowered_text.find(result[\"record\"][\"name\"].lower()) == -1:\n",
    "                        continue\n",
    "                    indices.append(token.i)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "        return indices\n",
    "\n",
    "    def is_species(self, token):\n",
    "        index = token.i\n",
    "        return index in self.species_indices\n",
    "        \n",
    "    def has_species(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.species_indices:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95190af-07ec-46f0-ad6d-6b0b784408a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:    \n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        # Unit\n",
    "        self.unit_literals = {\"unit\", \"%\", \"percent\"}\n",
    "        self.unit_keywords = [self.main.lg_nlp(literal) for literal in self.unit_literals]\n",
    "        self.unit_indices = []\n",
    "        # Quantitative\n",
    "        self.quantitative_literals = {\"tenfold\", \"half\", \"double\", \"triple\", \"quadruple\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"higher\", \"lower\"}\n",
    "        self.quantitative_keywords = [self.main.lg_nlp(literal) for literal in self.quantitative_literals]\n",
    "        self.quantitative_indices = []\n",
    "        # Change\n",
    "        self.change_literals = {\"increase\", \"decrease\", \"change\", \"weaken\", \"shift\", \"cause\", \"produce\"}\n",
    "        self.change_keywords = [self.main.lg_nlp(literal) for literal in self.change_literals]\n",
    "        self.change_indices = []\n",
    "        # Cause\n",
    "        self.cause_literals = {\"thus\"}\n",
    "        self.cause_keywords = [self.main.lg_nlp(literal) for literal in self.cause_literals]\n",
    "        self.cause_indices = []\n",
    "        # Trait\n",
    "        self.trait_literals = {\"diet\"}\n",
    "        self.trait_keywords = [self.main.lg_nlp(literal) for literal in self.trait_literals]\n",
    "        self.trait_indices = []\n",
    "        # Update\n",
    "        self.update()\n",
    "        \n",
    "    def update(self):\n",
    "        if not self.main.sp_doc:\n",
    "            return\n",
    "        self.unit_indices = self.load_unit_indices()\n",
    "        self.quantitative_indices = self.load_quantitative_indices()\n",
    "        self.change_indices = self.load_change_indices()\n",
    "        self.cause_indices = self.load_cause_indices()\n",
    "        self.trait_indices = self.load_trait_indices()\n",
    "        return\n",
    "\n",
    "    def is_unit(self, token):\n",
    "        return token.i in self.unit_indices\n",
    "\n",
    "    def has_unit(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.unit_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_unit_indices(self):\n",
    "        indices = []\n",
    "        for token in self.main.sp_doc:\n",
    "            if token.pos_ not in [\"NOUN\"]:\n",
    "                continue\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.unit_literals:\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.main.lg_nlp(token.lemma_)\n",
    "            for keyword in self.unit_keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                # print(f\"{lemma} and {keyword} Similarity: {similarity}\")\n",
    "                if similarity > 0.7:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def is_change(self, token):\n",
    "        return token.i in self.change_indices\n",
    "\n",
    "    def has_change(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.change_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_change_indices(self):\n",
    "        indices = []\n",
    "        for token in self.main.sp_doc:\n",
    "            if token.pos_ not in [\"NOUN\", \"VERB\", \"ADP\"]:\n",
    "                continue\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.change_literals:\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            # Handling Case: \"from\" ... \"to\" ...\n",
    "            if token.lower_ == \"to\" and token.head and token.head.i in indices:\n",
    "                for child in token.head.children:\n",
    "                    if child.lower_ == \"from\":\n",
    "                        indices.append(token.i)\n",
    "                        break\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.main.lg_nlp(token.lemma_)\n",
    "            for keyword in self.change_keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                # print(f\"{lemma} and {keyword} Similarity: {similarity}\")\n",
    "                if similarity > 0.7:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def is_quantitative(self, token):\n",
    "        return token.i in self.quantitative_indices\n",
    "\n",
    "    def has_quantitative(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.quantitative_indices:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def load_quantitative_indices(self):\n",
    "        indices = []\n",
    "        for token in self.main.sp_doc:\n",
    "            if token.pos_ not in [\"NOUN\", \"NUM\"]:\n",
    "                continue\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.quantitative_literals:\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.main.lg_nlp(token.lemma_)\n",
    "            for keyword in self.quantitative_keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                # print(f\"{lemma} and {keyword} Similarity: {similarity}\")\n",
    "                if similarity > 0.7:\n",
    "                    # We need to make sure that any noun that the \n",
    "                    # quantitative token modifies (the unit) is a unit.\n",
    "                    if token.head and self.is_unit(token.head):\n",
    "                        indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def is_cause(self, token):\n",
    "        return token.i in self.cause_indices\n",
    "\n",
    "    def has_cause(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.cause_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_cause_indices(self):\n",
    "        indices = []\n",
    "        for token in self.main.sp_doc:\n",
    "            if token.pos_ not in [\"ADP\", \"SCONJ\", \"PART\", \"PRON\", \"ADV\"]:\n",
    "                continue\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.cause_literals:\n",
    "                indices.append(token.i)\n",
    "            elif token.pos_ == \"SCONJ\":\n",
    "                # print(\"It's a SCONJ\")\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            elif token.pos_ == \"PART\":\n",
    "                if token.head and token.head.pos_ == \"VERB\":\n",
    "                    # print(\"It's a PART + VERB\")\n",
    "                    indices.append(token.i)\n",
    "                    continue\n",
    "            elif token.pos_ == \"ADP\":\n",
    "                if token.lower_ == \"due\" and token.i + 1 < len(self.main.sp_doc) and self.main.sp_doc[token.i + 1].lower_ == \"to\":\n",
    "                    # print(\"It's a DUE TO\")\n",
    "                    indices.append(token.i)\n",
    "                    continue\n",
    "                elif token.lower_ == \"in\" and token.i + 1 < len(self.main.sp_doc) and self.main.sp_doc[token.i + 1].lower_ == \"response\":\n",
    "                    # print(\"It's a DUE TO\")\n",
    "                    indices.append(token.i)\n",
    "                    continue\n",
    "                elif token.head:\n",
    "                    if token.head.pos_ == \"AUX\":\n",
    "                        # print(\"The head is an AUX\")\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "                    elif token.head.pos_ == \"VERB\" and token.head.i < token.i and self.is_change(token.head):\n",
    "                        # print(\"The head is a change-VERB\")\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "                    elif token.lower_ != \"to\" and \"AUX\" in [child.pos_ for child in list(filter(lambda t: t.i < token.i,token.head.children))]:\n",
    "                        # print(\"There's an AUX in the head's children\")\n",
    "                        # for child in token.head.children:\n",
    "                            # print(f\"\\t\\t{child}, {child.pos_}\")\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "                elif token.ancestors:\n",
    "                    # print(\"There's an AUX ancestor\")\n",
    "                    if \"AUX\" in [ancestor.pos_ for ancestor in token.ancestors]:\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "            elif token.pos_ == \"PRON\":\n",
    "                if token.head and token.head.pos_ == \"VERB\" and self.is_change(token.head):\n",
    "                    indices.append(token.i)\n",
    "                    continue\n",
    "        return indices\n",
    "\n",
    "    def is_trait(self, token):\n",
    "        return token.i in self.trait_indices\n",
    "\n",
    "    def has_trait(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.trait_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_trait_indices(self):\n",
    "        indices = []\n",
    "        for token in self.main.sp_doc:\n",
    "            if token.pos_ not in [\"NOUN\"]:\n",
    "                continue\n",
    "\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.trait_literals:\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "\n",
    "            # Comparing Similarity\n",
    "            lemma = self.main.lg_nlp(token.lemma_)\n",
    "            for keyword in self.trait_keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                # print(f\"{lemma} and {keyword} Similarity: {similarity}\")\n",
    "                if similarity > 0.7:\n",
    "                    indices.append(token.i)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a579e4bc-2b66-405a-b8da-e94805129c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.chunk_map = None\n",
    "\n",
    "    def update(self):\n",
    "        if not self.main.sp_doc:\n",
    "            return\n",
    "        self.chunk_map = self.load_chunk_map()\n",
    "        return\n",
    "\n",
    "    def load_chunk_map(self):\n",
    "        chunk_map = {}\n",
    "        for chunk in self.main.sp_doc.noun_chunks:\n",
    "            for token in chunk:\n",
    "                chunk_map[token.i] = list(filter(lambda t: t.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"], [token for token in chunk]))\n",
    "        return chunk_map\n",
    "\n",
    "    def get_chunk(self, token):\n",
    "        if not self.chunk_map or token.i not in self.chunk_map:\n",
    "            return None\n",
    "        return self.chunk_map[token.i]\n",
    "\n",
    "    def get_list(self, token, max_i=None):\n",
    "        max_i = len(self.main.sp_doc) - 1 if not max_i else max_i\n",
    "    \n",
    "        if not token or token.i >= max_i:\n",
    "            # print(-1)\n",
    "            return []\n",
    "        if token.nbor().text != \",\":\n",
    "            # print(0)\n",
    "            return []\n",
    "\n",
    "        and_or_found = False\n",
    "        conjunction_found = False\n",
    "        offset_at_last_comma = -1\n",
    "        number_commas_found = 0\n",
    "        \n",
    "        offset = 1\n",
    "        while token.i + offset <= max_i:\n",
    "            # print(f\"Offset: {offset}\")\n",
    "            # print(f\"Offset At Last Comma: {offset_at_last_comma}\")\n",
    "            nbor = token.nbor(offset)\n",
    "            if nbor.text == \",\":\n",
    "                # STOP\n",
    "                if nbor.head.i > nbor.i:\n",
    "                    # print(f\"Offset: {offset}\")\n",
    "                    # print(1, nbor, nbor.head)\n",
    "                    break\n",
    "                offset_at_last_comma = offset\n",
    "                number_commas_found += 1\n",
    "                offset += 1\n",
    "                continue\n",
    "            if nbor.pos_ in [\"ADJ\", \"NOUN\", \"PROPN\", \"PRON\"] or nbor.lower_ in [\"-\", \"/\"]:\n",
    "                # print(2, nbor)\n",
    "                offset += 1\n",
    "                continue\n",
    "            if nbor.pos_ == \"CCONJ\" and nbor.lower_ not in [\"but\"]:\n",
    "                # print(3, nbor)\n",
    "                if nbor.lower_ in [\"and\", \"or\"]:\n",
    "                    # print(4, nbor)\n",
    "                    if and_or_found:\n",
    "                        # print(5, nbor)\n",
    "                        break\n",
    "                    elif number_commas_found == 1 and not conjunction_found and offset_at_last_comma == offset + 1:\n",
    "                        # print(6, nbor)\n",
    "                        offset = offset_at_last_comma - 1\n",
    "                        break\n",
    "                    and_or_found = True\n",
    "                conjunction_found = True\n",
    "                offset += 1\n",
    "                continue\n",
    "            break\n",
    "        # print(7)\n",
    "        if offset <= 2:\n",
    "            return []\n",
    "        # print(self.main.sp_doc[token.i:token.i+offset])\n",
    "        return list(filter(lambda t: t.pos_ not in [\"PUNCT\"], [t for t in self.main.sp_doc[token.i:token.i+offset]]))\n",
    "\n",
    "    def get_conjunct(self, token):\n",
    "        has_conj = False\n",
    "        for child in token.children:\n",
    "            if child.pos_ == \"CCONJ\" and child.lower_ in [\"and\", \"or\"]:\n",
    "                has_conj = True\n",
    "                break\n",
    "        if not has_conj:\n",
    "            return []\n",
    "        for child in token.children:\n",
    "            if child.dep_ == \"conj\":\n",
    "                if child.i in self.chunk_map:\n",
    "                    return self.chunk_map[child.i]\n",
    "                return [child]\n",
    "        return []\n",
    "\n",
    "    def get_conjunct_and_list(self, token, max_i=None):\n",
    "        trail = self.get_list(token, max_i)\n",
    "        conjuct = self.get_conjunct(token)\n",
    "        if conjuct:\n",
    "            trail += conjuct\n",
    "        return trail\n",
    "\n",
    "    def get_context(self, token, max_i=None):\n",
    "        context = self.get_list(token, max_i)\n",
    "        conjuct = self.get_conjunct(token)\n",
    "        if conjuct:\n",
    "            context += conjuct\n",
    "        chunk = self.get_chunk(token)\n",
    "        if chunk:\n",
    "            context += list(filter(lambda t: t.i != token.i, chunk))\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0d752ef-11de-4c19-a0cc-b5c32a199dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit:\n",
    "    def __init__(self, *, species=None, trait=None, change=None, cause=None):\n",
    "        self.species = species\n",
    "        self.trait = trait\n",
    "        self.cause = cause\n",
    "        self.change = change\n",
    "\n",
    "    def empty(self):\n",
    "        if not self.species and not self.trait and not self.cause and not self.change:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def not_empty(self):\n",
    "        return not self.empty()\n",
    "\n",
    "    def can_merge(self, unit):\n",
    "        # Two units can merge if there's no\n",
    "        # overlap.\n",
    "        if self.species and unit.species:\n",
    "            return False\n",
    "        if self.trait and unit.trait:\n",
    "            return False\n",
    "        if self.cause and unit.cause:\n",
    "            return False\n",
    "        if self.change and unit.change:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def merge(self, unit):\n",
    "        # We take the parts that the other unit has; assuming that\n",
    "        # there's no overlap, there's no loss of information. This\n",
    "        # is likely not foolproof.\n",
    "        if unit.species:\n",
    "            self.species = unit.species\n",
    "        if unit.trait:\n",
    "            self.trait = unit.trait\n",
    "        if unit.cause:\n",
    "            self.cause = unit.cause\n",
    "        if unit.change:\n",
    "            self.change = unit.change\n",
    "\n",
    "    def get_score(self):\n",
    "        score = 0\n",
    "        if self.species:\n",
    "            score += 1\n",
    "        if self.trait:\n",
    "            score += 1\n",
    "        if self.cause:\n",
    "            score += 1\n",
    "        if self.change:\n",
    "            score += 1    \n",
    "        return score\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Species: {self.species}, Trait: {self.trait}, Cause: ({self.cause}), Change: {self.change}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc5cc71f-45c1-4cd7-b446-1e25b4468ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self, *, text=None, main=None, species=None, possession=None, references=None, keywords=None, context=None):\n",
    "        self.main = main if main else Main(text=text)\n",
    "        self.species = species if species else Species(self.main)\n",
    "        self.context = context if context else Context(self.main)\n",
    "        self.keywords = keywords if keywords else Keywords(self.main)\n",
    "        self.possession = possession if possession else Possession(self.main)\n",
    "        self.references = references if references else References(self.main, texts=list(filter(lambda t: t is not None,[text])))\n",
    "            \n",
    "    def update(self, text):\n",
    "        self.main.update(text)\n",
    "        self.species.update()\n",
    "        self.context.update()\n",
    "        self.keywords.update()\n",
    "        self.possession.update()\n",
    "        self.references.update(texts=[text])\n",
    "\n",
    "    def parse_species(self, tokens, species, used):\n",
    "        for token in tokens:\n",
    "            if token in used:\n",
    "                continue\n",
    "            if self.species.is_species(token):\n",
    "                if token.head:\n",
    "                    if token.head.pos_ in [\"SCONJ\", \"ADP\"] and token.head.lower_ != \"of\":\n",
    "                        continue\n",
    "                ancestors = [t for t in token.ancestors]\n",
    "                if self.keywords.has_cause(ancestors):\n",
    "                    # print(\"!!!\")\n",
    "                    continue\n",
    "                full_species = self.context.get_context(token)\n",
    "                used.update([token, *full_species])\n",
    "                species.update([token, *full_species])\n",
    "                break\n",
    "\n",
    "    def parse_change(self, tokens, change, used):\n",
    "        for token in tokens:\n",
    "            if token in used:\n",
    "                continue\n",
    "            if self.keywords.is_change(token):\n",
    "                used.add(token)\n",
    "                change.add(token)\n",
    "                # I only want one word that represents\n",
    "                # the change for simplicity\n",
    "                break\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in used:\n",
    "                continue\n",
    "            if token.pos_ == \"ADP\" and token.lower_ != \"of\":\n",
    "                end_i = token.i + 1\n",
    "                contenders = []\n",
    "                while end_i <= tokens[-1].i and self.main.sp_doc[end_i] not in used:\n",
    "                    if self.main.sp_doc[end_i].pos_ not in [\"NUM\", \"SYM\", \"NOUN\", \"ADP\", \"DET\"]:\n",
    "                        break\n",
    "                    contenders.append(self.main.sp_doc[end_i])\n",
    "                    end_i += 1\n",
    "                if not self.keywords.has_quantitative(contenders) and not self.keywords.has_change(contenders):\n",
    "                    continue\n",
    "                used.update([token, *contenders])\n",
    "                change.update([token, *contenders])\n",
    "                break\n",
    "\n",
    "    def parse_trait(self, tokens, trait, used, species, change):\n",
    "        if species:\n",
    "            contenders = list(filter(lambda t: t.i >= tokens[0].i and t.i <= tokens[-1].i and t not in used, self.possession.get_owned(species)))\n",
    "            for token in contenders:\n",
    "                for ancestor in token.ancestors:\n",
    "                    if ancestor in used:\n",
    "                        continue\n",
    "                    if self.keywords.is_change(ancestor):\n",
    "                        used.add(token)\n",
    "                        trait.add(token)\n",
    "                        break    \n",
    "        elif change:\n",
    "            # The trait could be listed before the change (i.e. \"diet shifts from ...\")\n",
    "            prev_i = list(change)[0].i - 1\n",
    "            prev_token = None if prev_i < 0 else self.main.sp_doc[prev_i]\n",
    "            if prev_token and prev_token not in used and prev_token.pos_ == \"NOUN\":\n",
    "                if self.keywords.is_trait(prev_token):\n",
    "                    used.add(prev_token)\n",
    "                    trait.add(prev_token)\n",
    "            else:\n",
    "                # Look for \"in\" (i.e. \"increase in ...\")\n",
    "                for ch in change:\n",
    "                    if ch in used or not self.keywords.is_change(ch):\n",
    "                        continue\n",
    "                    for child in ch.children:\n",
    "                        if child in used:\n",
    "                            continue\n",
    "                        if child.pos_ == \"ADP\" and child.children:\n",
    "                            children = list(child.children)\n",
    "                            if children[0] not in used:\n",
    "                                used.add(children[0])\n",
    "                                trait.add(children[0])\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                if token in used:\n",
    "                    continue\n",
    "                if token.pos_ in [\"NOUN\"] and token.head and self.keywords.is_change(token.head):\n",
    "                    used.add(token)\n",
    "                    trait.add(token)                        \n",
    "                    break\n",
    "\n",
    "        if trait:\n",
    "            full_trait = self.context.get_context(list(trait)[0])\n",
    "            for t in full_trait:\n",
    "                if t not in used and t.pos_ == \"NOUN\":\n",
    "                    used.add(t)\n",
    "                    trait.add(t)\n",
    "\n",
    "    def parse_species_by_trait(self, tokens, species, used, trait):\n",
    "        species_contenders = list(filter(lambda t: t.i >= tokens[0].i and t.i <= tokens[-1].i and t not in used, self.possession.get_owner(trait)))\n",
    "        for sp in species_contenders:\n",
    "            if self.species.is_species(sp):\n",
    "                full_sp = self.context.get_context(sp)\n",
    "                used.update(full_sp)\n",
    "                species.update(full_sp)\n",
    "\n",
    "    def parse_cause(self, tokens, cause, used):\n",
    "        for token in tokens:\n",
    "            if token in used:\n",
    "                continue\n",
    "            if token.pos_ in [\"SCONJ\", \"ADV\"] and self.keywords.is_cause(token):\n",
    "                end_i = token.i + 1\n",
    "                # Expanding (Doesn't Handle Conjunctions)\n",
    "                while end_i <= tokens[-1].i and self.main.sp_doc[end_i] not in used:\n",
    "                    if self.main.sp_doc[end_i].pos_ not in [\"ADP\", \"DET\", \"NOUN\", \"PROPN\", \"AUX\", \"ADV\", \"PRON\", \"ADJ\"]:\n",
    "                        break\n",
    "                    used.add(self.main.sp_doc[end_i])\n",
    "                    cause.add(self.main.sp_doc[end_i])\n",
    "                    end_i += 1\n",
    "                end_i -= 1\n",
    "                # Expanding (Handles Conjunctions)\n",
    "                if self.main.sp_doc[end_i].pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                    context = self.context.get_conjunct_and_list(self.main.sp_doc[end_i])\n",
    "                    # print(f\"Context: {self.main.sp_doc[end_i], context}\")\n",
    "                    for context_token in context:\n",
    "                        if context_token not in used:\n",
    "                            used.add(context_token)\n",
    "                            cause.add(context_token)\n",
    "                used.add(token)\n",
    "                cause.add(token)\n",
    "\n",
    "    def parse_cause_by_ADP(self, tokens, cause, used):\n",
    "        for token in tokens:\n",
    "            if token in used:\n",
    "                continue\n",
    "            if token.pos_ in [\"PRON\"] and self.keywords.is_cause(token):\n",
    "                used.add(token)\n",
    "            elif token.pos_ in [\"ADP\"] and self.keywords.is_cause(token):\n",
    "                end_i = token.i + 1\n",
    "                # Expanding\n",
    "                buffer = []\n",
    "                noun_found = False\n",
    "                while end_i <= tokens[-1].i and self.main.sp_doc[end_i] not in used:\n",
    "                    if self.main.sp_doc[end_i].pos_ not in [\"ADP\", \"DET\", \"NOUN\", \"PROPN\", \"AUX\", \"ADV\", \"PRON\", \"ADJ\"]:\n",
    "                        break\n",
    "                    if self.main.sp_doc[end_i].pos_ in [\"NOUN\", \"PROPN\", \"PRON\"]:\n",
    "                        noun_found = True\n",
    "                    buffer.append(self.main.sp_doc[end_i])\n",
    "                    end_i += 1\n",
    "                # Expanding (Handles Conjunctions)\n",
    "                end_i -= 1\n",
    "                if self.main.sp_doc[end_i].pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                    context = self.context.get_conjunct_and_list(self.main.sp_doc[end_i])\n",
    "                    for context_token in context:\n",
    "                        if context_token not in used:\n",
    "                            buffer.append(context_token)\n",
    "                \n",
    "                if noun_found:\n",
    "                    used.update([token, *buffer])\n",
    "                    cause.update([token, *buffer])\n",
    "                \n",
    "    def parse_segment(self, l_i, r_i):\n",
    "        # print(f\"\\nPARSING SEGMENT\")\n",
    "        # print(f\"Text: {self.main.sp_doc[l_i:r_i+1].text}\")\n",
    "\n",
    "        used, cause, change, species, trait = set(), set(), set(), set(), set()\n",
    "        tokens = self.main.sp_doc[l_i:r_i+1]\n",
    "        \n",
    "        # Parse\n",
    "        self.parse_cause(tokens, cause, used)\n",
    "        # print(f\"Cause: {cause}\")\n",
    "        self.parse_change(tokens, change, used)\n",
    "        # print(f\"Change: {change}\")\n",
    "        self.parse_species(tokens, species, used)        \n",
    "        # print(f\"Species: {species}\")\n",
    "        self.parse_trait(tokens, trait, used, species, change)\n",
    "        # print(f\"Trait: {trait}\")\n",
    "        if not species:\n",
    "            self.parse_species_by_trait(tokens, species, used, trait)\n",
    "            # print(f\"Species: {species}\")\n",
    "        self.parse_cause_by_ADP(tokens, cause, used)\n",
    "        # print(f\"Cause: {cause}\")\n",
    "\n",
    "        unit = Unit(species=species, trait=trait, change=change, cause=cause)\n",
    "        return unit\n",
    "\n",
    "    def parse_sentence(self, l_i, r_i):\n",
    "        units = []\n",
    "        \n",
    "        # Recursive Split\n",
    "        # We're extracting the core information\n",
    "        # in the sentence into units.\n",
    "        def recursive_split(r_l_i, r_r_i):\n",
    "            nonlocal units\n",
    "            # Find Verb\n",
    "            # The verb is used to divide\n",
    "            # the \"parsing\" space, which\n",
    "            # makes the work simpler.\n",
    "            verb = None\n",
    "            for token in self.main.sp_doc[r_l_i:r_r_i+1]:\n",
    "                if token.pos_ == \"VERB\":\n",
    "                    verb = token\n",
    "                    break\n",
    "    \n",
    "            # Base Case\n",
    "            # If there is no verb, we have\n",
    "            # reached the simplest case and\n",
    "            # can extract information.\n",
    "            if verb == None:\n",
    "                units.append(self.parse_segment(r_l_i, r_r_i))\n",
    "            else:\n",
    "                # print(f\"Verb: {verb}\")\n",
    "                recursive_split(r_l_i, verb.i - 1)\n",
    "                units.append(verb)\n",
    "                recursive_split(verb.i + 1, r_r_i)\n",
    "            return\n",
    "        recursive_split(l_i, r_i)\n",
    "\n",
    "        # Recursive Merge\n",
    "        # We are putting the pieces back together,\n",
    "        # so that we, the computer, can understand\n",
    "        # what's going on.\n",
    "        def recursive_merge():\n",
    "            nonlocal units\n",
    "            if len(units) < 3:\n",
    "                return\n",
    "            \n",
    "            l_unit = units[0]\n",
    "            verb = units[1]\n",
    "            r_unit = units[2]\n",
    "            verb_is_change = self.keywords.is_change(verb)\n",
    "            \n",
    "            if l_unit.empty() and r_unit.empty():\n",
    "                m_unit = Unit()  \n",
    "            elif l_unit.not_empty() and r_unit.empty():\n",
    "                # print(1)\n",
    "                if verb_is_change:\n",
    "                    l_unit.change.add(verb)\n",
    "                m_unit = l_unit\n",
    "            elif r_unit.not_empty() and l_unit.empty():\n",
    "                # print(2)\n",
    "                if verb_is_change:\n",
    "                    r_unit.change.add(verb)\n",
    "                m_unit = r_unit    \n",
    "            elif l_unit.can_merge(r_unit):\n",
    "                # print(3)\n",
    "                l_unit.merge(r_unit)\n",
    "                if verb_is_change:\n",
    "                    l_unit.change.add(verb)\n",
    "                m_unit = l_unit\n",
    "            elif verb_is_change:\n",
    "                # print(4)\n",
    "                r_unit.cause = l_unit\n",
    "                if verb_is_change:\n",
    "                    r_unit.change.add(verb)\n",
    "                m_unit = r_unit\n",
    "            else:\n",
    "                # print(5)\n",
    "                if l_unit.get_score() >= r_unit.get_score():\n",
    "                    m_unit = l_unit\n",
    "                else:\n",
    "                    m_unit = r_unit\n",
    "            units = [m_unit] + units[3:]\n",
    "            recursive_merge()\n",
    "            return\n",
    "        recursive_merge()\n",
    "\n",
    "        assert len(units) == 1\n",
    "        return units[0]\n",
    "\n",
    "    def parse(self):\n",
    "        units = []\n",
    "        for sent in self.main.sp_doc.sents:\n",
    "            # print(f\"Sentence: {self.main.sp_doc[sent.start:sent.end].text}\")\n",
    "            unit = self.parse_sentence(sent.start, sent.end - 1)\n",
    "            # print(unit)\n",
    "            units.append(unit)\n",
    "            # print()\n",
    "        return units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9903d921-8cd0-4038-bfef-4d711540d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "text00 = Main.clean_text(\"Acridoidea exhibited significant diet shifts from grass to herbs (Kruskal-Wallis test, P 0.01, df 3) when they were in the presence of the comparatively sedentary species (the smaller Pisaurina and the larger Hogna) compared to controls without spiders (Fig. 2).\")\n",
    "text01 = Main.clean_text(\"Our results show that phototrophs can indirectly decrease the population density of heterotrophic bacteria by modification of the nature of bacterial interactions with predators.\")\n",
    "text02 = Main.clean_text(\"Our results show that Selachii can indirectly decrease the population density of Selachimorpha by modification of the nature of bacterial interactions with predators.\")\n",
    "text03 = Main.clean_text(\"All predators inflicted significant mortality on the prey at each prey density compared to the predator-free control for that density\")\n",
    "text04 = Main.clean_text(\"Our results show that an increase in sediment organic matter content is associated to a decline in the abundance of Loripes lucinalis (lucinid bivalve) in the Cymodocea nodosa meadows studied, which potentially may weaken the mutualism between the two species.\")\n",
    "text05 = Main.clean_text(\"The abundance of lucinids showed a negative correlation with the organic matter content in vegetated sediments (Fig. 3a), but showed no correlation in bare ones (Fig. 3b).\")\n",
    "text06 = Main.clean_text(\"The MANOVA on the cattle tank experiment showed that the presence of Tramea, nonlethal Anax, and large bullfrog tadpoles all had significant effects on both small tadpole species (Table 1).\")\n",
    "text07 = Main.clean_text(\"Thus the presence of predators, both nonlethal Anax and lethal Tramea, modified the tank environment in a way that facilitated invasion by Nematocera, but only in the absence of large bullfrogs.\")\n",
    "text08 = Main.clean_text(\"We hypothesized that the presence of Anax would decrease foraging activity of small tadpoles, which in turn would decrease predation by Tramea on the small tadpoles.\")\n",
    "text09 = Main.clean_text('''Only a fraction of the individuals in a given prey population are likely to be killed and consumed by predators. In contrast, nearly all individuals experience the chronic effects of predation risk. When threatened by predators, prey adopt defensive tactics whole costs can lead to reduced growth, maturation rates, survivorship, fecundity, or population density. This nonconsumptive impact of predation risk on prey is known as a \"trait-mediated interaction\" (TMI) because it results from changes in prey traits such as behavior or physiology. Ecological theory suggests that the strength of TMI effects will reflect a balance between the conflicting demands of reproduction vs. predator avoidance. Competitor density and resource availability are expected to alter the balance between these conflicting forces. We conducted a meta-analysis of experimental studies that measured TMI effect size while varying competitor and/or resource density. The threat of predation had an overall negative effect on prey performance, but the strength of this effect varied with the level of competition. High competition exacerbated the negative effect of intimidation on prey density but moderated the negative effect of intimidation on prey life history and growth. We discuss these results in light of previously published theoretical expectations. Our results highlight the variable and context-dependent nature of interspecific interactions.''')\n",
    "text10 = Main.clean_text(\"Current theory on trophic interactions in food webs assumes that ecologically similar species can be treated collectively as a single functional unit such as a guild or trophic level. This theory implies that all species within that unit transmit identical direct and indirect effects throughout the community. We evaluated this assumption by conducting experiments to compare the direct and indirect effects of three top-predator species, belonging to the same hunting spider guild, on the same species of grasshopper and on old-field grasses and herbs. Observations under field conditions revealed that each spider species exhibited different hunting behavior (i.e., sit-and-wait, sit-and-pursue, and active hunting) and occupied different locations within the vegetation canopy. These differences resulted in different direct effects on grasshopper prey. Grasshoppers demonstrated significant behavioral (diet) shifts in the presence of sit-and-wait and sit-and-pursue species but not when faced with actively hunting species. Grasshopper density was significantly reduced by spider species that occupied lower parts of the vegetation canopy (sit-and-pursue and actively hunting species), but it was not significantly reduced by the sit-and-wait spider species that occupied the upper parts of the canopy. These direct effects manifested themselves differently in the plant trophic level. The sit-and-wait spider caused indirect effects on plants by changing grasshopper foraging behavior (a trait-mediated effect). The sit-and-pursue spider caused indirect effects by reducing grasshopper density (density-mediated effects); the effects of changes in grasshopper behavior were thus not reflected in the plant trophic level. The actively hunting spiders had strictly density-mediated indirect effects on plants. The study offers mechanistic insight into how predator species within the same guild can have very different trophic effects in food webs. Thus classical modeling approaches that treat all predator species as a single functional unit may not adequately capture biologically relevant details that influence community dynamics.\")\n",
    "text11 = Main.clean_text(\"Diversity and plasticity are hallmarks of cells of the monocyte-macrophage lineage. In response to IFNs, Toll-like receptor engagement, or IL-4/IL-13 signaling, macrophages undergo M1 (classical) or M2 (alternative) activation, which represent extremes of a continuum in a universe of activation states. Progress has now been made in defining the signaling pathways, transcriptional networks, and epigenetic mechanisms underlying M1-M2 or M2-like polarized activation. Functional skewing of mononuclear phagocytes occurs in vivo under physiological conditions (e.g., ontogenesis and pregnancy) and in pathology (allergic and chronic inflammation, tissue repair, infection, and cancer). However, in selected preclinical and clinical conditions, coexistence of cells in different activation states and unique or mixed phenotypes have been observed, a reflection of dynamic changes and complex tissue-derived signals. The identification of mechanisms and molecules associated with macrophage plasticity and polarized activation provides a basis for macrophage-centered diagnostic and therapeutic strategies.\")\n",
    "text12 = Main.clean_text(\"This investigation examines the role of trait-mediated indirect interactions in a simple aquatic food web. We conducted the experiments in cattle watering tanks in order to establish whether competitive and predator-prey interactions between two species are affected by other species in the system; i.e., are pairwise interaction strengths affected by the background species assemblage? We examined the survival and growth response of small bullfrog (Rana catesbeiana) and small green frog (Rana clamitans) tadpoles in the presence and absence of a competitor (large bullfrogs), the lethal presence of the larval odonate predator Tramea lacerata,and the nonlethal (caged) presence of the larval odonate predators Anax junius and Anax longipes. We demonstrate that large bullfrog competitors and caged Anax affect traits (foraging activity level) of small bullfrog and small green frog tadpoles and that these changes in traits, in turn, affect interactions of the small tadpole species with each other and with the other species. In particular, the following four trait- mediated indirect interactions were evident: (1) Presence of large bullfrog competitors increased the predation rate of Trameaon small green frogs and small bullfrogs. (2) Presence of nonlethal Anax reduced the predation rate of Tramea on small green frogs. (3) Presence of nonlethal Anax increased the competitive advantage of bullfrogs over green frogs. (4) Presence of nonlethal Anax facilitated midge invasion of the experimental units. The pro- posed mechanisms (changes in small tadpole activity) involved in these trait-mediated indirect interactions were supported by observational data on tadpole activity and resource levels in the experimental units, and in laboratory experiments examining tadpole activity responses to predators. The occurrence of strong trait-mediated indirect interactions in this simple food web underscores the potential importance of such interactions in animal com- munities.\")\n",
    "text13 = Main.clean_text(\"Presence of large bullfrog competitors increased the predation rate of Trameaon small green frogs and small bullfrogs.\")\n",
    "text14 = Main.clean_text(\"Presence of nonlethal Anax reduced the predation rate of Tramea on small green frogs.\")\n",
    "text15 = Main.clean_text(\"Presence of nonlethal Anax increased the competitive advantage of bullfrogs over green frogs.\")\n",
    "text16 = Main.clean_text(\"Presence of nonlethal Anax facilitated midge invasion of the experimental units.\") # Not 100% sure about this one.\n",
    "text17 = \"Pea aphids (Acyrthosiphon pisum, Harris) have been shown to produce winged dispersal morphs in response to the presence of ladybirds or parasitoid natural enemies.\"\n",
    "text18 = \"The results presented here clearly demonstrate that the presence of both lacewing larvae and hoverfly larvae can induce Lucinida to produce a higher proportion of winged offspring.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa58f33-377b-4691-860d-d65ff56f67f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\thinc\\shims\\pytorch.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n",
      "05/05/2025 22:18:04 - INFO - \t missing_keys: []\n",
      "05/05/2025 22:18:04 - INFO - \t unexpected_keys: []\n",
      "05/05/2025 22:18:04 - INFO - \t mismatched_keys: []\n",
      "05/05/2025 22:18:04 - INFO - \t error_msgs: []\n",
      "05/05/2025 22:18:04 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    }
   ],
   "source": [
    "main = Main(text=None)\n",
    "possession = Possession(main=main)\n",
    "keywords = Keywords(main=main)\n",
    "species = Species(main=main)\n",
    "references = References(main=main, texts=[])\n",
    "context = Context(main=main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b54981fc-2d82-4ea6-942b-b37a15023624",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser(main=main, possession=possession, keywords=keywords, species=species, references=references, context=context, text=None)\n",
    "\n",
    "# for text in [text18]:\n",
    "#     # Update\n",
    "#     parser.update(text)\n",
    "#     # Parse\n",
    "#     units = parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70aada39-4c77-4877-b6be-6f25d4e1c9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889 889 1379\n"
     ]
    }
   ],
   "source": [
    "# PDF to Text: PyMuPDF\n",
    "# This looks like it may be good. The documentation\n",
    "# looks great. I think PyMuPDF will be my saving grace\n",
    "# for this portion of the project. I will be choosing\n",
    "# PyMuPDF.\n",
    "import pymupdf\n",
    "\n",
    "def pdf_to_text(url):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        f = pdf_bytes(url)\n",
    "        doc = pymupdf.open(stream=f)\n",
    "        for d in doc:\n",
    "            text += d.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "# OpenAlex: Finding Papers\n",
    "# Let's say we have a set or an array of keywords.\n",
    "# We can use OpenAlex to find a large number of papers\n",
    "# that, in some way, match those keywords. This is an\n",
    "# example of how it could work. Furthermore, Veronica\n",
    "# mentioned how there's other characteristics that you\n",
    "# may be looking for, like how far back you want to go\n",
    "# in searching for papers.\n",
    "from pyalex import Works\n",
    "\n",
    "# Later, I'll use these URLs to try out the PDF to text\n",
    "# tools. If there's any.\n",
    "urls = []\n",
    "number_urls = 0\n",
    "\n",
    "keywords = [\"higher-order interactions\", \"trait-mediated interaction modification\", \"trait-mediated interaction\", \"polymorphism\", \"apparent competition\", \"resource competition\", \"keystone predation\", \"intraguild predation\", \"intransitive competition\", \"trophic chains\", \"competition chains\", \"mutual competition\"]\n",
    "pager = Works().search_filter(title=keywords[0]).paginate(per_page=200)\n",
    "number_works = 0\n",
    "number_unfiltered_works = 0\n",
    "\n",
    "texts = []\n",
    "\n",
    "for page in pager:\n",
    "    for work in page:\n",
    "        number_unfiltered_works += 1\n",
    "        \n",
    "        title = work['title']\n",
    "        abstract = work['abstract']\n",
    "        \n",
    "        # Find Full Text\n",
    "        url = None\n",
    "        if work[\"primary_location\"]:\n",
    "            url = work[\"primary_location\"][\"pdf_url\"]\n",
    "            if url:\n",
    "                urls.append(url)\n",
    "        full_text = \"\" if not url else pdf_to_text(url)\n",
    "        \n",
    "        if not abstract and not full_text:\n",
    "            continue\n",
    "        texts.append((title, abstract if abstract and not full_text else full_text))\n",
    "        number_works += 1\n",
    "    if number_works >= 10000:\n",
    "        break\n",
    "\n",
    "print(len(texts), number_works, number_unfiltered_works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d9863-ad69-4c97-8ae2-6fc46d2c8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(texts)\n",
    "\n",
    "valid_texts = []\n",
    "\n",
    "text_i = 0\n",
    "number_texts = len(texts)\n",
    "\n",
    "for title, text in texts[:number_texts]:\n",
    "    try:\n",
    "        text_i += 1\n",
    "        print(f\"{text_i}/{number_texts}\")\n",
    "        \n",
    "        parser.update(text)\n",
    "        units = parser.parse()\n",
    "\n",
    "        for unit in units:\n",
    "            if unit.get_score() == 4:\n",
    "                valid_texts.append((title, unit))\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "971d39ef-9b21-4ffd-8f38-150c45980729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b080e64-a589-4ee9-9244-f00d899e6fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct and higher‐order interactions in plant communities under increasing weather persistence\n",
      "Species: {species, interactions}, Trait: {role}, Cause: (Species: set(), Trait: {conditions}, Cause: (set()), Change: set()), Change: {destabilize}\n",
      "\n",
      "HOM (higher order mode) losses at the IR (interaction region) of the B-factory\n",
      "Species: {s}, Trait: {heating}, Cause: ({beam, the, wall, in, pipe}), Change: {producing}\n",
      "\n",
      "Multitrophic higher-order interactions modulate species persistence\n",
      "Species: {plant, species}, Trait: {interactions}, Cause: (Species: set(), Trait: {HOIs}, Cause: (set()), Change: set()), Change: {produced}\n",
      "\n",
      "Bias-Variance Trade-Off in Hierarchical Probabilistic Models Using Higher-Order Feature Interactions\n",
      "Species: {variance, trade, off, bias}, Trait: {complexity}, Cause: (Species: set(), Trait: {number}, Cause: ({that}), Change: {increasing}), Change: {increases}\n",
      "\n",
      "Competition Between Aquatic Insects and Vertebrates: Interaction Strength and Higher Order Interactions\n",
      "Species: {Insects}, Trait: {mass}, Cause: ({that, both, anurans}), Change: {reduced}\n",
      "\n",
      "The Telomere Capping Complex CST Has an Unusual Stoichiometry, Makes Multipartite Interaction with G-Tails, and Unfolds Higher-Order G-Tail Structures\n",
      "Species: {domain, albicans, fold, Candida, OB, Stn1}, Trait: {surface, DNA}, Cause: ({the, DNA, that}), Change: {caused, reduction}\n",
      "\n",
      "VERTEBRATES: INTERACTION STRENGTH AND HIGHER ORDER INTERACTIONS1\n",
      "Species: {Insects}, Trait: {mass}, Cause: ({that, both, anurans}), Change: {reduced}\n",
      "\n",
      "The role of higher-order biotic interactions on tropical tree growth\n",
      "Species: {species, most}, Trait: {diameter, growth, rates}, Cause: ({what, had, beyond, interactions, direct, already}), Change: {reduced, reduce}\n",
      "\n",
      "Higher order genes interaction in DNA repair and cytokine genes polymorphism and risk to lung cancer in North Indians\n",
      "Species: {lung}, Trait: {risk}, Cause: ({that, terminal, nodes, which}), Change: {increased}\n",
      "\n",
      "Higher order scaffoldin assembly in Ruminococcus flavefaciens cellulosome is coordinated by a discrete cohesin-dockerin interaction\n",
      "Species: {Ruminococcus, flavefaciens}, Trait: {cellulosome, complexity}, Cause: ({as}), Change: {increasing}\n",
      "\n",
      "Bivalve Growth and Higher Order Interactions: Importance of Density, Site, and Time\n",
      "Species: {Mercenaria, growth}, Trait: {mixing}, Cause: ({roughness, of, because, hydrodynamics}), Change: {increased}\n",
      "\n",
      "Experimental evidence for nature's hidden network of higher-order interactions\n",
      "Species: {distinct}, Trait: {densities}, Cause: (Species: set(), Trait: set(), Cause: (Species: set(), Trait: set(), Cause: ({network, interactions, multiple, a, how, within}), Change: set()), Change: {changed}), Change: {reducing}\n",
      "\n",
      "Connecting higher order interactions with ecological stability in experimental aquatic food webs\n",
      "Species: {more, species, third}, Trait: {interaction}, Cause: ({the, a, when, of, presence, third}), Change: {changes}\n",
      "\n",
      "Higher-Order Simulations of Interactional Aerodynamics on Full Helicopter Configurations Using a Hamiltonian Strand Approach\n",
      "Species: {flight, forward, medium, speed, case}, Trait: {conditions}, Cause: ({of, an, at, ratio, advance, μ}), Change: {reproduced}\n",
      "\n",
      "Intraspecific variation promotes species coexistence and trait clustering through higher order interactions\n",
      "Species: {species, coexistence}, Trait: {divergence, trait}, Cause: ({that, individual, variation, can}), Change: {reducing}\n",
      "\n",
      "Higher Order Calcium-Dependent Protein Kinases (CPKs) Mutant Lines Elucidate Roles of CPKs within Abscisic Acid Signal Transduction in Arabidopsis and In Vivo Interactions of Calcium-Dependent Protein Kinases, CPK6 and CPK23, with PP2C Protein Phosphatases within Abscisic Acid Signaling\n",
      "Species: {plants}, Trait: {fitness}, Cause: (Species: set(), Trait: {abscisic, acid, stress-, phytohormone}, Cause: (set()), Change: set()), Change: {reduces}\n",
      "\n",
      "Experimental evidence for nature's hidden network of higher-order interactions\n",
      "Species: {distinct}, Trait: {densities}, Cause: (Species: set(), Trait: set(), Cause: (Species: set(), Trait: set(), Cause: ({how, within, interactions, network, a, multiple}), Change: set()), Change: {changed}), Change: {reducing}\n",
      "\n",
      "Direct and higher-order interactions in plant communities under increasing weather persistence\n",
      "Species: {interactions, species}, Trait: {role}, Cause: (Species: set(), Trait: {conditions}, Cause: (set()), Change: set()), Change: {destabilize}\n",
      "\n",
      "Experimental evidence for nature's hidden network of higher-order interactions\n",
      "Species: {distinct}, Trait: {densities}, Cause: (Species: set(), Trait: set(), Cause: (Species: set(), Trait: set(), Cause: ({interactions, network, multiple, a, within, how}), Change: set()), Change: {changed}), Change: {reducing}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in valid_texts:\n",
    "    print(t[0])\n",
    "    print(t[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "090fa01e-e6f0-44a1-90f7-c429ccc9ced9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889\n"
     ]
    }
   ],
   "source": [
    "print(text_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18d6e09b-890b-49fe-befe-f77449753074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90305f73-c979-4ba2-9a2d-35f0671e3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('texts.pickle', 'wb') as file:\n",
    "#     pickle.dump(texts, file)\n",
    "\n",
    "# with open('valid_texts.pickle', 'wb') as file:\n",
    "#     pickle.dump([t[0] for t in valid_texts], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2aa2e1-afda-40b5-ae53-9f9ae19bfdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_texts.pickle\", 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
