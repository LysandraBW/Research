{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d888a6-3df7-4dfd-a573-811ad167fbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import textacy\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from fastcoref import FCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from fastcoref import spacy_component\n",
    "from spacy.matcher import Matcher, DependencyMatcher, PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d3a028-c928-4c47-9be9-e34b313f6efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5013e7f6-f39b-4671-98eb-0c46e494316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tools:\n",
    "    def __init__(self):\n",
    "        self.taxonerd = TaxoNERD(prefer_gpu=True)\n",
    "        self.nlp = self.taxonerd.load(model=\"en_ner_eco_biobert\")\n",
    "        self.nlp.add_pipe(\"fastcoref\")\n",
    "        print(self.nlp.pipe_names)\n",
    "        self.doc = None\n",
    "        self.token_map = None\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        cleaned_text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "        cleaned_text = re.sub(\"\\s+\", \" \", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"\\s+([?.!,])\", r\"\\1\", cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def update(self, doc):\n",
    "        self.doc = doc\n",
    "        # Map Tokens to Index\n",
    "        self.token_map = {}\n",
    "        for token in self.doc:\n",
    "            self.token_map[token.idx] = token.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c96853-5df0-4680-8d1e-2f9cf3c189fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class References:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        self.cluster_map = None\n",
    "\n",
    "    def update(self):\n",
    "        if not self.tools.doc:\n",
    "            return\n",
    "        self.cluster_map = self.get_cluster_map(self.tools.doc._.coref_clusters)\n",
    "        \n",
    "    def get_cluster_map(self, clusters):\n",
    "        cluster_map = {}\n",
    "        for cluster in clusters:\n",
    "            token_cluster = []\n",
    "            for span in cluster:\n",
    "                if span[0] not in self.tools.token_map:\n",
    "                    raise Exception(\"Invalid Token\")\n",
    "                index = self.tools.token_map[span[0]]\n",
    "                token_cluster.append(self.tools.doc[index])\n",
    "            # Mapping\n",
    "            for token in token_cluster:\n",
    "                cluster_map[token.i] = list(filter(lambda t: t != token, token_cluster))\n",
    "        return cluster_map\n",
    "            \n",
    "    def get_references(self, tokens):\n",
    "        refs = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.cluster_map:\n",
    "                refs += self.cluster_map[index]\n",
    "        return refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63830b96-590e-491b-bf60-90833967cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Possession:\n",
    "    # There's no definite names for these patterns as I do not know what\n",
    "    # to call them. These patterns are used to extract possessive\n",
    "    # relationships from a sentence. I also could not find better names for\n",
    "    # the two variables below.\n",
    "    OWNER = \"owner\"\n",
    "    OWNED = \"owned\"\n",
    "    \n",
    "    patterns = {\n",
    "        \"Pattern1\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": OWNED,\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"poss\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern2\": [\n",
    "             {\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": OWNED,\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": \"adp\",\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"prep\",\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"ADP\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"adp\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"pobj\",\n",
    "                    \"POS\": {\n",
    "                        \"IN\": [\"NOUN\", \"PROPN\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern3\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": \"verb\",\n",
    "                \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"nsubj\",\n",
    "                    \"POS\": {\"IN\": [\"PRON\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"dobj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"Pattern4\": [\n",
    "            {\n",
    "                \"RIGHT_ID\": \"verb\",\n",
    "                \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNED,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"nsubj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"verb\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": \"adp\",\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"prep\",\n",
    "                    \"POS\": {\"IN\": [\"ADP\"]}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"LEFT_ID\": \"adp\",\n",
    "                \"REL_OP\": \">\",\n",
    "                \"RIGHT_ID\": OWNER,\n",
    "                \"RIGHT_ATTRS\": {\n",
    "                    \"DEP\": \"pobj\",\n",
    "                    \"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        self.matcher = DependencyMatcher(self.tools.nlp.vocab)\n",
    "        for pattern_id, pattern in Possession.patterns.items():\n",
    "            self.matcher.add(pattern_id, [pattern])\n",
    "        self.owner_map = None\n",
    "        self.owned_map = None\n",
    "        self.update()\n",
    "    \n",
    "    def update(self):\n",
    "        if not self.tools.doc:\n",
    "            return\n",
    "        matches = self.matcher(self.tools.doc)\n",
    "        owner_map, owned_map = self.get_ownership_map(matches)\n",
    "        self.owner_map = owner_map # Maps Owner to Owned\n",
    "        self.owned_map = owned_map # Maps Owned to Owner\n",
    "        \n",
    "    def get_ownership_map(self, matches):\n",
    "        owner_map = {}\n",
    "        owned_map = {}\n",
    "\n",
    "        for match_id, token_ids in matches:\n",
    "            pattern_id = self.tools.nlp.vocab.strings[match_id]\n",
    "            # print(pattern_id)\n",
    "            owner = None\n",
    "            owned = None\n",
    "            for i in range(len(token_ids)):\n",
    "                right_id = Possession.patterns[pattern_id][i][\"RIGHT_ID\"]\n",
    "                if right_id == Possession.OWNER:\n",
    "                    owner = self.tools.doc[token_ids[i]]\n",
    "                if right_id == Possession.OWNED:\n",
    "                    owned = self.tools.doc[token_ids[i]]\n",
    "\n",
    "            # Owner to Owned\n",
    "            if owner.i not in owner_map:\n",
    "                owner_map[owner.i] = []\n",
    "            owner_map[owner.i].append(owned)\n",
    "\n",
    "            # Owned to Owner\n",
    "            if owned.i not in owned_map:\n",
    "                owned_map[owned.i] = []\n",
    "            owned_map[owned.i].append(owner)\n",
    "            \n",
    "        return (owner_map, owned_map)\n",
    "\n",
    "    def get_owner(self, tokens):\n",
    "        owners = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.owned_map:\n",
    "                owners += self.owned_map[index]\n",
    "        return owners\n",
    "\n",
    "    def get_owned(self, tokens):\n",
    "        owned = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.owner_map:\n",
    "                owned += self.owner_map[index]\n",
    "        return owned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11b24e95-8df1-4438-aaa5-7d8ba0759ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        self.species_indices = None\n",
    "        self.update()\n",
    "\n",
    "    def update(self):\n",
    "        if not self.tools.doc:\n",
    "            return\n",
    "        self.species_indices = self.get_species_indices()\n",
    "        \n",
    "    def get_species_indices(self):\n",
    "        indices = []\n",
    "\n",
    "        # lowered_text = self.tools.doc.text.lower()\n",
    "        # for token in self.tools.doc:\n",
    "        #     if token.pos_ not in [\"NOUN\", \"PROPN\"]:\n",
    "        #         continue\n",
    "        #     try:\n",
    "        #         results = requests.get(f\"https://api.inaturalist.org/v1/search?q={token.lemma_}&sources=taxa&include_taxon_ancestors=false\")\n",
    "        #         results = results.json()\n",
    "        #         results = results[\"results\"]\n",
    "        #         for result in results:\n",
    "        #             if \"record\" not in result or \"name\" not in result[\"record\"]:\n",
    "        #                 continue\n",
    "        #             if lowered_text.find(result[\"record\"][\"name\"].lower()) == -1:\n",
    "        #                 continue\n",
    "        #             indices.append(token.i)\n",
    "        #     except Exception as e:\n",
    "        #         print(\"Network Error\")\n",
    "                \n",
    "        for species_span in self.tools.doc.ents:\n",
    "            for species in species_span:\n",
    "                if species.idx not in self.tools.token_map:\n",
    "                    raise Exception(\"Invalid Token\")\n",
    "                index = self.tools.token_map[species.idx]\n",
    "                if index in indices:\n",
    "                    continue\n",
    "                indices.append(index)\n",
    "        return indices\n",
    "\n",
    "    def is_species(self, token):\n",
    "        index = token.i\n",
    "        return index in self.species_indices\n",
    "        \n",
    "    def contains_species(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.species_indices:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d802666c-0146-4d8b-80bd-4bd8fa689aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit:\n",
    "    def __init__(self, *, species=None, trait=None, change=None, cause=None):\n",
    "        self.species = species\n",
    "        self.trait = trait\n",
    "        self.cause = cause\n",
    "        self.change = change\n",
    "        # Flag\n",
    "        self.is_cause = False\n",
    "\n",
    "    def empty(self):\n",
    "        if not self.species and not self.trait and not self.cause and not self.change:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def not_empty(self):\n",
    "        return not self.empty()\n",
    "\n",
    "    def can_merge(self, unit):\n",
    "        # Two units can merge if there's no\n",
    "        # overlap.\n",
    "        if self.species and unit.species:\n",
    "            return False\n",
    "        if self.trait and unit.trait:\n",
    "            return False\n",
    "        if self.cause and unit.cause:\n",
    "            return False\n",
    "        if self.change and unit.change:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def merge(self, unit):\n",
    "        # We take the parts that the\n",
    "        # other unit has; assuming that\n",
    "        # there's no overlap, there's\n",
    "        # no loss of information.\n",
    "        if unit.species:\n",
    "            self.species = unit.species\n",
    "        if unit.trait:\n",
    "            self.trait = unit.trait\n",
    "        if unit.cause:\n",
    "            self.cause = unit.cause\n",
    "        if unit.change:\n",
    "            self.change = unit.change\n",
    "\n",
    "    def get_score(self):\n",
    "        score = 0\n",
    "        if self.species:\n",
    "            score += 1\n",
    "        if self.trait:\n",
    "            score += 1\n",
    "        if self.cause:\n",
    "            score += 1\n",
    "        if self.change:\n",
    "            score += 1    \n",
    "        return score\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Species: {self.species}, Trait: {self.trait}, Cause: ({self.cause}), Change: {self.change}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869e90fe-7ecb-4840-89f8-5ad104a7f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:    \n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        # References\n",
    "        self.unit_keywords = [self.tools.nlp(keyword) for keyword in {\"unit\", \"%\", \"percent\"}]\n",
    "        self.change_keywords = [self.tools.nlp(keyword) for keyword in {\"increase\", \"decrease\", \"change\", \"weaken\", \"shift\", \"cause\"}]\n",
    "        self.quantity_keywords = [self.tools.nlp(keyword) for keyword in {\"tenfold\", \"half\", \"double\", \"triple\", \"quadruple\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"}]\n",
    "        # Instances\n",
    "        self.unit_indices = []\n",
    "        self.change_indices = []\n",
    "        self.quantity_indices = []\n",
    "        self.cause_indices = []\n",
    "        self.update()\n",
    "        \n",
    "    def update(self):\n",
    "        if not self.tools.doc:\n",
    "            return\n",
    "        self.unit_indices = self.load_unit_indices()\n",
    "        self.change_indices = self.load_change_indices()\n",
    "        self.quantity_indices = self.load_quantity_indices()\n",
    "        self.cause_indices = self.load_cause_indices()\n",
    "        return\n",
    "\n",
    "    def is_unit(self, token):\n",
    "        return token.i in self.unit_indices\n",
    "\n",
    "    def has_unit(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.unit_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_unit_indices(self):\n",
    "        indices = []\n",
    "        for token in self.tools.doc:\n",
    "            if token.pos_ not in [\"NOUN\"]:\n",
    "                continue\n",
    "            lemma = self.tools.nlp(token.lemma_)\n",
    "            for keyword in self.unit_keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > 0.7:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def is_change(self, token):\n",
    "        return token.i in self.change_indices\n",
    "\n",
    "    def has_change(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.change_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_change_indices(self):\n",
    "        indices = []\n",
    "        for token in self.tools.doc:\n",
    "            if token.lower_ == \"to\" and token.head and token.head.lower_ == \"from\":\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            if token.pos_ not in [\"NOUN\", \"VERB\"]:\n",
    "                continue\n",
    "            lemma = self.tools.nlp(token.lemma_)\n",
    "            for keyword in self.change_keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > 0.7:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def is_quantity(self, token):\n",
    "        return token.i in self.quantity_indices\n",
    "\n",
    "    def has_quantity(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.quantity_indices:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def load_quantity_indices(self):\n",
    "        indices = []\n",
    "        for token in self.tools.doc:\n",
    "            if token.pos_ not in [\"NOUN\", \"NUM\"]:\n",
    "                continue\n",
    "            lemma = self.tools.nlp(token.lemma_)\n",
    "            for keyword in self.quantity_keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > 0.7:\n",
    "                    # Make sure that if there is a noun the quantity\n",
    "                    # modifies, and that it is a unit.\n",
    "                    if token.head and token.head.pos_ == \"NOUN\" and not self.is_unit(token.head):\n",
    "                        continue\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def is_cause(self, token):\n",
    "        return token.i in self.cause_indices\n",
    "\n",
    "    def has_cause(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.cause_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_cause_indices(self):\n",
    "        indices = []\n",
    "        for token in self.tools.doc:\n",
    "            if token.pos_ not in [\"ADP\", \"SCONJ\", \"PART\", \"PRON\"]:\n",
    "                continue\n",
    "            if token.pos_ == \"SCONJ\":\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            elif token.pos_ == \"PART\":\n",
    "                if token.head and token.head.pos_ == \"VERB\":\n",
    "                    indices.append(token.i)\n",
    "                    continue\n",
    "            elif token.pos_ == \"ADP\":\n",
    "                if token.lower_ == \"due\" and self.tools.doc[token.i + 1] and self.tools.doc[token.i + 1].lower_ == \"to\":\n",
    "                    indices.append(token.i)\n",
    "                    continue\n",
    "                elif token.head:\n",
    "                    if token.head.pos_ == \"AUX\":\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "                    elif token.head.pos_ == \"VERB\" and token.head.i < token.i and self.is_change(token.head):\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "                    elif token.lower_ != \"to\" and \"AUX\" in [child.pos_ for child in list(filter(lambda t: t.i < token.i,token.head.children))]:\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "                elif token.ancestors:\n",
    "                    if \"AUX\" in [ancestor.pos_ for ancestor in token.ancestors]:\n",
    "                        indices.append(token.i)\n",
    "                        continue\n",
    "            elif token.pos_ == \"PRON\":\n",
    "                if token.head and token.head.pos_ == \"VERB\" and self.is_change(token.head):\n",
    "                    indices.append(token.i)\n",
    "                    continue\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95c46620-832d-4d6c-8495-155fbcb68442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.tools = Tools()\n",
    "        self.species = Species(self.tools)\n",
    "        self.possession = Possession(self.tools)\n",
    "        self.references = References(self.tools)\n",
    "        self.keywords = Keywords(self.tools)\n",
    "            \n",
    "    def update(self, doc):\n",
    "        self.tools.update(doc)\n",
    "        self.species.update()\n",
    "        self.possession.update()\n",
    "        self.references.update()\n",
    "        self.keywords.update()\n",
    "                \n",
    "    def parse_segment(self, l_i, r_i):\n",
    "        # print(f\"\\nPARSING SEGMENT\\n\")\n",
    "        # print(f\"Text: {self.tools.doc[l_i:r_i+1].text}\")\n",
    "        used = []\n",
    "\n",
    "        # Find Cause\n",
    "        cause = []\n",
    "        for token in self.tools.doc[l_i:r_i+1]:\n",
    "            if token not in used and token.pos_ in [\"SCONJ\"] and self.keywords.is_cause(token):\n",
    "                start_i = token.i + 1\n",
    "                end_i = start_i\n",
    "                while end_i <= r_i and self.tools.doc[end_i] not in used and self.tools.doc[end_i].pos_ in [\"ADP\", \"DET\", \"NOUN\", \"PROPN\", \"AUX\", \"ADV\", \"PRON\", \"ADJ\"]:\n",
    "                    used.append(self.tools.doc[end_i])\n",
    "                    cause.append(self.tools.doc[end_i])\n",
    "                    end_i += 1\n",
    "                used.append(token)\n",
    "        # print(f\"Cause 1: {cause}\")\n",
    "        \n",
    "        # Find Species\n",
    "        species = []\n",
    "        for token in self.tools.doc[l_i:r_i+1]:\n",
    "            if self.species.is_species(token) and token not in used and (token.head and (token.head.pos_ not in [\"SCONJ\", \"ADP\"] or token.head.lower_ == \"of\")):\n",
    "                species.append(token)\n",
    "                used.append(species)\n",
    "                break\n",
    "        # print(f\"Species: {species}\")\n",
    "        \n",
    "        # Find Change\n",
    "        change = []\n",
    "        for token in self.tools.doc[l_i:r_i+1]:\n",
    "            if token not in used and not token.is_oov:\n",
    "                if self.keywords.is_change(token):\n",
    "                    change.append(token)\n",
    "                    used.append(token)\n",
    "            # I only want one word that represents\n",
    "            # the change for simplicity\n",
    "            if change:\n",
    "                break\n",
    "        # print(f\"Change 1: {change}\")\n",
    "\n",
    "        # Next Method to Find Change\n",
    "        for token in self.tools.doc[l_i:r_i+1]:\n",
    "            if token not in used and token.pos_ == \"ADP\" and token.lower_ != \"of\":\n",
    "                # print(\"In Next Method...\", token, token not in used, token.pos_)\n",
    "                start_i = token.i + 1\n",
    "                end_i = start_i\n",
    "                possible_changes = []\n",
    "                while end_i <= r_i and self.tools.doc[end_i].pos_ in [\"NUM\", \"SYM\", \"NOUN\", \"ADP\", \"DET\"]:\n",
    "                    possible_changes.append(self.tools.doc[end_i])\n",
    "                    end_i += 1\n",
    "                if not self.keywords.has_quantity(possible_changes):\n",
    "                    continue\n",
    "                # print(f\"Actual Changes: {possible_changes}\")\n",
    "                for possible_change in possible_changes:\n",
    "                    used.append(possible_change)\n",
    "                    change.append(possible_change)\n",
    "                used.append(token)\n",
    "                break\n",
    "        # print(f\"Change 2: {change}\")\n",
    "\n",
    "        # Find Trait\n",
    "        trait = []\n",
    "        if species:\n",
    "            possible_traits = list(filter(lambda t: t.i >= l_i and t.i <= r_i, self.possession.get_owned(species)))\n",
    "            valid_trait = False\n",
    "            for possible_trait in possible_traits:\n",
    "                for ancestor in possible_trait.ancestors:\n",
    "                    if self.keywords.is_change(ancestor):\n",
    "                        valid_trait = True\n",
    "                        break\n",
    "            if valid_trait:\n",
    "                trait = possible_traits\n",
    "        elif change:\n",
    "            # The trait is listed before the change (i.e. \"diet shifts from ...\")\n",
    "            prev_i = change[0].i - 1\n",
    "            prev_token = None if prev_i < 0 else self.tools.doc[prev_i]\n",
    "            if prev_token and prev_token not in used and prev_token.pos_ == \"NOUN\":\n",
    "                used.append(prev_token)\n",
    "                trait.append(prev_token)\n",
    "            else:\n",
    "                # Look for \"in\" (i.e. \"increase in ...\")\n",
    "                for child in change[0].children:\n",
    "                    # print(child, child.pos_, child.children)\n",
    "                    if child in used:\n",
    "                        continue\n",
    "                    if child.pos_ == \"ADP\" and child.children:\n",
    "                        children = list(child.children)\n",
    "                        if children[0] not in used:\n",
    "                            used.append(children[0])\n",
    "                            trait.append(children[0])\n",
    "        else:\n",
    "            for token in self.tools.doc[l_i:r_i+1]:\n",
    "                if token.head and self.keywords.is_change(token.head):\n",
    "                    trait.append(token)\n",
    "                    used.append(token)\n",
    "\n",
    "                    possible_species = self.possession.get_owner(trait)\n",
    "                    if self.species.contains_species(possible_species):\n",
    "                        for sp in possible_species:\n",
    "                            if token in sp.ancestors:\n",
    "                                species.append(sp)\n",
    "                                used.append(sp)\n",
    "                                break\n",
    "                    break\n",
    "        # print(f\"Trait: {trait}\")\n",
    "        \n",
    "        # Find Cause\n",
    "        is_cause = False\n",
    "        for token in self.tools.doc[l_i:r_i+1]:\n",
    "            if token not in used and token.pos_ in [\"PRON\"] and self.keywords.is_cause(token):\n",
    "                is_cause = True\n",
    "                used.append(token)\n",
    "            elif token not in used and token.pos_ in [\"ADP\"] and self.keywords.is_cause(token):\n",
    "                start_i = token.i + 1\n",
    "                end_i = start_i\n",
    "                buffer = []\n",
    "                noun_found = False\n",
    "                while end_i <= r_i and self.tools.doc[end_i] not in used and self.tools.doc[end_i].pos_ in [\"ADP\", \"DET\", \"NOUN\", \"PROPN\", \"AUX\", \"ADV\", \"PRON\", \"ADJ\"]:\n",
    "                    if self.tools.doc[end_i].pos_ in [\"NOUN\", \"PROPN\", \"PRON\"]:\n",
    "                        noun_found = True\n",
    "                    buffer.append(self.tools.doc[end_i])\n",
    "                    end_i += 1\n",
    "                if noun_found:\n",
    "                    for token in buffer:\n",
    "                        used.append(token)\n",
    "                        cause.append(token)\n",
    "                    used.append(token)\n",
    "        # print(f\"Cause 2: {cause}\")\n",
    "        \n",
    "        unit = Unit(species=species, trait=trait, change=change, cause=cause)\n",
    "        unit.is_cause = is_cause\n",
    "        return unit\n",
    "\n",
    "    def parse_sentence(self, l_i, r_i):\n",
    "        units = []\n",
    "        \n",
    "        # Recursive Split\n",
    "        # We're extracting the core information\n",
    "        # in the sentence into units.\n",
    "        def recursive_split(r_l_i, r_r_i):\n",
    "            nonlocal units\n",
    "            # Find Verb\n",
    "            # The verb is used to divide\n",
    "            # the \"parsing\" space, which\n",
    "            # makes the work simpler.\n",
    "            verb = None\n",
    "            for token in self.tools.doc[r_l_i:r_r_i+1]:\n",
    "                if token.pos_ == \"VERB\":\n",
    "                    verb = token\n",
    "                    break\n",
    "    \n",
    "            # Base Case\n",
    "            # If there is no verb, we have\n",
    "            # reached the simplest case and\n",
    "            # can extract information.\n",
    "            if verb == None:\n",
    "                units.append(self.parse_segment(r_l_i, r_r_i))\n",
    "            else:\n",
    "                recursive_split(r_l_i, verb.i - 1)\n",
    "                units.append(verb)\n",
    "                recursive_split(verb.i + 1, r_r_i)\n",
    "            return\n",
    "        recursive_split(l_i, r_i)\n",
    "\n",
    "        # Recursive Merge\n",
    "        # We are putting the pieces back together,\n",
    "        # so that we, the computer, can understand\n",
    "        # what's going on.\n",
    "        def recursive_merge():\n",
    "            nonlocal units\n",
    "            if len(units) < 3:\n",
    "                return\n",
    "            \n",
    "            l_unit = units[0]\n",
    "            verb = units[1]\n",
    "            r_unit = units[2]\n",
    "            verb_is_change = self.keywords.is_change(verb)\n",
    "            \n",
    "            if l_unit.empty() and r_unit.empty():\n",
    "                m_unit = Unit()  \n",
    "            elif l_unit.not_empty() and r_unit.empty():\n",
    "                # print(1)\n",
    "                if verb_is_change:\n",
    "                    l_unit.change.append(verb)\n",
    "                m_unit = l_unit\n",
    "            elif r_unit.not_empty() and l_unit.empty():\n",
    "                # print(2)\n",
    "                if verb_is_change:\n",
    "                    r_unit.change.append(verb)\n",
    "                m_unit = r_unit    \n",
    "            elif l_unit.can_merge(r_unit):\n",
    "                # print(3)\n",
    "                l_unit.merge(r_unit)\n",
    "                if verb_is_change:\n",
    "                    l_unit.change.append(verb)\n",
    "                m_unit = l_unit\n",
    "            elif verb_is_change:\n",
    "                # print(4)\n",
    "                r_unit.cause = l_unit\n",
    "                if verb_is_change:\n",
    "                    r_unit.change.append(verb)\n",
    "                m_unit = r_unit\n",
    "            else:\n",
    "                # print(5)\n",
    "                if l_unit.get_score() >= r_unit.get_score():\n",
    "                    m_unit = l_unit\n",
    "                else:\n",
    "                    m_unit = r_unit\n",
    "            units = [m_unit] + units[3:]\n",
    "            recursive_merge()\n",
    "            return\n",
    "        recursive_merge()\n",
    "\n",
    "        assert len(units) == 1\n",
    "        return units[0]\n",
    "\n",
    "    def parse(self):\n",
    "        units = []\n",
    "        for sent in self.tools.doc.sents:\n",
    "            print(f\"Sentence: {self.tools.doc[sent.start:sent.end].text}\")\n",
    "            unit = self.parse_sentence(sent.start, sent.end - 1)\n",
    "            print(unit)\n",
    "            units.append(unit)\n",
    "            print()\n",
    "        return units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a1dfd80-ba55-4a46-ab79-bb59f398844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2025 12:04:40 - INFO - \t GPU is available\n",
      "05/04/2025 12:04:40 - INFO - \t TaxoNERD will use GPU\n",
      "05/04/2025 12:05:06 - INFO - \t missing_keys: []\n",
      "05/04/2025 12:05:06 - INFO - \t unexpected_keys: []\n",
      "05/04/2025 12:05:06 - INFO - \t mismatched_keys: []\n",
      "05/04/2025 12:05:06 - INFO - \t error_msgs: []\n",
      "05/04/2025 12:05:06 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "05/04/2025 12:05:07 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformer', 'tagger', 'attribute_ruler', 'lemmatizer', 'pysbd_sentencizer', 'parser', 'ner', 'taxo_abbrev_detector', 'fastcoref']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 39.08 examples/s]\n",
      "05/04/2025 12:05:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.52it/s]\n",
      "05/04/2025 12:05:20 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 71.44 examples/s]\n",
      "05/04/2025 12:05:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.64it/s]\n",
      "05/04/2025 12:05:33 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 56.91 examples/s]\n",
      "05/04/2025 12:05:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.33it/s]\n",
      "05/04/2025 12:05:45 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 54.93 examples/s]\n",
      "05/04/2025 12:05:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.30it/s]\n",
      "05/04/2025 12:05:58 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 126.32 examples/s]\n",
      "05/04/2025 12:06:10 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.60it/s]\n",
      "05/04/2025 12:06:11 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 82.53 examples/s]\n",
      "05/04/2025 12:06:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.39it/s]\n",
      "05/04/2025 12:06:23 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 47.51 examples/s]\n",
      "05/04/2025 12:06:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.25it/s]\n",
      "05/04/2025 12:06:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 69.19 examples/s]\n",
      "05/04/2025 12:06:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.27it/s]\n",
      "05/04/2025 12:06:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 46.05 examples/s]\n",
      "05/04/2025 12:07:00 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.32it/s]\n",
      "05/04/2025 12:07:00 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 68.10 examples/s]\n",
      "05/04/2025 12:07:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.70it/s]\n",
      "05/04/2025 12:07:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 71.57 examples/s]\n",
      "05/04/2025 12:07:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.27it/s]\n",
      "05/04/2025 12:07:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 63.94 examples/s]\n",
      "05/04/2025 12:07:37 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.49it/s]\n",
      "05/04/2025 12:07:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 58.57 examples/s]\n",
      "05/04/2025 12:07:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.00it/s]\n",
      "05/04/2025 12:07:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 42.65 examples/s]\n",
      "05/04/2025 12:08:02 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.80it/s]\n",
      "05/04/2025 12:08:02 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 69.75 examples/s]\n",
      "05/04/2025 12:08:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.15it/s]\n",
      "05/04/2025 12:08:14 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 38.15 examples/s]\n",
      "05/04/2025 12:08:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.77it/s]\n",
      "05/04/2025 12:08:27 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 86.17 examples/s]\n",
      "05/04/2025 12:08:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.32it/s]\n",
      "05/04/2025 12:08:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 60.67 examples/s]\n",
      "05/04/2025 12:08:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.48it/s]\n",
      "05/04/2025 12:08:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 66.23 examples/s]\n",
      "05/04/2025 12:09:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.01it/s]\n",
      "05/04/2025 12:09:04 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 62.21 examples/s]\n",
      "05/04/2025 12:09:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.29it/s]\n",
      "05/04/2025 12:09:16 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 43.36 examples/s]\n",
      "05/04/2025 12:09:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.44it/s]\n",
      "05/04/2025 12:09:28 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 52.36 examples/s]\n",
      "05/04/2025 12:09:40 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.07it/s]\n",
      "05/04/2025 12:09:41 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 57.68 examples/s]\n",
      "05/04/2025 12:09:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.16it/s]\n",
      "05/04/2025 12:09:53 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 38.72 examples/s]\n",
      "05/04/2025 12:10:06 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization Took 326.11009454727173s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "parser = Parser()\n",
    "t1 = time.time()\n",
    "print(f\"Initialization Took {t1 - t0}s\")\n",
    "\n",
    "# for doc in parser.tools.nlp.pipe([\"\"], n_process=2):    \n",
    "#     # Update\n",
    "#     t0 = time.time()\n",
    "#     parser.update(doc)\n",
    "#     t1 = time.time()\n",
    "#     print(f\"Update Took {t1 - t0}s\")\n",
    "    \n",
    "#     # Parse\n",
    "#     t0 = time.time()\n",
    "#     units = parser.parse()\n",
    "#     t1 = time.time()\n",
    "#     print(f\"Parsing Took {t1 - t0}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad4ddf-01fe-4b17-ba2c-38491d3b0fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
