{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "292e1a6c-ae95-494f-b933-5bda81576d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import pickle\n",
    "import pymupdf\n",
    "import textacy\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "from pprint import pprint\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from pyalex import Works\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6f4b802e-decb-4351-a861-33b2ceef5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = text\n",
    "    cleaned_text = re.sub(r'http\\S+', '', cleaned_text) # Remove URLs\n",
    "    cleaned_text = re.sub(r'-\\n', '', cleaned_text) # Remove Hyphenations\n",
    "    cleaned_text = re.sub(\"\\s+\", \" \", cleaned_text) # Remove Duplicate Spaces\n",
    "    cleaned_text = re.sub(r\"\\s+([?.!,])\", r\"\\1\", cleaned_text) # Remove Spaces Before Punctuation\n",
    "    return cleaned_text\n",
    "\n",
    "def pdf_to_text(url):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        f = pdf_bytes(url)\n",
    "        doc = pymupdf.open(stream=f)\n",
    "        for d in doc:\n",
    "            text += d.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def load_local_documents(group=\"Cleared\"):\n",
    "    documents = []\n",
    "    filenames = glob.glob(f\"../Week 7/Examples/{group}/*.pdf\")\n",
    "    for filename in filenames:\n",
    "        full_text = \"\"\n",
    "        doc = pymupdf.open(filename)\n",
    "        for page in doc:\n",
    "            text = page.get_text()\n",
    "            full_text += \" \" + text\n",
    "        if full_text:\n",
    "            documents.append(clean_text(full_text))\n",
    "    return documents\n",
    "\n",
    "def load_documents():\n",
    "    # Cross Product\n",
    "    keywords = [\"higher-order interactions\", \"trait-mediated interaction modification\", \"trait-mediated interaction\", \"polymorphism\", \"apparent competition\", \"resource competition\", \"keystone predation\", \"intraguild predation\", \"intransitive competition\", \"trophic chains\", \"competition chains\", \"mutual competition\"]\n",
    "    number_keywords = len(keywords)\n",
    "    for i in range(4):\n",
    "        for j in range(4, number_keywords, 1):\n",
    "            keywords.append(f\"{keywords[i]} {keywords[j]}\")\n",
    "    \n",
    "\n",
    "    # Loading Texts\n",
    "    texts = []\n",
    "    number_works = 0\n",
    "    number_unfiltered_works = 0\n",
    "    number_keywords = len(keywords)\n",
    "    \n",
    "    for k, keyword in enumerate(keywords):\n",
    "        print(f\"({k + 1}/{number_keywords}) Searching Keyword '{keyword}'\")\n",
    "        pager = Works().search_filter(title=keyword).paginate(per_page=200)\n",
    "        for page in pager:\n",
    "            for work in page:\n",
    "                number_unfiltered_works += 1\n",
    "                \n",
    "                title = work['title']\n",
    "                abstract = work['abstract']\n",
    "                doi = work['doi']\n",
    "                \n",
    "                # Find Full Text\n",
    "                url = None\n",
    "                if work[\"primary_location\"]:\n",
    "                    url = work[\"primary_location\"][\"pdf_url\"]\n",
    "                full_text = \"\" if not url else pdf_to_text(url)\n",
    "                \n",
    "                if not abstract and not full_text:\n",
    "                    continue\n",
    "                texts.append((number_works, title, doi, abstract if abstract and not full_text else full_text))\n",
    "                number_works += 1\n",
    "        k += 1\n",
    "        clear_output(wait=True)        \n",
    "\n",
    "    assert len(texts) == number_works\n",
    "    print(f\"Number Documents: {number_works}, Number Unfiltered Documents: {number_unfiltered_works}\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "41873874-541c-4576-bf2e-791b3d31d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def on_topic(text):\n",
    "    # Topic and Threshold\n",
    "    topics = [(\"ecology\", 0.75), (\"interaction\", 0.75)]\n",
    "\n",
    "    for topic, threshold in topics:\n",
    "        # Break Into Parts\n",
    "        chunks = []\n",
    "        chunk_length = len(text)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            chunk = text[i:i+chunk_length]\n",
    "    \n",
    "            # Ensure Full Words\n",
    "            j = i + chunk_length\n",
    "            while j < len(text) and text[j] != \" \":\n",
    "                chunk += text[j]\n",
    "                j += 1\n",
    "    \n",
    "            i = j\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "        # Classify\n",
    "        labels = [topic]\n",
    "        scores = {}\n",
    "        for label in labels:\n",
    "            scores[label] = 0\n",
    "            \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            result = classifier(chunk, labels)\n",
    "            for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
    "                scores[label] += score\n",
    "        \n",
    "        mean_score = np.mean(np.array(list(scores.values())) / len(chunks))\n",
    "        if mean_score < threshold:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9090ee67-7e4a-4fc7-b52b-ae6215464bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller:\n",
    "    def __init__(self):\n",
    "        # print(\"Loading SP_NLP\")\n",
    "        t0 = time.time()\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        t1 = time.time()\n",
    "        # print(f\"SP_NLP: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Loading TN_NLP\")\n",
    "        t0 = time.time()\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "        t1 = time.time()\n",
    "        # print(f\"TN_NLP: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Loading FCOREF\")\n",
    "        t0 = time.time()\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        t1 = time.time()\n",
    "        # print(f\"FCOREF: {t1-t0}s\")\n",
    "        \n",
    "        self.sp_doc = None\n",
    "        self.tn_doc = None\n",
    "        self.tk_map = None\n",
    "    \n",
    "    def update(self, doc):\n",
    "        self.sp_doc = doc\n",
    "        # print(\"Updating TN_DOC\")\n",
    "        t0 = time.time()\n",
    "        self.tn_doc = self.tn_nlp(doc.text)\n",
    "        t1 = time.time()\n",
    "        # print(f\"TN_DOC: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Updating TK_MAP\")\n",
    "        t0 = time.time()\n",
    "        self.tk_map = self.load_token_map()\n",
    "        t1 = time.time()\n",
    "        # print(f\"TK_MAP: {t1-t0}s\")\n",
    "\n",
    "    def load_token_map(self):\n",
    "        tk_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            tk_map[token.idx] = token.i\n",
    "        return tk_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "799ea3cf-f021-4de9-bf5c-1baa31fed886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species:\n",
    "    def __init__(self, controller):\n",
    "        self.controller = controller\n",
    "        self.species_spans = None\n",
    "        self.species_indices = None\n",
    "\n",
    "    def update(self):\n",
    "        if not self.controller.sp_doc or not self.controller.tn_doc:\n",
    "            return\n",
    "        # print(\"Updating Species Indices and Spans\")\n",
    "        t0 = time.time()\n",
    "        self.species_spans, self.species_indices = self.load_species_spans()\n",
    "        t1 = time.time()\n",
    "        # print(f\"Load Species Indices and Span: {t1-t0}s\")\n",
    "        \n",
    "    def load_species_spans(self):\n",
    "        spans = []\n",
    "        indices = []\n",
    "        for species_span in self.controller.tn_doc.ents:\n",
    "            l_species_idx = species_span[0].idx\n",
    "            r_species_idx = species_span[-1].idx\n",
    "            \n",
    "            if l_species_idx not in self.controller.tk_map or r_species_idx not in self.controller.tk_map:\n",
    "                raise Exception(\"Invalid Token\")\n",
    "                \n",
    "            l_species_i = self.controller.tk_map[l_species_idx]\n",
    "            r_species_i = self.controller.tk_map[r_species_idx]\n",
    "\n",
    "            span = self.controller.sp_doc[l_species_i:r_species_i+1]\n",
    "            spans.append(span)\n",
    "            indices += [token.i for token in span]\n",
    "        return (spans, indices)\n",
    "\n",
    "    def is_species(self, token):\n",
    "        return token.i in self.species_indices\n",
    "        \n",
    "    def has_species(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.species_indices:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7d864d6e-4d78-4ab2-a84c-1e24a66fe989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    def __init__(self, controller, literals, pos_types, threshold=0.7):\n",
    "        self.controller = controller\n",
    "        self.literals = literals\n",
    "        self.threshold = threshold\n",
    "        self.pos_types = pos_types\n",
    "        self.keywords = [self.controller.sp_nlp(literal) for literal in self.literals]\n",
    "        self.keyword_indices = []\n",
    "\n",
    "    def update(self):\n",
    "        if not self.controller.sp_doc or not self.controller.sp_nlp:\n",
    "            return\n",
    "        # print(\"Updating Keyword Indices\")\n",
    "        t0 = time.time()\n",
    "        self.keyword_indices = self.load_keyword_indices()\n",
    "        t1 = time.time()\n",
    "        # print(f\"Keyword Indices: {t1-t0}s\")\n",
    "        \n",
    "    def is_keyword(self, token):\n",
    "        return token.i in self.keyword_indices\n",
    "\n",
    "    def has_keyword(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.keyword_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_keyword_indices(self):\n",
    "        indices = []\n",
    "        for token in self.controller.sp_doc:\n",
    "            if token.pos_ not in self.pos_types or self.do_not_check(token):\n",
    "                continue\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.literals:\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.controller.sp_nlp(token.lemma_)\n",
    "            for keyword in self.keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > self.threshold:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def find_keyword_indices(self, tokens):\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token.pos_ not in self.pos_types or self.do_not_check(token):\n",
    "                continue\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.literals:\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.controller.sp_nlp(token.lemma_)\n",
    "            for keyword in self.keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > self.threshold:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def do_not_check(self, token):\n",
    "        return len(token) <= 5 or re.match('^[\\w]+$', token.text) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4348c355-0b76-4b59-a72a-8ce8766ff2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, controller):\n",
    "        super().__init__(controller, {\"increase\", \"decrease\", \"change\", \"shift\", \"cause\", \"produce\"}, [\"NOUN\", \"VERB\"], 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b389f8c8-8b1a-4fb9-9676-fea3cf8f01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class References:\n",
    "    def __init__(self, controller, texts=None):\n",
    "        self.controller = controller\n",
    "        self.predictions = None\n",
    "        self.cluster_map = None\n",
    "        self.text_size_in_tokens = 100\n",
    "        if texts:\n",
    "            self.update(texts)\n",
    "\n",
    "    def update(self, text):\n",
    "        if not self.controller.sp_doc:\n",
    "            return\n",
    "        # print(\"Updating Predictions\")\n",
    "        t0 = time.time()\n",
    "        texts = []\n",
    "        offsets = []\n",
    "        for i in range(0, len(self.controller.sp_doc), self.text_size_in_tokens):\n",
    "            texts.append(self.controller.sp_doc[i:i+self.text_size_in_tokens].text)\n",
    "            offsets.append(self.controller.sp_doc[i].idx)\n",
    "        self.predictions = self.controller.fcoref.predict(texts=texts)\n",
    "        t1 = time.time()\n",
    "        # print(f\"Predictions: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Updating Cluster Map\")\n",
    "        t0 = time.time()\n",
    "        self.cluster_map = self.load_cluster_map(self.predictions, offsets)\n",
    "        t1 = time.time()\n",
    "        # print(f\"Cluster Map: {t1-t0}s\")\n",
    "\n",
    "    def load_cluster_map(self, predictions, offsets):\n",
    "        cluster_map = {}\n",
    "        for prediction, offset in zip(predictions, offsets):\n",
    "            clusters = prediction.get_clusters(as_strings=False)\n",
    "            for cluster in clusters:\n",
    "                # Converting Spans to Tokens\n",
    "                token_cluster = []\n",
    "                for span in cluster:\n",
    "                    if not span:\n",
    "                        continue\n",
    "                    index = span[0] + offset\n",
    "                    if index not in self.controller.tk_map:\n",
    "                        continue\n",
    "                        # raise Exception(\"Invalid Token\")\n",
    "                    index = self.controller.tk_map[index]\n",
    "                    token_cluster.append(self.controller.sp_doc[index])\n",
    "                # Mapping\n",
    "                for token in token_cluster:\n",
    "                    cluster_map[token.i] = list(filter(lambda t: t != token, token_cluster))\n",
    "        return cluster_map\n",
    "            \n",
    "    def get_references(self, tokens):\n",
    "        refs = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.cluster_map:\n",
    "                refs += self.cluster_map[index]\n",
    "        return refs\n",
    "\n",
    "    def same_reference(self, token_a, token_b):\n",
    "        if token_a.lemma_.lower() == token_b.lemma_.lower():\n",
    "            return True\n",
    "        if token_a.i in self.cluster_map and token_b in self.cluster_map[token_a.i]:\n",
    "            return True\n",
    "        if token_b.i in self.cluster_map and token_a in self.cluster_map[token_b.i]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def same_reference_span(self, span_a, span_b):\n",
    "        if span_a.text.lower() == span_b.text.lower():\n",
    "            return True\n",
    "        for token_a in span_a:\n",
    "            for token_b in span_b:\n",
    "                if self.same_reference(token_a, token_b):\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c9d48c9e-1ffa-4a71-b537-c5f2aed3ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scanner():\n",
    "    def __init__(self, controller):\n",
    "        # Helpers\n",
    "        self.controller = controller\n",
    "        self.species = Species(self.controller)\n",
    "        self.references = References(self.controller)\n",
    "        self.changes = ChangeKeywords(self.controller)\n",
    "\n",
    "        # Used to Evaluate Points\n",
    "        self.level1 = {\"ecological\", \"community\", \"interaction\", \"trait\", \"ecosystem\", \"ecology\"}\n",
    "        self.level2 = {\"model\"}\n",
    "        self.level3 = {\"predator\", \"prey\", \"competitor\", \"resource\", \"predation\", \"specie\", \"result\", \"effect\", \"population\", \"species\", \"invade\", \"presence\"}\n",
    "\n",
    "    def update(self, doc):\n",
    "        self.controller.update(doc)\n",
    "        self.references.update(doc.text)\n",
    "        self.species.update()\n",
    "\n",
    "    def get_full_species(self):\n",
    "        full_species = [*self.species.species_spans]\n",
    "        full_indices = [*self.species.species_indices]\n",
    "        \n",
    "        for k, v in self.references.cluster_map.items():\n",
    "            if k in self.species.species_indices:\n",
    "                for token in v:\n",
    "                    if token.i not in full_indices:\n",
    "                        token_span = self.controller.sp_doc[token.i:token.i+1]\n",
    "                        full_species.append(token_span)\n",
    "                        full_indices.append(token.i)\n",
    "            if self.species.has_species(v):\n",
    "                if k not in full_indices:\n",
    "                    token_span = self.controller.sp_doc[k:k+1]\n",
    "                    full_species.append(token_span)\n",
    "                    full_indices.append(k)\n",
    "        \n",
    "        return (full_species, full_indices)\n",
    "\n",
    "    def get_points(self):\n",
    "        points = 0\n",
    "\n",
    "        # Species Work\n",
    "        visited_species = {}\n",
    "        species_spans, species_indices = self.get_full_species()\n",
    "        \n",
    "        for species_span in species_spans:\n",
    "            # Adjusting Species if Vague\n",
    "            if species_span[0].text.lower() in [\"species\"]:\n",
    "                i = species_span[0].i\n",
    "                j = i\n",
    "                while j > 0:\n",
    "                    prev_token = self.controller.sp_doc[j-1]\n",
    "                    if prev_token.pos_ not in [\"NOUN\", \"ADJ\", \"PROPN\", \"SYM\"]:\n",
    "                        break\n",
    "                    species_span = self.controller.sp_doc[j-1:i+1]\n",
    "                    # print(f\"Adjusted Species: {species_span}\")\n",
    "                    j -= 1\n",
    "                \n",
    "            # Repeating Species\n",
    "            past_visits = 0\n",
    "            for sp in visited_species.keys():\n",
    "                visited_span = self.controller.sp_doc[sp[0]:sp[1]+1]\n",
    "                if self.references.same_reference_span(species_span, visited_span):\n",
    "                    past_visits = visited_species[sp]\n",
    "                    visited_species[sp] += 1\n",
    "                    break\n",
    "            if past_visits == 0:\n",
    "                visited_species[(species_span[0].i, species_span[-1].i)] = 1\n",
    "            if past_visits > 50:\n",
    "                continue\n",
    "                    \n",
    "            li = species_span[0].sent.start\n",
    "            ri = species_span[-1].sent.end\n",
    "\n",
    "            sli = species_span[0].i\n",
    "            sri = species_span[-1].i\n",
    "\n",
    "            l_token_indices = set([token.i for token in self.controller.sp_doc[li:sli]])\n",
    "            r_token_indices = set([token.i for token in self.controller.sp_doc[sri+1:ri]])\n",
    "\n",
    "            # Nearby Actions (Modification)\n",
    "            change_indices = set(self.changes.find_keyword_indices(self.controller.sp_doc[li:ri]))\n",
    "            l_changes = l_token_indices.intersection(change_indices)\n",
    "            r_changes = r_token_indices.intersection(change_indices)\n",
    "\n",
    "            # There must be a change.\n",
    "            if not l_changes and not r_changes:\n",
    "                continue\n",
    "                \n",
    "            # Nearby Species (Interaction)\n",
    "            l_species = l_token_indices.intersection(species_indices)\n",
    "            r_species = r_token_indices.intersection(species_indices)\n",
    "\n",
    "            if l_changes or r_changes:\n",
    "                points += 10 * (past_visits + 1)\n",
    "                change_found = True\n",
    "            if l_species or r_species:\n",
    "                points += 10 * (past_visits + 1)\n",
    "                other_species_found = True\n",
    "            points += min(past_visits * 10, 100)\n",
    "            \n",
    "        print(f\"Species Points After: {points}\")\n",
    "        \n",
    "        # Adjustments\n",
    "        points /= len(list(self.controller.sp_doc.sents))\n",
    "        print(f\"Adjust Points After: {points}\")\n",
    "\n",
    "        print(f\"Visited Species: {visited_species}\")\n",
    "        if len(visited_species) < 3:\n",
    "            return 0\n",
    "        return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d1e50093-6889-4ddd-bd5e-f58992413644",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # (1) Load and (2) Filter Documents by Topic\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# labeled_documents = list(filter(lambda d: on_topic(d[-1]), load_documents()))\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# documents = [d[-1] for d in labeled_documents]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Scan Documents\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m scanner \u001b[38;5;241m=\u001b[39m Scanner(\u001b[43mController\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     12\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[128], line 11\u001b[0m, in \u001b[0;36mController.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# print(f\"SP_NLP: {t1-t0}s\")\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# print(\"Loading TN_NLP\")\u001b[39;00m\n\u001b[0;32m     10\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtn_nlp \u001b[38;5;241m=\u001b[39m \u001b[43mTaxoNERD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_ner_eco_biobert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtagger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattribute_ruler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlemmatizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# print(f\"TN_NLP: {t1-t0}s\")\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(\"Loading FCOREF\")\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\taxonerd\\taxonerd.py:48\u001b[0m, in \u001b[0;36mTaxoNERD.load\u001b[1;34m(self, model, exclude, linker, neighbours, threshold)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     42\u001b[0m     model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[0;32m     47\u001b[0m ):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpysbd_sentencizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscispacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_sentence_segmenter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pysbd_sentencizer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\spacy\\util.py:465\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))()\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_package(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\spacy\\util.py:501\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mname (str): The package name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(name)\n\u001b[1;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\en_ner_eco_biobert\\__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_init_py(\u001b[38;5;18m__file__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\spacy\\util.py:682\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[1;32m--> 682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\spacy\\util.py:547\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    538\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[0;32m    539\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[0;32m    540\u001b[0m     config,\n\u001b[0;32m    541\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[0;32m    546\u001b[0m )\n\u001b[1;32m--> 547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\spacy\\language.py:2209\u001b[0m, in \u001b[0;36mLanguage.from_disk\u001b[1;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[0;32m   2206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     exclude \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(exclude) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m-> 2209\u001b[0m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserializers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   2210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m path  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m   2211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_link_components()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\spacy\\util.py:1390\u001b[0m, in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, reader \u001b[38;5;129;01min\u001b[39;00m readers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1388\u001b[0m     \u001b[38;5;66;03m# Split to support file names like meta.json\u001b[39;00m\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:\n\u001b[1;32m-> 1390\u001b[0m         \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\spacy\\language.py:2203\u001b[0m, in \u001b[0;36mLanguage.from_disk.<locals>.<lambda>\u001b[1;34m(p, proc)\u001b[0m\n\u001b[0;32m   2201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_disk\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2202\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 2203\u001b[0m     deserializers[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m p, proc\u001b[38;5;241m=\u001b[39mproc: \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[misc]\u001b[39;49;00m\n\u001b[0;32m   2204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvocab\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     exclude \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(exclude) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy_transformers\\pipeline_component.py:416\u001b[0m, in \u001b[0;36mTransformer.from_disk\u001b[1;34m(self, path, exclude)\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, hf_model)\n\u001b[0;32m    411\u001b[0m deserialize \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mfrom_disk,\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcfg\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mupdate(deserialize_config(p)),\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: load_model,\n\u001b[0;32m    415\u001b[0m }\n\u001b[1;32m--> 416\u001b[0m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserialize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\spacy\\util.py:1390\u001b[0m, in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, reader \u001b[38;5;129;01min\u001b[39;00m readers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1388\u001b[0m     \u001b[38;5;66;03m# Split to support file names like meta.json\u001b[39;00m\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:\n\u001b[1;32m-> 1390\u001b[0m         \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy_transformers\\pipeline_component.py:390\u001b[0m, in \u001b[0;36mTransformer.from_disk.<locals>.load_model\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(p, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m mfile:\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE149) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\thinc\\model.py:638\u001b[0m, in \u001b[0;36mModel.from_bytes\u001b[1;34m(self, bytes_data)\u001b[0m\n\u001b[0;32m    636\u001b[0m msg \u001b[38;5;241m=\u001b[39m srsly\u001b[38;5;241m.\u001b[39mmsgpack_loads(bytes_data)\n\u001b[0;32m    637\u001b[0m msg \u001b[38;5;241m=\u001b[39m convert_recursive(is_xp_array, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39masarray, msg)\n\u001b[1;32m--> 638\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\thinc\\model.py:676\u001b[0m, in \u001b[0;36mModel.from_dict\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    674\u001b[0m         node\u001b[38;5;241m.\u001b[39mset_param(param_name, value)\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, shim_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshims\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]):\n\u001b[1;32m--> 676\u001b[0m         \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshims\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshim_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy_transformers\\layers\\hf_shim.py:90\u001b[0m, in \u001b[0;36mHFShim.from_bytes\u001b[1;34m(self, bytes_data)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, bytes_data):\n\u001b[1;32m---> 90\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[43msrsly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmsgpack_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     92\u001b[0m     tok_dict \u001b[38;5;241m=\u001b[39m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\srsly\\_msgpack_api.py:27\u001b[0m, in \u001b[0;36mmsgpack_loads\u001b[1;34m(data, use_list)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# msgpack-python docs suggest disabling gc before unpacking large messages\u001b[39;00m\n\u001b[0;32m     26\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m---> 27\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mmsgpack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m gc\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m msg\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\srsly\\msgpack\\__init__.py:85\u001b[0m, in \u001b[0;36munpackb\u001b[1;34m(packed, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         object_hook \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(decoder, chain\u001b[38;5;241m=\u001b[39mobject_hook)\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m object_hook\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _unpackb(packed, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\3.10\\lib\\site-packages\\srsly\\msgpack\\_unpacker.pyx:206\u001b[0m, in \u001b[0;36msrsly.msgpack._unpacker.unpackb\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # (1) Load and (2) Filter Documents by Topic\n",
    "# labeled_documents = list(filter(lambda d: on_topic(d[-1]), load_documents()))\n",
    "# documents = [d[-1] for d in labeled_documents]\n",
    "# print(f\"Number of Documents: {len(documents)}\")\n",
    "\n",
    "# # Save Documents\n",
    "# with open('documents.pickle', 'wb') as file:\n",
    "#     pickle.dump(labeled_documents, file)\n",
    "\n",
    "# Load Documents\n",
    "with open('documents.pickle', 'rb') as file:\n",
    "    labeled_documents = pickle.load(file)\n",
    "    documents = [d[-1] for d in labeled_documents]\n",
    "    print(f\"Number of Documents: {len(documents)}\")\n",
    "\n",
    "# Scan Documents\n",
    "scanner = Scanner(Controller())\n",
    "i = 0\n",
    "output = []\n",
    "for doc in scanner.controller.sp_nlp.pipe(documents):\n",
    "    print(f\"{i+1}/{len(documents)}\")\n",
    "    # t0 = time.time()\n",
    "    scanner.update(doc)\n",
    "    # t1 = time.time()\n",
    "    # print(f\"Total Time: {t1-t0}\")\"\n",
    "    \n",
    "    points = scanner.get_points()\n",
    "    output.append((labeled_documents[i][0], labeled_documents[i][1], labeled_documents[i][2], points))\n",
    "    # print(f\"'{labeled_documents[i][0]}' Points: {points}\\n\")\n",
    "    i += 1\n",
    "    clear_output(wait=True)\n",
    "\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "693f8cc0-5036-4d41-a563-f8e255a22cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 2072 Ecology, 82(7), 2001, pp. 20722081 q 2001 by the Ecological Society of America EFFECTS OF TOP PREDATOR SPECIES ON DIRECT AND INDIRECT INTERACTIONS IN A FOOD WEB OSWALD J. SCHMITZ1 AND K. BLAKE SUTTLE2 Yale University, School of Forestry and Environmental Studies and Department of Ecology and Evolutionary Biology, New Haven, Connecticut 06511 USA Abstract. Current theory on trophic interactions in food webs assumes that ecologically similar species can be treated collectively as a single f'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # (1) Load and (2) Filter Documents by Topic\n",
    "# # documents = list(filter(lambda d: on_topic(d), load_local_documents()))\n",
    "# print(f\"Number of Documents: {len(documents)}\")\n",
    "\n",
    "# scanner = Scanner(Controller())\n",
    "# i = 0\n",
    "# output = []\n",
    "# for doc in scanner.controller.sp_nlp.pipe(documents):\n",
    "#     print(f\"{i+1}/{len(documents)}\")\n",
    "#     scanner.update(doc)\n",
    "    \n",
    "#     points = scanner.get_points()\n",
    "#     output.append((i, points))\n",
    "#     print(f\"'{i}' Points: {points}\\n\")\n",
    "#     i += 1\n",
    "#     # clear_output(wait=True)\n",
    "    \n",
    "# # clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c25172-f317-4a60-83a8-e78c372cdaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.sort(key=lambda d: d[-1], reverse=True)\n",
    "\n",
    "# data = [[\"Index\", \"Points\"], *output]\n",
    "data = [[\"Index\", \"Title\", \"DOI\", \"Points\"], *output]\n",
    "with open('test_output.csv', 'w', encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
