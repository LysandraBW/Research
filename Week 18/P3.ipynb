{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbecf5be-df89-4d1a-b75f-d92a5c5fff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trait-Mediated Interaction Modification\n",
    "# Empirical:\n",
    "# You could look for words like \"hypothesis\", \"experiment\", \"found\", and \"discovered\". That may point\n",
    "# towards there being an experiment in the paper. There are also words like \"control group\", \"compared\",\n",
    "# \"findings\", \"results\", \"study\", and more.\n",
    "# Qualitative vs. Quantitative:\n",
    "# To infer whether something is quantitative, you could look for numeric tokens and units.\n",
    "# However, you can only do so much with the abstract. Therefore, this is likely not good enough.\n",
    "# Yet, you could still take advantage of words like \"fewer\" and \"increased\" to show that there is a change.\n",
    "# However, this would be more suited for the above category.\n",
    "# Traits:\n",
    "# There is no NLP tool for traits that I can use or create so I think that I could instead use keywords.\n",
    "# For example, \"snail feeding rates\" is a trait. You may be able to spot this by looking for a word like\n",
    "# \"rate\". You'd expand that word to include \"snail feeding rates\". As \"snail\" is a species you can infer\n",
    "# that \"rates\" is a trait. I would be more decisive and use a dependency parser to ensure that the trait\n",
    "# is a property of the species (like before). However, with all the cases that may exist, I think checking\n",
    "# to see whether a species can be found by traveling back and/or forward without finding certain tokens could\n",
    "# work well enough.\n",
    "# 3 Species or More:\n",
    "# This is simple. However, I think using a dictionary and TaxoNerd would be beneficial (for higher accuracy).\n",
    "# To handle the potential differences in tokenization, character offsets should be used.\n",
    "# Standardization:\n",
    "# There is a lot of variance in the scores. To squash this issue, I think that we could assign each sentence\n",
    "# a value from 0 to 1. We would add these values and divide by the number of sentences. This would result in\n",
    "# a number that is also from 0 to 1. However, there are categories that we would like to inspect. So, we must\n",
    "# create an overall score in the interval from [0, 1] while also scoring each category. Well, for each sentence\n",
    "# we could add a point for each category that is observed. The sentence would receive said score divided by the\n",
    "# number of categories. At the end, we add up all the sentence scores and divide by the number of sentences.\n",
    "# The aggregate score for each category would also be divided by the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "637ccb94-4c7d-49b1-a727-f6e25b021357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from spacy.language import Language\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "%run -i \"./utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0612988-8cd7-4322-b98a-dbb5dc7d1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Help:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        # Zero Plurals\n",
    "        # The singular and plural versions of the words below are the same.\n",
    "        self.zero_plurals = [\n",
    "            \"species\", \n",
    "            \"deer\", \n",
    "            \"fish\", \n",
    "            \"moose\", \n",
    "            \"sheep\", \n",
    "            \"swine\", \n",
    "            \"buffalo\", \n",
    "            \"trout\", \n",
    "            \"cattle\"\n",
    "        ]\n",
    "        # Irregular Nouns\n",
    "        # There's not a defined conversion method.\n",
    "        self.irregular_nouns = {\n",
    "            \"ox\": \"oxen\",\n",
    "            \"goose\": \"geese\",\n",
    "            \"mouse\": \"mice\",\n",
    "            \"bacterium\": \"bacteria\"\n",
    "        }\n",
    "        self.irregular_nouns_rev = {v: k for k, v in self.irregular_nouns.items()}\n",
    "        self.irregular_singular_nouns = self.irregular_nouns.keys()\n",
    "        self.irregular_plural_nouns = self.irregular_nouns.values()\n",
    "\n",
    "    def remove_extra_spaces(self, string):\n",
    "        # Remove Duplicate Spaces\n",
    "        string = re.sub(r\"\\s+\", \" \", string)\n",
    "        # Remove Spaces Before Punctuation\n",
    "        string = re.sub(r\"\\s+([?.!,])\", r\"\\1\", string)\n",
    "        # Remove Outside Spaces\n",
    "        return string.strip()\n",
    "\n",
    "    def remove_outer_non_alnum(self, string):\n",
    "        while string:\n",
    "            start_len = len(string)\n",
    "            # Remove Leading Non-Alphanumeric Character\n",
    "            if string and not string[0].isalnum():\n",
    "                string = string[1:]\n",
    "            # Remove Trailing Non-Alphanumeric Character\n",
    "            if string and not string[-1].isalnum():\n",
    "                string = string[:-1]\n",
    "            # No Changes Made\n",
    "            if start_len == len(string):\n",
    "                break\n",
    "        return string\n",
    "\n",
    "    def group_text(self, text, flatten=False):\n",
    "        # The parenthetical would be the content inside of a pair of\n",
    "        # matching parentheses, brackets, or braces.\n",
    "        parentheticals = []\n",
    "        \n",
    "        # This contains the text that's not inside of\n",
    "        # parentheses and co.\n",
    "        base_text = []\n",
    "        \n",
    "        # Used for building groups,\n",
    "        # handles a nested structure.\n",
    "        stacks = []\n",
    "        \n",
    "        # These are the characters we recognize\n",
    "        # in terms of grouping.\n",
    "        pairs = {\n",
    "            \"(\": \")\",\n",
    "            \"[\": \"]\",\n",
    "            \"{\": \"}\"\n",
    "        }\n",
    "        open_chars = pairs.keys()\n",
    "        close_chars = pairs.values()\n",
    "        \n",
    "        # This contains the opening characters\n",
    "        # of the groups that are currently open\n",
    "        # (e.g. '(', '['). We use it so that we know\n",
    "        # whether we open or close a group.\n",
    "        opened = []\n",
    "        \n",
    "        for i, char in enumerate(text):\n",
    "            # Opening Character\n",
    "            if char in open_chars:\n",
    "                stacks.append([])\n",
    "                opened.append(char)\n",
    "            # Closing Character\n",
    "            elif opened and char == pairs.get(opened[-1], \"\"):\n",
    "                parentheticals.append(stacks.pop())\n",
    "                opened.pop()\n",
    "            # Add Character to Group\n",
    "            elif opened:\n",
    "                stacks[-1].append(i)\n",
    "            # Add Character to Ungrouped Text\n",
    "            else:\n",
    "                base_text.append(i)\n",
    "        \n",
    "        # If an opening character hasn't been closed,\n",
    "        # we just close all the remaining opened groups.\n",
    "        # This is moreso a problem regarding the text.\n",
    "        while stacks:\n",
    "            parentheticals.append(stacks.pop())\n",
    "            \n",
    "        # Merge\n",
    "        groups = [*parentheticals, base_text]\n",
    "        tuple_groups = []\n",
    "        for group in groups:\n",
    "            if not group:\n",
    "                continue\n",
    "            \n",
    "            tuples = [[group[0], group[0] + 1]]\n",
    "            for index in group[1:]:\n",
    "                if tuples[-1][1] == index:\n",
    "                    tuples[-1][1] = index + 1\n",
    "                else:\n",
    "                    tuples.append([index, index + 1])\n",
    "            tuple_groups.append(tuples)\n",
    "            \n",
    "        if flatten:\n",
    "            flattened_tuple_groups = []\n",
    "            for tuple_group in tuple_groups:\n",
    "                for tuple in tuple_group:\n",
    "                    flattened_tuple_groups.append(tuple)\n",
    "            tuple_groups = flattened_tuple_groups\n",
    "        \n",
    "        return tuple_groups\n",
    "\n",
    "    def singularize(self, string):\n",
    "        string = string.lower()\n",
    "        \n",
    "        # The string to singularize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        if not words:\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is a zero plural\n",
    "        # or a singular irregular noun, there's no changes\n",
    "        # to make. For example, \"red sheep\" and \"ox\" are \n",
    "        # already singular.\n",
    "        if (\n",
    "            words[-1] in self.zero_plurals or \n",
    "            words[-1] in self.irregular_singular_nouns\n",
    "        ):\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is an irregular\n",
    "        # plural noun, we rely on a dictionary with the\n",
    "        # corresponding mapping.\n",
    "        if words[-1] in self.irregular_plural_nouns:\n",
    "            words[-1] = self.irregular_nouns_rev[words[-1]]\n",
    "            return [self.remove_extra_spaces(\" \".join(words))]\n",
    "        \n",
    "        singulars = []\n",
    "\n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to error), we need to\n",
    "        # handle them all.\n",
    "        singular_forms = self.singular_form(words[-1])\n",
    "\n",
    "        if not singular_forms:\n",
    "            return [string]\n",
    "        \n",
    "        for singular_form in singular_forms:\n",
    "            singular = self.remove_extra_spaces(\" \".join([*words[:-1], singular_form]))\n",
    "            singulars.append(singular)\n",
    "            \n",
    "        return singulars\n",
    "\n",
    "    def singular_form(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Change -ies to -y\n",
    "        if re.fullmatch(r\".*ies$\", string):\n",
    "            versions.append(f'{string[:-3]}y')\n",
    "            return versions\n",
    "\n",
    "        # Change -ves to -f and -fe\n",
    "        if re.fullmatch(r\".*ves$\", string):\n",
    "            versions.append(f'{string[:-3]}f')\n",
    "            versions.append(f'{string[:-3]}fe')\n",
    "            return versions\n",
    "\n",
    "        # Remove -es \n",
    "        if re.fullmatch(r\".*es$\", string):\n",
    "            versions.append(f'{string[:-2]}')\n",
    "            return versions\n",
    "\n",
    "        # Change -i to -us\n",
    "        if re.fullmatch(r\".*i$\", string):\n",
    "            versions.append(f'{string[:-1]}us')\n",
    "            return versions\n",
    "\n",
    "        # Remove -s\n",
    "        if re.fullmatch(r\".*s$\", string):\n",
    "            versions.append(f'{string[:-1]}')\n",
    "            return versions\n",
    "\n",
    "        return versions\n",
    "\n",
    "    def pluralize(self, string):\n",
    "        string = string.lower()\n",
    "        \n",
    "        # The string to pluralize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        if not words:\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is a zero plural\n",
    "        # or a plural irregular noun, there's no changes\n",
    "        # to make. For example, \"red sheep\" and \"oxen\" are \n",
    "        # already singular.\n",
    "        if (\n",
    "            words[-1] in self.zero_plurals or \n",
    "            words[-1] in self.irregular_plural_nouns\n",
    "        ):\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is an irregular\n",
    "        # singular noun, we rely on a dictionary with the\n",
    "        # corresponding mapping.\n",
    "        if words[-1] in self.irregular_singular_nouns:\n",
    "            words[-1] = self.irregular_nouns[words[-1]]\n",
    "            return [self.remove_extra_spaces(\" \".join(words))]\n",
    "        \n",
    "        plurals = []\n",
    "        \n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to error), we need to\n",
    "        # handle them all.\n",
    "        plural_forms = self.plural_form(words[-1])\n",
    "\n",
    "        if not plural_forms:\n",
    "            return [string]\n",
    "            \n",
    "        for plural_form in plural_forms:\n",
    "            plural = self.remove_extra_spaces(\" \".join([*words[:-1], plural_form]))\n",
    "            plurals.append(plural)\n",
    "            \n",
    "        return plurals\n",
    "        \n",
    "    def plural_form(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Words that end with -us often have\n",
    "        # two different plural versions: -es and -i.\n",
    "        # For example, the plural version of cactus \n",
    "        # can be cactuses or cacti.\n",
    "        if re.fullmatch(r\".*us$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            versions.append(f'{string[:-2]}i')\n",
    "            return versions\n",
    "\n",
    "        # The -es ending is added to the words below.\n",
    "        if re.fullmatch(r\".*([^l]s|sh|ch|x|z)$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # Words that end with a consonant followed by 'y'\n",
    "        # are made plural by replacing the 'y' with -ies.\n",
    "        # For example, the plural version of canary is\n",
    "        # canaries.\n",
    "        if re.fullmatch(r\".*([^aeiou])(y)$\", string):\n",
    "            versions.append(f'{string[:-1]}ies')\n",
    "            return versions\n",
    "            \n",
    "        # The plural version of words ending with -f\n",
    "        # and -fe aren't clear. To be safe, I will add\n",
    "        # both versions.\n",
    "        if (re.fullmatch(r\".*(f)(e?)$\", string) and not re.fullmatch(r\".*ff$\", string)):\n",
    "            last_clean = re.sub(r\"(f)(e?)$\", \"\", string)\n",
    "            versions.append(f'{last_clean}fs')\n",
    "            versions.append(f'{last_clean}ves')\n",
    "            return versions\n",
    "\n",
    "        # People add -s or -es to words that end with 'o'.\n",
    "        # To be safe, both versions are added.\n",
    "        if re.fullmatch(r\".*([^aeiou])o$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # If there's no -s at the end of the string and\n",
    "        # the other cases didn't run, we add an -s.\n",
    "        if re.fullmatch(r\".*[^s]$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "        \n",
    "        return versions\n",
    "\n",
    "    def expand_unit(self, *, il_unit, ir_unit, il_boundary, ir_boundary, speech=[], literals=[], include=True, direction='BOTH', verbose=False):\n",
    "        assert il_unit <= ir_unit\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            assert il_boundary <= il_unit\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            assert ir_boundary >= ir_unit\n",
    "        \n",
    "        # Move Left\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            # The indices are inclusive, therefore, when \n",
    "            # the condition fails, il_unit will be equal\n",
    "            # to il_boundary.\n",
    "            while il_unit > il_boundary:\n",
    "                # We assume that the current token is allowed,\n",
    "                # and look to the token to the left.\n",
    "                l_token = self.main.sp_doc[il_unit-1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = l_token.pos_ in speech or l_token.lower_ in literals\n",
    "\n",
    "                # Case 1: include=False, in_set=True\n",
    "                # If we're not meant to include the defined tokens, and the\n",
    "                # current token is in that set, we stop expanding.\n",
    "                # Case 2: include=True, in_set=False\n",
    "                # If we're meant to include the defined tokens, and the current\n",
    "                # token is not in that set, we stop expanding.\n",
    "                # Case 3: include=in_set\n",
    "                # If we're meant to include the defined tokens, and the current\n",
    "                # token is in that set, we continue expanding. If we're not meant\n",
    "                # to include the defined tokens, and the current token is not\n",
    "                # in that set, we continue expanding.\n",
    "                if include ^ in_set:\n",
    "                    break\n",
    "                \n",
    "                # Else, the left token is valid, and\n",
    "                # we continue to expand.\n",
    "                il_unit -= 1\n",
    "\n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            # Likewise, when the condition fails,\n",
    "            # ir_unit will be equal to the ir_boundary.\n",
    "            # The ir_boundary is also inclusive.\n",
    "            while ir_unit < ir_boundary:\n",
    "                # Assuming that the current token is valid,\n",
    "                # we look to the right to see if we can\n",
    "                # expand.\n",
    "                r_token = self.main.sp_doc[ir_unit+1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = r_token.pos_ in speech or r_token.lower_ in literals\n",
    "                if include ^ in_set:\n",
    "                    break\n",
    "\n",
    "                # Else, the token is valid and\n",
    "                # we continue.\n",
    "                ir_unit += 1\n",
    "\n",
    "        assert il_unit >= il_boundary and ir_unit <= ir_boundary\n",
    "        expanded_unit = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        return expanded_unit\n",
    "\n",
    "    def contract_unit(self, *, il_unit, ir_unit, speech=[], literals=[], include=True, direction='BOTH', verbose=False):\n",
    "        if il_unit > ir_unit:\n",
    "            print(f\"il_unit of {il_unit} greater than ir_unit of {ir_unit}\")\n",
    "            return None\n",
    "        \n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit < ir_unit:\n",
    "                # We must check if the current token\n",
    "                # is not allowed. If it's not allowed,\n",
    "                # we contract (remove).\n",
    "                token = self.main.sp_doc[il_unit]\n",
    "\n",
    "                # The token is invalid, thus we stop\n",
    "                # contracting.\n",
    "                # include = True means that we want the tokens that match\n",
    "                # the speech and/or literals in the contracted unit.\n",
    "                # include = False means that we don't want the tokens that\n",
    "                # match the speech and/or literals in the contracted unit.\n",
    "                # Case 1: include = True, in_set = True\n",
    "                # We have a token that's meant to be included in the set.\n",
    "                # However, we're contracting, which means we would end up\n",
    "                # removing the token if we continue. Therefore, we break.\n",
    "                # Case 2: include = False, in_set = False\n",
    "                # We have a token that's not in the set which defines the\n",
    "                # tokens that aren't meant to be included. Therefore, we \n",
    "                # have a token that is meant to be included. If we continue,\n",
    "                # we would end up removing this token. Therefore, we break.\n",
    "                # Default:\n",
    "                # If we have a token that's in the set (in_set=True) of\n",
    "                # tokens we're not supposed to include in the contracted \n",
    "                # unit (include=False), we need to remove it. Likewise, if\n",
    "                # we have a token that's not in the set (in_set=False) of\n",
    "                # tokens to include in the contracted unit (include=True),\n",
    "                # we need to remove it.\n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include == in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid, thus we continue.\n",
    "                il_unit += 1\n",
    "\n",
    "        # Move Left      \n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit > il_unit:\n",
    "                token = self.main.sp_doc[ir_unit]\n",
    "\n",
    "                # The token is invalid and we\n",
    "                # stop contracting.\n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include == in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid and we continue.\n",
    "                ir_unit -= 1\n",
    "\n",
    "        assert il_unit <= ir_unit\n",
    "        contracted_unit = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        return contracted_unit\n",
    "\n",
    "    def find_unit_context(self, *, il_unit, ir_unit, il_boundary, ir_boundary, verbose=False):\n",
    "        assert il_unit <= ir_unit\n",
    "        assert il_boundary <= il_unit\n",
    "        assert ir_boundary >= ir_unit\n",
    "        \n",
    "        # Caveat: Parentheticals\n",
    "        # The context of a unit inside of parentheses should not\n",
    "        # go farther than the boundaries of those parentheses.\n",
    "        # However, we need to manually determine whether the unit\n",
    "        # is in parentheses (or any set of the matching symbols\n",
    "        # below).\n",
    "        matching_puncts = {\n",
    "            \"[\": \"]\", \n",
    "            \"(\": \")\", \n",
    "            \"-\": \"-\", \n",
    "            \"--\": \"--\",\n",
    "            \"{\": \"}\",\n",
    "            \",\": \",\"\n",
    "        }\n",
    "        \n",
    "        # The opening symbols for group punctuation.\n",
    "        opening_puncts = list(matching_puncts.keys())\n",
    "\n",
    "        # The closing symbols for group punctuation.\n",
    "        closing_puncts = list(matching_puncts.values())\n",
    "\n",
    "        # Both the opening and closing symbols above.\n",
    "        puncts = [*closing_puncts, *opening_puncts]\n",
    "\n",
    "        # Look for Group Punctuation on the Left\n",
    "        i = il_unit\n",
    "        l_punct = None\n",
    "        while i >= il_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.lower_ in puncts and token.lower_ != \",\":\n",
    "                l_punct = token\n",
    "                break\n",
    "            i -= 1\n",
    "\n",
    "        # Look for Group Punctuation on the Right\n",
    "        i = ir_unit + 1 if l_punct and il_unit == ir_unit else ir_unit\n",
    "        r_punct = None\n",
    "        while i <= ir_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.lower_ in puncts and token.lower_ != \",\":\n",
    "                r_punct = token\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        # If there's a group punctuation on the left\n",
    "        # and right, and they match each other (e.g. '(' and ')'),\n",
    "        # we return the text between the punctuations.\n",
    "        parenthetical = l_punct and r_punct and matching_puncts.get(l_punct.lower_, '') == r_punct.text\n",
    "        if parenthetical:\n",
    "            return self.main.sp_doc[l_punct.i:r_punct.i+1]\n",
    "\n",
    "        # As the unit is not a parenthetical, we will expand\n",
    "        # outwards until we run into a stopping token. The exclude\n",
    "        # list contains tokens that should be excluded from the\n",
    "        # context. Currently, it will contain any parentheticals\n",
    "        # that we run into.\n",
    "        exclude = []\n",
    "\n",
    "        # If a token's POS falls into these categories, we will\n",
    "        # continue. If not, we stop expanding.\n",
    "        speech = [\"ADJ\", \"NOUN\", \"ADP\", \"ADV\", \"PART\", \"PROPN\", \"VERB\", \"PRON\", \"DET\", \"AUX\", \"PART\", \"SCONJ\"]\n",
    "        \n",
    "        # Expand Left\n",
    "        while il_unit > il_boundary:\n",
    "            # Assuming that the current token is fine,\n",
    "            # we look to the left.\n",
    "            l_token = self.main.sp_doc[il_unit-1]\n",
    "\n",
    "            # If it's a closing punctuation (e.g. ')', ']'),\n",
    "            # we need to skip over whatever is contained in\n",
    "            # that punctuation.\n",
    "            if l_token.lower_ in closing_puncts:\n",
    "                i = il_unit - 1\n",
    "                \n",
    "                token = self.main.sp_doc[i]\n",
    "                exclude.append(token)\n",
    "\n",
    "                # We continue until we reach the boundary or\n",
    "                # we find the matching opening punctuation.\n",
    "                opening_punct_found = matching_puncts.get(token.lower_, '') == l_token.lower_\n",
    "                \n",
    "                while i > il_boundary and (not opening_punct_found or (opening_punct_found and l_token == token)):\n",
    "                    i -= 1\n",
    "                    token = self.main.sp_doc[i]\n",
    "                    exclude.append(token)\n",
    "                    opening_punct_found = matching_puncts.get(token.lower_, '') == l_token.lower_\n",
    "                \n",
    "                exclude.append(token)\n",
    "\n",
    "                # After we've gone past the parenthetical,\n",
    "                # we can jump to the next position.\n",
    "                il_unit = i\n",
    "                continue\n",
    "            # If it's not a closing punctuation, we check\n",
    "            # whether it's a stopping token\n",
    "            else:\n",
    "                if l_token.pos_ not in speech:\n",
    "                    break\n",
    "                else:\n",
    "                    il_unit -= 1\n",
    "\n",
    "        # Expand Right\n",
    "        while ir_unit < ir_boundary:\n",
    "            # We're checking the token to the right\n",
    "            # to see if we can expand or not.\n",
    "            r_token = self.main.sp_doc[ir_unit+1]\n",
    "            \n",
    "            # If the token to the right is an opening\n",
    "            # punctuation (e.g. '(', '['), we must skip\n",
    "            # it, the parenthetical inside, and the\n",
    "            # closing punctuation.\n",
    "            if r_token.lower_ in opening_puncts:\n",
    "                i = ir_unit + 1\n",
    "                \n",
    "                token = self.main.sp_doc[i]\n",
    "                exclude.append(token)\n",
    "\n",
    "                closing_punct_found = token.lower_ == matching_puncts.get(r_token.lower_, '')\n",
    "                \n",
    "                while i < ir_boundary and (not closing_punct_found or (closing_punct_found and r_token == token)):\n",
    "                    i += 1\n",
    "                    token = self.main.sp_doc[i]\n",
    "                    exclude.append(token)\n",
    "                    closing_punct_found = token.lower_ == matching_puncts.get(r_token.lower_, '')\n",
    "                \n",
    "                exclude.append(token)\n",
    "\n",
    "                ir_unit = i\n",
    "                continue\n",
    "            # If it's not an opening punctuation, we check\n",
    "            # whether we can continue expanding.\n",
    "            else:\n",
    "                if r_token.pos_ not in speech:\n",
    "                    break\n",
    "                else:\n",
    "                    ir_unit += 1\n",
    "        \n",
    "        # We remove the excluded tokens\n",
    "        # and return the context.\n",
    "        context = [t for t in self.main.sp_doc[il_unit:ir_unit+1] if t not in exclude]\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26736310-0964-4184-a9c1-f6b0b38d7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the Dictionary\n",
    "@Language.component(\"lower_case_lemmas\")\n",
    "def lower_case_lemmas(doc) :\n",
    "    for token in doc :\n",
    "        token.lemma_ = token.lemma_.lower()\n",
    "    return doc\n",
    "\n",
    "class Species:\n",
    "    def __init__(self, main):\n",
    "        # Tools\n",
    "        self.main = main\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\"])\n",
    "        self.tn_nlp.add_pipe(\"lower_case_lemmas\", after=\"lemmatizer\")\n",
    "        self.tn_doc = None\n",
    "        \n",
    "        # Contains any spans that have been identified\n",
    "        # as a species.\n",
    "        self.spans = None\n",
    "        \n",
    "        # Contains any tokens that have been identified\n",
    "        # as a species or being a part of a species.\n",
    "        self.tokens = None\n",
    "        \n",
    "        # Used to quickly access the span that a token\n",
    "        # belongs to.\n",
    "        self.token_to_span = None\n",
    "        \n",
    "        # Maps a string to an array of strings wherein\n",
    "        # the strings involved in the key-value pair \n",
    "        # have been identified as an alternate name of each other.\n",
    "        self.alternate_names = None\n",
    "        \n",
    "        # Used to increase TaxoNERD's accuracy.\n",
    "        self.dictionary = None\n",
    "        self.load_dictionary()\n",
    "\n",
    "    def load_dictionary(self):\n",
    "        self.dictionary = [\"juvenile\", \"adult\", \"prey\", \"predator\", \"species\", \"tree\", \"cat\", \"dog\"]\n",
    "        # df = pd.read_csv(\"VernacularNames.csv\")\n",
    "        # self.dictionary += df.VernacularName.to_list()\n",
    "\n",
    "        patterns = []\n",
    "        for name in self.dictionary:\n",
    "            doc = self.tn_nlp(name)\n",
    "            patterns.append({\"label\": \"LIVB\", \"pattern\": [{\"LEMMA\": token.lemma_} for token in doc]})\n",
    "        ruler = self.tn_nlp.add_pipe(\"entity_ruler\")\n",
    "        ruler.add_patterns(patterns)\n",
    "        \n",
    "    def update(self, text, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        self.spans, self.tokens, self.token_to_span, self.alternate_names = self.load_species(verbose=verbose)\n",
    "\n",
    "    def load_species(self, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # We'll search for species in the text.\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "\n",
    "        # These three contain the species that have been\n",
    "        # identified in the text. Tokens that aren't adjectives,\n",
    "        # nouns, or proper nouns will be stripped.\n",
    "        spans = []\n",
    "        tokens = []\n",
    "        token_to_span = {}\n",
    "\n",
    "        # It's useful to know if a different name refers to a\n",
    "        # species we have already seen. For example, in\n",
    "        # \"predatory crab (Carcinus maenas)\", \"predatory crab\"\n",
    "        # is an alternative name for \"Carcinus maenas\" and\n",
    "        # vice versa. This is used so that the species can be\n",
    "        # properly tracked and redundant points are less\n",
    "        # likely to be given.\n",
    "        alternate_names = {}\n",
    "\n",
    "        # We convert the spans that TaxoNerd has recognized\n",
    "        # to spans under a different parent document. This is\n",
    "        # because we're largely using said parent document and\n",
    "        # there is more functionality in that parent document.\n",
    "        species_spans = []\n",
    "        for tn_species_span in self.tn_doc.ents:\n",
    "            # print(f\"TN Species Span: {tn_species_span}\")\n",
    "            \n",
    "            char_i0 = self.tn_doc[tn_species_span.start].idx\n",
    "            char_i1 = char_i0 + len(tn_species_span.text) - 1\n",
    "\n",
    "            sp_token_i0 = self.main.token_at_char(char_i0).i\n",
    "            sp_token_i1 = self.main.token_at_char(char_i1).i\n",
    "\n",
    "            sp_species_span = self.main.sp_doc[sp_token_i0:sp_token_i1+1]\n",
    "            \n",
    "            # Although they have different parent documents,\n",
    "            # they should still have the same text.\n",
    "            if sp_species_span.text.lower() != tn_species_span.text.lower():\n",
    "                print(sp_species_span.text.lower(), tn_species_span.text.lower())\n",
    "            assert sp_species_span.text.lower() == tn_species_span.text.lower()\n",
    "\n",
    "            # Sometimes, TaxoNerd recognizes two names of a species in one span.\n",
    "            # If they're separated with parentheses, we can handle the case here.\n",
    "            # The naming is difficult, so I'll just call it species_tuples.\n",
    "            species_tuples = self.main.help.group_text(sp_species_span.text, flatten=True)\n",
    "\n",
    "            # print(f\"Species Tuples: {species_tuples}\")\n",
    "            \n",
    "            species_span_chunks = []\n",
    "            for species_tuple in species_tuples:\n",
    "                species_span_chunk_text = sp_species_span.text[species_tuple[0]:species_tuple[1]]\n",
    "                # print(f\"Species Tuple Text: {species_span_chunk_text}\")\n",
    "                \n",
    "                if species_span_chunk_text.isspace():\n",
    "                    continue\n",
    "                \n",
    "                group_char_i0 = char_i0 + species_tuple[0]\n",
    "                group_char_i1 = char_i0 + species_tuple[1] - 1\n",
    "\n",
    "                # Update L Index to Exclude Whitespace Characters\n",
    "                while text[group_char_i0].isspace():\n",
    "                    group_char_i0 += 1\n",
    "\n",
    "                # Update R Index to Exclude Whitespace Characters\n",
    "                while text[group_char_i1].isspace():\n",
    "                    group_char_i1 -= 1\n",
    "\n",
    "                group_token_i0 = self.main.token_at_char(group_char_i0).i\n",
    "                group_token_i1 = self.main.token_at_char(group_char_i1).i\n",
    "\n",
    "                # print(f\"Species Span Chunk Appended: {self.main.sp_doc[group_token_i0:group_token_i1+1]}\")\n",
    "                \n",
    "                species_span_chunks.append(self.main.sp_doc[group_token_i0:group_token_i1+1])\n",
    "\n",
    "            for species_span_chunk in species_span_chunks:\n",
    "                species_spans.append(species_span_chunk)\n",
    "    \n",
    "                # TaxoNERD will recognize the full species (i.e. \"brown squirrels\"),\n",
    "                # and we can use this to find more instances of a species in the text\n",
    "                # by extracting the last noun or proper noun from that span \n",
    "                # (i.e. \"squirrels\"). Now, we can find \"brown squirrels\" and \n",
    "                # \"squirrels\".\n",
    "                reversed_span = [t for t in species_span_chunk]\n",
    "                reversed_span.reverse()\n",
    "                for token in reversed_span:\n",
    "                    # print(f\"Token Reversed Span: {token}\")\n",
    "                    if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                        # print(f\"\\tADDED\")\n",
    "                        species_spans.append(self.main.sp_doc[token.i:token.i+1])\n",
    "                        break\n",
    "\n",
    "        # TaxoNerd sometimes recognizes one instance of a species\n",
    "        # and fails to recognize it elsewhere. To fix this, I'll\n",
    "        # search the text for all the species that TaxoNerd sees.\n",
    "        # This should resolve that issue. To make this more robust,\n",
    "        # I'll include the singular and plural versions of the\n",
    "        # recognized species. Furthermore, the species being used\n",
    "        # to search for other instances of species in the text will\n",
    "        # be called search_species. Using a database I downloaded,\n",
    "        # I've initialized search_species with a set of english\n",
    "        # vernacular names (e.g., \"dog\", \"cat\"). I'm removing it for\n",
    "        # now because there's seemingly a lot of bogus values.\n",
    "        # df = pd.read_csv(\"EnglishVernacularNames-2.csv\")\n",
    "        # search_species = df.Name.to_list()\n",
    "        search_species = [\"juvenile\", \"adult\", \"prey\", \"predator\", \"predators\", \"species\", \"tree\", \"cat\", \"dog\", \"flies\", \"plants\", \"plant\", \"fly\"]\n",
    "\n",
    "        for species_span in species_spans:\n",
    "            species_text = species_span.text.lower()\n",
    "            species_text = self.main.help.remove_extra_spaces(self.main.help.remove_outer_non_alnum(species_text))\n",
    "            \n",
    "            # print(f\"Species Text: {species_text}\")\n",
    "\n",
    "            # not [c for c in species_text if c.isalpha()]\n",
    "            if not species_text or not [c for c in species_text if c.isalpha()]:\n",
    "                # print(f\"Continued\")\n",
    "                continue\n",
    "            \n",
    "            search_species.append(species_text)\n",
    "\n",
    "            # Add Singular and/or Plural Version\n",
    "            if species_span[-1].pos_ == \"NOUN\":\n",
    "                # Plural\n",
    "                if species_span[-1].tag_ == \"NNS\":\n",
    "                    singular_species = self.main.help.singularize(species_text)\n",
    "                    search_species.extend(singular_species)\n",
    "                # Singular\n",
    "                if species_span[-1].tag_ == \"NN\":\n",
    "                    plural_species = self.main.help.pluralize(species_text)\n",
    "                    search_species.extend(plural_species)\n",
    "\n",
    "        # Now, we have the species to search for in the text.\n",
    "        search_species = list(set(search_species))\n",
    "        # print(f\"Search Species: {search_species}\")\n",
    "        \n",
    "        for species in search_species:\n",
    "            matches = re.finditer(re.escape(species), text, re.IGNORECASE)\n",
    "            \n",
    "            for char_i0, char_i1, matched_text in [(match.start(), match.end(), match.group()) for match in matches]:\n",
    "                # The full word must match, not just a substring inside of it.\n",
    "                # So, if the species we're looking for is \"ant\", only \"ant\"\n",
    "                # will match -- not \"pants\" or \"antebellum\". Therefore, the\n",
    "                # characters to the left and right of the matched string must be\n",
    "                # non-alphanumeric.\n",
    "                l_char_is_letter = char_i0 > 0 and text[char_i0-1].isalpha()\n",
    "                r_char_is_letter = char_i1 < len(text) and text[char_i1].isalpha()\n",
    "                \n",
    "                if l_char_is_letter or r_char_is_letter or not matched_text:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    sp_li = self.main.token_at_char(char_i0).i\n",
    "                    sp_ri = self.main.token_at_char(char_i1-1).i\n",
    "                except Exception as e:\n",
    "                    print(f\"Matched Text: '{matched_text}'\")\n",
    "                    print(e)\n",
    "                    continue\n",
    "\n",
    "                # This is the matched substring (which would be\n",
    "                # a species) as a span in the parent document.\n",
    "                species_span = self.main.sp_doc[sp_li:sp_ri+1]\n",
    "                \n",
    "                # Expand Species\n",
    "                # Let's say there's a word like \"squirrel\". That's a bit ambiguous. \n",
    "                # Is it a brown squirrel, a bonobo? If the species is possibly missing\n",
    "                # information (like an adjective to the left of it), we should expand\n",
    "                # in order to get a full picture of the species.\n",
    "                unclear_1 = len(species_span) == 1 and species_span[0].pos_ == \"NOUN\"\n",
    "                unclear_2 = species_span.start > 0 and self.main.sp_doc[species_span.start-1].pos_ in [\"ADJ\"]\n",
    "                \n",
    "                if unclear_1 or unclear_2:\n",
    "                    species_span = self.main.help.expand_unit(\n",
    "                        il_unit=species_span.start, \n",
    "                        ir_unit=species_span.end-1,\n",
    "                        il_boundary=0,\n",
    "                        ir_boundary=len(self.main.sp_doc),\n",
    "                        speech=[\"ADJ\", \"PROPN\"],\n",
    "                        literals=[\"-\"],\n",
    "                        include=True,\n",
    "                        direction=\"LEFT\",\n",
    "                        verbose=verbose\n",
    "                    )\n",
    "                \n",
    "                # Remove Outer Symbols\n",
    "                # There are times where a species is identified with a parenthesis\n",
    "                # nearby. Here, we remove that parenthesis (and any other symbols).\n",
    "                species_span = self.main.help.contract_unit(\n",
    "                    il_unit=species_span.start, \n",
    "                    ir_unit=species_span.end-1, \n",
    "                    speech=[\"PUNCT\", \"SYM\", \"DET\", \"PART\"],\n",
    "                    include=False,\n",
    "                    verbose=verbose\n",
    "                )\n",
    "\n",
    "                if not species_span:\n",
    "                    print(f\"Matched Text: '{matched_text}'\")\n",
    "                    print(char_i0)\n",
    "                    continue\n",
    "            \n",
    "                # A species must have a noun or a\n",
    "                # proper noun. This may help discard\n",
    "                # bogus results.\n",
    "                letter_found = False\n",
    "                for token in species_span:\n",
    "                    if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                        letter_found = True\n",
    "                        break\n",
    "\n",
    "                if not letter_found:\n",
    "                    continue\n",
    "\n",
    "                # Adding Species\n",
    "                spans.append(species_span)\n",
    "                for token in species_span:\n",
    "                    if token in tokens or token.pos_ in [\"PUNCT\", \"SYM\", \"DET\", \"PART\"]:\n",
    "                        continue\n",
    "                    tokens.append(token)\n",
    "                    token_to_span[token] = species_span\n",
    "\n",
    "        # Removing Duplicates and Sorting \n",
    "        spans = list({span.start: span for span in spans}.values())\n",
    "        spans.sort(key=lambda span: span.start)\n",
    "        \n",
    "        # Finding and Storing Alternative Names\n",
    "        for i, species_span in enumerate(spans):\n",
    "            # There's not a next species to\n",
    "            # evaluate.\n",
    "            if i + 1 >= len(spans):\n",
    "                break\n",
    "            \n",
    "            next_species_span = spans[i+1]\n",
    "            \n",
    "            # If there's one token between the species and the next species,\n",
    "            # we check if the next species is surrounded by punctuation.\n",
    "            if next_species_span.start - species_span.end == 1:\n",
    "                # Token Before and After the Next Species\n",
    "                before_next = self.main.sp_doc[next_species_span.start-1]\n",
    "                after_next = self.main.sp_doc[next_species_span.end]\n",
    "\n",
    "                if before_next.pos_ in [\"PUNCT\", \"SYM\"] and after_next.pos_ in [\"PUNCT\", \"SYM\"]:\n",
    "                    sp_1_text = species_span.text.lower()\n",
    "                    sp_2_text = next_species_span.text.lower()\n",
    "                    \n",
    "                    if sp_1_text not in alternate_names:\n",
    "                        alternate_names[sp_1_text] = []\n",
    "                    \n",
    "                    if sp_2_text not in alternate_names:\n",
    "                        alternate_names[sp_2_text] = []\n",
    "                    \n",
    "                    alternate_names[sp_1_text].append(sp_2_text)\n",
    "                    alternate_names[sp_2_text].append(sp_1_text)\n",
    "            # If there's no token between the species and the next,\n",
    "            # species we assume that they refer to the same species.\n",
    "            elif next_species_span.start - species_span.end == 0:\n",
    "                sp_1_text = species_span.text.lower()\n",
    "                sp_2_text = next_species_span.text.lower()\n",
    "                \n",
    "                if sp_1_text not in alternate_names:\n",
    "                    alternate_names[sp_1_text] = []\n",
    "                \n",
    "                if sp_2_text not in alternate_names:\n",
    "                    alternate_names[sp_2_text] = []\n",
    "\n",
    "                alternate_names[sp_1_text].append(sp_2_text)\n",
    "                alternate_names[sp_2_text].append(sp_1_text)\n",
    "       \n",
    "        return (spans, tokens, token_to_span, alternate_names)\n",
    "\n",
    "    def span_at_token(self, token):\n",
    "        if token in self.token_to_span:\n",
    "            return self.token_to_span[token]\n",
    "        return None\n",
    "    \n",
    "    def is_species(self, token):\n",
    "        return token in self.tokens\n",
    "        \n",
    "    def has_species(self, tokens, verbose=False):\n",
    "        for token in tokens:\n",
    "            if token in self.tokens:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def find_same_species(self, sp_A, sp_b, verbose=False):\n",
    "        # METHOD 1: Check for Literal Matches\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        \n",
    "        for sp_a in sp_A:\n",
    "            # Verbatim Text\n",
    "            sp_a_text = sp_a.text.lower()\n",
    "\n",
    "            if sp_a_text == sp_b_text:\n",
    "                return sp_a\n",
    "\n",
    "            # Singularized Text\n",
    "            sp_a_singular_texts = sp_a_text if sp_a[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(sp_a_text)\n",
    "            sp_b_singular_texts = sp_b_text if sp_b[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(sp_b_text)\n",
    "\n",
    "            if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "                return sp_a\n",
    "\n",
    "        # METHOD 2: Check Alternate Names\n",
    "        for sp_a in sp_A:\n",
    "            # Species B is an alternate name for Species A\n",
    "            if sp_b_text in self.alternate_names.get(sp_a_text, []):\n",
    "                return sp_a\n",
    "            # Species A is an alternate name for Species B\n",
    "            if sp_a_text in self.alternate_names.get(sp_b_text, []):\n",
    "                return sp_a\n",
    "        \n",
    "        # METHOD 3: Check Nouns\n",
    "        # This is used if one or none of the species being compared\n",
    "        # has 1 adjective.\n",
    "        sp_b_0_text = sp_b[0].lower_\n",
    "        sp_b_is_noun = sp_b[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "        sp_b_nouns = []\n",
    "        sp_b_num_adjectives = 0\n",
    "        for token in sp_b:\n",
    "            if not sp_b_nouns and token.pos_ == \"ADJ\":\n",
    "                sp_b_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_b_nouns.append(token)\n",
    "        sp_b_nouns_str = [noun.lower_ for noun in sp_b_nouns]\n",
    "        sp_b_singular_texts = \" \".join(sp_b_nouns_str) if sp_b_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(\" \".join(sp_b_nouns_str))\n",
    "        \n",
    "        for sp_a in sp_A:\n",
    "            sp_a_0_text = sp_a[0].lower_\n",
    "            sp_a_is_noun = sp_a[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "            # Case Example: 'Hyla' v. 'Hyla tadpoles'\n",
    "            if sp_a_0_text == sp_b_0_text and (sp_a_is_noun or sp_b_is_noun):\n",
    "                if sp_a_text in sp_b_text or sp_b_text in sp_a_text:\n",
    "                    return sp_a\n",
    "            # Case Example: 'dogs' v. 'red dogs'\n",
    "            else:\n",
    "                sp_a_nouns = []\n",
    "                sp_a_num_adjectives = 0\n",
    "                for token in sp_a:\n",
    "                    if not sp_a_nouns and token.pos_ == \"ADJ\":\n",
    "                        sp_a_num_adjectives += 1\n",
    "                    elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                        sp_a_nouns.append(token)\n",
    "                sp_a_nouns_str = [noun.lower_ for noun in sp_a_nouns]\n",
    "                \n",
    "                if sp_a_nouns and sp_b_nouns and (\n",
    "                    (sp_a_num_adjectives == 1 and sp_b_num_adjectives == 0) or \n",
    "                    (sp_b_num_adjectives == 1 and sp_a_num_adjectives == 0)\n",
    "                ):\n",
    "                    sp_a_singular_texts = \" \".join(sp_a_nouns_str) if sp_a_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(\" \".join(sp_a_nouns_str))\n",
    "                    \n",
    "                    if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "                        return sp_a\n",
    "\n",
    "        # METHOD 3: Last Ditch Effort\n",
    "        # If there's been no matches, we just look for one string inside of\n",
    "        # another.\n",
    "        for sp_a in sp_A:\n",
    "            sp_a_text = sp_a.text.lower()\n",
    "            if sp_b_text in sp_a_text or sp_a_text in sp_b_text:\n",
    "                return sp_a\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c31521a-3d38-4087-b63e-bb974ad3a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    def __init__(self, main, *, regexes=[], vocab=[], patterns=[], def_pos=[], def_tag=[], def_threshold=0.7):\n",
    "        self.main = main\n",
    "        # When comparing two words, SpaCy returns a value\n",
    "        # from 0 to 1, representing how similar the two\n",
    "        # embeddings are. The threshold below determines\n",
    "        # the minimum number of similarity before two words\n",
    "        # are considered as being equivalent. This is the\n",
    "        # default values, specific values can be provided for\n",
    "        # each word.\n",
    "        self.def_threshold = def_threshold\n",
    "        # To decrease our \"search space\", we can specify the\n",
    "        # type of tokens we'd want to consider via the\n",
    "        # tag_ and pos_ attributes. The tag and pos keys for a\n",
    "        # vocab word is used for more specificity.\n",
    "        self.def_tag = def_tag\n",
    "        self.def_pos = def_pos\n",
    "        # The words are divided into two categories: vocabulary\n",
    "        # and regex. The vocabulary matches words\n",
    "        # by definition, whereas the regexes match words by\n",
    "        # content.\n",
    "        self.regex = regexes\n",
    "        self.vocab = []\n",
    "        for vocab_word in vocab:\n",
    "            if isinstance(vocab_word, str):\n",
    "                doc = self.main.sp_nlp(vocab_word)\n",
    "                self.vocab.append({\n",
    "                    \"doc\": doc,\n",
    "                    \"lemma\": \" \".join([t.lemma_ for t in doc])\n",
    "                })\n",
    "            else:\n",
    "                doc = self.main.sp_nlp(vocab_word[\"word\"])\n",
    "                self.vocab.append({\n",
    "                    \"doc\": doc,\n",
    "                    \"tag\": vocab_word.get(\"tag\"),\n",
    "                    \"pos\": vocab_word.get(\"pos\"),\n",
    "                    \"threshold\": vocab_word.get(\"threshold\"),\n",
    "                    \"lemma\": \" \".join([t.lemma_ for t in doc])\n",
    "                })\n",
    "\n",
    "        # Rule-Based Matching\n",
    "        self.matcher = Matcher(self.main.sp_nlp.vocab)\n",
    "        self.matcher.add(\"Pattern\", patterns)\n",
    "\n",
    "        # Matched Tokens\n",
    "        self.tokens = []\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tokens = self.match_tokens(verbose=verbose)\n",
    "\n",
    "    def match_tokens(self, verbose=False):\n",
    "        verbose=True\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        matched_tokens = []\n",
    "\n",
    "        # Match by Regex\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "        for regex in self.regex:\n",
    "            if verbose:\n",
    "                print(regex, [match.start() for match in re.finditer(regex, text, re.IGNORECASE)])\n",
    "            for char_index in [match.start() for match in re.finditer(regex, text, re.IGNORECASE)]:\n",
    "                adj_char_index = char_index\n",
    "                while text[adj_char_index].isspace():\n",
    "                    adj_char_index += 1\n",
    "                token = self.main.token_at_char(adj_char_index)\n",
    "                if (\n",
    "                    (self.def_pos and token.pos_ not in self.def_pos) or \n",
    "                    (self.def_tag and token.tag_ not in self.def_tag)\n",
    "                ):\n",
    "                    continue\n",
    "                matched_tokens.append(token)\n",
    "\n",
    "        # Match by Matcher\n",
    "        matches = self.matcher(self.main.sp_doc)\n",
    "        for match_id, start, end in matches:\n",
    "            span = self.main.sp_doc[start:end]  # The matched span\n",
    "            print(f\"Matched Span: {span}\")\n",
    "            matched_tokens.append(span[0])\n",
    "\n",
    "        # Match by Vocab\n",
    "        for token in self.main.sp_doc:\n",
    "            if (\n",
    "                (self.def_pos and token.pos_ not in self.def_pos) or \n",
    "                (self.def_tag and token.tag_ not in self.def_tag) or \n",
    "                (token in matched_tokens)\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            token_doc = self.main.sp_nlp(token.lower_)\n",
    "            token_lemma = \" \".join([t.lemma_ for t in token_doc])\n",
    "            \n",
    "            for vocab_word in self.vocab:\n",
    "                # Ensure Correct Tag\n",
    "                if vocab_word.get(\"tag\"):\n",
    "                    if not [t for t in token_doc if t.tag_ in vocab_word.get(\"tag\")]:\n",
    "                        continue\n",
    "                \n",
    "                # Ensure Correct PoS\n",
    "                if vocab_word.get(\"pos\"):\n",
    "                    if not [t for t in token_doc if t.pos_ in vocab_word.get(\"pos\")]:\n",
    "                        continue\n",
    "\n",
    "                # Check Lemma\n",
    "                if token.lower_ == \"amplified\":\n",
    "                    print(token_lemma, vocab_word[\"lemma\"])\n",
    "                    \n",
    "                if token_lemma == vocab_word[\"lemma\"]:\n",
    "                    matched_tokens.append(token)\n",
    "                    break\n",
    "\n",
    "                # Check Similarity\n",
    "                similarity = vocab_word[\"doc\"].similarity(token_doc)\n",
    "\n",
    "                if token.lower_ == \"amplified\":\n",
    "                    print(token.lower, vocab_word[\"doc\"], similarity)\n",
    "                    \n",
    "                if similarity >= vocab_word.get(\"threshold\", self.def_threshold):\n",
    "                    matched_tokens.append(token)\n",
    "                    break\n",
    "                \n",
    "        return matched_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6699dce8-5827-4757-becd-a016f9a06cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"study\", \n",
    "                \"hypothesis\", \n",
    "                \"experiment\", \n",
    "                \"found\", \n",
    "                \"discover\", \n",
    "                \"compare\", \n",
    "                \"finding\", \n",
    "                \"result\", \n",
    "                \"test\", \n",
    "                \"examine\", \n",
    "                \"model\",\n",
    "                \"measure\",\n",
    "                \"manipulate\",\n",
    "                \"assess\",\n",
    "                \"conduct\",\n",
    "                \"data\",\n",
    "                \"analyze\",\n",
    "                \"sample\",\n",
    "                \"observe\",\n",
    "                \"predict\",\n",
    "                \"suggest\",\n",
    "                \"method\",\n",
    "                \"investigation\",\n",
    "                \"trial\",\n",
    "                \"experimental\",\n",
    "                \"evidence\",\n",
    "                \"demonstrate\",\n",
    "                \"analysis\",\n",
    "                \"show\",\n",
    "                \"compare\",\n",
    "                \"comparable\",\n",
    "                \"control group\", \n",
    "                \"independent\",\n",
    "                \"dependent\",\n",
    "                \"applied\",\n",
    "                \"treatment\",\n",
    "                \"survery\"\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a4551e-5af3-4d29-92d1-485f3c1f25b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"theory\",\n",
    "                \"review\",\n",
    "                \"analysis\",\n",
    "                \"meta-analysis\"\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d158d0-272e-48c4-b27f-eb5c3d8a7b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeTopicKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            regexes=[\n",
    "                r\"co-?evolution\",\n",
    "                r\"evolution\",\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93ae4dd9-f73e-41d5-926e-96c383d65c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CauseKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"increase\", \n",
    "                \"decrease\", \n",
    "                \"change\", \n",
    "                \"shift\", \n",
    "                \"cause\", \n",
    "                \"produce\", \n",
    "                \"trigger\", \n",
    "                \"suppress\", \n",
    "                \"inhibit\",\n",
    "                \"encourage\",\n",
    "                \"allow\",\n",
    "                \"influence\",\n",
    "                \"affect\",\n",
    "                \"alter\",\n",
    "                \"induce\",\n",
    "                \"produce\",\n",
    "                \"result in\",\n",
    "                \"associated\",\n",
    "                \"correlated\",\n",
    "                \"contribute\",\n",
    "                \"impact\",\n",
    "                \"deter\",\n",
    "                \"depressed\",\n",
    "                \"when\",\n",
    "                \"because\",\n",
    "                # \"reduce\",\n",
    "                # \"killed\",\n",
    "                # \"supported\"\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"SCONJ\", \"NOUN\"],\n",
    "            # def_tag=[\"VB\", \"VBD\", \"WRB\", \"IN\", \"VBG\"],\n",
    "            # def_threshold=0.75\n",
    "            def_threshold=0.8\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            # I'm not sure what cause words should be filtered out, because\n",
    "            # I haven't seen everything, but this word should be filtered out,\n",
    "            # it's not really reflective the changes that we're looking for. But,\n",
    "            # sometimes it is, so it's up in the air. However, I feel like the\n",
    "            # writer would use more clear language like \"decrease\" or something.\n",
    "            if token.lemma_ in [\"kill\"]:\n",
    "                continue\n",
    "            filtered.append(token)\n",
    "            \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bb56df8-95d1-4ac9-956a-f7aeddb3ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"few\", \n",
    "                \"more\", \n",
    "                \"increase\", \n",
    "                \"decrease\", \n",
    "                \"less\", \n",
    "                \"short\", \n",
    "                \"long\", \n",
    "                \"greater\"\n",
    "                \"shift\",\n",
    "                \"fluctuate\",\n",
    "                \"adapt\",\n",
    "                \"grow\",\n",
    "                \"rise\"\n",
    "                \"surge\",\n",
    "                \"intensify\",\n",
    "                \"amplify\",\n",
    "                \"multiply\",\n",
    "                \"decline\",\n",
    "                \"reduce\",\n",
    "                \"drop\",\n",
    "                \"diminish\",\n",
    "                \"fall\",\n",
    "                \"lessen\",\n",
    "                \"doubled\",\n",
    "                \"tripled\",\n",
    "                \"lower\",\n",
    "            ],\n",
    "            regexes=[\n",
    "                # Match Examples:\n",
    "                # 1. \"one... as...\"\n",
    "                # 2. \"2x than...\"\n",
    "                r\"(one|two|three|four|five|six|seven|eight|nine|ten|twice|thrice|([0-9]+|[0-9]+.[0-9]+)(x|%))[\\s-]+[^\\s]*[\\s-]+(as|more|than|likely)([\\s-]+|$)\"\n",
    "            ],\n",
    "            def_pos=[\"NOUN\", \"ADJ\", \"ADV\"],\n",
    "            def_threshold=0.75\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose=True)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in self.main.sp_doc:\n",
    "            # Already Matched\n",
    "            if token in tokens:\n",
    "                filtered.append(token)\n",
    "            \n",
    "            # Comparative Adjective\n",
    "            # Looking for words like \"bigger\" and \"better\".\n",
    "            elif token.pos_ == \"ADJ\" and token.tag_ == \"JJR\":\n",
    "                filtered.append(token)\n",
    "                continue\n",
    "            \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba70792f-d32b-4638-a929-c7499506e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraitKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            regexes=[\n",
    "                r\"behaviou?r\", \n",
    "                r\"[^A-Za-z]+rate\", \n",
    "                \"colou?r\",\n",
    "                \"biomass\",\n",
    "                r\"[^A-Za-z]+mass\", \n",
    "                r\"[^A-Za-z]+size\",\n",
    "                \"number\",\n",
    "                \"length\", \n",
    "                \"pattern\", \n",
    "                \"weight\",\n",
    "                \"shape\", \n",
    "                \"efficiency\", \n",
    "                \"trait\",\n",
    "                \"phenotype\",\n",
    "                \"demography\",\n",
    "                \"population (structure|mechanic)s?\",\n",
    "                \"ability\", \n",
    "                \"capacity\", \n",
    "                \"height\", \n",
    "                \"width\", \n",
    "                \"[A-Za-z]+span\",\n",
    "                \"diet\",\n",
    "                # \"food\",\n",
    "                \"feeding\",\n",
    "                \"nest\",\n",
    "                \"substrate\",\n",
    "                \"breeding\",\n",
    "                r\"[^A-Za-z]+age[^A-Za-z]+\",\n",
    "                \"lifespan\",\n",
    "                \"development\",\n",
    "                \"output\",\n",
    "                \"time\",\n",
    "                \"period\"\n",
    "                # \"mating\",\n",
    "                # \"[^A-Za-z]+fur\",\n",
    "                # \"feathers\",\n",
    "                # \"scales\",\n",
    "                # \"skin\",\n",
    "                # \"limb\",\n",
    "                \"level\",\n",
    "                \"configuration\",\n",
    "                \"dimorphism\",\n",
    "                \"capability\",\n",
    "                # \"appendages\",\n",
    "                # \"blood\",\n",
    "                \"regulation\",\n",
    "                \"excretion\",\n",
    "                \"luminescence\",\n",
    "                r\"[^A-Za-z]+role\",\n",
    "                # \"reproduction\",\n",
    "                # \"courtship\",\n",
    "                # \"pollination\",\n",
    "                # \"mechanism\",\n",
    "                \"sensitivity\",\n",
    "                \"resistance\",\n",
    "                r\"(un|(^|\\s)[A-Za-z]*-)infected\",\n",
    "                # \"temperature\"\n",
    "            ],\n",
    "            def_pos=[\"NOUN\", \"ADJ\"]\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=True):\n",
    "        verbose = True\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        print(f\"Tokens to Filter: {tokens}\")\n",
    "        \n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            expanded_token = self.main.help.expand_unit(\n",
    "                il_unit=token.i, \n",
    "                ir_unit=token.i, \n",
    "                il_boundary=0, \n",
    "                ir_boundary=len(self.main.sp_doc) - 1, \n",
    "                speech=[\"PUNCT\"],\n",
    "                include=False,\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "            print(f\"\\tToken: {token}\\n\\tExpanded Token: {expanded_token}\")\n",
    "            if self.main.species.has_species(expanded_token):\n",
    "                filtered.append(token)\n",
    "\n",
    "        print(f\"Filtered Tokens: {filtered}\")\n",
    "        \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "224f1fde-3ca0-4306-a9cb-dde37bb3a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main,\n",
    "            vocab=[\n",
    "                \"compare\",\n",
    "                \"examine\",\n",
    "                \"evaluate\",\n",
    "                \"assess\",\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce08e27a-c07c-42d7-8b82-3ccd12f6e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main,\n",
    "            regexes=[\n",
    "                r\"between\",\n",
    "                r\"against\",\n",
    "                r\"independen(t|ts|tly|cy)\",\n",
    "                r\"dependen(t|ts|tly|cy)\",\n",
    "                r\"treatments?\",\n",
    "                r\"effect\"\n",
    "            ],\n",
    "            patterns=[\n",
    "                [{\"LOWER\": {\"IN\": [\"with\", \"without\"]}}, {\"OP\": \"*\", \"IS_PUNCT\": False}, {\"LOWER\": {\"IN\": [\"with\", \"without\"]}}],\n",
    "                [{\"LOWER\": {\"IN\": [\"at\"]}}, {\"POS\": \"NUM\"}],\n",
    "                [{\"LOWER\": {\"IN\": [\"at\"]}}, {\"LOWER\": {\"IN\": [\"several\", \"unique\", \"multiple\", \"different\"]}}],\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea51e8cc-3625-4117-8e10-83d78fbcc785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"therefore\",\n",
    "                \"thus\",\n",
    "                \"results\",\n",
    "                \"showed\",\n",
    "                \"displayed\",\n",
    "                \"due\",\n",
    "                \"hence\"\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\", \"SCONJ\", \"ADV\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fdf8e07-a326-4fea-8615-faa5d549852d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Main:\n",
    "    def __init__(self):\n",
    "        # Tools\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        self.sp_doc = None\n",
    "\n",
    "        # Maps Character Position to Token in Document\n",
    "        # Used to handle differences between different\n",
    "        # pipelines and tools.\n",
    "        self.index_map = None\n",
    "    \n",
    "        # Parsers\n",
    "        self.species = Species(self)\n",
    "        self.trait = TraitKeywords(self)\n",
    "        self.cause = CauseKeywords(self)\n",
    "        self.change = ChangeKeywords(self)\n",
    "        self.experiment = ExperimentKeywords(self)\n",
    "        self.neg_experiment = NegativeExperimentKeywords(self)\n",
    "        self.neg_topic = NegativeTopicKeywords(self)\n",
    "        self.result = ResultKeywords(self)\n",
    "        self.variable = VariableKeywords(self)\n",
    "        self.test = TestKeywords(self)\n",
    "\n",
    "        # Helper\n",
    "        self.help = Help(self)\n",
    "\n",
    "    def update_doc(self, doc, verbose=False):\n",
    "        self.sp_doc = doc\n",
    "        self.index_map = self.load_index_map()\n",
    "        self.species.update(doc.text, verbose=True)\n",
    "        self.trait.update(verbose=False)\n",
    "        self.cause.update(verbose=False)\n",
    "        self.change.update(verbose=False)\n",
    "        self.experiment.update(verbose=False)\n",
    "        self.neg_experiment.update(verbose=False)\n",
    "        self.neg_topic.update(verbose=False)\n",
    "        self.result.update(verbose=False)\n",
    "        self.variable.update(verbose=False)\n",
    "        self.test.update(verbose=False)\n",
    "\n",
    "    def update_text(self, text, verbose=False):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.update_doc(self.sp_doc, verbose=verbose)\n",
    "        \n",
    "    def token_at_char(self, char_index):\n",
    "        # SpaCy Doc or Indexing Map Not Found\n",
    "        if not self.sp_doc or not self.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        if char_index in self.index_map:\n",
    "            return self.index_map[char_index]\n",
    "\n",
    "        raise Exception(f\"Token at Index {char_index} Not Found\")\n",
    "        \n",
    "    def load_index_map(self):\n",
    "        # SpaCy Doc Not Found\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Map Character Index to Token\n",
    "        index_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            # char_i0 is the index of the token's starting character.\n",
    "            # char_i1 is the index of the character after the token's ending character.\n",
    "            char_i0 = token.idx\n",
    "            char_i1 = token.idx + len(token)\n",
    "\n",
    "            for i in range(char_i0, char_i1):\n",
    "                index_map[i] = token\n",
    "\n",
    "        return index_map\n",
    "\n",
    "    def score(self, verbose=False):\n",
    "        NUM_CATEGORIES = 6\n",
    "\n",
    "        # Requires the mention of a trait and a cause or change word.\n",
    "        # The cause or change word indicates some variation.\n",
    "        # Index 0 in Array\n",
    "        TRAIT = 0\n",
    "\n",
    "        # Requires the mention of a species and a cause or change word.\n",
    "        # The cause or change word indicates that the species is being\n",
    "        # affected or is affecting something else.\n",
    "        # Index 1 in Array\n",
    "        SPECIES = 1\n",
    "\n",
    "        # Requires a word that has been defined as \"experiment\"-related.\n",
    "        # Index 2 in Array\n",
    "        EXPERIMENT = 2\n",
    "\n",
    "        # Requires the mention of several species (more or less).\n",
    "        # Index 3 in Array\n",
    "        INTERACTION = 3\n",
    "\n",
    "        # There are certain topics that we may not want.\n",
    "        NEGATIVE_TOPIC = 4\n",
    "\n",
    "        # I'm going to redo the comments anyway.\n",
    "        TRAIT_VARIATION = 5\n",
    "\n",
    "        # Max # of Points of Category per Sentence (MPC)\n",
    "        # A category can collect points from each sentence. However,\n",
    "        # there's a maximum number of points it can collect. This is\n",
    "        # determined by the MPC.\n",
    "        MPC = [1] * NUM_CATEGORIES\n",
    "\n",
    "        # TODO: This is no longer needed as the scores are first calculated vertically,\n",
    "        # rather than horizontally. Previously, we'd add up the points a sentence received\n",
    "        # across its categories. In order to have a final score in the range [0, 1], the\n",
    "        # maximum points a sentence received had to be in the range [0, 1]. However, \n",
    "        # since we no longer add up all the points a sentence received across all\n",
    "        # categories (to apply the weights at the end), they don't need to add up\n",
    "        # to 1. It's hard to explain without an example.\n",
    "        # assert np.sum(MPC) == 1\n",
    "        \n",
    "        # Points per Instance of Category (PIC)\n",
    "        # Each token is evaluated to check whether a category\n",
    "        # can be given points. The number of points given, if\n",
    "        # the token is determined to be satisfactory, is the PIC.\n",
    "        # The PIC is less than or equal to the MPC for the corresponding\n",
    "        # category. The idea behind the PIC and MPC is similar to how\n",
    "        # sets work in tennis: you're not immediately awarded the full points\n",
    "        # for the set (MPC) if your opponent fails to return the ball,\n",
    "        # instead you're given a smaller # of points (PIC) that allow you to\n",
    "        # incrementally win the set (category).\n",
    "        PIC = [0] * NUM_CATEGORIES\n",
    "        PIC[TRAIT] = MPC[TRAIT]*1.0\n",
    "        PIC[SPECIES] = MPC[SPECIES]/3.0\n",
    "        PIC[EXPERIMENT] = MPC[EXPERIMENT]*0.625\n",
    "        PIC[INTERACTION] = MPC[INTERACTION]/3.0\n",
    "        PIC[NEGATIVE_TOPIC] = MPC[NEGATIVE_TOPIC]*1.0\n",
    "\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            assert 0 <= PIC[i] <= MPC[i]\n",
    "\n",
    "        # Category Weights (CW)\n",
    "        # It may be helpful to weigh a certain category's fraction of total points\n",
    "        # more or less than another's. Thus, at the end, we'll take a\n",
    "        # weighted average of the category's FTP. The weights must add up to 1.\n",
    "        CW = [0] * NUM_CATEGORIES\n",
    "        CW[TRAIT] = 0.3\n",
    "        CW[SPECIES] = 0.1\n",
    "        CW[EXPERIMENT] = 0.1\n",
    "        CW[INTERACTION] = 0.1\n",
    "        CW[NEGATIVE_TOPIC] = 0.1\n",
    "        CW[TRAIT_VARIATION] = 0.3\n",
    "\n",
    "        assert round(np.sum(CW)) == 1\n",
    "\n",
    "        # Leniency\n",
    "        # There are certain categories that aren't going to be as frequent as others.\n",
    "        # For example, the trait category. You could try and decrease the influence\n",
    "        # of said category by lowering its MPC and/or increasing the PIC (so that it's\n",
    "        # easier to achieve the FTP). However, this could make it harder to meaningfully\n",
    "        # represent the category. The idea of leniency is to remove (some) sentences that had 0\n",
    "        # points from the scoring. This increases the FTP as, for example, instead of comparing\n",
    "        # 0.5 points to a total of 2.5 points, you can compare 0.5 to 2.0 points, and so on.\n",
    "        # A leniency of 1 means that all sentences that received 0 points will be removed from\n",
    "        # the scoring. A leniency of 0 means that all the sentences are included in the scoring.\n",
    "        LEN = [0] * NUM_CATEGORIES\n",
    "        LEN[TRAIT] = 0\n",
    "        LEN[SPECIES] = 0\n",
    "        LEN[EXPERIMENT] = 0\n",
    "        LEN[INTERACTION] = 0\n",
    "        LEN[NEGATIVE_TOPIC] = 0\n",
    "\n",
    "        # Used for Leniency\n",
    "        zero_pt_sents = [0] * NUM_CATEGORIES\n",
    "        \n",
    "        # Points\n",
    "        points = [0] * NUM_CATEGORIES\n",
    "\n",
    "        # Extracted Information\n",
    "        cause_tokens = self.cause.tokens\n",
    "        change_tokens = self.change.tokens\n",
    "        trait_tokens = self.trait.tokens\n",
    "        species_tokens = [self.sp_doc[span.start] for span in self.species.spans]\n",
    "        experiment_tokens = self.experiment.tokens\n",
    "        neg_experiment_tokens = self.neg_experiment.tokens\n",
    "        neg_topic_tokens = self.neg_topic.tokens\n",
    "        result_tokens = self.result.tokens\n",
    "        test_tokens = self.test.tokens\n",
    "        var_tokens = self.variable.tokens\n",
    "\n",
    "        print(f\"Cause Tokens: {cause_tokens}\")\n",
    "        print(f\"Change Tokens: {change_tokens}\")\n",
    "        print(f\"Experiment Tokens: {experiment_tokens}\")\n",
    "        print(f\"Negative Experiment Tokens: {neg_experiment_tokens}\")\n",
    "        print(f\"Negative Topic Tokens: {neg_topic_tokens}\")\n",
    "        print(f\"Trait Tokens: {trait_tokens}\")\n",
    "        print(f\"Species Tokens: {species_tokens}\")\n",
    "         \n",
    "        # This is used to ensure that at least three species\n",
    "        # are mentioned.\n",
    "        seen_species = {}\n",
    "\n",
    "        for sent in self.sp_doc.sents:\n",
    "            # This contains the number of points\n",
    "            # each category has accumulated in the sentence.\n",
    "            curr_points = [0] * NUM_CATEGORIES\n",
    "\n",
    "            # Contains the tokens in the sentence.\n",
    "            sent_tokens = [token for token in sent]\n",
    "\n",
    "            # This is used for the species (must have a nearby cause and/or\n",
    "            # change word).\n",
    "            sent_cause_tokens = set(sent_tokens).intersection(cause_tokens)\n",
    "            sent_change_tokens = set(sent_tokens).intersection(change_tokens)\n",
    "\n",
    "            # We don't want to visit the same species more than one\n",
    "            # in the same sentence as to avoid redundant points.\n",
    "            sent_seen_species = []\n",
    "            sent_num_unique_species = 0\n",
    "            \n",
    "            print(f\"Sentence Tokens: {sent_tokens}\")\n",
    "            print(f\"Sentence Cause Tokens: {sent_cause_tokens}\")\n",
    "            print(f\"Sentence Change Tokens: {sent_change_tokens}\")\n",
    "            \n",
    "            for token in sent_tokens:\n",
    "                # If each category has reached their maximum number of points,\n",
    "                # we can end the loop early.\n",
    "                all_maxed = True\n",
    "                for i in range(NUM_CATEGORIES):\n",
    "                    if curr_points[i] < MPC[i]:\n",
    "                        all_maxed = False\n",
    "\n",
    "                if all_maxed:\n",
    "                    break\n",
    "\n",
    "                # NEGATIVE TOPIC CATEGORY\n",
    "                if curr_points[NEGATIVE_TOPIC] < MPC[NEGATIVE_TOPIC] and token in neg_topic_tokens:\n",
    "                    curr_points[NEGATIVE_TOPIC] += PIC[NEGATIVE_TOPIC]\n",
    "                    print(f\"Added Points for Negative Topic via Token '{token}'\\n\")\n",
    "\n",
    "                # TRAIT CATEGORY\n",
    "                if curr_points[TRAIT] < MPC[TRAIT] and token in trait_tokens:\n",
    "                    print(\"TRAIT CATEGORY\")\n",
    "                    # To get points in the trait category, there must \n",
    "                    # be (1) a trait; and (2) a change or cause in the token's\n",
    "                    # context.\n",
    "                    token_context = set(self.help.find_unit_context(\n",
    "                        il_unit=token.i, \n",
    "                        ir_unit=token.i, \n",
    "                        il_boundary=token.sent.start, \n",
    "                        ir_boundary=token.sent.end-1, \n",
    "                        verbose=verbose)\n",
    "                    )\n",
    "                    cause_tokens_in_context = set(sent_cause_tokens).intersection(token_context)\n",
    "                    change_tokens_in_context = set(sent_change_tokens).intersection(token_context)\n",
    "\n",
    "                    print(f\"Token ({token}) Context: {token_context}\")\n",
    "                    print(f\"Cause Tokens in Context: {cause_tokens_in_context}\")\n",
    "                    print(f\"Change Tokens in Context: {change_tokens_in_context}\")\n",
    "\n",
    "                    if cause_tokens_in_context or change_tokens_in_context:\n",
    "                        curr_points[TRAIT] += PIC[TRAIT]\n",
    "                        print(f\"Added Points for Trait via Token '{token}'\")\n",
    "\n",
    "                    print()\n",
    "\n",
    "                # EXPERIMENT CATEGORY\n",
    "                if token in neg_experiment_tokens:\n",
    "                    curr_points[EXPERIMENT] -= 2 * PIC[EXPERIMENT]\n",
    "                    print(f\"Deducted Points for Experiment via Token '{token}'\\n\")\n",
    "                elif curr_points[EXPERIMENT] < MPC[EXPERIMENT] and token in experiment_tokens:\n",
    "                    curr_points[EXPERIMENT] += PIC[EXPERIMENT]\n",
    "                    print(f\"Added Points for Experiment via Token '{token}'\\n\")\n",
    "\n",
    "                # SPECIES CATEGORY\n",
    "                if token in species_tokens:\n",
    "                    print(\"SPECIES CATEGORY\")\n",
    "                    # Find Species Span\n",
    "                    species_span = self.species.span_at_token(token)           \n",
    "\n",
    "                    # Updating Seen Species (in Entire Text)\n",
    "                    past_visits = 0\n",
    "\n",
    "                    # Find Previous Instance of Species (if Any)\n",
    "                    print(\"Seen Species Updated\")\n",
    "                    print(seen_species)\n",
    "                    print()\n",
    "                    \n",
    "                    seen_species_span = self.species.find_same_species(seen_species.keys(), species_span)\n",
    "                    if seen_species_span:\n",
    "                        past_visits = seen_species[seen_species_span]\n",
    "                        seen_species[seen_species_span] += 1\n",
    "                    \n",
    "                    if not past_visits:\n",
    "                        seen_species[species_span] = 1\n",
    "\n",
    "                    print(\"Seen Species Updated\")\n",
    "                    print(seen_species)\n",
    "                    print()\n",
    "                    \n",
    "                    # Checking Seen Species (in Sentence)\n",
    "                    # We only add points if it's a species that has not been seen\n",
    "                    # in the sentence. This is to avoid redundant points. \n",
    "                    # Also, if it species has not been seen at all (is_new_species),\n",
    "                    # then it cannot be a redundant species (we couldn't have seen it in the sentence\n",
    "                    # either).\n",
    "                    redundant_species = False\n",
    "\n",
    "                    if not past_visits:\n",
    "                        if self.species.find_same_species(sent_seen_species, species_span):\n",
    "                            redundant_species = True\n",
    "                    \n",
    "                    sent_seen_species.append(species_span)\n",
    "\n",
    "                    print(\"Seen Species in Sentence\")\n",
    "                    print(sent_seen_species)\n",
    "                    print()\n",
    "                    \n",
    "                    if redundant_species:\n",
    "                        continue\n",
    "                    sent_num_unique_species += 1\n",
    "                    \n",
    "                    # INTERACTION CATEGORY\n",
    "                    # It is helpful to have this category here because (if we've reached here)\n",
    "                    # we're dealing with a new species in the sentence.\n",
    "                    if curr_points[INTERACTION] < MPC[INTERACTION] and sent_num_unique_species > 1:\n",
    "                        curr_points[INTERACTION] += PIC[INTERACTION]\n",
    "                        print(f\"Added Points for Interaction via Token '{token}'\\n\")\n",
    "                        \n",
    "                    if curr_points[SPECIES] < MPC[SPECIES]:\n",
    "                        # To get points in the species category, there must be \n",
    "                        # (1) a species; and (2) a change or cause in the phrase\n",
    "                        # (or clause) that the token is a part of.\n",
    "                        token_context = set(self.help.find_unit_context(\n",
    "                            il_unit=token.i, \n",
    "                            ir_unit=token.i, \n",
    "                            il_boundary=token.sent.start, \n",
    "                            ir_boundary=token.sent.end-1, \n",
    "                            verbose=verbose)\n",
    "                        )\n",
    "                        cause_tokens_in_context = set(sent_cause_tokens).intersection(token_context)\n",
    "                        change_tokens_in_context = set(sent_change_tokens).intersection(token_context)\n",
    "\n",
    "                        print(f\"Token ({token}) Context: {token_context}\")\n",
    "                        print(f\"Cause Tokens in Context: {cause_tokens_in_context}\")\n",
    "                        print(f\"Change Tokens in Context: {change_tokens_in_context}\")\n",
    "                        \n",
    "                        if cause_tokens_in_context or change_tokens_in_context:\n",
    "                            curr_points[SPECIES] += PIC[SPECIES]\n",
    "                            print(f\"Added Points for Species via Token '{token}'\")\n",
    "\n",
    "                        print()\n",
    "         \n",
    "            # SENTENCE DONE\n",
    "            # Add Sentence Points to Total Points\n",
    "            for category in [TRAIT, SPECIES, EXPERIMENT, INTERACTION, NEGATIVE_TOPIC]:\n",
    "                if round(curr_points[category]) == 0:\n",
    "                    zero_pt_sents[category] += 1\n",
    "                points[category] += max(0, min(curr_points[category], MPC[category]))\n",
    "\n",
    "        # Trait Variation Score\n",
    "        NUM_SENTENCES = len(list(self.sp_doc.sents))\n",
    "        \n",
    "        max_trait_var_points = 0\n",
    "        for i in range(NUM_SENTENCES):\n",
    "            sent_i = list(self.sp_doc.sents)[i]\n",
    "            print(f\"Trying Sentence: {sent_i}\")\n",
    "            sent_i_tokens = [token for token in sent_i]\n",
    "\n",
    "            sent_i_neg_exp_tokens = set(sent_i_tokens).intersection(neg_experiment_tokens)\n",
    "            sent_i_neg_topic_tokens = set(sent_i_tokens).intersection(neg_topic_tokens)\n",
    "            sent_i_cause_tokens = set(sent_i_tokens).intersection(cause_tokens)\n",
    "            sent_i_result_tokens = set(sent_i_tokens).intersection(result_tokens) - sent_i_cause_tokens\n",
    "\n",
    "            if sent_i_neg_topic_tokens or sent_i_neg_exp_tokens:\n",
    "                print(sent_i_result_tokens)\n",
    "                print(sent_i_neg_topic_tokens)\n",
    "                print(sent_i_neg_exp_tokens)\n",
    "                print('A')\n",
    "                continue\n",
    "\n",
    "            sent_trait_tokens = set(sent_i_tokens).intersection(trait_tokens)\n",
    "            if not sent_trait_tokens:\n",
    "                print('B')\n",
    "                continue\n",
    "\n",
    "            print(\"!!!\")\n",
    "            \n",
    "            trait_var_points = 0\n",
    "            \n",
    "            sent_exp_tokens = set(sent_i_tokens).intersection(experiment_tokens)\n",
    "            sent_test_tokens = set(sent_i_tokens).intersection(test_tokens)\n",
    "\n",
    "            if sent_exp_tokens:\n",
    "                trait_var_points = 0.1\n",
    "            if sent_test_tokens:\n",
    "                trait_var_points = 0.35\n",
    "\n",
    "            # PUT BACK\n",
    "            if trait_var_points == 0:\n",
    "                continue\n",
    "\n",
    "            print(sent_exp_tokens)\n",
    "            print(sent_test_tokens)\n",
    "            \n",
    "            sent_var_tokens = set(sent_i_tokens).intersection(var_tokens)\n",
    "\n",
    "            print(sent_var_tokens)\n",
    "            \n",
    "            if sent_var_tokens:\n",
    "                trait_var_points += 0.15\n",
    "\n",
    "            assert trait_var_points <= 0.5\n",
    "\n",
    "            print(f\"Trait Variation Points: {trait_var_points}\")\n",
    "            if trait_var_points == 0:\n",
    "                print('C')\n",
    "                continue\n",
    "            \n",
    "            for j in range(i+1, NUM_SENTENCES):\n",
    "                sent_j = list(self.sp_doc.sents)[j]\n",
    "                print(f\"Comparing to Sentence: {sent_j}\")\n",
    "                sent_j_tokens = [token for token in sent_j]\n",
    "            \n",
    "                sent_species_tokens = set(sent_j_tokens).intersection(species_tokens)\n",
    "                sent_cause_tokens = set(sent_j_tokens).intersection(cause_tokens)\n",
    "                sent_change_tokens = set(sent_j_tokens).intersection(change_tokens)\n",
    "                sent_result_tokens = set(sent_j_tokens).intersection(result_tokens) - sent_cause_tokens\n",
    "\n",
    "                print(sent_species_tokens)\n",
    "                print(sent_cause_tokens)\n",
    "                print(sent_change_tokens)\n",
    "                print(sent_result_tokens)\n",
    "\n",
    "                if (\n",
    "                    not sent_species_tokens\n",
    "                    or \n",
    "                    (\n",
    "                        not sent_cause_tokens and not sent_change_tokens and not sent_result_tokens\n",
    "                    )\n",
    "                ):\n",
    "                    print('D')\n",
    "                    continue\n",
    "\n",
    "                _trait_var_points = trait_var_points + (1 - ((j - i - 1) / (NUM_SENTENCES - 1))) * 0.5\n",
    "                print(f\"Trait Variation Points after Sentence 2: {_trait_var_points}\")\n",
    "                _trait_var_points *= (1 - i/(NUM_SENTENCES - 1))\n",
    "                print(f\"Trait Variation Points after Adjustment: {_trait_var_points}\")\n",
    "                print(f\"Sentence 1 ({i}): {list(self.sp_doc.sents)[i]}\\nSentence 2 ({j}): {list(self.sp_doc.sents)[j]}\\nTrait Variation Points: {_trait_var_points}\")\n",
    "                max_trait_var_points = max(max_trait_var_points, _trait_var_points)\n",
    "\n",
    "        points[TRAIT_VARIATION] = max_trait_var_points\n",
    "        # Calculating Score            \n",
    "        # NUM_SENTENCES = len(list(self.sp_doc.sents))\n",
    "\n",
    "        print(f\"Points B4 Normalization: {points}\")\n",
    "\n",
    "        score = 0\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            # The trait variation category is already \"normalized\".\n",
    "            if i != TRAIT_VARIATION:\n",
    "                # This is the number of sentences that did not receive\n",
    "                # 0 points (for the category). The number of sentences\n",
    "                # we divide by must minimally account for these sentences.\n",
    "                num_non_zero_pt_sents = NUM_SENTENCES - zero_pt_sents[i]\n",
    "                \n",
    "                # This is the number of sentences to calculate the\n",
    "                # FTP with, with leniency applied.\n",
    "                lenient_num_sentences = max(num_non_zero_pt_sents, (1 - LEN[i]) * NUM_SENTENCES)\n",
    "    \n",
    "                # Calculating FTP\n",
    "                points[i] = points[i] / (MPC[i] * lenient_num_sentences)\n",
    "    \n",
    "                # Take the Inverse\n",
    "                if i == NEGATIVE_TOPIC:\n",
    "                    points[i] = 1 - points[i]\n",
    "    \n",
    "            # Add onto Score\n",
    "            score += max(0, min(points[i], 1)) * CW[i]\n",
    "\n",
    "        # Enforcing 3 or More Species            \n",
    "        if len(seen_species) < 3:\n",
    "            return 0, points\n",
    "            \n",
    "        assert 0.0 <= score <= 1.0\n",
    "        \n",
    "        return score, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73cdd412-e94a-4ead-9b99-a0e8edf3448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(name, save_output=False, version=\"\"):\n",
    "    # Redirect Print Statements\n",
    "    # https://stackoverflow.com/questions/7152762/how-to-redirect-print-output-to-a-file\n",
    "    if save_output:\n",
    "        initial_stdout = sys.stdout\n",
    "        f = open(f'./Print{name}{\"\" if not version else f\"-{version}\"}.txt', 'w')\n",
    "        sys.stdout = f\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "\n",
    "    # Load Dataset\n",
    "    data = load_preprocessed_dataset(name)\n",
    "\n",
    "    # We'll be running the points algorithm\n",
    "    # on the abstracts of these papers.\n",
    "    texts = list(data['Abstract'].to_numpy())\n",
    "    \n",
    "    # The scores for each paper will be stored here,\n",
    "    # we'll set this as a column of the dataframe.\n",
    "    scores = []\n",
    "    points = []\n",
    "    trait_points = []\n",
    "    species_points = []\n",
    "    experiment_points = []\n",
    "    interaction_points = []\n",
    "    neg_topic_points = []\n",
    "    trait_var_points = []\n",
    "    \n",
    "    # Scan and Evaluate Documents\n",
    "    main = Main()\n",
    "    for i, doc in enumerate(main.sp_nlp.pipe(texts)):\n",
    "        print(f\"{i+1}/{data.shape[0]} - {data.iloc[i]['Title']}\\n\")\n",
    "        main.update_doc(doc, verbose=save_output)\n",
    "\n",
    "        # Empty string literals cause errors, so it's\n",
    "        # being handled here.\n",
    "        if not main.sp_doc or not main.species.tn_doc:\n",
    "            scores.append(0)\n",
    "        else:\n",
    "            score, _points = main.score(verbose=save_output)\n",
    "            scores.append(score)\n",
    "            points.append(_points)\n",
    "            trait_points.append(_points[0])\n",
    "            species_points.append(_points[1])\n",
    "            experiment_points.append(_points[2])\n",
    "            interaction_points.append(_points[3])\n",
    "            neg_topic_points.append(_points[4])\n",
    "            trait_var_points.append(_points[5])\n",
    "\n",
    "        if not save_output:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    # Reset Standard Output\n",
    "    if save_output:\n",
    "        sys.stdout = initial_stdout\n",
    "        f.close()\n",
    "\n",
    "    data[\"Score\"] = scores\n",
    "    data[\"Trait Points\"] = trait_points\n",
    "    data[\"Species Points\"] = species_points\n",
    "    data[\"Experiment Points\"] = experiment_points\n",
    "    data[\"Interaction Points\"] = interaction_points\n",
    "    data[\"Negative Topic Points\"] = neg_topic_points\n",
    "    data[\"Trait Variation Points\"] = trait_var_points\n",
    "    data.sort_values(by='Score', ascending=False, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ced84b80-fd17-4774-940f-64037ee22411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 - The effect of copper stress on inter-trophic relationships in a model tri-trophic food chain.\n",
      "\n",
      "behaviou?r []\n",
      "[^A-Za-z]+rate [974, 1612]\n",
      "colou?r []\n",
      "biomass []\n",
      "[^A-Za-z]+mass [1068, 1630]\n",
      "[^A-Za-z]+size []\n",
      "number []\n",
      "length [1004, 1509]\n",
      "pattern []\n",
      "weight [1082]\n",
      "shape []\n",
      "efficiency []\n",
      "trait []\n",
      "phenotype []\n",
      "demography []\n",
      "population (structure|mechanic)s? []\n",
      "ability []\n",
      "capacity []\n",
      "height []\n",
      "width []\n",
      "[A-Za-z]+span []\n",
      "diet []\n",
      "feeding []\n",
      "nest []\n",
      "substrate []\n",
      "breeding []\n",
      "[^A-Za-z]+age[^A-Za-z]+ []\n",
      "lifespan []\n",
      "development []\n",
      "output []\n",
      "time []\n",
      "periodlevel []\n",
      "configuration []\n",
      "dimorphism []\n",
      "capability []\n",
      "regulation []\n",
      "excretion []\n",
      "luminescence []\n",
      "[^A-Za-z]+role []\n",
      "sensitivity []\n",
      "resistance []\n",
      "(un|(^|\\s)[A-Za-z]*-)infected []\n",
      "Tokens to Filter: [rate, rate, mass, mass, length, length, weights]\n",
      "\tToken: rate\n",
      "\tExpanded Token: The rate of growth and flag leaf length were affected by levels of Cu in the soil but total plant mass and ear weights were not\n",
      "\tToken: rate\n",
      "\tExpanded Token: Ladybirds also appeared to be unaffected by the levels of Cu in soils as consumption rate or change in mass between the treatments was not significant\n",
      "\tToken: mass\n",
      "\tExpanded Token: The rate of growth and flag leaf length were affected by levels of Cu in the soil but total plant mass and ear weights were not\n",
      "\tToken: mass\n",
      "\tExpanded Token: Ladybirds also appeared to be unaffected by the levels of Cu in soils as consumption rate or change in mass between the treatments was not significant\n",
      "\tToken: length\n",
      "\tExpanded Token: The rate of growth and flag leaf length were affected by levels of Cu in the soil but total plant mass and ear weights were not\n",
      "\tToken: length\n",
      "\tExpanded Token: induced plants and no significant relationships between levels of N in plant tissues or flag leaf length were found\n",
      "\tToken: weights\n",
      "\tExpanded Token: The rate of growth and flag leaf length were affected by levels of Cu in the soil but total plant mass and ear weights were not\n",
      "Filtered Tokens: [rate, rate, mass, mass, length, length, weights]\n",
      "(one|two|three|four|five|six|seven|eight|nine|ten|twice|thrice|([0-9]+|[0-9]+.[0-9]+)(x|%))[\\s-]+[^\\s]*[\\s-]+(as|more|than|likely)([\\s-]+|$) []\n",
      "co-?evolution []\n",
      "evolution []\n",
      "between [1459, 1636]\n",
      "against []\n",
      "independen(t|ts|tly|cy) []\n",
      "dependen(t|ts|tly|cy) []\n",
      "treatments? [1648]\n",
      "effect [451]\n",
      "Cause Tokens: [induced, affected, affected, induced, change]\n",
      "Change Tokens: []\n",
      "Experiment Tokens: [study, experiments, conducted, experiments, measurements, conducted, analysed, found, found, treatments, study, control, methods]\n",
      "Negative Experiment Tokens: [analysed]\n",
      "Negative Topic Tokens: []\n",
      "Trait Tokens: [rate, rate, mass, mass, length, length, weights]\n",
      "Species Tokens: [wheat, plants, total, wheat, seedlings, Triticum, plant, herbivore, predator, plants, grain, aphids, Sitobion, avenae, predatory, Adalia, bipunctata, plant, total, aphid, aphid, plants, plant, Ladybirds]\n",
      "Sentence Tokens: [Soil, fertility, and, management, are, paramount, in, ensuring, food, security, for, the, growing, populations, .]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: set()\n",
      "Sentence Tokens: [The, use, of, agri, -, chemical, and, products, containing, heavy, metals, inadvertently, threaten, both, food, security, and, the, surrounding, ecosystems, from, contamination, ,, loss, of, productivity, or, ecosystem, service, .]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: set()\n",
      "Sentence Tokens: [In, the, present, study, a, series, of, experiments, on, the, toxic, and, adaptive, responses, of, wheat, plants, to, copper, -, induced, stress, were, conducted, to, establish, the, effects, of, different, levels, of, Cu, (, 0, , 200, mg, kg, -1, ), on, growth, ,, nutrient, levels, and, the, total, plant, proteins, of, wheat, seedlings, (, Triticum, aestivum, ), using, pot, experiments, .]\n",
      "Sentence Cause Tokens: {induced}\n",
      "Sentence Change Tokens: set()\n",
      "Added Points for Experiment via Token 'study'\n",
      "\n",
      "Added Points for Experiment via Token 'experiments'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[wheat plants]\n",
      "\n",
      "Token (wheat) Context: {adaptive, copper, wheat, of, to, responses, plants}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[wheat plants, wheat plants]\n",
      "\n",
      "Added Points for Interaction via Token 'plants'\n",
      "\n",
      "Token (plants) Context: {adaptive, copper, wheat, of, to, responses, plants}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 2}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[wheat plants, wheat plants, total plant]\n",
      "\n",
      "Added Points for Interaction via Token 'total'\n",
      "\n",
      "Token (total) Context: {the, experiments, proteins, seedlings, pot, plant, wheat, using, of, total}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 1, wheat seedlings: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[wheat plants, wheat plants, total plant, wheat seedlings]\n",
      "\n",
      "Added Points for Interaction via Token 'wheat'\n",
      "\n",
      "Token (wheat) Context: {the, experiments, proteins, seedlings, pot, plant, wheat, using, of, total}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 1, wheat seedlings: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 1, wheat seedlings: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[wheat plants, wheat plants, total plant, wheat seedlings, wheat seedlings]\n",
      "\n",
      "Token (seedlings) Context: {the, experiments, proteins, seedlings, pot, plant, wheat, using, of, total}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 1, wheat seedlings: 2}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 2, wheat seedlings: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[wheat plants, wheat plants, total plant, wheat seedlings, wheat seedlings, Triticum aestivum]\n",
      "\n",
      "Token (Triticum) Context: {aestivum, ), (, Triticum}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "Sentence Tokens: [A, tri, -, trophic, food, chain, soil, , plant, , herbivore, , predator, was, established, as, plants, were, infested, with, grain, aphids, (, Sitobion, avenae, ), which, were, subsequently, fed, to, predatory, ladybirds, (, Adalia, bipunctata, ), .]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: set()\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 2, wheat seedlings: 2}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 3, wheat seedlings: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[plant]\n",
      "\n",
      "Token (plant) Context: {plant}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 3, wheat seedlings: 2}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 4, wheat seedlings: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[plant, herbivore]\n",
      "\n",
      "Added Points for Interaction via Token 'herbivore'\n",
      "\n",
      "Token (herbivore) Context: {herbivore}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 4, wheat seedlings: 2}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 5, wheat seedlings: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[plant, herbivore, predator]\n",
      "\n",
      "Added Points for Interaction via Token 'predator'\n",
      "\n",
      "Token (predator) Context: {to, established, grain, were, was, which, ladybirds, plants, fed, with, as, subsequently, predator, infested, predatory, aphids, were}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 2, total plant: 5, wheat seedlings: 2}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 5, wheat seedlings: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[plant, herbivore, predator, plants]\n",
      "\n",
      "Added Points for Interaction via Token 'plants'\n",
      "\n",
      "Token (plants) Context: {to, established, grain, were, was, which, ladybirds, plants, fed, with, as, subsequently, predator, infested, predatory, aphids, were}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 5, wheat seedlings: 2}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 5, wheat seedlings: 2, grain aphids: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[plant, herbivore, predator, plants, grain aphids]\n",
      "\n",
      "Token (grain) Context: {to, established, grain, were, was, which, ladybirds, plants, fed, with, as, subsequently, predator, infested, predatory, aphids, were}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 5, wheat seedlings: 2, grain aphids: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 5, wheat seedlings: 2, grain aphids: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[plant, herbivore, predator, plants, grain aphids, aphids]\n",
      "\n",
      "Token (aphids) Context: {to, established, grain, were, was, which, ladybirds, plants, fed, with, as, subsequently, predator, infested, predatory, aphids, were}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 5, wheat seedlings: 2, grain aphids: 2}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 6, wheat seedlings: 2, grain aphids: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[plant, herbivore, predator, plants, grain aphids, aphids, Sitobion avenae]\n",
      "\n",
      "Token (Sitobion) Context: {(, Sitobion, avenae, )}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 6, wheat seedlings: 2, grain aphids: 2}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 7, wheat seedlings: 2, grain aphids: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[plant, herbivore, predator, plants, grain aphids, aphids, Sitobion avenae, Sitobion avenae]\n",
      "\n",
      "Token (avenae) Context: {(, Sitobion, avenae, )}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 7, wheat seedlings: 2, grain aphids: 2}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 7, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[plant, herbivore, predator, plants, grain aphids, aphids, Sitobion avenae, Sitobion avenae, predatory ladybirds]\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 7, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 8, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[plant, herbivore, predator, plants, grain aphids, aphids, Sitobion avenae, Sitobion avenae, predatory ladybirds, Adalia bipunctata]\n",
      "\n",
      "Token (Adalia) Context: {Adalia, bipunctata, ), (}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 8, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 9, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[plant, herbivore, predator, plants, grain aphids, aphids, Sitobion avenae, Sitobion avenae, predatory ladybirds, Adalia bipunctata, Adalia bipunctata]\n",
      "\n",
      "Token (bipunctata) Context: {Adalia, bipunctata, ), (}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "Sentence Tokens: [Multiple, measurements, were, conducted, which, deduced, that, Cu, was, taken, up, from, soil, into, the, plant, tissues, accumulating, in, the, shoot, and, ear, .]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: set()\n",
      "Added Points for Experiment via Token 'measurements'\n",
      "\n",
      "Added Points for Experiment via Token 'conducted'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 9, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 10, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[plant]\n",
      "\n",
      "Token (plant) Context: {conducted, Multiple, that, taken, plant, soil, in, were, was, deduced, from, the, shoot, accumulating, measurements, which, up, Cu, into, tissues, the}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "Sentence Tokens: [The, rate, of, growth, and, flag, leaf, length, were, affected, by, levels, of, Cu, in, the, soil, but, total, plant, mass, and, ear, weights, were, not, .]\n",
      "Sentence Cause Tokens: {affected}\n",
      "Sentence Change Tokens: set()\n",
      "TRAIT CATEGORY\n",
      "Token (rate) Context: {The, rate, of, growth}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "TRAIT CATEGORY\n",
      "Token (length) Context: {length, Cu, soil, leaf, affected, the, flag, of, were, levels, in, by}\n",
      "Cause Tokens in Context: {affected}\n",
      "Change Tokens in Context: set()\n",
      "Added Points for Trait via Token 'length'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 10, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 11, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[total plant]\n",
      "\n",
      "Token (total) Context: {total, plant, mass}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "Sentence Tokens: [Wheat, shoots, and, ears, were, analysed, for, N, (, crude, protein, ), P, ,, K, and, it, was, found, that, the, levels, of, Cu, in, soils, affected, the, levels, of, protein, in, both, the, shoot, and, the, ear, while, the, levels, of, P, and, K, remained, unaffected, .]\n",
      "Sentence Cause Tokens: {affected}\n",
      "Sentence Change Tokens: set()\n",
      "Deducted Points for Experiment via Token 'analysed'\n",
      "\n",
      "Added Points for Experiment via Token 'found'\n",
      "\n",
      "Sentence Tokens: [Total, populations, of, aphid, and, aphid, fecundity, appeared, to, be, unaffected, by, the, Cu, stress, -, induced, plants, and, no, significant, relationships, between, levels, of, N, in, plant, tissues, or, flag, leaf, length, were, found, .]\n",
      "Sentence Cause Tokens: {induced}\n",
      "Sentence Change Tokens: set()\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 11, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 12, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[aphid]\n",
      "\n",
      "Token (aphid) Context: {Total, populations, of, aphid}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 12, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 13, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[aphid, aphid]\n",
      "\n",
      "Added Points for Interaction via Token 'aphid'\n",
      "\n",
      "Token (aphid) Context: {be, the, aphid, by, to, stress, appeared, Cu, unaffected, fecundity}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 3, total plant: 13, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 4, total plant: 13, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[aphid, aphid, plants]\n",
      "\n",
      "Added Points for Interaction via Token 'plants'\n",
      "\n",
      "Token (plants) Context: {induced, plants}\n",
      "Cause Tokens in Context: {induced}\n",
      "Change Tokens in Context: set()\n",
      "Added Points for Species via Token 'plants'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 4, total plant: 13, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 4, total plant: 14, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[aphid, aphid, plants, plant]\n",
      "\n",
      "Added Points for Interaction via Token 'plant'\n",
      "\n",
      "Token (plant) Context: {between, tissues, relationships, of, significant, plant, levels, in, no, N}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "TRAIT CATEGORY\n",
      "Token (length) Context: {leaf, found, flag, were, length}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "Added Points for Experiment via Token 'found'\n",
      "\n",
      "Sentence Tokens: [Ladybirds, also, appeared, to, be, unaffected, by, the, levels, of, Cu, in, soils, as, consumption, rate, or, change, in, mass, between, the, treatments, was, not, significant, .]\n",
      "Sentence Cause Tokens: {change}\n",
      "Sentence Change Tokens: set()\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{wheat plants: 4, total plant: 14, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{wheat plants: 4, total plant: 14, wheat seedlings: 2, grain aphids: 2, predatory ladybirds: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[Ladybirds]\n",
      "\n",
      "Token (Ladybirds) Context: {appeared, levels, unaffected, in, also, consumption, be, Cu, the, as, to, Ladybirds, by, of, rate, soils}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "TRAIT CATEGORY\n",
      "Token (rate) Context: {appeared, levels, unaffected, in, also, consumption, be, Cu, the, as, to, Ladybirds, by, of, rate, soils}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "TRAIT CATEGORY\n",
      "Token (mass) Context: {in, the, not, between, change, was, treatments, mass, significant}\n",
      "Cause Tokens in Context: {change}\n",
      "Change Tokens in Context: set()\n",
      "Added Points for Trait via Token 'mass'\n",
      "\n",
      "Added Points for Experiment via Token 'treatments'\n",
      "\n",
      "Sentence Tokens: [While, the, present, study, does, not, support, a, critical, threshold, for, Cu, levels, in, agricultural, soils, it, can, conclude, that, biological, control, methods, are, unaffected, by, levels, of, Cu, in, the, soil, .]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: set()\n",
      "Added Points for Experiment via Token 'study'\n",
      "\n",
      "Added Points for Experiment via Token 'control'\n",
      "\n",
      "Trying Sentence: Soil fertility and management are paramount in ensuring food security for the growing populations.\n",
      "B\n",
      "Trying Sentence: The use of agri-chemical and products containing heavy metals inadvertently threaten both food security and the surrounding ecosystems from contamination, loss of productivity or ecosystem service.\n",
      "B\n",
      "Trying Sentence: In the present study a series of experiments on the toxic and adaptive responses of wheat plants to copper-induced stress were conducted to establish the effects of different levels of Cu (0  200 mg kg -1) on growth, nutrient levels and the total plant proteins of wheat seedlings (Triticum aestivum) using pot experiments.\n",
      "B\n",
      "Trying Sentence: A tri-trophic food chain soil  plant  herbivore  predator was established as plants were infested with grain aphids (Sitobion avenae) which were subsequently fed to predatory ladybirds (Adalia bipunctata).\n",
      "B\n",
      "Trying Sentence: Multiple measurements were conducted which deduced that Cu was taken up from soil into the plant tissues accumulating in the shoot and ear.\n",
      "B\n",
      "Trying Sentence: The rate of growth and flag leaf length were affected by levels of Cu in the soil but total plant mass and ear weights were not.\n",
      "!!!\n",
      "Trying Sentence: Wheat shoots and ears were analysed for N (crude protein) P, K and it was found that the levels of Cu in soils affected the levels of protein in both the shoot and the ear while the levels of P and K remained unaffected.\n",
      "set()\n",
      "set()\n",
      "{analysed}\n",
      "A\n",
      "Trying Sentence: Total populations of aphid and aphid fecundity appeared to be unaffected by the Cu stress-induced plants and no significant relationships between levels of N in plant tissues or flag leaf length were found.\n",
      "!!!\n",
      "{found}\n",
      "set()\n",
      "{between}\n",
      "Trait Variation Points: 0.25\n",
      "Comparing to Sentence: Ladybirds also appeared to be unaffected by the levels of Cu in soils as consumption rate or change in mass between the treatments was not significant.\n",
      "{Ladybirds}\n",
      "{change}\n",
      "set()\n",
      "set()\n",
      "Trait Variation Points after Sentence 2: 0.75\n",
      "Trait Variation Points after Adjustment: 0.16666666666666666\n",
      "Sentence 1 (7): Total populations of aphid and aphid fecundity appeared to be unaffected by the Cu stress-induced plants and no significant relationships between levels of N in plant tissues or flag leaf length were found.\n",
      "Sentence 2 (8): Ladybirds also appeared to be unaffected by the levels of Cu in soils as consumption rate or change in mass between the treatments was not significant.\n",
      "Trait Variation Points: 0.16666666666666666\n",
      "Comparing to Sentence: While the present study does not support a critical threshold for Cu levels in agricultural soils it can conclude that biological control methods are unaffected by levels of Cu in the soil.\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "D\n",
      "Trying Sentence: Ladybirds also appeared to be unaffected by the levels of Cu in soils as consumption rate or change in mass between the treatments was not significant.\n",
      "!!!\n",
      "{treatments}\n",
      "set()\n",
      "{between, treatments}\n",
      "Trait Variation Points: 0.25\n",
      "Comparing to Sentence: While the present study does not support a critical threshold for Cu levels in agricultural soils it can conclude that biological control methods are unaffected by levels of Cu in the soil.\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "D\n",
      "Trying Sentence: While the present study does not support a critical threshold for Cu levels in agricultural soils it can conclude that biological control methods are unaffected by levels of Cu in the soil.\n",
      "B\n",
      "Points B4 Normalization: [2.0, 0.3333333333333333, 4.25, 3.0, 0, 0.16666666666666666]\n"
     ]
    }
   ],
   "source": [
    "# Dataset Names: \"Examples\", \"Baseline-1\", \"SubA\", \"SubAFiltered\", \"SubB\", \"SubBFiltered\", \"C\", \"CFiltered\", \"D\", \"DFiltered\"]\n",
    "scored_data = score_dataset(\"Baseline-1\", save_output=False, version='')\n",
    "store_scored_dataset(scored_data, \"Baseline-3\", version='9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78034f48-0224-4a95-b0b1-6d90cbedc72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (28, 4)\n"
     ]
    }
   ],
   "source": [
    "data = load_preprocessed_dataset(\"Baseline-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "903acfa6-1963-4781-9a34-c241b21ab12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Temperature dependency of intraguild predation...</td>\n",
       "      <td>Environmental factors such as temperature can ...</td>\n",
       "      <td>https://doi.org/10.1002/ecy.2157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Impact of intraspecific and intraguild predati...</td>\n",
       "      <td>Exotic predators are more likely to replace re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "18  Temperature dependency of intraguild predation...   \n",
       "23  Impact of intraspecific and intraguild predati...   \n",
       "\n",
       "                                             Abstract  \\\n",
       "18  Environmental factors such as temperature can ...   \n",
       "23  Exotic predators are more likely to replace re...   \n",
       "\n",
       "                                 DOI  Score  \n",
       "18  https://doi.org/10.1002/ecy.2157      0  \n",
       "23                               NaN      0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['Title'].str.contains('native')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09abccd9-be79-4cdb-89a7-7e0617880e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Temperature dependency of intraguild predation between native and invasive crabs\n",
      "Abstract: Environmental factors such as temperature can affect the geographical distribution of species directly by exceeding physiological tolerances, or indirectly by altering physiological rates that dictate the sign and strength of species interactions. Although the direct effects of environmental conditions are relatively well studied, the effects of environmentally mediated species interactions have garnered less attention. In this study, we examined the temperature dependency of size-structured intraguild predation (IGP) between native blue crabs (Callinectes sapidus, the IG predator) and invasive green crabs (Carcinus maenas, the IG prey) to evaluate how the effect of temperature on competitive and predatory rates may influence the latitudinal distribution of these species. In outdoor mesocosm experiments, we quantified interactions between blue crabs, green crabs, and shared prey (mussels) at three temperatures reflective of those across their range, using two size classes of blue crab. At low temperatures, green crabs had a competitive advantage and IGP by blue crabs on green crabs was low. At high temperatures, size-matched blue and green crabs were competitively similar, large blue crabs had a competitive advantage, and IGP on green crabs was high. We then used parameter values generated from these experiments (temperature- and size-dependent attack rates and handling times) in a size-structured IGP model in which we varied IGP attack rate, maturation rate of the blue crab from the non-predatory to predatory size class, and resource carrying capacity at each of the three temperatures. In the model, green crabs were likely to competitively exclude blue crabs at low temperature, whereas blue crabs were likely to competitively and consumptively exclude green crabs at higher temperatures, particularly when resource productivities and rates of IGP were high. While many factors may play a role in delimiting species ranges, our results suggest that temperature-dependent interactions can influence local coexistence and are worth considering when developing mechanistic species distribution models and evaluating responses to environmental change.\n"
     ]
    }
   ],
   "source": [
    "index = 18\n",
    "\n",
    "title = data.iloc[index].Title\n",
    "abstract = data.iloc[index].Abstract\n",
    "\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Abstract: {abstract}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe27736d-19f7-4050-a356-fa17767d77c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/28/2025 18:41:30 - INFO - \t missing_keys: []\n",
      "06/28/2025 18:41:30 - INFO - \t unexpected_keys: []\n",
      "06/28/2025 18:41:30 - INFO - \t mismatched_keys: []\n",
      "06/28/2025 18:41:30 - INFO - \t error_msgs: []\n",
      "06/28/2025 18:41:30 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behaviou?r []\n",
      "[^A-Za-z]+rate [181, 715, 1373, 1460, 1477, 1863]\n",
      "colou?r []\n",
      "biomass []\n",
      "[^A-Za-z]+mass []\n",
      "[^A-Za-z]+size [480, 973, 1128, 1351, 1404, 1535]\n",
      "number []\n",
      "length []\n",
      "pattern []\n",
      "weight []\n",
      "shape []\n",
      "efficiency []\n",
      "trait []\n",
      "phenotype []\n",
      "demography []\n",
      "population (structure|mechanic)s? []\n",
      "ability []\n",
      "capacity [1570]\n",
      "height []\n",
      "width []\n",
      "[A-Za-z]+span []\n",
      "diet []\n",
      "feeding []\n",
      "nest []\n",
      "substrate []\n",
      "breeding []\n",
      "[^A-Za-z]+age[^A-Za-z]+ []\n",
      "lifespan []\n",
      "development []\n",
      "output []\n",
      "time [1393]\n",
      "periodlevel []\n",
      "configuration []\n",
      "dimorphism []\n",
      "capability []\n",
      "regulation []\n",
      "excretion []\n",
      "luminescence []\n",
      "[^A-Za-z]+role [1917]\n",
      "sensitivity []\n",
      "resistance []\n",
      "(un|(^|\\s)[A-Za-z]*-)infected []\n",
      "Tokens to Filter: [rates, rates, rates, rate, rate, rates, size, size, size, size, size, capacity, times, role]\n",
      "\tToken: rates\n",
      "\tExpanded Token: or indirectly by altering physiological rates that dictate the sign and strength of species interactions\n",
      "\tToken: rates\n",
      "\tExpanded Token: to evaluate how the effect of temperature on competitive and predatory rates may influence the latitudinal distribution of these species\n",
      "\tToken: rates\n",
      "\tExpanded Token: dependent attack rates and handling times\n",
      "\tToken: rate\n",
      "\tExpanded Token: structured IGP model in which we varied IGP attack rate\n",
      "\tToken: rate\n",
      "\tExpanded Token: maturation rate of the blue crab from the non-predatory to predatory size class\n",
      "\tToken: rates\n",
      "\tExpanded Token: particularly when resource productivities and rates of IGP were high\n",
      "\tToken: size\n",
      "\tExpanded Token: we examined the temperature dependency of size\n",
      "\tToken: size\n",
      "\tExpanded Token: using two size classes of blue crab\n",
      "\tToken: size\n",
      "\tExpanded Token: temperature- and size\n",
      "\tToken: size\n",
      "\tExpanded Token: in a size\n",
      "\tToken: size\n",
      "\tExpanded Token: maturation rate of the blue crab from the non-predatory to predatory size class\n",
      "\tToken: capacity\n",
      "\tExpanded Token: and resource carrying capacity at each of the three temperatures\n",
      "\tToken: times\n",
      "\tExpanded Token: dependent attack rates and handling times\n",
      "\tToken: role\n",
      "\tExpanded Token: While many factors may play a role in delimiting species ranges\n",
      "Filtered Tokens: [rates, rates, rate, size, size, role]\n",
      "(one|two|three|four|five|six|seven|eight|nine|ten|twice|thrice|([0-9]+|[0-9]+.[0-9]+)(x|%))[\\s-]+[^\\s]*[\\s-]+(as|more|than|likely)([\\s-]+|$) []\n",
      "co-?evolution []\n",
      "evolution []\n",
      "between [524, 843]\n",
      "against []\n",
      "independen(t|ts|tly|cy) []\n",
      "dependen(t|ts|tly|cy) [467, 1357, 1990]\n",
      "treatments? []\n",
      "effect [268, 337, 665]\n",
      "Matched Span: at three\n",
      "Cause Tokens: [affect, altering, influence, when, influence, when, change]\n",
      "Change Tokens: [less, low, low, low, higher]\n",
      "Experiment Tokens: [studied, study, examined, dependency, evaluate, experiments, experiments, dependent, model, model, results, suggest, dependent, models]\n",
      "Negative Experiment Tokens: []\n",
      "Negative Topic Tokens: []\n",
      "Trait Tokens: [rates, rates, rate, size, size, role]\n",
      "Species Tokens: [species, species, species, native, Callinectes, IG, invasive, Carcinus, IG, species, blue, green, prey, mussels, blue, green, blue, green, green, large, green, blue, green, blue, blue, green, species, mechanistic]\n",
      "Sentence Tokens: [Environmental, factors, such, as, temperature, can, affect, the, geographical, distribution, of, species, directly, by, exceeding, physiological, tolerances, ,, or, indirectly, by, altering, physiological, rates, that, dictate, the, sign, and, strength, of, species, interactions, .]\n",
      "Sentence Cause Tokens: {altering, affect}\n",
      "Sentence Change Tokens: set()\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[species]\n",
      "\n",
      "Token (species) Context: {tolerances, affect, directly, distribution, physiological, can, such, geographical, species, factors, exceeding, temperature, of, Environmental, the, by, as}\n",
      "Cause Tokens in Context: {affect}\n",
      "Change Tokens in Context: set()\n",
      "Added Points for Species via Token 'species'\n",
      "\n",
      "TRAIT CATEGORY\n",
      "Token (rates) Context: {physiological, dictate, that, altering, sign, by, rates, indirectly, the}\n",
      "Cause Tokens in Context: {altering}\n",
      "Change Tokens in Context: set()\n",
      "Added Points for Trait via Token 'rates'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[species, species]\n",
      "\n",
      "Added Points for Interaction via Token 'species'\n",
      "\n",
      "Token (species) Context: {interactions, strength, of, species}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "Sentence Tokens: [Although, the, direct, effects, of, environmental, conditions, are, relatively, well, studied, ,, the, effects, of, environmentally, mediated, species, interactions, have, garnered, less, attention, .]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: {less}\n",
      "Added Points for Experiment via Token 'studied'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 2}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 3}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[species]\n",
      "\n",
      "Token (species) Context: {species, garnered, mediated, effects, have, attention, the, environmentally, less, interactions, of}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: {less}\n",
      "Added Points for Species via Token 'species'\n",
      "\n",
      "Sentence Tokens: [In, this, study, ,, we, examined, the, temperature, dependency, of, size, -, structured, intraguild, predation, (, IGP, ), between, native, blue, crabs, (, Callinectes, sapidus, ,, the, IG, predator, ), and, invasive, green, crabs, (, Carcinus, maenas, ,, the, IG, prey, ), to, evaluate, how, the, effect, of, temperature, on, competitive, and, predatory, rates, may, influence, the, latitudinal, distribution, of, these, species, .]\n",
      "Sentence Cause Tokens: {influence}\n",
      "Sentence Change Tokens: set()\n",
      "Added Points for Experiment via Token 'study'\n",
      "\n",
      "Added Points for Experiment via Token 'examined'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 3}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 3, native blue crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[native blue crabs]\n",
      "\n",
      "Token (native) Context: {native, structured, between, crabs, predation, blue, intraguild}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 3, native blue crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 4, native blue crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[native blue crabs, Callinectes sapidus]\n",
      "\n",
      "Added Points for Interaction via Token 'Callinectes'\n",
      "\n",
      "Token (Callinectes) Context: {,, (, predator, sapidus, IG, Callinectes, the, )}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 4, native blue crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 4, native blue crabs: 1, IG predator: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[native blue crabs, Callinectes sapidus, IG predator]\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 4, native blue crabs: 1, IG predator: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 4, native blue crabs: 1, IG predator: 1, invasive green crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[native blue crabs, Callinectes sapidus, IG predator, invasive green crabs]\n",
      "\n",
      "Added Points for Interaction via Token 'invasive'\n",
      "\n",
      "Token (invasive) Context: {temperature, how, of, invasive, competitive, evaluate, on, crabs, effect, to, green, the}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 4, native blue crabs: 1, IG predator: 1, invasive green crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 5, native blue crabs: 1, IG predator: 1, invasive green crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[native blue crabs, Callinectes sapidus, IG predator, invasive green crabs, Carcinus maenas]\n",
      "\n",
      "Added Points for Interaction via Token 'Carcinus'\n",
      "\n",
      "Token (Carcinus) Context: {the, ), ,, (, prey, maenas, IG, Carcinus}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 5, native blue crabs: 1, IG predator: 1, invasive green crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 5, native blue crabs: 1, IG predator: 2, invasive green crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[native blue crabs, Callinectes sapidus, IG predator, invasive green crabs, Carcinus maenas, IG prey]\n",
      "\n",
      "Token (IG) Context: {the, ), ,, (, prey, maenas, IG, Carcinus}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "TRAIT CATEGORY\n",
      "Token (rates) Context: {may, latitudinal, these, the, rates, of, predatory, influence, species, distribution}\n",
      "Cause Tokens in Context: {influence}\n",
      "Change Tokens in Context: set()\n",
      "Added Points for Trait via Token 'rates'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 5, native blue crabs: 1, IG predator: 2, invasive green crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 1, IG predator: 2, invasive green crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[native blue crabs, Callinectes sapidus, IG predator, invasive green crabs, Carcinus maenas, IG prey, species]\n",
      "\n",
      "Token (species) Context: {may, latitudinal, these, the, rates, of, predatory, influence, species, distribution}\n",
      "Cause Tokens in Context: {influence}\n",
      "Change Tokens in Context: set()\n",
      "Added Points for Species via Token 'species'\n",
      "\n",
      "Sentence Tokens: [In, outdoor, mesocosm, experiments, ,, we, quantified, interactions, between, blue, crabs, ,, green, crabs, ,, and, shared, prey, (, mussels, ), at, three, temperatures, reflective, of, those, across, their, range, ,, using, two, size, classes, of, blue, crab, .]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: set()\n",
      "Added Points for Experiment via Token 'experiments'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 1, IG predator: 2, invasive green crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 2, IG predator: 2, invasive green crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[blue crabs]\n",
      "\n",
      "Token (blue) Context: {interactions, crabs, quantified, blue, we, between}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 2, IG predator: 2, invasive green crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 2, IG predator: 2, invasive green crabs: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[blue crabs, green crabs]\n",
      "\n",
      "Added Points for Interaction via Token 'green'\n",
      "\n",
      "Token (green) Context: {outdoor, In, crabs, experiments, green, using, mesocosm}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 2, IG predator: 2, invasive green crabs: 2}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 2, IG predator: 3, invasive green crabs: 2}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[blue crabs, green crabs, prey]\n",
      "\n",
      "Added Points for Interaction via Token 'prey'\n",
      "\n",
      "Token (prey) Context: {at, shared, prey}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 2, IG predator: 3, invasive green crabs: 2}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 2, IG predator: 3, invasive green crabs: 2, mussels: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[blue crabs, green crabs, prey, mussels]\n",
      "\n",
      "TRAIT CATEGORY\n",
      "Token (size) Context: {blue, of, classes, crab, size}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 2, IG predator: 3, invasive green crabs: 2, mussels: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 2, IG predator: 4, invasive green crabs: 2, mussels: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[blue crabs, green crabs, prey, mussels, blue crab]\n",
      "\n",
      "Added Points for Interaction via Token 'blue'\n",
      "\n",
      "Token (blue) Context: {blue, of, classes, crab, size}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "Sentence Tokens: [At, low, temperatures, ,, green, crabs, had, a, competitive, advantage, and, IGP, by, blue, crabs, on, green, crabs, was, low, .]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: {low, low}\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 2, IG predator: 4, invasive green crabs: 2, mussels: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 2, IG predator: 4, invasive green crabs: 3, mussels: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[green crabs]\n",
      "\n",
      "Token (green) Context: {a, had, advantage, crabs, competitive, green}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 2, IG predator: 4, invasive green crabs: 3, mussels: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 4, invasive green crabs: 3, mussels: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[green crabs, blue crabs]\n",
      "\n",
      "Added Points for Interaction via Token 'blue'\n",
      "\n",
      "Token (blue) Context: {blue, green, low, by, was, on, IGP, crabs, crabs}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: {low}\n",
      "Added Points for Species via Token 'blue'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 4, invasive green crabs: 3, mussels: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 4, invasive green crabs: 4, mussels: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[green crabs, blue crabs, green crabs]\n",
      "\n",
      "Added Points for Interaction via Token 'green'\n",
      "\n",
      "Token (green) Context: {blue, green, low, by, was, on, IGP, crabs, crabs}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: {low}\n",
      "Added Points for Species via Token 'green'\n",
      "\n",
      "Sentence Tokens: [At, high, temperatures, ,, size, -, matched, blue, and, green, crabs, were, competitively, similar, ,, large, blue, crabs, had, a, competitive, advantage, ,, and, IGP, on, green, crabs, was, high, .]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: set()\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 4, invasive green crabs: 4, mussels: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 4, invasive green crabs: 5, mussels: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[green crabs]\n",
      "\n",
      "Token (green) Context: {were, crabs, similar, green, competitively}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 4, invasive green crabs: 5, mussels: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 4, invasive green crabs: 5, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[green crabs, large blue crabs]\n",
      "\n",
      "Added Points for Interaction via Token 'large'\n",
      "\n",
      "Token (large) Context: {large, advantage, crabs, high, competitive, blue, At, a, had, temperatures}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 4, invasive green crabs: 5, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 4, invasive green crabs: 6, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[green crabs, large blue crabs, green crabs]\n",
      "\n",
      "Added Points for Interaction via Token 'green'\n",
      "\n",
      "Token (green) Context: {IGP, crabs, green, high, was, on}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "Sentence Tokens: [We, then, used, parameter, values, generated, from, these, experiments, (, temperature-, and, size, -, dependent, attack, rates, and, handling, times, ), in, a, size, -, structured, IGP, model, in, which, we, varied, IGP, attack, rate, ,, maturation, rate, of, the, blue, crab, from, the, non, -, predatory, to, predatory, size, class, ,, and, resource, carrying, capacity, at, each, of, the, three, temperatures, .]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: set()\n",
      "Added Points for Experiment via Token 'experiments'\n",
      "\n",
      "Added Points for Experiment via Token 'dependent'\n",
      "\n",
      "TRAIT CATEGORY\n",
      "Token (rate) Context: {-, we, model, attack, maturation, from, the, -, IGP, IGP, which, ,, of, non, crab, structured, in, varied, rate, rate, blue, the}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 4, invasive green crabs: 6, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 5, invasive green crabs: 6, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[blue crab]\n",
      "\n",
      "Token (blue) Context: {-, we, model, attack, maturation, from, the, -, IGP, IGP, which, ,, of, non, crab, structured, in, varied, rate, rate, blue, the}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "TRAIT CATEGORY\n",
      "Token (size) Context: {We, from, parameter, predatory, predatory, a, used, to, in, generated, class, experiments, size, values, size, then, these}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "Sentence Tokens: [In, the, model, ,, green, crabs, were, likely, to, competitively, exclude, blue, crabs, at, low, temperature, ,, whereas, blue, crabs, were, likely, to, competitively, and, consumptively, exclude, green, crabs, at, higher, temperatures, ,, particularly, when, resource, productivities, and, rates, of, IGP, were, high, .]\n",
      "Sentence Cause Tokens: {when}\n",
      "Sentence Change Tokens: {higher, low}\n",
      "Added Points for Experiment via Token 'model'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 5, invasive green crabs: 6, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 5, invasive green crabs: 7, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[green crabs]\n",
      "\n",
      "Token (green) Context: {blue, particularly, likely, productivities, green, exclude, at, were, crabs, resource, competitively, temperature, crabs, when, to, low}\n",
      "Cause Tokens in Context: {when}\n",
      "Change Tokens in Context: {low}\n",
      "Added Points for Species via Token 'green'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 3, IG predator: 5, invasive green crabs: 7, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 4, IG predator: 5, invasive green crabs: 7, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[green crabs, blue crabs]\n",
      "\n",
      "Added Points for Interaction via Token 'blue'\n",
      "\n",
      "Token (blue) Context: {blue, particularly, likely, productivities, green, exclude, at, were, crabs, resource, competitively, temperature, crabs, when, to, low}\n",
      "Cause Tokens in Context: {when}\n",
      "Change Tokens in Context: {low}\n",
      "Added Points for Species via Token 'blue'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 4, IG predator: 5, invasive green crabs: 7, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 5, IG predator: 5, invasive green crabs: 7, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[green crabs, blue crabs, blue crabs]\n",
      "\n",
      "Added Points for Interaction via Token 'blue'\n",
      "\n",
      "Token (blue) Context: {whereas, the, were, In, competitively, crabs, to, blue, model, likely}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 5, IG predator: 5, invasive green crabs: 7, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 5, IG predator: 5, invasive green crabs: 8, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[green crabs, blue crabs, blue crabs, green crabs]\n",
      "\n",
      "Added Points for Interaction via Token 'green'\n",
      "\n",
      "Token (green) Context: {green, higher, exclude, at, consumptively, temperatures, crabs}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: {higher}\n",
      "Added Points for Species via Token 'green'\n",
      "\n",
      "Sentence Tokens: [While, many, factors, may, play, a, role, in, delimiting, species, ranges, ,, our, results, suggest, that, temperature, -, dependent, interactions, can, influence, local, coexistence, and, are, worth, considering, when, developing, mechanistic, species, distribution, models, and, evaluating, responses, to, environmental, change, .]\n",
      "Sentence Cause Tokens: {when, change, influence}\n",
      "Sentence Change Tokens: set()\n",
      "TRAIT CATEGORY\n",
      "Token (role) Context: {factors, delimiting, a, many, play, in, ranges, While, role, may, species}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 6, native blue crabs: 5, IG predator: 5, invasive green crabs: 8, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 7, native blue crabs: 5, IG predator: 5, invasive green crabs: 8, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[species]\n",
      "\n",
      "Token (species) Context: {factors, delimiting, a, many, play, in, ranges, While, role, may, species}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "\n",
      "Added Points for Experiment via Token 'results'\n",
      "\n",
      "Added Points for Experiment via Token 'suggest'\n",
      "\n",
      "SPECIES CATEGORY\n",
      "Seen Species Updated\n",
      "{species: 7, native blue crabs: 5, IG predator: 5, invasive green crabs: 8, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species Updated\n",
      "{species: 8, native blue crabs: 5, IG predator: 5, invasive green crabs: 8, mussels: 1, large blue crabs: 1}\n",
      "\n",
      "Seen Species in Sentence\n",
      "[species, mechanistic species]\n",
      "\n",
      "Added Points for Interaction via Token 'mechanistic'\n",
      "\n",
      "Token (mechanistic) Context: {when, species, considering, mechanistic, models, worth, distribution, developing, are}\n",
      "Cause Tokens in Context: {when}\n",
      "Change Tokens in Context: set()\n",
      "Added Points for Species via Token 'mechanistic'\n",
      "\n",
      "Trying Sentence: Environmental factors such as temperature can affect the geographical distribution of species directly by exceeding physiological tolerances, or indirectly by altering physiological rates that dictate the sign and strength of species interactions.\n",
      "!!!\n",
      "Trying Sentence: Although the direct effects of environmental conditions are relatively well studied, the effects of environmentally mediated species interactions have garnered less attention.\n",
      "B\n",
      "Trying Sentence: In this study, we examined the temperature dependency of size-structured intraguild predation (IGP) between native blue crabs (Callinectes sapidus, the IG predator) and invasive green crabs (Carcinus maenas, the IG prey) to evaluate how the effect of temperature on competitive and predatory rates may influence the latitudinal distribution of these species.\n",
      "!!!\n",
      "{study, evaluate, examined, dependency}\n",
      "{evaluate, examined}\n",
      "{dependency, effect, between}\n",
      "Trait Variation Points: 0.5\n",
      "Comparing to Sentence: In outdoor mesocosm experiments, we quantified interactions between blue crabs, green crabs, and shared prey (mussels) at three temperatures reflective of those across their range, using two size classes of blue crab.\n",
      "{blue, mussels, green, blue, prey}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "D\n",
      "Comparing to Sentence: At low temperatures, green crabs had a competitive advantage and IGP by blue crabs on green crabs was low.\n",
      "{blue, green, green}\n",
      "set()\n",
      "{low, low}\n",
      "set()\n",
      "Trait Variation Points after Sentence 2: 0.9375\n",
      "Trait Variation Points after Adjustment: 0.703125\n",
      "Sentence 1 (2): In this study, we examined the temperature dependency of size-structured intraguild predation (IGP) between native blue crabs (Callinectes sapidus, the IG predator) and invasive green crabs (Carcinus maenas, the IG prey) to evaluate how the effect of temperature on competitive and predatory rates may influence the latitudinal distribution of these species.\n",
      "Sentence 2 (4): At low temperatures, green crabs had a competitive advantage and IGP by blue crabs on green crabs was low.\n",
      "Trait Variation Points: 0.703125\n",
      "Comparing to Sentence: At high temperatures, size-matched blue and green crabs were competitively similar, large blue crabs had a competitive advantage, and IGP on green crabs was high.\n",
      "{green, green, large}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "D\n",
      "Comparing to Sentence: We then used parameter values generated from these experiments (temperature- and size-dependent attack rates and handling times) in a size-structured IGP model in which we varied IGP attack rate, maturation rate of the blue crab from the non-predatory to predatory size class, and resource carrying capacity at each of the three temperatures.\n",
      "{blue}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "D\n",
      "Comparing to Sentence: In the model, green crabs were likely to competitively exclude blue crabs at low temperature, whereas blue crabs were likely to competitively and consumptively exclude green crabs at higher temperatures, particularly when resource productivities and rates of IGP were high.\n",
      "{green, blue, blue, green}\n",
      "{when}\n",
      "{higher, low}\n",
      "set()\n",
      "Trait Variation Points after Sentence 2: 0.75\n",
      "Trait Variation Points after Adjustment: 0.5625\n",
      "Sentence 1 (2): In this study, we examined the temperature dependency of size-structured intraguild predation (IGP) between native blue crabs (Callinectes sapidus, the IG predator) and invasive green crabs (Carcinus maenas, the IG prey) to evaluate how the effect of temperature on competitive and predatory rates may influence the latitudinal distribution of these species.\n",
      "Sentence 2 (7): In the model, green crabs were likely to competitively exclude blue crabs at low temperature, whereas blue crabs were likely to competitively and consumptively exclude green crabs at higher temperatures, particularly when resource productivities and rates of IGP were high.\n",
      "Trait Variation Points: 0.5625\n",
      "Comparing to Sentence: While many factors may play a role in delimiting species ranges, our results suggest that temperature-dependent interactions can influence local coexistence and are worth considering when developing mechanistic species distribution models and evaluating responses to environmental change.\n",
      "{mechanistic, species}\n",
      "{when, change, influence}\n",
      "set()\n",
      "{results}\n",
      "Trait Variation Points after Sentence 2: 0.6875\n",
      "Trait Variation Points after Adjustment: 0.515625\n",
      "Sentence 1 (2): In this study, we examined the temperature dependency of size-structured intraguild predation (IGP) between native blue crabs (Callinectes sapidus, the IG predator) and invasive green crabs (Carcinus maenas, the IG prey) to evaluate how the effect of temperature on competitive and predatory rates may influence the latitudinal distribution of these species.\n",
      "Sentence 2 (8): While many factors may play a role in delimiting species ranges, our results suggest that temperature-dependent interactions can influence local coexistence and are worth considering when developing mechanistic species distribution models and evaluating responses to environmental change.\n",
      "Trait Variation Points: 0.515625\n",
      "Trying Sentence: In outdoor mesocosm experiments, we quantified interactions between blue crabs, green crabs, and shared prey (mussels) at three temperatures reflective of those across their range, using two size classes of blue crab.\n",
      "!!!\n",
      "{experiments}\n",
      "set()\n",
      "{between, at}\n",
      "Trait Variation Points: 0.25\n",
      "Comparing to Sentence: At low temperatures, green crabs had a competitive advantage and IGP by blue crabs on green crabs was low.\n",
      "{blue, green, green}\n",
      "set()\n",
      "{low, low}\n",
      "set()\n",
      "Trait Variation Points after Sentence 2: 0.75\n",
      "Trait Variation Points after Adjustment: 0.46875\n",
      "Sentence 1 (3): In outdoor mesocosm experiments, we quantified interactions between blue crabs, green crabs, and shared prey (mussels) at three temperatures reflective of those across their range, using two size classes of blue crab.\n",
      "Sentence 2 (4): At low temperatures, green crabs had a competitive advantage and IGP by blue crabs on green crabs was low.\n",
      "Trait Variation Points: 0.46875\n",
      "Comparing to Sentence: At high temperatures, size-matched blue and green crabs were competitively similar, large blue crabs had a competitive advantage, and IGP on green crabs was high.\n",
      "{green, green, large}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "D\n",
      "Comparing to Sentence: We then used parameter values generated from these experiments (temperature- and size-dependent attack rates and handling times) in a size-structured IGP model in which we varied IGP attack rate, maturation rate of the blue crab from the non-predatory to predatory size class, and resource carrying capacity at each of the three temperatures.\n",
      "{blue}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "D\n",
      "Comparing to Sentence: In the model, green crabs were likely to competitively exclude blue crabs at low temperature, whereas blue crabs were likely to competitively and consumptively exclude green crabs at higher temperatures, particularly when resource productivities and rates of IGP were high.\n",
      "{green, blue, blue, green}\n",
      "{when}\n",
      "{higher, low}\n",
      "set()\n",
      "Trait Variation Points after Sentence 2: 0.5625\n",
      "Trait Variation Points after Adjustment: 0.3515625\n",
      "Sentence 1 (3): In outdoor mesocosm experiments, we quantified interactions between blue crabs, green crabs, and shared prey (mussels) at three temperatures reflective of those across their range, using two size classes of blue crab.\n",
      "Sentence 2 (7): In the model, green crabs were likely to competitively exclude blue crabs at low temperature, whereas blue crabs were likely to competitively and consumptively exclude green crabs at higher temperatures, particularly when resource productivities and rates of IGP were high.\n",
      "Trait Variation Points: 0.3515625\n",
      "Comparing to Sentence: While many factors may play a role in delimiting species ranges, our results suggest that temperature-dependent interactions can influence local coexistence and are worth considering when developing mechanistic species distribution models and evaluating responses to environmental change.\n",
      "{mechanistic, species}\n",
      "{when, change, influence}\n",
      "set()\n",
      "{results}\n",
      "Trait Variation Points after Sentence 2: 0.5\n",
      "Trait Variation Points after Adjustment: 0.3125\n",
      "Sentence 1 (3): In outdoor mesocosm experiments, we quantified interactions between blue crabs, green crabs, and shared prey (mussels) at three temperatures reflective of those across their range, using two size classes of blue crab.\n",
      "Sentence 2 (8): While many factors may play a role in delimiting species ranges, our results suggest that temperature-dependent interactions can influence local coexistence and are worth considering when developing mechanistic species distribution models and evaluating responses to environmental change.\n",
      "Trait Variation Points: 0.3125\n",
      "Trying Sentence: At low temperatures, green crabs had a competitive advantage and IGP by blue crabs on green crabs was low.\n",
      "B\n",
      "Trying Sentence: At high temperatures, size-matched blue and green crabs were competitively similar, large blue crabs had a competitive advantage, and IGP on green crabs was high.\n",
      "B\n",
      "Trying Sentence: We then used parameter values generated from these experiments (temperature- and size-dependent attack rates and handling times) in a size-structured IGP model in which we varied IGP attack rate, maturation rate of the blue crab from the non-predatory to predatory size class, and resource carrying capacity at each of the three temperatures.\n",
      "!!!\n",
      "{experiments, dependent, model}\n",
      "set()\n",
      "{dependent}\n",
      "Trait Variation Points: 0.25\n",
      "Comparing to Sentence: In the model, green crabs were likely to competitively exclude blue crabs at low temperature, whereas blue crabs were likely to competitively and consumptively exclude green crabs at higher temperatures, particularly when resource productivities and rates of IGP were high.\n",
      "{green, blue, blue, green}\n",
      "{when}\n",
      "{higher, low}\n",
      "set()\n",
      "Trait Variation Points after Sentence 2: 0.75\n",
      "Trait Variation Points after Adjustment: 0.1875\n",
      "Sentence 1 (6): We then used parameter values generated from these experiments (temperature- and size-dependent attack rates and handling times) in a size-structured IGP model in which we varied IGP attack rate, maturation rate of the blue crab from the non-predatory to predatory size class, and resource carrying capacity at each of the three temperatures.\n",
      "Sentence 2 (7): In the model, green crabs were likely to competitively exclude blue crabs at low temperature, whereas blue crabs were likely to competitively and consumptively exclude green crabs at higher temperatures, particularly when resource productivities and rates of IGP were high.\n",
      "Trait Variation Points: 0.1875\n",
      "Comparing to Sentence: While many factors may play a role in delimiting species ranges, our results suggest that temperature-dependent interactions can influence local coexistence and are worth considering when developing mechanistic species distribution models and evaluating responses to environmental change.\n",
      "{mechanistic, species}\n",
      "{when, change, influence}\n",
      "set()\n",
      "{results}\n",
      "Trait Variation Points after Sentence 2: 0.6875\n",
      "Trait Variation Points after Adjustment: 0.171875\n",
      "Sentence 1 (6): We then used parameter values generated from these experiments (temperature- and size-dependent attack rates and handling times) in a size-structured IGP model in which we varied IGP attack rate, maturation rate of the blue crab from the non-predatory to predatory size class, and resource carrying capacity at each of the three temperatures.\n",
      "Sentence 2 (8): While many factors may play a role in delimiting species ranges, our results suggest that temperature-dependent interactions can influence local coexistence and are worth considering when developing mechanistic species distribution models and evaluating responses to environmental change.\n",
      "Trait Variation Points: 0.171875\n",
      "Trying Sentence: In the model, green crabs were likely to competitively exclude blue crabs at low temperature, whereas blue crabs were likely to competitively and consumptively exclude green crabs at higher temperatures, particularly when resource productivities and rates of IGP were high.\n",
      "B\n",
      "Trying Sentence: While many factors may play a role in delimiting species ranges, our results suggest that temperature-dependent interactions can influence local coexistence and are worth considering when developing mechanistic species distribution models and evaluating responses to environmental change.\n",
      "!!!\n",
      "{dependent, results, suggest, models}\n",
      "{evaluating}\n",
      "{dependent}\n",
      "Trait Variation Points: 0.5\n",
      "Points B4 Normalization: [2.0, 3.0, 4.875, 4.999999999999999, 0, 0.703125]\n",
      "(0.5206597222222222, [0.2222222222222222, 0.3333333333333333, 0.5416666666666666, 0.5555555555555555, 1.0, 0.703125])\n"
     ]
    }
   ],
   "source": [
    "main = Main()\n",
    "main.update_text(str(abstract))\n",
    "print(main.score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3be39281-ba1e-42a2-974b-06095bf8f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in main.sp_doc:\n",
    "    if token.lower_ == \"uninfected\":\n",
    "        print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d5598-955e-4819-8037-04bd854b30d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
