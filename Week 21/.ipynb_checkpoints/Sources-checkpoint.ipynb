{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a19bf17-5833-4831-a063-8d98ee8dbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "%run \"./DataFrame.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad882e-def5-4cd8-9036-d5582db9f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'-\\n', '', text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+([?.!,])\", r\"\\1\", text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d52891-3ab5-4c96-add6-486d6dd7cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Papers(DataFrame):\n",
    "    COLUMNS = {\"DOI\": str, \"Title\": str, \"Abstract\": str, \"ID\": str}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Papers, self).__init__(columns=Papers.COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bec877-2f2f-4690-ad87-0e0f46be938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Source(Papers):\n",
    "    CLF_MDL = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "    CLF = pipeline(\"zero-shot-classification\", model=CLF_MDL, device=\"cpu\")\n",
    "\n",
    "    SEARCH_TERMS = [\n",
    "        [\n",
    "            \"trait\",\n",
    "            \"phenotype\"\n",
    "        ],\n",
    "        [\n",
    "            \"trait-mediated\",\n",
    "            \"higher-order interaction\",\n",
    "            \"polymorphism\",\n",
    "            \"interaction modification\",\n",
    "            \"indirect effect\"\n",
    "        ],\n",
    "        [\n",
    "            \"apparent competition\",\n",
    "            \"resource competition\",\n",
    "            \"intransitive competition\",\n",
    "            \"mutual competition\",\n",
    "            \"keystone predation\",\n",
    "            \"intraguild predation\",\n",
    "            \"trophic chain\",\n",
    "            \"competition chain\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "\n",
    "\n",
    "    \n",
    "    def on_topic(self, text):\n",
    "        topics = [(\"ecology\", 0.9)]\n",
    "        scores = {}\n",
    "\n",
    "        good = True\n",
    "    \n",
    "        for topic, threshold in topics:\n",
    "            output = Source.CLF(text, [topic])\n",
    "            \n",
    "            score = output[\"scores\"][0]\n",
    "            scores[topic] = score\n",
    "            \n",
    "            if score < threshold:\n",
    "                good = False\n",
    "                \n",
    "        return (good, scores)\n",
    "\n",
    "\n",
    "    \n",
    "    def clean(self):\n",
    "        print(\"Cleaning...\")\n",
    "\n",
    "        self.df.drop_duplicates(subset=[\"DOI\"], inplace=True)\n",
    "        print(f\"Dropped Duplicates, Number Papers: {self.df.shape[0]}\")\n",
    "        \n",
    "        self.df.dropna(inplace=True)\n",
    "        print(f\"Dropped NAs, Number Papers: {self.df.shape[0]}\")\n",
    "\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "    def save_filter_results(self, name):\n",
    "        # Ecology\n",
    "        df_eco = DataFrame(df=self.df.loc[self.df[\"Is Ecology\"] == True])\n",
    "        df_eco.write(f\"{name}-Ecology\", reset_index=True)\n",
    "\n",
    "        # Not Ecology\n",
    "        df_not_eco = DataFrame(df=self.df.loc[self.df[\"Is Ecology\"] == False])\n",
    "        df_not_eco.write(f\"{name}-NotEcology\", reset_index=True)\n",
    "\n",
    "        return (df_eco, df_not_eco)\n",
    "\n",
    "\n",
    "    \n",
    "    def filter(self, name, auto_save=500):\n",
    "        print(\"Filtering...\")\n",
    "\n",
    "        \n",
    "        if \"Is Ecology\" not in self.df.columns:\n",
    "            self.add_col(\"Is Ecology\", False, dtype=bool)\n",
    "\n",
    "        if \"Ecology Score\" not in self.df.columns:\n",
    "            self.add_col(\"Ecology Score\", math.nan, dtype=float)\n",
    "\n",
    "        \n",
    "        i = 0\n",
    "        num_filtered = 0\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            print(f\"{i+1}/{self.df.shape[0]}\")\n",
    "\n",
    "            # Abstract\n",
    "            abstract = self.df.loc[idx].Abstract\n",
    "            abstract_NaN = type(abstract) in [type(0), type(0.0)] and math.isnan(abstract)\n",
    "            \n",
    "            if abstract_NaN or not abstract:\n",
    "                abstract = \"\"\n",
    "            \n",
    "            abstract = abstract[:3000]\n",
    "            \n",
    "            # Deters Redundancy\n",
    "            unfiltered = math.isnan(self.df.loc[idx, \"Ecology Score\"])\n",
    "            \n",
    "            if not abstract:\n",
    "                self.df.loc[idx, \"Is Ecology\"] = False\n",
    "                self.df.loc[idx, \"Ecology Score\"] = -1\n",
    "            elif unfiltered:\n",
    "                out = self.on_topic(abstract)\n",
    "                self.df.loc[idx, \"Is Ecology\"] = out[0]\n",
    "                self.df.loc[idx, \"Ecology Score\"] = out[1][\"ecology\"]\n",
    "                num_filtered += 1\n",
    "\n",
    "                if num_filtered % auto_save == 0:\n",
    "                    self.save_filter_results(name)\n",
    "\n",
    "            i += 1\n",
    "            \n",
    "        df_eco, df_not_eco = self.save_filter_results(name)\n",
    "        \n",
    "        print(f\"Number Ecology Papers: {df_eco.df.shape[0]}\")\n",
    "        print(f\"Number Not-Ecology Papers: {df_not_eco.df.shape[0]}\")\n",
    "\n",
    "        self.df = df_eco.df\n",
    "\n",
    "    def search(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def expand(self, max_hops=3, max_citations=3):\n",
    "        # max_hops: The maximum level we can\n",
    "        # reach in the citation tree.\n",
    "        # max_citations: The maximum number of\n",
    "        # citations we can visit for each work.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def identify(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47199c16-9d25-4ff8-a123-88b24524038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAlex(Source):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"OpenAlex\")\n",
    "\n",
    "\n",
    "    \n",
    "    def extract_id(self, url):\n",
    "        if not url:\n",
    "            return \"\"\n",
    "        \n",
    "        url_prefix = \"https://openalex.org/\"\n",
    "        url_prefix_len = len(url_prefix)\n",
    "        return url[url_prefix_len:]\n",
    "\n",
    "\n",
    "    \n",
    "    def revert_abstract(self, inverted_abstract):\n",
    "        if not inverted_abstract:\n",
    "            return \"\"\n",
    "\n",
    "        i = 0\n",
    "        abstract = \"\"\n",
    "        \n",
    "        while True:\n",
    "            index_found = False\n",
    "            for k, v in inverted_abstract.items():\n",
    "                if i in v:\n",
    "                    abstract += \" \"\n",
    "                    abstract += k\n",
    "                    i += 1\n",
    "                    index_found = True\n",
    "            if not index_found:\n",
    "                break\n",
    "        \n",
    "        return abstract\n",
    "\n",
    "\n",
    "    \n",
    "    def get_search_filter(self, search_terms_2D, from_date=\"1800-01-01\", to_date=\"2030-01-01\"):\n",
    "        search_terms_1D = []\n",
    "        \n",
    "        for search_terms in search_terms_2D:\n",
    "            search_terms_1D.append(\"|\".join(search_terms).replace(\" \", \"%20\"))\n",
    "        search_filter = \",\".join([f\"title_and_abstract.search:{search}\" for search in search_terms_1D])\n",
    "\n",
    "        search_filter += f',has_abstract:true'\n",
    "        search_filter += f',to_publication_date:{to_date}'\n",
    "        search_filter += f',from_publication_date:{from_date}'\n",
    "        search_filter += f',type:article'\n",
    "\n",
    "        return search_filter\n",
    "\n",
    "\n",
    "    \n",
    "    def search(self, search_filter, max_num_papers=math.inf):\n",
    "        print(\"Search...\")\n",
    "        \n",
    "        # Exponential Backoff\n",
    "        # c is the number of adverse events. An adverse event\n",
    "        # is a thrown exception. If c >= max_c, we exit the \n",
    "        # process.\n",
    "        c = 0\n",
    "        max_c = 6\n",
    "        \n",
    "        URL = f\"https://api.openalex.org/works?filter={search_filter}\"\n",
    "        print(URL)\n",
    "    \n",
    "        page = 1\n",
    "        while self.df.shape[0] < max_num_papers:\n",
    "            if c > 0:\n",
    "                t = 2 ** c\n",
    "                print(f\"Sleeping for {t}s\")\n",
    "                time.sleep(t)\n",
    "\n",
    "            # Requesting the API\n",
    "            url = f\"{URL}&page={page}\"\n",
    "            print(url)\n",
    "\n",
    "            try:\n",
    "                out = requests.get(url)\n",
    "                out = out.json()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "                c += 1\n",
    "                if c >= max_c:\n",
    "                    # Things aren't working out, and we\n",
    "                    # should exit the process.\n",
    "                    break\n",
    "                else:\n",
    "                    # We can still try again.\n",
    "                    continue\n",
    "\n",
    "            # If the output is null or there are no\n",
    "            # results, we exit the search.\n",
    "            if not out or not out.get(\"results\"):\n",
    "                print(\"No Results\")\n",
    "                break\n",
    "\n",
    "            # This prints the number of papers\n",
    "            # the API identified.\n",
    "            count = out['meta']['count']\n",
    "            if page == 1:\n",
    "                print(f\"Count: {count}\")\n",
    "\n",
    "            # Storing the Results\n",
    "            i = 0\n",
    "            papers = out[\"results\"]\n",
    "            num_papers = len(papers)\n",
    "            \n",
    "            while i < num_papers and self.df.shape[0] < max_num_papers:\n",
    "                paper = papers[i]\n",
    "                \n",
    "                # The abstract needs to be converted\n",
    "                # into readable text (and cleaned).\n",
    "                abstract = paper.get(\"abstract_inverted_index\", \"\")\n",
    "                abstract = self.revert_abstract(abstract)\n",
    "                abstract = clean(abstract)\n",
    "\n",
    "                # The ID is returned as an URL. We take\n",
    "                # the minimal portion of that ID.\n",
    "                id_ = paper.get(\"id\", \"\")\n",
    "                id_ = self.extract_id(id_)\n",
    "                \n",
    "                self.add_row({\n",
    "                    \"DOI\": paper.get(\"doi\", \"\"), \n",
    "                    \"Title\": paper.get(\"title\", \"\"),\n",
    "                    \"Abstract\": abstract,\n",
    "                    \"ID\": id_\n",
    "                })\n",
    "                \n",
    "                i += 1\n",
    "      \n",
    "            page += 1\n",
    "\n",
    "        print(f\"Retrieved {self.size()} Paper(s)\")\n",
    "\n",
    "    def expand(self, max_hops=3, max_citations=3):\n",
    "        # This list contains the IDs that we have\n",
    "        # already seen and thus cannot visit.\n",
    "        seen = self.df[\"ID\"].tolist()\n",
    "\n",
    "        # This stack contains the IDs that we need\n",
    "        # to visit.\n",
    "        stack = [(start_id, 0) for start_id in seen]\n",
    "\n",
    "        # These two variables are used as a progress\n",
    "        # indicator. We monitor the number of start\n",
    "        # IDs that have been searched.\n",
    "        start_seen = 0\n",
    "        start_size = len(seen)\n",
    "        \n",
    "        while stack:\n",
    "            curr_id, curr_hops = stack.pop()\n",
    "\n",
    "            # Print Progress\n",
    "            # The starting IDs have a hop value\n",
    "            # of 0.\n",
    "            if curr_hops == 0:\n",
    "                start_seen += 1\n",
    "                print(f\"({start_seen}/{start_size}) Searching...\")\n",
    "\n",
    "            # We have reached the maximum level\n",
    "            # of the citation tree.\n",
    "            if curr_hops >= max_hops:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                url = f\"https://api.openalex.org/works/{curr_id}?select=referenced_works\"\n",
    "                print(url)\n",
    "                out = requests.get(url)\n",
    "                out = out.json()\n",
    "            except Exception as e:\n",
    "                # We can continue as we have already popped the\n",
    "                # troublesome ID from the stack.\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            if not out or not out.get('referenced_works'):\n",
    "                    continue\n",
    "\n",
    "            num_citations = 0\n",
    "            for paper_id in out['referenced_works']:\n",
    "                if num_citations >= max_citations:\n",
    "                    break\n",
    "    \n",
    "                paper_id = self.extract_id(paper_id)\n",
    "                print(f\"\\t{paper_id}\")\n",
    "                \n",
    "                if paper_id in seen:\n",
    "                    continue\n",
    "\n",
    "                seen.append(paper_id)\n",
    "                stack.append((paper_id, curr_hops + 1))\n",
    "                num_citations += 1\n",
    "\n",
    "        # Storing Papers\n",
    "        print(\"Storing Papers...\")\n",
    "        \n",
    "        FIELDS = \"id,doi,title,abstract_inverted_index\"\n",
    "        \n",
    "        for i, paper_id in enumerate(seen):\n",
    "            print(f\"{i+1}/{len(seen)}\")\n",
    "            \n",
    "            url = f\"https://api.openalex.org/works/{paper_id}?select={FIELDS}\"\n",
    "            print(url)\n",
    "\n",
    "            try:\n",
    "                out = requests.get(url)\n",
    "                out = out.json()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "                \n",
    "            if not out:\n",
    "                continue\n",
    "\n",
    "            # Process Abstract\n",
    "            abstract = out.get(\"abstract_inverted_index\", \"\")\n",
    "            abstract = self.revert_abstract(abstract)\n",
    "            abstract = clean(abstract)\n",
    "            \n",
    "            self.add_row({\n",
    "                \"DOI\": out[\"doi\"], \n",
    "                \"Title\": out[\"title\"],\n",
    "                \"Abstract\": abstract, \n",
    "                \"ID\": paper_id\n",
    "            })\n",
    "\n",
    "        self.write(f\"{self.name} Expanded\", reset_index=True)\n",
    "        print(f\"Expanded, Number of Papers: {self.size()}\")\n",
    "\n",
    "    def identify(self):\n",
    "        # NOTE: This likely will not be called as\n",
    "        # running all of the functions below without\n",
    "        # running out of memory (or bug) is unlikely.\n",
    "        \n",
    "        # 1. Search\n",
    "        search_filter = self.make_search_filter(Source.SEARCH_TERMS)\n",
    "        self.search(search_filter)\n",
    "        self.clean()\n",
    "        self.filter(f\"{self.name}\")\n",
    "\n",
    "        # 2. Expand\n",
    "        self.expand()\n",
    "        self.filter(f\"{self.name} Expanded\")\n",
    "        \n",
    "        # self.write(self.name, reset_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48b6cc-d464-4335-b954-532104320059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticScholar(Source):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Semantic Scholar\")\n",
    "\n",
    "    def make_search_filter(self):\n",
    "        # NOTE: No hate, but this source is questionable.\n",
    "        # Maybe it's me. I am not create a more adaptable\n",
    "        # function as the API is finicky.\n",
    "        # Hence, it's a literal.\n",
    "        a = \"(trait %7C phenotype)\"\n",
    "        b = \"(higher order interaction %7C interaction modification %7C indirect effect %7C trait mediated %7C polymorphism)\"\n",
    "        c = \"(competition chain %7C trophic chain %7C intraguild predation %7C keystone predation %7C apparent competition %7C resource competition %7C intransitive competition %7C mutual competition)\"\n",
    "        fields = \"title,abstract,externalIds\"\n",
    "        types = \"CaseReport,Book,BookSection,JournalArticle,Study\"\n",
    "        return f\"query={a} {b} {c}&fields={fields}&publicationTypes={types}\"\n",
    "    \n",
    "    def search(self, search_filter, max_num_papers=math.inf):\n",
    "        print(\"Search...\")\n",
    "        \n",
    "        # Exponential Backoff\n",
    "        # c is the number of adverse events. An adverse event\n",
    "        # is a thrown exception. If c >= max_c, we exit the \n",
    "        # process.\n",
    "        c = 0\n",
    "        max_c = 6\n",
    "        \n",
    "        URL = f\"https://api.semanticscholar.org/graph/v1/paper/search/bulk?{search_filter}\"\n",
    "        print(URL)\n",
    "\n",
    "        token = \"\"\n",
    "        while self.size() < max_num_papers:\n",
    "            # Sleep\n",
    "            if c > 0:\n",
    "                t = 2 ** c\n",
    "                print(f\"Sleeping for {t}s\")\n",
    "                time.sleep(t)\n",
    "\n",
    "            # Add Token\n",
    "            url = URL\n",
    "            if token:\n",
    "                url += f\"&token={token}\"\n",
    "\n",
    "            # Fetch Works\n",
    "            try:\n",
    "                out = requests.get(url)\n",
    "                out = out.json()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "                c += 1\n",
    "                if c >= max_c:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # If there are no results, we end\n",
    "            # the search.\n",
    "            if not out or not out.get(\"data\"):\n",
    "                print(\"No Results\")\n",
    "                break\n",
    "\n",
    "            # Print Count\n",
    "            if not token:\n",
    "                print(f\"Count: {out['total']}\")\n",
    "\n",
    "            i = 0\n",
    "            papers = out[\"data\"]\n",
    "            num_papers = len(papers)\n",
    "\n",
    "            while i < num_papers and self.size() < max_num_papers:    \n",
    "                paper = papers[i]\n",
    "\n",
    "                # Process DOI\n",
    "                # If there is no DOI, we do not convert it\n",
    "                # into an URL.\n",
    "                doi = paper[\"externalIds\"].get(\"DOI\", \"\")\n",
    "                doi = not doi or f'https://doi.org/{doi}'\n",
    "                \n",
    "                self.add_row({\n",
    "                    \"DOI\": doi, \n",
    "                    \"Title\": paper[\"title\"], \n",
    "                    \"Abstract\": paper.get(\"abstract\"), \n",
    "                    \"ID\": paper[\"paperId\"]\n",
    "                })\n",
    "                    \n",
    "                i += 1\n",
    "\n",
    "            token = out.get(\"token\")\n",
    "            if not token:\n",
    "                break\n",
    "\n",
    "        print(f\"Retrieved {self.size()} Paper(s)\")\n",
    "\n",
    "    def expand(self, max_hops=3, max_citations=3):\n",
    "        seen = self.df[\"ID\"].tolist()\n",
    "        stack = [(start_id, 0) for start_id in seen]\n",
    "\n",
    "        start_seen = 0\n",
    "        start_size = len(seen)\n",
    "\n",
    "        while stack:\n",
    "            curr_id, curr_hops = stack.pop()\n",
    "\n",
    "            if curr_hops == 0:\n",
    "                start_seen += 1\n",
    "                print(f\"{start_seen}/{start_size} Searching...\")\n",
    "            \n",
    "            if curr_hops >= max_hops:\n",
    "                continue\n",
    "\n",
    "            # Exponential Backoff\n",
    "            # Ideally, I wouldn't need this, but\n",
    "            # Semantic Scholar has strict request rates.\n",
    "            c = 0\n",
    "            max_c = 6\n",
    "            cont_loop = False\n",
    "            \n",
    "            while c < max_c:\n",
    "                try:\n",
    "                    # Sleep\n",
    "                    if c > 0:\n",
    "                        t = 2 ** c\n",
    "                        print(f\"Sleeping for {t}s\")\n",
    "                        time.sleep(t)\n",
    "\n",
    "                    url = f\"https://api.semanticscholar.org/graph/v1/paper/{curr_id}/citations\"\n",
    "                    out = requests.get(url)\n",
    "                    out = out.json()\n",
    "                    \n",
    "                    if not out or not out.get(\"data\"):\n",
    "                        cont_loop = True\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    c += 1\n",
    "                    if c >= max_c:\n",
    "                        cont_loop = True\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            if cont_loop:\n",
    "                continue\n",
    "\n",
    "            num_citations = 0\n",
    "            for citation in out['data']:\n",
    "                if num_citations >= max_citations:\n",
    "                    break\n",
    "                \n",
    "                paper_id = citation['citingPaper']['paperId']\n",
    "                \n",
    "                if paper_id in seen:\n",
    "                    continue\n",
    "                \n",
    "                seen.append(paper_id)\n",
    "                stack.append((paper_id, curr_hops + 1))\n",
    "                num_citations += 1\n",
    "\n",
    "        \n",
    "        # Storing Papers\n",
    "        print(\"Storing Papers...\")\n",
    "\n",
    "        # Exponential Backoff\n",
    "        c = 0\n",
    "        max_c = 6\n",
    "        \n",
    "        FIELDS = \"title,externalIds,abstract\"\n",
    "        \n",
    "        for i, paper_id in enumerate(seen):\n",
    "            if c > 0:\n",
    "                t = 2 ** c\n",
    "                print(f\"Sleeping for {t}s\")\n",
    "                time.sleep(t)\n",
    "            \n",
    "            print(f\"{i+1}/{len(seen)}\")\n",
    "            \n",
    "            try:\n",
    "                url = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields={FIELDS}\"\n",
    "                out = requests.get(url)\n",
    "                out = out.json()\n",
    "                \n",
    "                if not out:\n",
    "                    print(f\"No Output\")\n",
    "                    continue\n",
    "\n",
    "                doi = paper[\"externalIds\"].get(\"DOI\", \"\")\n",
    "                doi = not doi or f'https://doi.org/{doi}'\n",
    "                \n",
    "                self.add_row({\n",
    "                    \"DOI\": doi, \n",
    "                    \"Title\": out[\"title\"],\n",
    "                    \"Abstract\": out[\"abstract\"], \n",
    "                    \"ID\": out[\"paperId\"]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                c += 1\n",
    "                if c >= max_c:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        self.write(f\"{self.name} Expanded\", reset_index=True)\n",
    "        print(f\"Expanded, Number of Papers: {self.size()}\")\n",
    "\n",
    "    def run(self):\n",
    "        # 1. Search\n",
    "        search_filter = self.make_search_filter()\n",
    "        self.find(search_filter)\n",
    "        self.clean()\n",
    "        self.filter(name=f\"{self.name}\")\n",
    "\n",
    "        # 2. Expand\n",
    "        self.expand()\n",
    "        self.filter(name=f\"{self.name} Expanded\")\n",
    "        \n",
    "        # self.write(self.name, reset_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452e79a-da67-4615-bc95-3cee93ec1568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Completing Interrupted Work\n",
    "# Apparently, it did finish. When I was running it yesterday morning,\n",
    "# the page ran out of memory. I never closed the page, but it seemed\n",
    "# like I'd have to run it again. It appears that this is not the case.\n",
    "# oa = OpenAlex()\n",
    "# oa.df = pd.read_csv(\"OpenAlex Expanded.csv\")\n",
    "# oa.filter(name=f\"{oa.name} Expanded\")\n",
    "# oa.write(oa.name, reset_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b7c2f-a204-4207-83e4-c1654db5e095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sc = SemanticScholar()\n",
    "# sc.expand()\n",
    "# sc.df = pd.read_csv(\"Semantic Scholar Expanded-5.csv\")\n",
    "# sc.filter(name=f\"{sc.name} Expanded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd87b9-6be5-4666-b8ef-d7d7f4d9d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the Two Sources\n",
    "df1 = pd.read_csv(\"OpenAlex Expanded-Ecology.csv\")\n",
    "df2 = pd.read_csv(\"Semantic Scholar Expanded-Ecology-1.csv\")\n",
    "df = pd.concat([df1, df2], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dfc178-ce4b-4b0a-b8c0-00f2f316fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wrapper = DataFrame(df=df)\n",
    "df_wrapper.write(\"Papers\", reset_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e040a24-9086-4d09-b29b-a8605e00c541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
