{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c1ee796-e7a4-4a73-a1e8-113c6d4956e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import pickle\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from spacy.language import Language\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1439c540-4646-4079-8c86-7576d6718bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE_LEVEL = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "834c33d0-ab0f-4715-a886-7b8013477317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    # There is not a defined conversion method for these words.\n",
    "    # This is the default list of irregular nouns. It maps the\n",
    "    # the singular version to the plural version (SP).\n",
    "    IRREGULAR_NOUNS_SP = {\n",
    "        \"ox\": \"oxen\",\n",
    "        \"goose\": \"geese\",\n",
    "        \"mouse\": \"mice\",\n",
    "        \"bacterium\": \"bacteria\"\n",
    "    }\n",
    "\n",
    "    # This is the reversed version of the dictionary above, meaning \n",
    "    # that the plural version is mapped to the singular version \n",
    "    # (PS).\n",
    "    IRREGULAR_NOUNS_PS = {v: k for k, v in IRREGULAR_NOUNS_SP.items()}\n",
    "    \n",
    "    # The singular and plural versions of these words are the same. \n",
    "    # This is the default list of zero plural nouns.\n",
    "    ZERO_PLURAL_NOUNS = [\n",
    "        \"species\", \n",
    "        \"deer\", \n",
    "        \"fish\", \n",
    "        \"moose\", \n",
    "        \"sheep\", \n",
    "        \"swine\", \n",
    "        \"buffalo\", \n",
    "        \"trout\", \n",
    "        \"cattle\"\n",
    "    ]\n",
    "\n",
    "    # These pairs of characters define symbols that enclose other\n",
    "    # information in a text.\n",
    "    ENCLOSURES = {\n",
    "        \"(\": \")\",\n",
    "        \"[\": \"]\",\n",
    "        \"{\": \"}\"\n",
    "    }\n",
    "\n",
    "    LAX_ENCLOSURES = {\n",
    "        \"(\": \")\",\n",
    "        \"[\": \"]\",\n",
    "        \"{\": \"}\",\n",
    "        \"窶能": \"窶能"\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self, main, irregular_nouns_sp=IRREGULAR_NOUNS_SP, irregular_nouns_ps=IRREGULAR_NOUNS_PS, zero_plural_nouns=ZERO_PLURAL_NOUNS):\n",
    "        self.main = main\n",
    "        self.zero_plural_nouns = zero_plural_nouns\n",
    "        self.irregular_nouns_sp = irregular_nouns_sp\n",
    "        self.irregular_nouns_ps = irregular_nouns_ps\n",
    "        self.irregular_plural_nouns = list(self.irregular_nouns_sp.values())\n",
    "        self.irregular_singular_nouns = list(self.irregular_nouns_sp.keys())\n",
    "\n",
    "\n",
    "\n",
    "    def delete_extra_whitespace(self, string):\n",
    "        # Duplicate spaces, spaces before punctuation marks,\n",
    "        # and outside spaces are removed.\n",
    "        string = re.sub(r\"\\s+\", \" \", string)\n",
    "        string = re.sub(r\"\\s+([?.!,])\", r\"\\1\", string)\n",
    "        string = string.strip()\n",
    "        return string\n",
    "\n",
    "\n",
    "\n",
    "    def delete_outer_non_alnum(self, string):\n",
    "        while string:\n",
    "            start_len = len(string)\n",
    "            # Remove Leading Non-Alphanumeric Character\n",
    "            if string and not string[0].isalnum():\n",
    "                string = string[1:]\n",
    "            # Remove Trailing Non-Alphanumeric Character\n",
    "            if string and not string[-1].isalnum():\n",
    "                string = string[:-1]\n",
    "            # No Changes Made\n",
    "            if start_len == len(string):\n",
    "                break\n",
    "        return string\n",
    "\n",
    "\n",
    "\n",
    "    def get_parentheticals(self, text, enclosures=ENCLOSURES, flatten=False):\n",
    "        # The parenthetical would be the content inside of a pair\n",
    "        # of matching parentheses, brackets, or braces.\n",
    "        parentheticals = []\n",
    "        \n",
    "        # This contains the text that's not inside of any\n",
    "        # enclosure.\n",
    "        base_text = []\n",
    "        \n",
    "        # This is used for building groups, which often has a \n",
    "        # nested structure.\n",
    "        stacks = []\n",
    "        \n",
    "        # These are the pairs of characters that we recognize\n",
    "        # as defining the parenthetical.\n",
    "        openers = list(enclosures.keys())\n",
    "        closers = list(enclosures.values())\n",
    "        \n",
    "        # This contains the opening characters of the groups \n",
    "        # that are currently open (e.g. '(', '['). We use it \n",
    "        # so that we know whether to open or close a group.\n",
    "        opened = []\n",
    "        \n",
    "        for i, char in enumerate(text):\n",
    "            # Open Group\n",
    "            if char in openers:\n",
    "                stacks.append([])\n",
    "                opened.append(char)\n",
    "            # Close Group\n",
    "            elif opened and char == enclosures.get(opened[-1], \"\"):\n",
    "                parentheticals.append(stacks.pop())\n",
    "                opened.pop()\n",
    "            # Add to Group\n",
    "            elif opened:\n",
    "                stacks[-1].append(i)\n",
    "            # Add to Base Text\n",
    "            else:\n",
    "                base_text.append(i)\n",
    "        \n",
    "        # We close the remaining groups that have not\n",
    "        # been closed.\n",
    "        while stacks:\n",
    "            parentheticals.append(stacks.pop())\n",
    "            \n",
    "        # Cluster Groups' Indices\n",
    "        # A list in the lists of indices (where each list represents a group of text) could have \n",
    "        # an interruption (e.g. [0, 1, 2, 10 15]) because of a parenthetical. So, we cluster the\n",
    "        # indices in each list to make the output more useful (e.g. [(0, 3), (10, 16)]).\n",
    "        lists_of_indices = [*parentheticals, base_text]        \n",
    "        lists_of_clustered_indices = []\n",
    "\n",
    "        for list_of_indices in lists_of_indices:\n",
    "            if not list_of_indices:\n",
    "                continue\n",
    "\n",
    "            # We start off with a single cluster that is made up of the\n",
    "            # first index. If the next index follows the first index, \n",
    "            # we continue the cluster. If it doesn't, we create a new cluster.\n",
    "            clustered_indices = [[list_of_indices[0], list_of_indices[0] + 1]]\n",
    "            \n",
    "            for index in list_of_indices[1:]:\n",
    "                if clustered_indices[-1][1] == index:\n",
    "                    clustered_indices[-1][1] = index + 1\n",
    "                else:\n",
    "                    clustered_indices.append([index, index + 1])\n",
    "\n",
    "            # Add Clustered Indices\n",
    "            lists_of_clustered_indices.append(clustered_indices)\n",
    "            \n",
    "        if flatten:\n",
    "            flattened_clusters = []\n",
    "            # We are placing each cluster of indices into one list.\n",
    "            # This removes the context of the larger parenthetical,\n",
    "            # but the context may be cumbersome instead of useful.\n",
    "            for list_of_clustered_indices in lists_of_clustered_indices:\n",
    "                for clustered_indices in list_of_clustered_indices:\n",
    "                    flattened_clusters.append(clustered_indices)\n",
    "            lists_of_clustered_indices = flattened_clusters\n",
    "        \n",
    "        return lists_of_clustered_indices\n",
    "\n",
    "\n",
    "\n",
    "    def separate_span_by_parenthetical(self, span):\n",
    "        span_parentheticals = []\n",
    "        \n",
    "        # The clusters of the span represented with tuples of char indices\n",
    "        # (e.g. [(0, 1), (1, 5), (5, 10)]. This is a list of clustered\n",
    "        # indices (like above).\n",
    "        text_clusters = self.get_parentheticals(span.text, flatten=True)\n",
    "        \n",
    "        for cluster in text_clusters:\n",
    "            if span.text[cluster[0]:cluster[1]].isspace():\n",
    "                continue\n",
    "\n",
    "            l_char_index = span[0].idx + cluster[0]\n",
    "            r_char_index = span[0].idx + cluster[1] - 1\n",
    "\n",
    "            # Instead of having a tuple dictating the start and end of a cluster,\n",
    "            # we can use a span -- it's much simpler.\n",
    "            cluster_as_span = self.get_span_at_indices(l_char_index, r_char_index)\n",
    "            if not cluster_as_span:\n",
    "                continue\n",
    "            \n",
    "            span_parentheticals.append(cluster_as_span)\n",
    "\n",
    "        return span_parentheticals\n",
    "\n",
    "\n",
    "\n",
    "    def separate_spans_by_parenthetical(self, spans):\n",
    "        all_span_parentheticals = []\n",
    "        for span in spans:\n",
    "            all_span_parentheticals.extend(self.separate_span_by_parenthetical(span))\n",
    "        return all_span_parentheticals\n",
    "\n",
    "    \n",
    " \n",
    "    def singularize(self, string):\n",
    "        string = string.lower()\n",
    "        \n",
    "        # The string to singularize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        if not words:\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is a zero plural\n",
    "        # or a singular irregular noun, there's no changes\n",
    "        # to make. For example, \"red sheep\" and \"ox\" are \n",
    "        # already singular.\n",
    "        if (\n",
    "            words[-1] in self.zero_plural_nouns or \n",
    "            words[-1] in self.irregular_singular_nouns\n",
    "        ):\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is an irregular\n",
    "        # plural noun, we rely on a dictionary with the\n",
    "        # corresponding mapping.\n",
    "        if words[-1] in self.irregular_plural_nouns:\n",
    "            words[-1] = self.irregular_nouns_ps[words[-1]]\n",
    "            singulars = [self.delete_extra_whitespace(\" \".join(words))]\n",
    "            return singulars\n",
    "        \n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to uncertainty), we need to\n",
    "        # include all possible versions.\n",
    "        singulars = []\n",
    "        singular_endings = self.get_singular(words[-1])\n",
    "\n",
    "        if not singular_endings:\n",
    "            return [string]\n",
    "        \n",
    "        for singular_ending in singular_endings:\n",
    "            singular = self.delete_extra_whitespace(\" \".join([*words[:-1], singular_ending]))\n",
    "            singulars.append(singular)\n",
    "            \n",
    "        return singulars\n",
    "\n",
    "\n",
    "\n",
    "    def get_singular(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Replace -ies with -y\n",
    "        if re.fullmatch(r\".*ies$\", string):\n",
    "            versions.append(f'{string[:-3]}y')\n",
    "            return versions\n",
    "\n",
    "        # Replace -ves with -f and -fe\n",
    "        if re.fullmatch(r\".*ves$\", string):\n",
    "            versions.append(f'{string[:-3]}f')\n",
    "            versions.append(f'{string[:-3]}fe')\n",
    "            return versions\n",
    "\n",
    "        # Delete -es \n",
    "        if re.fullmatch(r\".*es$\", string):\n",
    "            versions.append(f'{string[:-2]}')\n",
    "            return versions\n",
    "\n",
    "        # Replace -i with -us\n",
    "        if re.fullmatch(r\".*i$\", string):\n",
    "            versions.append(f'{string[:-1]}us')\n",
    "            return versions\n",
    "\n",
    "        # Delete -s\n",
    "        if re.fullmatch(r\".*s$\", string):\n",
    "            versions.append(f'{string[:-1]}')\n",
    "            return versions\n",
    "\n",
    "        return versions\n",
    "\n",
    "\n",
    "    \n",
    "    def pluralize(self, string):\n",
    "        string = string.lower()\n",
    "        \n",
    "        # The string to pluralize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        if not words:\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is a zero plural\n",
    "        # or a plural irregular noun, there's no changes\n",
    "        # to make. For example, \"red sheep\" and \"oxen\" are \n",
    "        # already singular.\n",
    "        if (\n",
    "            words[-1] in self.zero_plural_nouns or \n",
    "            words[-1] in self.irregular_plural_nouns\n",
    "        ):\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is an irregular\n",
    "        # singular noun, we rely on a dictionary with the\n",
    "        # corresponding mapping.\n",
    "        if words[-1] in self.irregular_singular_nouns:\n",
    "            words[-1] = self.irregular_nouns_sp[words[-1]]\n",
    "            return [self.delete_extra_whitespace(\" \".join(words))]\n",
    "        \n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to error), we need to\n",
    "        # handle them all.\n",
    "        plurals = []\n",
    "        plural_endings = self.get_plural(words[-1])\n",
    "\n",
    "        if not plural_endings:\n",
    "            return [string]\n",
    "            \n",
    "        for plural_ending in plural_endings:\n",
    "            plural = self.delete_extra_whitespace(\" \".join([*words[:-1], plural_ending]))\n",
    "            plurals.append(plural)\n",
    "            \n",
    "        return plurals\n",
    "\n",
    "    \n",
    "  \n",
    "    def get_plural(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Words that end with -us often have\n",
    "        # two different plural versions: -es and -i.\n",
    "        # For example, the plural version of cactus \n",
    "        # can be cactuses or cacti.\n",
    "        if re.fullmatch(r\".*us$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            versions.append(f'{string[:-2]}i')\n",
    "            return versions\n",
    "\n",
    "        # The -es ending is added to the words below.\n",
    "        if re.fullmatch(r\".*([^l]s|sh|ch|x|z)$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # Words that end with a consonant followed by 'y'\n",
    "        # are made plural by replacing the 'y' with -ies.\n",
    "        # For example, the plural version of canary is\n",
    "        # canaries.\n",
    "        if re.fullmatch(r\".*([^aeiou])(y)$\", string):\n",
    "            versions.append(f'{string[:-1]}ies')\n",
    "            return versions\n",
    "            \n",
    "        # The plural version of words ending with -f\n",
    "        # and -fe aren't clear. To be safe, I will add\n",
    "        # both versions.\n",
    "        if (re.fullmatch(r\".*(f)(e?)$\", string) and not re.fullmatch(r\".*ff$\", string)):\n",
    "            last_clean = re.sub(r\"(f)(e?)$\", \"\", string)\n",
    "            versions.append(f'{last_clean}fs')\n",
    "            versions.append(f'{last_clean}ves')\n",
    "            return versions\n",
    "\n",
    "        # People add -s or -es to words that end with 'o'.\n",
    "        # To be safe, both versions are added.\n",
    "        if re.fullmatch(r\".*([^aeiou])o$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # If there's no -s at the end of the string and\n",
    "        # the other cases didn't run, we add an -s.\n",
    "        if re.fullmatch(r\".*[^s]$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "        \n",
    "        return versions\n",
    "\n",
    "\n",
    " \n",
    "    def expand_unit(self, *, il_unit, ir_unit, il_boundary, ir_boundary, speech=[], literals=[], include=True, direction='BOTH', verbose=False):\n",
    "        UNIT = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if il_unit > ir_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} greater than ir_unit of {ir_unit}\")\n",
    "            return None\n",
    "        \n",
    "        if direction in ['BOTH', 'LEFT'] and il_boundary > il_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} less than il_boundary of {il_boundary}\")\n",
    "            return None\n",
    "        \n",
    "        if direction in ['BOTH', 'RIGHT'] and ir_boundary < ir_unit:\n",
    "            print(f\"Error: ir_unit of {ir_unit} greater than ir_boundary of {ir_boundary}\")\n",
    "            return None\n",
    "        \n",
    "        # Move Left\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            # The indices are inclusive, therefore, when \n",
    "            # the condition fails, il_unit will be equal\n",
    "            # to il_boundary.\n",
    "            while il_unit > il_boundary:\n",
    "                # We assume that the current token is allowed,\n",
    "                # and look to the token to the left.\n",
    "                l_token = self.main.sp_doc[il_unit-1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = l_token.pos_ in speech or l_token.lower_ in literals\n",
    "\n",
    "                # Case 1: include=False, in_set=True\n",
    "                # If we're not meant to include the defined tokens, and the\n",
    "                # current token is in that set, we stop expanding.\n",
    "                # Case 2: include=True, in_set=False\n",
    "                # If we're meant to include the defined tokens, and the current\n",
    "                # token is not in that set, we stop expanding.\n",
    "                # Case 3: include=in_set\n",
    "                # If we're meant to include the defined tokens, and the current\n",
    "                # token is in that set, we continue expanding. If we're not meant\n",
    "                # to include the defined tokens, and the current token is not\n",
    "                # in that set, we continue expanding.\n",
    "                if include ^ in_set:\n",
    "                    break\n",
    "                \n",
    "                # Else, the left token is valid, and\n",
    "                # we continue to expand.\n",
    "                il_unit -= 1\n",
    "\n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            # Likewise, when the condition fails,\n",
    "            # ir_unit will be equal to the ir_boundary.\n",
    "            # The ir_boundary is also inclusive.\n",
    "            while ir_unit < ir_boundary:\n",
    "                # Assuming that the current token is valid,\n",
    "                # we look to the right to see if we can\n",
    "                # expand.\n",
    "                r_token = self.main.sp_doc[ir_unit+1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = r_token.pos_ in speech or r_token.lower_ in literals\n",
    "                if include ^ in_set:\n",
    "                    break\n",
    "\n",
    "                # Else, the token is valid and\n",
    "                # we continue.\n",
    "                ir_unit += 1\n",
    "\n",
    "        assert il_unit >= il_boundary and ir_unit <= ir_boundary\n",
    "        \n",
    "        expanded_unit = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Expanded Unit of '{UNIT}': {expanded_unit}\")\n",
    "        \n",
    "        return expanded_unit\n",
    "\n",
    "\n",
    "    \n",
    "    def contract_unit(self, *, il_unit, ir_unit, speech=[], literals=[], include=True, direction='BOTH', verbose=False):\n",
    "        UNIT = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if il_unit > ir_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} greater than ir_unit of {ir_unit}\")\n",
    "            return None\n",
    "        \n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit < ir_unit:\n",
    "                # We must check if the current token is not allowed. If it's\n",
    "                # not allowed, we contract (remove).\n",
    "                token = self.main.sp_doc[il_unit]\n",
    "\n",
    "                # include = True means that we want the tokens that match\n",
    "                # the speech and/or literals in the contracted unit.\n",
    "                \n",
    "                # include = False means that we don't want the tokens that\n",
    "                # match the speech and/or literals in the contracted unit.\n",
    "                \n",
    "                # Case 1: include = True, in_set = True\n",
    "                # We have a token that's meant to be included in the set.\n",
    "                # However, we're contracting, which means we would end up\n",
    "                # removing the token if we continue. Therefore, we break.\n",
    "                \n",
    "                # Case 2: include = False, in_set = False\n",
    "                # We have a token that's not in the set which defines the\n",
    "                # tokens that aren't meant to be included. Therefore, we \n",
    "                # have a token that is meant to be included. If we continue,\n",
    "                # we would end up removing this token. Therefore, we break.\n",
    "                \n",
    "                # Default:\n",
    "                # If we have a token that's in the set (in_set=True) of\n",
    "                # tokens we're not supposed to include in the contracted \n",
    "                # unit (include=False), we need to remove it. Likewise, if\n",
    "                # we have a token that's not in the set (in_set=False) of\n",
    "                # tokens to include in the contracted unit (include=True),\n",
    "                # we need to remove it.\n",
    "                \n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include == in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid, thus we continue.\n",
    "                il_unit += 1\n",
    "\n",
    "        # Move Left      \n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit > il_unit:\n",
    "                token = self.main.sp_doc[ir_unit]\n",
    "\n",
    "                # The token is invalid and we\n",
    "                # stop contracting.\n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include == in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid and we continue.\n",
    "                ir_unit -= 1\n",
    "\n",
    "        assert il_unit <= ir_unit\n",
    "        \n",
    "        contracted_unit = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Contracted Unit of '{UNIT}': {contracted_unit}\")\n",
    "        \n",
    "        return contracted_unit\n",
    "\n",
    "\n",
    "    \n",
    "    def find_unit_context(self, *, il_unit, ir_unit, il_boundary, ir_boundary, speech=[\"ADJ\", \"NOUN\", \"ADP\", \"ADV\", \"PART\", \"PROPN\", \"VERB\", \"PRON\", \"DET\", \"AUX\", \"PART\", \"SCONJ\"], literals=[], include=True, enclosures=LAX_ENCLOSURES, comma_encloses=False, verbose=False):\n",
    "        UNIT = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if il_unit > ir_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} greater than ir_unit of {ir_unit}\")\n",
    "            return None\n",
    "        \n",
    "        if il_boundary > il_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} less than il_boundary of {il_boundary}\")\n",
    "            return None\n",
    "        \n",
    "        if ir_boundary < ir_unit:\n",
    "            print(f\"Error: ir_unit of {ir_unit} greater than ir_boundary of {ir_boundary}\")\n",
    "            return None\n",
    "        \n",
    "        # Caveat: Parentheticals\n",
    "        # The context of a unit inside a set of enclosures should\n",
    "        # not go farther than the boundaries of those enclosures.\n",
    "        # However, we need to manually determine whether the unit\n",
    "        # is in parentheses (or any set of the matching symbols\n",
    "        # below).\n",
    "        openers = list(enclosures.keys())\n",
    "        closers = list(enclosures.values())\n",
    "        enclosing_chars = [*closers, *openers]\n",
    "\n",
    "        # Look for Group Punctuation on the Left\n",
    "        i = il_unit\n",
    "        opener = None\n",
    "        while i > il_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.lower_ in enclosing_chars and token.lower_ != \",\":\n",
    "                opener = token\n",
    "                break\n",
    "            i -= 1\n",
    "\n",
    "        # Look for Group Punctuation on the Right\n",
    "        i = ir_unit\n",
    "        closer = None\n",
    "        while i < ir_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.lower_ in enclosing_chars and token.lower_ != \",\":\n",
    "                closer = token\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        # If there's a group punctuation on the left\n",
    "        # and right, and they match each other (e.g. '(' and ')'),\n",
    "        # we return the text between the punctuations.\n",
    "        parenthetical = opener and closer and enclosures.get(opener.lower_) == closer.text\n",
    "        if parenthetical:\n",
    "            context = [t for t in self.main.sp_doc[opener.i:closer.i+1]]\n",
    "            \n",
    "            if verbose and VERBOSE_LEVEL >= 1:\n",
    "                print(f\"Parenthetical - Unit Context of '{UNIT}': {context}\")\n",
    "            \n",
    "            return context\n",
    "\n",
    "        # We can also check whether the unit it enclosed\n",
    "        # in a comma or two, only if a comma can enclose.\n",
    "        if comma_encloses:\n",
    "            i = il_unit\n",
    "            while i > il_boundary:\n",
    "                i_token = self.main.sp_doc[i]\n",
    "                if i_token.lower_ in [\",\", \";\", \"窶能"]:\n",
    "                    break\n",
    "                i -= 1\n",
    "\n",
    "            j = ir_unit\n",
    "            while j < ir_boundary:\n",
    "                j_token = self.main.sp_doc[j]\n",
    "                if j_token.lower_ in [\",\", \";\", \"窶能"]:\n",
    "                    break\n",
    "                j += 1\n",
    "\n",
    "            if i_token.lower_ == \",\" or j_token.lower_ == \",\":\n",
    "                context = [t for t in self.main.sp_doc[i:j+1]]\n",
    "            \n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Comma - Unit Context of '{UNIT}': {context}\")\n",
    "                    \n",
    "                return context\n",
    "            \n",
    "        # As the unit is not a parenthetical, we will expand\n",
    "        # outwards until we run into a stopping token. The exclude\n",
    "        # list contains tokens that should be excluded from the\n",
    "        # context. Currently, it will contain any parentheticals\n",
    "        # that we run into.\n",
    "        exclude = []\n",
    "\n",
    "        # We can modify the enclosures after handling the parenthetical\n",
    "        # situation to make the code easier.\n",
    "        if comma_encloses:\n",
    "            enclosures[\",\"] : \",\"\n",
    "        \n",
    "        # Expand Left\n",
    "        while il_unit > il_boundary:\n",
    "            # Assuming that the current token is fine,\n",
    "            # we look to the left.\n",
    "            l_token = self.main.sp_doc[il_unit-1]\n",
    "\n",
    "            if l_token.lower_ not in closers:\n",
    "                in_set = l_token.pos_ in speech or l_token.lower_ in literals\n",
    "                if in_set ^ include:\n",
    "                    break\n",
    "                il_unit -= 1\n",
    "            # If it's a closing enclosure (e.g. ')', ']'),\n",
    "            # we need to skip over whatever is contained in\n",
    "            # that punctuation.\n",
    "            else:\n",
    "                i = il_unit - 1\n",
    "                \n",
    "                token = self.main.sp_doc[i]\n",
    "                exclude.append(token)\n",
    "\n",
    "                # We continue until we reach the boundary or\n",
    "                # we find the matching opening character.\n",
    "                closed = []\n",
    "                \n",
    "                while i > il_boundary:\n",
    "                    token = self.main.sp_doc[i]\n",
    "                    # Found Closer\n",
    "                    if token.lower_ in closers:\n",
    "                        exclude.append(token)\n",
    "                        closed.append(token.lower_)\n",
    "                    # Currently Closed\n",
    "                    elif closed:\n",
    "                        exclude.append(token)\n",
    "                        # Found Opener\n",
    "                        if token.lower_ == enclosures.get(closed[-1]):\n",
    "                            closed.pop()\n",
    "                    else:\n",
    "                        break\n",
    "                    i -= 1\n",
    "                \n",
    "                il_unit = i\n",
    "\n",
    "        # Expand Right\n",
    "        while ir_unit < ir_boundary:\n",
    "            # We're checking the token to the right\n",
    "            # to see if we can expand or not.\n",
    "            r_token = self.main.sp_doc[ir_unit+1]\n",
    "\n",
    "            if r_token.lower_ not in openers:\n",
    "                in_set = r_token.pos_ in speech or r_token.lower_ in literals\n",
    "                if in_set ^ include:\n",
    "                    break\n",
    "                ir_unit += 1\n",
    "            # If the token to the right is an opener (e.g. '(', '['), we must skip\n",
    "            # it, the parenthetical inside, and the closer.\n",
    "            else:\n",
    "                i = ir_unit + 1\n",
    "                \n",
    "                token = self.main.sp_doc[i]\n",
    "                exclude.append(token)\n",
    "\n",
    "                # We continue until we reach the boundary or\n",
    "                # we find all the closers for the openers.\n",
    "                opened = []\n",
    "                \n",
    "                while i < ir_boundary:\n",
    "                    token = self.main.sp_doc[i]\n",
    "                    # Found Opener\n",
    "                    if token.lower_ in openers:\n",
    "                        exclude.append(token)\n",
    "                        opened.append(token.lower_)\n",
    "                    # Currently Opened\n",
    "                    elif opened:\n",
    "                        exclude.append(token)\n",
    "                        # Found Closer\n",
    "                        if token.lower_ == enclosures.get(opened[-1]):\n",
    "                            opened.pop()\n",
    "                    else:\n",
    "                        break\n",
    "                    i += 1\n",
    "                \n",
    "                ir_unit = i\n",
    "        \n",
    "        # We remove the excluded tokens and return the context.\n",
    "        context = [t for t in self.main.sp_doc[il_unit:ir_unit+1] if t not in exclude]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Unit Context of '{UNIT}': {context}\")\n",
    "        \n",
    "        return context\n",
    "\n",
    "\n",
    "    \n",
    "    def get_span_at_indices(self, l_index, r_index):\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "\n",
    "        while text[l_index].isspace():\n",
    "            l_index += 1\n",
    "\n",
    "        while text[r_index].isspace():\n",
    "            r_index -= 1\n",
    "\n",
    "        if l_index > r_index:\n",
    "            print(f\"Error: l_index of {l_index} greater than r_index of {r_index}\")\n",
    "            return None\n",
    "            \n",
    "        l_token_i = self.main.token_at_char(l_index).i\n",
    "        r_token_i = self.main.token_at_char(r_index).i\n",
    "        \n",
    "        return self.main.sp_doc[l_token_i:r_token_i+1]\n",
    "\n",
    "\n",
    "    \n",
    "    def get_base_nouns(self, span, return_tokens=False, immediate_stop=False):\n",
    "        ending_nouns = []\n",
    "        \n",
    "        reversed_span = [t for t in span]\n",
    "        reversed_span.reverse()\n",
    "        \n",
    "        for token in reversed_span:\n",
    "            if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                ending_nouns.append(token if return_tokens else self.main.sp_doc[token.i:token.i+1])\n",
    "                if immediate_stop:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return ending_nouns\n",
    "\n",
    "\n",
    "\n",
    "    def flatten(self, arr):\n",
    "        flat_arr = []\n",
    "\n",
    "        if not isinstance(arr, list):\n",
    "            return [arr]\n",
    "\n",
    "        for element in arr:\n",
    "            flat_arr.extend(self.flatten(element))\n",
    "\n",
    "        return flat_arr\n",
    "\n",
    "\n",
    "    def is_same_text(self, sp_a, sp_b):\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        sp_a_text = sp_a.text.lower()\n",
    "\n",
    "        if sp_a_text == sp_b_text:\n",
    "            return True\n",
    "            \n",
    "        sp_a_singular_texts = [sp_a_text] if sp_a[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(sp_a_text)\n",
    "        sp_b_singular_texts = [sp_b_text] if sp_b[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(sp_b_text)\n",
    "\n",
    "        if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def has_same_base_nouns(self, sp_a, sp_b):\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        sp_b_0_text = sp_b[0].lower_\n",
    "        sp_b_0_is_noun = sp_b[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "        \n",
    "        sp_b_nouns = []\n",
    "        sp_b_num_adjectives = 0\n",
    "        \n",
    "        for token in sp_b:\n",
    "            if not sp_b_nouns and token.pos_ == \"ADJ\":\n",
    "                sp_b_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_b_nouns.append(token)\n",
    "\n",
    "        if not sp_b_nouns:\n",
    "            return False\n",
    "\n",
    "        sp_b_nouns_text = [noun.lower_ for noun in sp_b_nouns]\n",
    "        sp_b_singular_texts = [\" \".join(sp_b_nouns_text)] if sp_b_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(\" \".join(sp_b_nouns_text))\n",
    "\n",
    "        sp_a_text = sp_a.text.lower()\n",
    "        sp_a_0_text = sp_a[0].lower_\n",
    "        sp_a_0_is_noun = sp_a[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "        # Case Example: 'Hyla' v. 'Hyla tadpoles'\n",
    "        if sp_a_0_text == sp_b_0_text and (sp_a_0_is_noun or sp_b_0_is_noun):\n",
    "            if sp_a_text in sp_b_text or sp_b_text in sp_a_text:\n",
    "                return True\n",
    "        \n",
    "        # Case Example: 'dogs' v. 'red dogs'\n",
    "        sp_a_nouns = []\n",
    "        sp_a_num_adjectives = 0\n",
    "        for token in sp_a:\n",
    "            if not sp_a_nouns and token.pos_ == \"ADJ\":\n",
    "                sp_a_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_a_nouns.append(token)\n",
    "        \n",
    "        if not sp_a_nouns:\n",
    "            return False\n",
    "        \n",
    "        sp_a_nouns_text = [noun.lower_ for noun in sp_a_nouns]\n",
    "        \n",
    "        if sp_a_nouns and sp_b_nouns and (\n",
    "            (sp_a_num_adjectives == 1 and sp_b_num_adjectives == 0) or \n",
    "            (sp_b_num_adjectives == 1 and sp_a_num_adjectives == 0)\n",
    "        ):\n",
    "            sp_a_singular_texts = [\" \".join(sp_a_nouns_text)] if sp_a_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(\" \".join(sp_a_nouns_text))\n",
    "            if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "                return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a954dc-05f6-4e70-a974-f71c5976d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species:\n",
    "    def __init__(self, main):\n",
    "        # Tools\n",
    "        self.main = main\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\"])\n",
    "        self.tn_doc = None\n",
    "        \n",
    "        # Contains any spans that have been identified\n",
    "        # as a species.\n",
    "        self.spans = None\n",
    "        self.span_starts = None\n",
    "        \n",
    "        # Contains any tokens that have been identified\n",
    "        # as a species or being a part of a species.\n",
    "        self.tokens = None\n",
    "        \n",
    "        # Used to quickly access the span that a token\n",
    "        # belongs to.\n",
    "        self.token_to_span = None\n",
    "        \n",
    "        # Maps a string to an array of strings wherein\n",
    "        # the strings involved in the key-value pair \n",
    "        # have been identified as an alternate name of each other.\n",
    "        self.alternate_names = None\n",
    "        \n",
    "        # Includes words that (1) are to be identified as species; and\n",
    "        # (2) are sometimes not identified as species, more or less.\n",
    "        self.dictionary = [\"juvenile\", \"juveniles\", \"adult\", \"adults\", \"prey\", \"predator\", \"predators\", \"species\", \"tree\", \"cat\", \"dog\", \"fly\", \"flies\", \"plant\", \"plants\"]\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, text, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        self.spans, self.tokens, self.token_to_span, self.span_starts = self.load_species(verbose=verbose)\n",
    "        self.alternate_names = self.load_alternate_names(self.spans)\n",
    "\n",
    "\n",
    "\n",
    "    def convert_tn_spans_to_sp_spans(self, tn_spans):\n",
    "        sp_spans = []\n",
    "\n",
    "        for tn_span in tn_spans:\n",
    "            l_char_index = self.tn_doc[tn_span.start].idx\n",
    "            r_char_index = l_char_index + len(tn_span.text) - 1\n",
    "\n",
    "            try:\n",
    "                l_sp_token_i = self.main.token_at_char(l_char_index).i\n",
    "                r_sp_token_i = self.main.token_at_char(r_char_index).i\n",
    "            except Exception as e:\n",
    "                print(f\"Error: Couldn't find token at character index of {l_char_index} and token index of {l_sp_token_i}.\")\n",
    "                print(f\"Error: Couldn't find token at character index of {r_char_index} and token index of {r_sp_token_i}.\")\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            sp_span = self.main.sp_doc[l_sp_token_i:r_sp_token_i+1]\n",
    "            if sp_span.text != tn_span:\n",
    "                print(f\"Error: SpaCy span does not match TaxoNerd span.\")\n",
    "                continue\n",
    "            \n",
    "            sp_spans.append(sp_span)\n",
    "\n",
    "        return sp_spans\n",
    "\n",
    "\n",
    "\n",
    "    def load_search_strings(self, verbose=False):\n",
    "        search_strings = [*self.dictionary]\n",
    "        \n",
    "        # Creating a Broad Set of Species\n",
    "        spans = self.convert_tn_spans_to_sp_spans(self.tn_doc.ents)\n",
    "        spans = self.main.separate_spans_by_parenthetical(spans)\n",
    "\n",
    "        # Add Ending Nouns to Set\n",
    "        all_nouns = []\n",
    "        for span in spans:\n",
    "            nouns = self.main.get_base_nouns(span)\n",
    "            if nouns:\n",
    "                all_nouns.extend(nouns)\n",
    "        spans.extend(all_nouns)\n",
    "\n",
    "        # Adding Plural and Singular Versions of Spans\n",
    "        for span in spans:\n",
    "            text = span.text.lower()\n",
    "            text = self.main.delete_extra_whitespace(self.main.delete_outer_non_alnum(text))\n",
    "\n",
    "            # Blank Text or No Letters\n",
    "            if not text or not [c for c in text if c.isalpha()]:\n",
    "                continue\n",
    "\n",
    "            search_strings.append(text)\n",
    "\n",
    "            # Add Plural Version\n",
    "            singular = span[-1].pos_ == \"NOUN\" and span[-1].tag_ == \"NN\"\n",
    "            if singular:\n",
    "                plural_version = self.main.pluralize(text)\n",
    "                search_strings.extend(plural_version)\n",
    "\n",
    "            # Add Singular Version\n",
    "            plural = span[-1].pos_ == \"NOUN\" and span[-1].tag_ == \"NNS\"\n",
    "            if plural:\n",
    "                singular_version = self.main.singularize(text)\n",
    "                search_strings.extend(singular_version)\n",
    "\n",
    "        # Remove Duplicates\n",
    "        search_strings = list(set(search_strings))\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Search Strings: {search_strings}\")\n",
    "        \n",
    "        return search_strings\n",
    "\n",
    "\n",
    "\n",
    "    def load_alternate_names(self, spans, verbose=False):\n",
    "        spans.sort(key=lambda span: span.start)\n",
    "\n",
    "        # It's useful to know if a different name refers to a\n",
    "        # species we have already seen. For example, in\n",
    "        # \"predatory crab (Carcinus maenas)\", \"predatory crab\"\n",
    "        # is an alternative name for \"Carcinus maenas\" and\n",
    "        # vice versa. This is used so that the species can be\n",
    "        # properly tracked and redundant points are less\n",
    "        # likely to be given.\n",
    "        alternate_names = {}\n",
    "        \n",
    "        # Finding and Storing Alternative Names\n",
    "        for i, species_span in enumerate(spans):\n",
    "            # There's not a next species to\n",
    "            # evaluate.\n",
    "            if i + 1 >= len(spans):\n",
    "                break\n",
    "            \n",
    "            next_species_span = spans[i+1]\n",
    "            \n",
    "            # If there's one token between the species and the next species,\n",
    "            # we check if the next species is surrounded by punctuation.\n",
    "            if next_species_span.start - species_span.end == 1:\n",
    "                # Token Before and After the Next Species\n",
    "                before_next = self.main.sp_doc[next_species_span.start-1]\n",
    "                after_next = self.main.sp_doc[next_species_span.end]\n",
    "\n",
    "                if before_next.lower_ in [\"(\"] and after_next.lower_ in [\")\"]:\n",
    "                    sp_1_text = species_span.text.lower()\n",
    "                    sp_2_text = next_species_span.text.lower()\n",
    "                    \n",
    "                    if sp_1_text not in alternate_names:\n",
    "                        alternate_names[sp_1_text] = []\n",
    "                    \n",
    "                    if sp_2_text not in alternate_names:\n",
    "                        alternate_names[sp_2_text] = []\n",
    "                    \n",
    "                    alternate_names[sp_1_text].append(sp_2_text)\n",
    "                    alternate_names[sp_2_text].append(sp_1_text)\n",
    "            # If there's no token between the species and the next,\n",
    "            # species we assume that they refer to the same species.\n",
    "            elif next_species_span.start - species_span.end == 0:\n",
    "                sp_1_text = species_span.text.lower()\n",
    "                sp_2_text = next_species_span.text.lower()\n",
    "                \n",
    "                if sp_1_text not in alternate_names:\n",
    "                    alternate_names[sp_1_text] = []\n",
    "                \n",
    "                if sp_2_text not in alternate_names:\n",
    "                    alternate_names[sp_2_text] = []\n",
    "\n",
    "                alternate_names[sp_1_text].append(sp_2_text)\n",
    "                alternate_names[sp_2_text].append(sp_1_text)\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Alternate Names: {alternate_names}\")\n",
    "\n",
    "        return alternate_names\n",
    "\n",
    "\n",
    "\n",
    "    def bar(self, foo):\n",
    "        if not foo or len(foo) == 1:\n",
    "            return foo\n",
    "\n",
    "        foo.sort()\n",
    "        b = [foo[0]]\n",
    "        \n",
    "        l = 0\n",
    "        \n",
    "        for i in range(1, len(foo)):\n",
    "            if foo[i] - foo[l] <= 1:\n",
    "                l = i\n",
    "            else:\n",
    "                b.append(foo[i])\n",
    "                l = i\n",
    "            \n",
    "        return b\n",
    "\n",
    "\n",
    "    \n",
    "    def load_species(self, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Load Search Strings from Species Spans\n",
    "        search_strings = self.load_search_strings(verbose=verbose)\n",
    "\n",
    "        # Search for Species\n",
    "        # The results are stored in different \n",
    "        # forms below.\n",
    "        spans = []\n",
    "        tokens = []\n",
    "        token_to_span = {}\n",
    "\n",
    "        # Where we're searching for species.\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "\n",
    "        for string in search_strings:\n",
    "            matches = re.finditer(re.escape(string), text, re.IGNORECASE)\n",
    "\n",
    "            for l_char_index, r_char_index, matched_text in [(match.start(), match.end(), match.group()) for match in matches]:    \n",
    "                # The full word must match, not just a substring inside of it.\n",
    "                # So, if the species we're looking for is \"ant\", only \"ant\"\n",
    "                # will match -- not \"pants\" or \"antebellum\". Therefore, the\n",
    "                # characters to the left and right of the matched string cannot\n",
    "                # be letters.\n",
    "                l_char_is_letter = l_char_index > 0 and text[l_char_index-1].isalpha()\n",
    "                r_char_is_letter = r_char_index < len(text) and text[r_char_index].isalpha()\n",
    "                \n",
    "                if l_char_is_letter or r_char_is_letter or not matched_text:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    l_token_i = self.main.token_at_char(l_char_index).i\n",
    "                    r_token_i = self.main.token_at_char(r_char_index-1).i\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: Unable to find token at index of {l_char_index}.\")\n",
    "                    print(f\"Error: Unable to find token at index of {r_char_index}.\")\n",
    "                    print(f\"\\tMatched: '{matched_text}'\")\n",
    "                    print(e)\n",
    "                    continue\n",
    "\n",
    "                # This is the matched substring (which would be\n",
    "                # a species) as a span in the parent document.\n",
    "                span = self.main.sp_doc[l_token_i:r_token_i+1]\n",
    "                \n",
    "                # Expand Species\n",
    "                # Let's say there's a word like \"squirrel\". That's a bit ambiguous. \n",
    "                # Is it a brown squirrel, a bonobo? If the species is possibly missing\n",
    "                # information (like an adjective to the left of it), we should expand\n",
    "                # in order to get a full picture of the species.\n",
    "                unclear_1 = len(span) == 1 and span[0].pos_ == \"NOUN\"\n",
    "                unclear_2 = span.start > 0 and self.main.sp_doc[span.start-1].pos_ in [\"ADJ\"]\n",
    "                \n",
    "                if unclear_1 or unclear_2:\n",
    "                    span = self.main.expand_unit(\n",
    "                        il_unit=span.start, \n",
    "                        ir_unit=span.end-1,\n",
    "                        il_boundary=0,\n",
    "                        ir_boundary=len(self.main.sp_doc),\n",
    "                        speech=[\"ADJ\", \"PROPN\"],\n",
    "                        literals=[\"-\"],\n",
    "                        include=True,\n",
    "                        direction=\"LEFT\",\n",
    "                        verbose=verbose\n",
    "                    )\n",
    "                \n",
    "                # Remove Outer Symbols\n",
    "                # There are times where a species is identified with a parenthesis\n",
    "                # nearby. Here, we remove that parenthesis (and any other symbols).\n",
    "                span = self.main.contract_unit(\n",
    "                    il_unit=span.start, \n",
    "                    ir_unit=span.end-1, \n",
    "                    speech=[\"PUNCT\", \"SYM\", \"DET\", \"PART\"],\n",
    "                    include=False,\n",
    "                    verbose=verbose\n",
    "                )\n",
    "\n",
    "                if not span:\n",
    "                    print(f\"Error: Span does not exist; left character index {l_char_index}.\")\n",
    "                    print(f\"\\tMatched: '{matched_text}'\")\n",
    "                    continue\n",
    "            \n",
    "                # A species must have a noun or a\n",
    "                # proper noun. This may help discard\n",
    "                # bad results.\n",
    "                letter_found = False\n",
    "                for token in span:\n",
    "                    if token.pos_ in [\"NOUN\", \"PROPN\"] or token.lower_ in self.dictionary:\n",
    "                        letter_found = True\n",
    "                        break\n",
    "\n",
    "                if not letter_found:\n",
    "                    continue\n",
    "\n",
    "                # Adding Species\n",
    "                spans.append(span)\n",
    "                for token in span:\n",
    "                    if token in tokens or token.pos_ in [\"PUNCT\", \"SYM\", \"DET\", \"PART\"]:\n",
    "                        continue\n",
    "                    tokens.append(token)\n",
    "                    token_to_span[token] = span\n",
    "        \n",
    "        spans = list({span.start: span for span in spans}.values())\n",
    "        spans.sort(key=lambda span: span.start)\n",
    "        \n",
    "        span_starts = [span[0] for span in spans]\n",
    "        span_indices = self.bar([span[0].i for span in spans])\n",
    "        span_starts = [self.main.sp_doc[i] for i in span_indices]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"Output of load_species:\")\n",
    "            print(f\"Spans: {spans}\")\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "            print(f\"Mapped Tokens: {token_to_span}\")\n",
    "            print(f\"Span Starts: {span_starts}\")\n",
    "        \n",
    "        return (spans, tokens, token_to_span, span_starts)\n",
    "\n",
    "\n",
    "\n",
    "    def is_alternate(self, sp_a, sp_b):\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        sp_a_text = sp_a.text.lower()\n",
    "            \n",
    "        # Species B is an alternate name for Species A\n",
    "        if sp_b_text in self.alternate_names.get(sp_a_text, []):\n",
    "            return True\n",
    "        \n",
    "        # Species A is an alternate name for Species B\n",
    "        if sp_a_text in self.alternate_names.get(sp_b_text, []):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def is_same_text(self, sp_a, sp_b):\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        sp_a_text = sp_a.text.lower()\n",
    "\n",
    "        if sp_a_text == sp_b_text:\n",
    "            return True\n",
    "            \n",
    "        sp_a_singular_texts = [sp_a_text] if sp_a[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(sp_a_text)\n",
    "        sp_b_singular_texts = [sp_b_text] if sp_b[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(sp_b_text)\n",
    "\n",
    "        if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def has_same_base_nouns(self, sp_a, sp_b):\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        sp_b_0_text = sp_b[0].lower_\n",
    "        sp_b_0_is_noun = sp_b[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "        \n",
    "        sp_b_nouns = []\n",
    "        sp_b_num_adjectives = 0\n",
    "        \n",
    "        for token in sp_b:\n",
    "            if not sp_b_nouns and token.pos_ == \"ADJ\":\n",
    "                sp_b_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_b_nouns.append(token)\n",
    "\n",
    "        if not sp_b_nouns:\n",
    "            return False\n",
    "\n",
    "        sp_b_nouns_text = [noun.lower_ for noun in sp_b_nouns]\n",
    "        sp_b_singular_texts = [\" \".join(sp_b_nouns_text)] if sp_b_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(\" \".join(sp_b_nouns_text))\n",
    "\n",
    "        sp_a_text = sp_a.text.lower()\n",
    "        sp_a_0_text = sp_a[0].lower_\n",
    "        sp_a_0_is_noun = sp_a[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "        # Case Example: 'Hyla' v. 'Hyla tadpoles'\n",
    "        if sp_a_0_text == sp_b_0_text and (sp_a_0_is_noun or sp_b_0_is_noun):\n",
    "            if sp_a_text in sp_b_text or sp_b_text in sp_a_text:\n",
    "                return True\n",
    "        \n",
    "        # Case Example: 'dogs' v. 'red dogs'\n",
    "        sp_a_nouns = []\n",
    "        sp_a_num_adjectives = 0\n",
    "        for token in sp_a:\n",
    "            if not sp_a_nouns and token.pos_ == \"ADJ\":\n",
    "                sp_a_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_a_nouns.append(token)\n",
    "        \n",
    "        if not sp_a_nouns:\n",
    "            return False\n",
    "        \n",
    "        sp_a_nouns_text = [noun.lower_ for noun in sp_a_nouns]\n",
    "        \n",
    "        if sp_a_nouns and sp_b_nouns and (\n",
    "            (sp_a_num_adjectives == 1 and sp_b_num_adjectives == 0) or \n",
    "            (sp_b_num_adjectives == 1 and sp_a_num_adjectives == 0)\n",
    "        ):\n",
    "            sp_a_singular_texts = [\" \".join(sp_a_nouns_text)] if sp_a_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(\" \".join(sp_a_nouns_text))\n",
    "            if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def find_same_species(self, sp_A, sp_b, verbose=False):\n",
    "        # METHOD 1: Check for Literal Matches\n",
    "        for sp_a in sp_A:\n",
    "            if self.is_same_text(sp_a, sp_b):\n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Method 1: Match Between '{sp_a}' and '{sp_b}'\")\n",
    "                return sp_a\n",
    "\n",
    "        # METHOD 2: Check Alternate Names\n",
    "        for sp_a in sp_A:\n",
    "            if self.is_alternate(sp_a, sp_b):\n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Method 2: Match Between '{sp_a}' and '{sp_b}'\")\n",
    "                return sp_a\n",
    "        \n",
    "        # METHOD 3: Check Nouns\n",
    "        # This is used if one or none of the species being compared\n",
    "        # has 1 adjective.\n",
    "        for sp_a in sp_A:\n",
    "            if self.has_same_base_nouns(sp_a, sp_b):\n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Method 3: Match Between '{sp_a}' and '{sp_b}'\")\n",
    "                return sp_a\n",
    "\n",
    "        # METHOD 4: Last Ditch Effort\n",
    "        # If there's been no matches, we just look for one string inside of\n",
    "        # another.\n",
    "        for sp_a in sp_A:\n",
    "            sp_a_text = sp_a.text.lower()\n",
    "            sp_b_text = sp_b.text.lower()\n",
    "            \n",
    "            r_sp_a_text = re.compile(f\"(\\s|^){sp_a_text}(\\s|$)\", re.IGNORECASE)\n",
    "            r_sp_b_text = re.compile(f\"(\\s|^){sp_b_text}(\\s|$)\", re.IGNORECASE)\n",
    "            \n",
    "            if re.match(r_sp_a_text, sp_b_text) or re.match(r_sp_b_text, sp_a_text):\n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Method 4: Match Between '{sp_a}' and '{sp_b}'\")\n",
    "                return sp_a\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"No Matches Between {sp_A} and {sp_b}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    def span_at_token(self, token):\n",
    "        if token in self.token_to_span:\n",
    "            return self.token_to_span[token]\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    def is_species(self, token):\n",
    "        return token in self.tokens\n",
    "\n",
    "\n",
    "\n",
    "    def has_species(self, tokens, verbose=False):\n",
    "        for token in tokens:\n",
    "            if token in self.tokens:\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tToken '{token}' is Species\")\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c50028be-77a2-452c-9808-0a9ebad62620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    REGEX = \"regex\"\n",
    "    VOCAB = \"vocab\"\n",
    "    RULES = \"rules\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, main, *, regexes=[], vocab=[], patterns=[], def_pos=[], def_tag=[], def_threshold=0.7, def_weight=1.0):\n",
    "        self.main = main\n",
    "\n",
    "        # Constraints\n",
    "        self.def_threshold = def_threshold\n",
    "        self.def_tag = def_tag\n",
    "        self.def_pos = def_pos\n",
    "        self.def_weight = def_weight\n",
    "        \n",
    "        # Three Types of Matching\n",
    "        self.vocab, self.vocab_data = self.load_vocab(vocab)\n",
    "        self.regex, self.regex_data = self.load_regex(regexes)\n",
    "        self.rules, self.rules_data = self.load_rules(patterns)\n",
    "\n",
    "        # Quick Lookup\n",
    "        self.match_type_to_data = {\n",
    "            Keywords.REGEX: self.regex_data,\n",
    "            Keywords.VOCAB: self.vocab_data,\n",
    "            Keywords.RULES: self.rules_data\n",
    "        }\n",
    "\n",
    "    \n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        # Matched Tokens in Different Forms\n",
    "        self.token_data, self.mapped_token_data, self.tokens = self.match_tokens(verbose=verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def load_regex(self, regexes):\n",
    "        r = []\n",
    "        r_data = {}\n",
    "\n",
    "        for unit in regexes:\n",
    "            if isinstance(unit, str):\n",
    "                r.append(unit)\n",
    "            else:\n",
    "                regex = unit[\"regex\"]\n",
    "                r.append(regex)\n",
    "                r_data[regex] = {\n",
    "                    \"types\": unit.get(\"types\", []),\n",
    "                    \"weight\": unit.get(\"weight\", self.def_weight)\n",
    "                }\n",
    "\n",
    "        return r, r_data\n",
    "\n",
    "\n",
    "\n",
    "    def load_vocab(self, vocab):\n",
    "        v = []\n",
    "        v_data = {}\n",
    "        \n",
    "        for unit in vocab:\n",
    "            if isinstance(unit, str):\n",
    "                doc = self.main.sp_nlp(unit)\n",
    "                v.append({\n",
    "                    \"doc\": doc,\n",
    "                    \"lemma\": \" \".join([t.lemma_ for t in doc])\n",
    "                })\n",
    "            else:\n",
    "                doc = self.main.sp_nlp(unit[\"word\"])\n",
    "                v.append({\n",
    "                    \"doc\": doc,\n",
    "                    \"tag\": unit.get(\"tag\", self.def_tag),\n",
    "                    \"pos\": unit.get(\"pos\", self.def_pos),\n",
    "                    \"threshold\": unit.get(\"threshold\", self.def_threshold),\n",
    "                    \"lemma\": \" \".join([t.lemma_ for t in doc])\n",
    "                })\n",
    "                v_data[unit[\"word\"]] = {\n",
    "                    \"types\": unit.get(\"types\") or [],\n",
    "                    \"weight\": unit.get(\"weight\", self.def_weight),\n",
    "                }\n",
    "        \n",
    "        return v, v_data\n",
    "\n",
    "\n",
    "\n",
    "    def load_rules(self, patterns):\n",
    "        r = Matcher(self.main.sp_nlp.vocab)\n",
    "        r_data = {}\n",
    "        \n",
    "        for i, unit in enumerate(patterns):\n",
    "            if isinstance(unit, list):\n",
    "                r.add(f\"{i}\", unit)\n",
    "            else:\n",
    "                r.add(unit[\"name\"], unit[\"pattern\"])\n",
    "                r_data[unit[\"name\"]] = {\n",
    "                    \"types\": unit.get(\"types\") or [],\n",
    "                    \"weight\": unit.get(\"weight\", self.def_weight),\n",
    "                }\n",
    "\n",
    "        return r, r_data\n",
    "\n",
    "\n",
    "\n",
    "    def get_match_data(self, token, match_id, match_type):\n",
    "        match_type_data = self.match_type_to_data[match_type]\n",
    "        \n",
    "        if match_id in match_type_data:\n",
    "            return {\n",
    "                \"token\": token,\n",
    "                \"types\": match_type_data[match_id].get(\"types\", []),\n",
    "                \"weight\": match_type_data[match_id].get(\"weight\", self.def_weight)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"token\": token,\n",
    "                \"types\": [],\n",
    "                \"weight\": self.def_weight\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "    def bad_pos(self, pos):\n",
    "        return self.def_pos and pos not in self.def_pos\n",
    "\n",
    "\n",
    "\n",
    "    def bad_tag(self, tag):\n",
    "        return self.def_tag and tag not in self.def_tag\n",
    "\n",
    "\n",
    "\n",
    "    def bad_token(self, token):\n",
    "        return self.bad_pos(token.pos_) or self.bad_tag(token.tag_)\n",
    "\n",
    "\n",
    "\n",
    "    def match_tokens(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        matched_data = []\n",
    "        matched_tokens = []\n",
    "\n",
    "        # Match by Regex\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "        \n",
    "        for regex in self.regex:\n",
    "            matches = [(match.start(), match.end()) for match in re.finditer(regex, text, re.IGNORECASE)]\n",
    "            \n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\t'{regex}' Regex Matches: {matches}\")\n",
    "            \n",
    "            for l_char_index, r_char_index in matches:\n",
    "                span = self.main.get_span_at_indices(l_char_index, r_char_index - 1)\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tSpan Matched: {span}\")\n",
    "\n",
    "                for token in span:\n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tPossible Regex Match for Token '{token}' (Position: {token.pos_} and Tag: {token.tag_})\")\n",
    "                        \n",
    "                    if self.bad_token(token):\n",
    "                        continue\n",
    "                    \n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tRegex Matched Token '{token}'\")\n",
    "                        \n",
    "                    matched_tokens.append(token)\n",
    "                    matched_data.append(self.get_match_data(token, regex, Keywords.REGEX))\n",
    "\n",
    "        # Match by Rules\n",
    "        matches = self.rules(self.main.sp_doc)\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 2:\n",
    "            print(f\"\\tRule Matches: {matches}\")\n",
    "        \n",
    "        for match_id, start, end in matches:\n",
    "            span = self.main.sp_doc[start:end]\n",
    "            name = self.main.sp_nlp.vocab.strings[match_id]\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tPattern '{name}' Matched Span: {span}\")\n",
    "            \n",
    "            for token in span:\n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tPossible Rule Match for Token '{token}' (Position: {token.pos_} and Tag: {token.tag_})\")\n",
    "                    \n",
    "                if self.bad_token(token):\n",
    "                    continue\n",
    "                \n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tRule Matched Token '{token}'\")\n",
    "\n",
    "                matched_tokens.append(token)\n",
    "                matched_data.append(self.get_match_data(token, name, Keywords.RULES))\n",
    "\n",
    "        # Match by Vocab\n",
    "        for token in self.main.sp_doc:\n",
    "            if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tPossible Vocab Match for Token '{token}' (Position: {token.pos_} and Tag: {token.tag_})\")\n",
    "                    \n",
    "            if self.bad_token(token) or token in matched_tokens:\n",
    "                continue\n",
    "\n",
    "            token_doc = self.main.sp_nlp(token.lower_)\n",
    "            token_lemma = \" \".join([t.lemma_ for t in token_doc])\n",
    "            \n",
    "            for vocab_word in self.vocab:\n",
    "                # Ensure Correct Tag\n",
    "                if vocab_word.get(\"tag\"):\n",
    "                    if not [t for t in token_doc if t.tag_ in vocab_word.get(\"tag\")]:\n",
    "                        if verbose and VERBOSE_LEVEL >= 4:\n",
    "                            print(f\"\\t\\t\\tToken '{token_doc}' not in Vocab Word '{vocab_word['doc']}' Tags ({vocab_word.get('tag')})\")\n",
    "                        continue\n",
    "                \n",
    "                # Ensure Correct PoS\n",
    "                if vocab_word.get(\"pos\"):\n",
    "                    if not [t for t in token_doc if t.pos_ in vocab_word.get(\"pos\")]:\n",
    "                        if verbose and VERBOSE_LEVEL >= 4:\n",
    "                            print(f\"\\t\\t\\tToken '{token_doc}' not in Vocab Word '{vocab_word['doc']}' Speech ({vocab_word.get('pos')})\")\n",
    "                        continue\n",
    "\n",
    "                # Check Lemma\n",
    "                if verbose and VERBOSE_LEVEL >= 4:\n",
    "                    print(f\"\\t\\t\\t{token_doc} Lemma ({token_lemma}) and {vocab_word['doc']} Lemma ({vocab_word['lemma']})\")\n",
    "                    \n",
    "                if token_lemma == vocab_word[\"lemma\"]:\n",
    "                    matched_tokens.append(token)\n",
    "                    matched_data.append(self.get_match_data(token, vocab_word[\"doc\"].text, Keywords.VOCAB))\n",
    "\n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tVocab (Lemma) Matched Token '{token}'\")\n",
    "                    \n",
    "                    break\n",
    "\n",
    "                # Check Similarity\n",
    "                similarity = vocab_word[\"doc\"].similarity(token_doc)\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 4:\n",
    "                    print(f\"\\t\\t\\t{token_doc} and {vocab_word['doc']} Similarity: {similarity}\")\n",
    "                    \n",
    "                if similarity >= vocab_word.get(\"threshold\", self.def_threshold):\n",
    "                    matched_tokens.append(token)\n",
    "                    matched_data.append(self.get_match_data(token, vocab_word[\"doc\"].text, Keywords.VOCAB))\n",
    "\n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tVocab Matched Token '{token}'\")\n",
    "                        \n",
    "                    break\n",
    "\n",
    "        # Mapping Match(ed Token) Data\n",
    "        mapped_matched_data = {}\n",
    "        for matched_token_data in matched_data:\n",
    "            mapped_matched_data[matched_token_data[\"token\"]] = matched_token_data\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"Output of match_tokens\")\n",
    "            print(f\"Token Data: {matched_data}\")\n",
    "            print(f\"Mapped Token Data: {mapped_matched_data}\")\n",
    "            print(f\"Token: {matched_tokens}\")\n",
    "        \n",
    "        return matched_data, mapped_matched_data, matched_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83658627-3be1-4c6f-9b7e-37ad93ceb57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"study\", \n",
    "                \"hypothesis\", \n",
    "                \"experiment\", \n",
    "                \"found\", \n",
    "                \"discover\", \n",
    "                \"compare\", \n",
    "                \"finding\", \n",
    "                \"result\", \n",
    "                \"test\", \n",
    "                \"examine\", \n",
    "                \"model\",\n",
    "                \"measure\",\n",
    "                \"manipulate\",\n",
    "                \"assess\",\n",
    "                \"conduct\",\n",
    "                \"data\",\n",
    "                \"analyze\",\n",
    "                \"sample\",\n",
    "                \"observe\",\n",
    "                \"observation\",\n",
    "                \"predict\",\n",
    "                \"suggest\",\n",
    "                \"method\",\n",
    "                \"investigation\",\n",
    "                \"trial\",\n",
    "                \"experimental\",\n",
    "                \"evidence\",\n",
    "                \"demonstrate\",\n",
    "                \"analysis\",\n",
    "                \"show\",\n",
    "                \"compare\",\n",
    "                \"comparable\",\n",
    "                \"control group\", \n",
    "                \"independent\",\n",
    "                \"dependent\",\n",
    "                \"applied\",\n",
    "                \"treatment\",\n",
    "                \"survery\",\n",
    "                \"evaluate\",\n",
    "                \"ran\"\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924641b9-9216-4b44-9cb9-0fa1bb303b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"theory\",\n",
    "                \"review\",\n",
    "                \"meta-analysis\"\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a6c38cf-af1d-40b4-9cec-62c644855534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeTopicKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            regexes=[\n",
    "                r\"co-?evolution\",\n",
    "                r\"evolution\",\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b474d22b-6eb4-4df7-9d90-28424dea1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CauseKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"increase\", \n",
    "                \"decrease\", \n",
    "                \"change\", \n",
    "                \"shift\", \n",
    "                \"cause\", \n",
    "                \"produce\", \n",
    "                \"trigger\", \n",
    "                \"suppress\", \n",
    "                \"inhibit\",\n",
    "                \"encourage\",\n",
    "                \"allow\",\n",
    "                \"influence\",\n",
    "                \"affect\",\n",
    "                \"alter\",\n",
    "                \"induce\",\n",
    "                \"produce\",\n",
    "                \"result in\",\n",
    "                # \"associated with\",\n",
    "                # \"correlated with\",\n",
    "                \"contribute\",\n",
    "                \"impact\",\n",
    "                \"deter\",\n",
    "                \"depressed\",\n",
    "                \"when\",\n",
    "                \"because\",\n",
    "                \"reduce\",\n",
    "                \"killed\",\n",
    "                # \"supported\"\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"SCONJ\", \"NOUN\"],\n",
    "            # def_tag=[\"VB\", \"VBD\", \"WRB\", \"IN\", \"VBG\"],\n",
    "            # def_threshold=0.75\n",
    "            def_threshold=0.8\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "\n",
    "    \n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a586e5f-2dcb-42c4-ac6f-575febea3079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"few\", \n",
    "                \"more\", \n",
    "                \"increase\", \n",
    "                \"decrease\", \n",
    "                \"less\", \n",
    "                \"short\", \n",
    "                \"long\", \n",
    "                \"greater\"\n",
    "                \"shift\",\n",
    "                \"fluctuate\",\n",
    "                \"adapt\",\n",
    "                \"grow\",\n",
    "                \"rise\"\n",
    "                \"surge\",\n",
    "                \"intensify\",\n",
    "                \"amplify\",\n",
    "                \"multiply\",\n",
    "                \"decline\",\n",
    "                \"reduce\",\n",
    "                \"drop\",\n",
    "                \"diminish\",\n",
    "                \"fall\",\n",
    "                \"lessen\",\n",
    "                \"doubled\",\n",
    "                \"tripled\",\n",
    "                \"lower\",\n",
    "                \"adjust\",\n",
    "                \"reject\",\n",
    "            ],\n",
    "            regexes=[\n",
    "                # Match Examples:\n",
    "                # 1. \"one... as...\"\n",
    "                # 2. \"2x than...\"\n",
    "                r\"(one|two|three|four|five|six|seven|eight|nine|ten|twice|thrice|([0-9]+|[0-9]+.[0-9]+)(x|%))[\\s-]+[^\\s]*[\\s-]+(as|more|than|likely)([\\s-]+|$)\"\n",
    "            ],\n",
    "            def_pos=[\"NOUN\", \"ADJ\", \"ADV\", \"VERB\", \"NUM\"],\n",
    "            def_threshold=0.75\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose=verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "\n",
    "    \n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in self.main.sp_doc:\n",
    "            # Already Matched\n",
    "            if token in tokens:\n",
    "                filtered.append(token)\n",
    "            \n",
    "            # Comparative Adjective\n",
    "            # Looking for words like \"bigger\" and \"better\".\n",
    "            elif token.pos_ == \"ADJ\" and token.tag_ == \"JJR\":\n",
    "                filtered.append(token)\n",
    "                continue\n",
    "            \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf8be9de-1c7a-4246-a104-e9dd18e00d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraitKeywords(Keywords):\n",
    "    FOOD = \"Food\"\n",
    "    NOT_APPLICABLE = \"N/A\"\n",
    "    \n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            regexes=[\n",
    "                r\"behaviou?r\", \n",
    "                r\"[^A-Za-z]+rate\", \n",
    "                \"colou?r\",\n",
    "                \"biomass\",\n",
    "                r\"[^A-Za-z]+mass\", \n",
    "                r\"[^A-Za-z]+size\",\n",
    "                \"number\",\n",
    "                \"length\", \n",
    "                \"pattern\", \n",
    "                \"weight\",\n",
    "                \"shape\", \n",
    "                \"efficiency\", \n",
    "                \"trait\",\n",
    "                \"phenotype\",\n",
    "                \"demography\",\n",
    "                \"scent\",\n",
    "                \"population (structure|mechanic)s?\",\n",
    "                \"ability\", \n",
    "                \"capacity\", \n",
    "                \"height\", \n",
    "                \"width\", \n",
    "                \"[A-Za-z]+span\",\n",
    "                {\"regex\": \"diet\", \"types\": [TraitKeywords.FOOD]},\n",
    "                {\"regex\": \"food\", \"types\": [TraitKeywords.FOOD, TraitKeywords.NOT_APPLICABLE]},\n",
    "                {\"regex\": \"feeding\", \"types\": [TraitKeywords.FOOD]},\n",
    "                \"nest\",\n",
    "                \"substrate\",\n",
    "                \"breeding\",\n",
    "                r\"[^A-Za-z]+age[^A-Za-z]+\",\n",
    "                \"lifespan\",\n",
    "                \"development\",\n",
    "                \"output\",\n",
    "                \"time\",\n",
    "                \"period\",\n",
    "                \"level\",\n",
    "                \"configuration\",\n",
    "                \"dimorphism\",\n",
    "                \"capability\",\n",
    "                \"regulation\",\n",
    "                \"excretion\",\n",
    "                \"luminescence\",\n",
    "                r\"[^A-Za-z]+role\",\n",
    "                \"sensitivity\",\n",
    "                \"resistance\",\n",
    "                r\"(un|(^|\\s)[A-Za-z]*-)infected\",\n",
    "                \"temperature\",\n",
    "                \"density\"\n",
    "            ],\n",
    "            def_pos=[\"NOUN\"]\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "\n",
    "    \n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Unfiltered Trait Tokens: {tokens}\")\n",
    "        \n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            expanded_token = self.main.expand_unit(\n",
    "                il_unit=token.i, \n",
    "                ir_unit=token.i, \n",
    "                il_boundary=token.sent.start, \n",
    "                ir_boundary=token.sent.end-1, \n",
    "                speech=[\"PUNCT\"],\n",
    "                include=False,\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "            if self.main.species.has_species(expanded_token, verbose=verbose):\n",
    "                filtered.append(token)\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Filtered Trait Tokens: {filtered}\")\n",
    "        \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2a68dbd-47ea-4396-a860-4ac6876b71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main,\n",
    "            vocab=[\n",
    "                \"compare\",\n",
    "                \"examine\",\n",
    "                \"evaluate\",\n",
    "                \"assess\",\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b5dd3fe-54f9-4376-b381-1bb35a1d0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariabilityKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main,\n",
    "            vocab=[\n",
    "                {\"word\": \"different\", \"pos\": [\"ADJ\", \"NOUN\"]},\n",
    "                {\"word\": \"vary\", \"pos\": [\"VERB\", \"NOUN\"]},\n",
    "                {\"word\": \"varied\", \"pos\": [\"VERB\", \"NOUN\"]},\n",
    "                {\"word\": \"compare\", \"pos\": [\"VERB\"]}\n",
    "            ],\n",
    "            regexes=[\n",
    "                r\"between\",\n",
    "                r\"against\",\n",
    "                r\"independen(t|ts|tly|cy)\",\n",
    "                r\"dependen(t|ts|tly|cy)\",\n",
    "                r\"treatments?\",\n",
    "                r\"effect\",\n",
    "                r\"control\",\n",
    "                r\"(with|without)[A-Za-z]*(with|without)\",\n",
    "                r\"(^| )(un|not)[-| ]?([A-Za-z]+) [^!;?.\\n]* \\3\",\n",
    "                r\"([A-Za-z]+) [^!;?.\\n]* (un|not)[-| ]?\\1( |$)\",\n",
    "                \n",
    "            ],\n",
    "            patterns=[\n",
    "                [[{\"LOWER\": {\"IN\": [\"neither\", \"either\", \"both\"]}}, {\"OP\": \"*\", \"TAG\": {\"NOT_IN\": [\".\"]}}, {\"LOWER\": {\"IN\": [\"or\", \"and\"]}}]],\n",
    "                [[{\"LOWER\": {\"IN\": [\"with\", \"without\"]}}, {\"OP\": \"*\", \"TAG\": {\"NOT_IN\": [\".\"]}}, {\"LOWER\": {\"IN\": [\"with\", \"without\"]}}]],\n",
    "                [[{\"LOWER\": {\"IN\": [\"at\"]}}, {\"POS\": \"NUM\"}]],\n",
    "                [[{\"LOWER\": {\"IN\": [\"at\"]}}, {\"LOWER\": {\"IN\": [\"several\", \"unique\", \"multiple\", \"different\"]}}]],\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "401890b6-bfa1-4d81-97fb-ad7652e14504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_dataset(name):\n",
    "    # Load Dataset\n",
    "    data = pd.read_csv(f\"../Week 16/Datasets/{name}.csv\")\n",
    "    print(f\"Data Shape: {data.shape}\")\n",
    "\n",
    "    if data.shape[0] == 0:\n",
    "        print(\"Nothing to Score\")\n",
    "        return\n",
    "\n",
    "    # Preprocess Data\n",
    "    data.drop_duplicates(subset=['Abstract'], inplace=True)\n",
    "    data.dropna(subset=['Abstract'], inplace=True)\n",
    "\n",
    "    # Drop Unnamed Columns\n",
    "    # https://www.geeksforgeeks.org/how-to-drop-unnamed-column-in-pandas-dataframe/\n",
    "    data.drop(data.columns[data.columns.str.contains('unnamed', case=False)], axis=1, inplace=True)\n",
    "\n",
    "    # Reset Index\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79ff77d7-be07-41b5-aa7d-8ea134dda6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Main(Base):\n",
    "    def __init__(self):\n",
    "        # Tools\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        self.sp_doc = None\n",
    "\n",
    "        # Maps Character Position to Token in Document\n",
    "        # Used to handle differences between different\n",
    "        # pipelines and tools.\n",
    "        self.index_map = None\n",
    "    \n",
    "        # Parsers\n",
    "        self.species = Species(self)\n",
    "        self.trait = TraitKeywords(self)\n",
    "        self.cause = CauseKeywords(self)\n",
    "        self.change = ChangeKeywords(self)\n",
    "        self.experiment = ExperimentKeywords(self)\n",
    "        self.not_experiment = NegativeExperimentKeywords(self)\n",
    "        self.not_topic = NegativeTopicKeywords(self)\n",
    "        self.variability = VariabilityKeywords(self)\n",
    "        self.test = TestKeywords(self)\n",
    "\n",
    "        # Helper\n",
    "        super().__init__(self)\n",
    "\n",
    "\n",
    "    \n",
    "    def update_doc(self, doc, verbose=False):\n",
    "        self.sp_doc = doc\n",
    "        self.index_map = self.load_index_map()\n",
    "        self.species.update(doc.text, verbose=False)\n",
    "        self.trait.update(verbose=verbose)\n",
    "        self.cause.update(verbose=False)\n",
    "        self.change.update(verbose=False)\n",
    "        self.experiment.update(verbose=False)\n",
    "        self.not_experiment.update(verbose=False)\n",
    "        self.not_topic.update(verbose=False)\n",
    "        self.variability.update(verbose=False)\n",
    "        self.test.update(verbose=False)\n",
    "\n",
    "\n",
    "        \n",
    "    def update_text(self, text, verbose=False):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.update_doc(self.sp_doc, verbose=verbose)\n",
    "\n",
    "\n",
    "        \n",
    "    def token_at_char(self, char_index):\n",
    "        # SpaCy Doc or Indexing Map Not Found\n",
    "        if not self.sp_doc or not self.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        if char_index in self.index_map:\n",
    "            return self.index_map[char_index]\n",
    "\n",
    "        raise Exception(f\"Token at Index {char_index} Not Found\")\n",
    "\n",
    "\n",
    "        \n",
    "    def load_index_map(self):\n",
    "        # SpaCy Doc Not Found\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Map Character Index to Token\n",
    "        index_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            l_char_index = token.idx\n",
    "            r_char_index = token.idx + len(token)\n",
    "\n",
    "            for i in range(l_char_index, r_char_index):\n",
    "                index_map[i] = token\n",
    "\n",
    "        return index_map\n",
    "\n",
    "\n",
    "        \n",
    "    def valid_trait_token(self, token, sent_cause_tokens, sent_change_tokens, verbose=False):\n",
    "        if token not in self.trait.tokens:\n",
    "            return 0\n",
    "\n",
    "        token_data = self.trait.mapped_token_data[token]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Token '{token}' Types: {token_data['types']}\")\n",
    "            \n",
    "        if TraitKeywords.NOT_APPLICABLE in token_data[\"types\"]:\n",
    "            return 0\n",
    "\n",
    "        token_context = set(self.find_unit_context(\n",
    "            il_unit=token.i, \n",
    "            ir_unit=token.i, \n",
    "            il_boundary=token.sent.start, \n",
    "            ir_boundary=token.sent.end-1, \n",
    "            verbose=verbose)\n",
    "        )\n",
    "        \n",
    "        causes = set(sent_cause_tokens).intersection(token_context)\n",
    "        changes = set(sent_change_tokens).intersection(token_context)\n",
    "\n",
    "        if causes or changes:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.25\n",
    "\n",
    "\n",
    "    \n",
    "    def valid_species_token(self, token, sent_cause_tokens, sent_change_tokens, verbose=False):\n",
    "        if token not in self.species.tokens:\n",
    "            return 0\n",
    "        \n",
    "        token_context = set(self.find_unit_context(\n",
    "            il_unit=token.i, \n",
    "            ir_unit=token.i, \n",
    "            il_boundary=token.sent.start, \n",
    "            ir_boundary=token.sent.end-1, \n",
    "            verbose=verbose)\n",
    "        )\n",
    "        \n",
    "        causes = set(sent_cause_tokens).intersection(token_context)\n",
    "        changes = set(sent_change_tokens).intersection(token_context)\n",
    "\n",
    "        if causes or changes:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    \n",
    "    def update_seen_species(self, token, seen_species, sent_seen_species, sent_num_unique_species, verbose=False):\n",
    "        # Update Seen Species in Text\n",
    "        span = self.species.span_at_token(token)\n",
    "        prev_ref = self.species.find_same_species(seen_species.keys(), span, verbose=verbose)\n",
    "        \n",
    "        if prev_ref:\n",
    "            seen_species[prev_ref] += 1\n",
    "        else:\n",
    "            seen_species[span] = 1\n",
    "\n",
    "        # Check Seen Species in Sentence\n",
    "        # We only add points if it's a species that has not been seen\n",
    "        # in the sentence. This is to avoid redundant points. \n",
    "        # Also, if it species has not been seen at all (is_new_species),\n",
    "        # then it cannot be a redundant species (we couldn't have seen it in the sentence\n",
    "        # either).\n",
    "\n",
    "        prev_sent_ref = self.species.find_same_species(sent_seen_species.keys(), span, verbose=verbose)\n",
    "        if prev_sent_ref:\n",
    "            sent_seen_species[prev_sent_ref][\"visits\"] += 1\n",
    "            ref = prev_sent_ref\n",
    "        else:\n",
    "            sent_seen_species[span] = {\n",
    "                \"visits\": 1,\n",
    "                \"points\": 0\n",
    "            }\n",
    "            ref = span\n",
    "        \n",
    "        # Update Number of Unique Species in Sentence\n",
    "        if not prev_sent_ref:\n",
    "            sent_num_unique_species += 1\n",
    "\n",
    "        return seen_species, sent_seen_species, prev_sent_ref, sent_num_unique_species, ref\n",
    "\n",
    "\n",
    "    \n",
    "    def valid_trait_variation(self, verbose=False):\n",
    "        max_trait_variation_points = 0\n",
    "        \n",
    "        sentences = list(self.sp_doc.sents)\n",
    "        num_sentences = len(sentences)\n",
    "\n",
    "        for i in range(num_sentences):\n",
    "            print()\n",
    "            print()\n",
    "            print()\n",
    "            print()\n",
    "            print()\n",
    "            \n",
    "            sent_i = sentences[i]\n",
    "            sent_i_tokens = set([token for token in sent_i])\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tSentence I: {sent_i}\")\n",
    "            \n",
    "            sent_i_test_tokens = sent_i_tokens.intersection(self.test.tokens)\n",
    "            sent_i_experiment_tokens = sent_i_tokens.intersection(self.experiment.tokens)\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tSentence I Test Tokens: {sent_i_test_tokens}\")\n",
    "                print(f\"\\tSentence I Experiment Tokens: {sent_i_experiment_tokens}\")\n",
    "\n",
    "            if not sent_i_test_tokens and not sent_i_experiment_tokens:\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tNo Experiment or Test Tokens in Sentence I\")\n",
    "                continue\n",
    "\n",
    "            trait_variation_points_i = 0\n",
    "\n",
    "            if sent_i_experiment_tokens:\n",
    "                trait_variation_points_i = 0.10\n",
    "            \n",
    "            if sent_i_test_tokens:\n",
    "                trait_variation_points_i = 0.25\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tTrait Variation Points for I: {trait_variation_points_i}\")\n",
    "\n",
    "            sent_i_trait_tokens = sent_i_tokens.intersection(self.trait.tokens)\n",
    "            sent_i_species_tokens = sent_i_tokens.intersection(self.species.tokens)\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tSentence I Trait Tokens: {sent_i_trait_tokens}\")\n",
    "\n",
    "            if not sent_i_trait_tokens and not sent_i_species_tokens:\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tNo Trait or Specie Tokens in Sentence I\")\n",
    "                continue\n",
    "\n",
    "            t_variables = []\n",
    "            s_variables = []\n",
    "            sent_i_variability_tokens = sent_i_tokens.intersection(self.variability.tokens)\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tSentence I Variability Tokens: {sent_i_variability_tokens}\")\n",
    "\n",
    "            deduct_points = not sent_i_variability_tokens\n",
    "            \n",
    "            if sent_i_variability_tokens:\n",
    "                for token in sent_i_variability_tokens:\n",
    "                    trait_in_context = set(self.find_unit_context(\n",
    "                        il_unit=token.i, \n",
    "                        ir_unit=token.i, \n",
    "                        il_boundary=token.sent.start,\n",
    "                        ir_boundary=token.sent.end-1, \n",
    "                        speech=[\"NOUN\", \"ADJ\", \"ADV\", \"PROPN\", \"ADP\", \"CCONJ\", \"DET\" \"NUM\", \"SYM\"],\n",
    "                        include=True,\n",
    "                        comma_encloses=False,\n",
    "                        verbose=verbose\n",
    "                    )).intersection(self.trait.tokens)\n",
    "\n",
    "                    specie_in_context = set(self.find_unit_context(\n",
    "                        il_unit=token.i, \n",
    "                        ir_unit=token.i, \n",
    "                        il_boundary=token.sent.start,\n",
    "                        ir_boundary=token.sent.end-1, \n",
    "                        speech=[\"NOUN\", \"ADJ\", \"ADV\", \"PROPN\", \"ADP\", \"CCONJ\", \"DET\", \"NUM\", \"SYM\"],\n",
    "                        include=True,\n",
    "                        comma_encloses=False,\n",
    "                        verbose=verbose\n",
    "                    )).intersection(self.species.tokens)\n",
    "                    \n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tVariability Token '{token}' Traits in Context: {trait_in_context}\") \n",
    "                        print(f\"\\t\\tVariability Token '{token}' Specie in Context: {specie_in_context}\") \n",
    "\n",
    "                    if not specie_in_context and not trait_in_context:\n",
    "                        if verbose and VERBOSE_LEVEL >= 3:\n",
    "                            print(f\"\\t\\tNo Traits in Variability Token '{token}' Context\")\n",
    "                        continue\n",
    "\n",
    "                    deduct_points = False\n",
    "                    t_variables.extend(trait_in_context)\n",
    "                    s_variables.extend(specie_in_context)\n",
    "\n",
    "            t_variables = list(set(t_variables))\n",
    "            s_variables = list(set(s_variables))\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tt_variables: {t_variables}\")\n",
    "                print(f\"\\ts_variables: {s_variables}\")\n",
    "            \n",
    "            if t_variables or s_variables:\n",
    "                trait_variation_points_i += 0.25\n",
    "            else:\n",
    "                trait_variation_points_i += 0.15\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tTrait Variation Points for I: {trait_variation_points_i}\")\n",
    "\n",
    "            assert trait_variation_points_i <= 0.5\n",
    "\n",
    "            if trait_variation_points_i <= 0.49:\n",
    "                print(f\"Nope\")\n",
    "                continue\n",
    "\n",
    "            for j in range(i, num_sentences):\n",
    "                sent_j = sentences[j]\n",
    "                sent_j_tokens = set([token for token in sent_j])\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tSentence J: {sent_j}\")\n",
    "\n",
    "                sent_j_cause_tokens = sent_j_tokens.intersection(self.cause.tokens)\n",
    "                sent_j_change_tokens = sent_j_tokens.intersection(self.change.tokens)\n",
    "                sent_j_species_tokens = sent_j_tokens.intersection(self.species.span_starts)\n",
    "                sent_j_trait_tokens = sent_j_tokens.intersection(self.trait.tokens)\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tSentence J Cause Tokens: {sent_j_cause_tokens}\")\n",
    "                    print(f\"\\tSentence J Change Tokens: {sent_j_change_tokens}\")\n",
    "                    print(f\"\\tSentence J Species Tokens: {sent_j_species_tokens}\")\n",
    "                    print(f\"\\tSentence J Trait Tokens: {sent_j_trait_tokens}\")\n",
    "                \n",
    "                if not sent_j_species_tokens or (not sent_j_cause_tokens and not sent_j_change_tokens):\n",
    "                    if verbose and VERBOSE_LEVEL >= 2:\n",
    "                        print(f\"\\tUnsatisfied Conditions for Sentence J\")\n",
    "                    continue\n",
    "\n",
    "                trait_variation_points_j = 0\n",
    "                \n",
    "                if (not sent_j_species_tokens and not sent_j_trait_tokens) or (not t_variables and not s_variables):\n",
    "                    trait_variation_points_j += 0.25\n",
    "                elif i != j:\n",
    "                    # Check if Variable Referenced Again via Types\n",
    "                    variable_types = set(self.flatten([self.trait.mapped_token_data[token][\"types\"] for token in t_variables]))\n",
    "                    sent_j_trait_types = set(self.flatten([self.trait.mapped_token_data[token][\"types\"] for token in sent_j_trait_tokens]))\n",
    "\n",
    "                    if verbose and VERBOSE_LEVEL >= 2:\n",
    "                        print(f\"\\tVariable Types: {variable_types}\")\n",
    "                        print(f\"\\tTrait Types in Sentence J: {sent_j_trait_types}\")\n",
    "                        \n",
    "                    # Check if Variable Referenced Again via Literals\n",
    "                    variable_strings = set([token.lower_ for token in t_variables])\n",
    "                    sent_j_trait_strings = set([token.lower_ for token in sent_j_trait_tokens])\n",
    "\n",
    "                    if verbose and VERBOSE_LEVEL >= 2:\n",
    "                        print(f\"\\tVariable Trait (as Strings): {variable_strings}\")\n",
    "                        print(f\"\\tTrait (as Strings) in Sentence J: {sent_j_trait_strings}\")\n",
    "                    \n",
    "                    variable_referenced = bool(variable_types & sent_j_trait_types) or bool(variable_strings & sent_j_trait_strings)\n",
    "\n",
    "                    sent_j_species_spans = [self.sp_doc[token.i:token.i+1] for token in sent_j_species_tokens]\n",
    "                    s_variables_spans = [self.sp_doc[token.i:token.i+1] for token in s_variables]\n",
    "\n",
    "                    overlap = [self.species.find_same_species(sent_j_species_spans, species) for species in s_variables_spans]\n",
    "                    s_variable_referenced = any(overlap)\n",
    "\n",
    "                    if verbose and VERBOSE_LEVEL >= 2:\n",
    "                        print(f\"\\tT Variable Referenced? {variable_referenced}\")\n",
    "                        print(f\"\\tS Variable Referenced? {s_variable_referenced}\")\n",
    "                    \n",
    "                    if variable_referenced or s_variable_referenced:\n",
    "                        trait_variation_points_j += 0.50\n",
    "                    else:\n",
    "                        trait_variation_points_j += 0.25\n",
    "                elif i == j:\n",
    "                    trait_variation_points_j += 0.25\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tTrait Variation Points for J: {trait_variation_points_j}\")\n",
    "\n",
    "                assert trait_variation_points_j <= 0.5\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\ti: {i}\")\n",
    "                    print(f\"\\tj: {j}\")\n",
    "\n",
    "                # Scale by Distance In-Between\n",
    "                # if i == j:\n",
    "                #     j_scale = 0.5\n",
    "                # else:\n",
    "                #     j_scale = (1 - ((j - i) / (num_sentences - 1)))\n",
    "                \n",
    "                # assert 0 <= j_scale <= 1\n",
    "\n",
    "                # if verbose and VERBOSE_LEVEL >= 2:\n",
    "                #     print(f\"\\tScale for Sentence J Points: {j_scale}\")\n",
    "                    \n",
    "                # trait_variation_points = trait_variation_points_i + j_scale * trait_variation_points_j\n",
    "                trait_variation_points = trait_variation_points_i + trait_variation_points_j\n",
    "\n",
    "                # Scale by Distance from Top\n",
    "                # if num_sentences == 1:\n",
    "                #     i_scale = 1\n",
    "                # else:\n",
    "                #     i_scale = (1 - i/(num_sentences - 1))\n",
    "                    \n",
    "                # assert 0 <= i_scale <= 1\n",
    "\n",
    "                # if verbose and VERBOSE_LEVEL >= 2:\n",
    "                #     print(f\"\\tScale for Sentence I Points: {i_scale}\")\n",
    "                \n",
    "                # trait_variation_points *= i_scale\n",
    "\n",
    "                if deduct_points:\n",
    "                    trait_variation_points *= 0.6375\n",
    "                \n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tTrait Variation Points: {trait_variation_points}\")\n",
    "                \n",
    "                max_trait_variation_points = max(max_trait_variation_points, trait_variation_points)\n",
    "\n",
    "                if max_trait_variation_points >= 1:\n",
    "                    return 1\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Max Trait Variation Points: {max_trait_variation_points}\")\n",
    "            \n",
    "        return max_trait_variation_points\n",
    "\n",
    "\n",
    "        \n",
    "    def score(self, verbose=False):\n",
    "        NUM_CATEGORIES = 6\n",
    "\n",
    "        TRAIT = 0\n",
    "        SPECIES = 1\n",
    "        EXPERIMENT = 2\n",
    "        INTERACTION = 3\n",
    "        NOT_TOPIC = 4\n",
    "        TRAIT_VARIATION = 5\n",
    "\n",
    "        # Max # of Points of Category per Sentence (MPC)\n",
    "        # A category can collect points from each sentence. However,\n",
    "        # there's a maximum number of points it can collect. This is\n",
    "        # determined by the MPC.\n",
    "        MPC = [1] * NUM_CATEGORIES\n",
    "    \n",
    "        # Points per Instance of Category (PIC)\n",
    "        # Each token is evaluated to check whether a category\n",
    "        # can be given points. The number of points given, if\n",
    "        # the token is determined to be satisfactory, is the PIC.\n",
    "        # The PIC is less than or equal to the MPC for the corresponding\n",
    "        # category. The idea behind the PIC and MPC is similar to how\n",
    "        # sets work in tennis: you're not immediately awarded the full points\n",
    "        # for the set (MPC) if your opponent fails to return the ball,\n",
    "        # instead you're given a smaller # of points (PIC) that allow you to\n",
    "        # incrementally win the set (category).\n",
    "        PIC = [0] * NUM_CATEGORIES\n",
    "        PIC[TRAIT] = MPC[TRAIT]\n",
    "        PIC[SPECIES] = MPC[SPECIES]/2.0\n",
    "        PIC[EXPERIMENT] = MPC[EXPERIMENT]*0.625\n",
    "        PIC[INTERACTION] = MPC[INTERACTION]/3.0\n",
    "        PIC[NOT_TOPIC] = MPC[NOT_TOPIC]\n",
    "\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            assert 0 <= PIC[i] <= MPC[i]\n",
    "\n",
    "        # Category Weights (CW)\n",
    "        # It may be helpful to weigh a certain category's fraction of total points\n",
    "        # more or less than another's. Thus, at the end, we'll take a\n",
    "        # weighted average of the category's FTP. The weights must add up to 1.\n",
    "        CW = [0] * NUM_CATEGORIES\n",
    "        CW[TRAIT] = 0.3\n",
    "        CW[SPECIES] = 0.1\n",
    "        CW[EXPERIMENT] = 0.1\n",
    "        CW[INTERACTION] = 0.1\n",
    "        CW[NOT_TOPIC] = 0.1\n",
    "        CW[TRAIT_VARIATION] = 0.3\n",
    "\n",
    "        assert round(np.sum(CW)) == 1\n",
    "\n",
    "        # Leniency\n",
    "        # There are certain categories that aren't going to be as frequent as others.\n",
    "        # For example, the trait category. You could try and decrease the influence\n",
    "        # of said category by lowering its MPC and/or increasing the PIC (so that it's\n",
    "        # easier to achieve the FTP). However, this could make it harder to meaningfully\n",
    "        # represent the category. The idea of leniency is to remove (some) sentences that had 0\n",
    "        # points from the scoring. This increases the FTP as, for example, instead of comparing\n",
    "        # 0.5 points to a total of 2.5 points, you can compare 0.5 to 2.0 points, and so on.\n",
    "        # A leniency of 1 means that all sentences that received 0 points will be removed from\n",
    "        # the scoring. A leniency of 0 means that all the sentences are included in the scoring.\n",
    "        LEN = [0] * NUM_CATEGORIES\n",
    "        LEN[TRAIT] = 0\n",
    "        LEN[SPECIES] = 0.5\n",
    "        LEN[EXPERIMENT] = 0.5\n",
    "        LEN[INTERACTION] = 0\n",
    "        LEN[NOT_TOPIC] = 0\n",
    "\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            assert 0 <= LEN[i] <= 1\n",
    "\n",
    "        # Banned Sentences\n",
    "        # Not allowed to benefit from leniency\n",
    "        banned = [[0] * len(list(self.sp_doc.sents)) for _ in range(NUM_CATEGORIES)]\n",
    "\n",
    "        # Points\n",
    "        points = [0] * NUM_CATEGORIES\n",
    "        num_zero_pt_sents = [0] * NUM_CATEGORIES\n",
    "        seen_species = {}\n",
    "        seen_exp = set()\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"Extracted Information\")\n",
    "            print(f\"Cause Tokens: {self.cause.tokens}\")\n",
    "            print(f\"Change Tokens: {self.change.tokens}\")\n",
    "            print(f\"Trait Tokens: {self.trait.tokens}\")\n",
    "            print(f\"Species Tokens: {self.species.tokens}\")\n",
    "            print(f\"Experiment Tokens: {self.experiment.tokens}\")\n",
    "            print(f\"Not-Experiment Tokens: {self.not_experiment.tokens}\")\n",
    "            print(f\"Not-Topic Tokens: {self.not_topic.tokens}\")\n",
    "            print(f\"Variability Tokens: {self.variability.tokens}\")\n",
    "            print(f\"Test Tokens: {self.test.tokens}\")\n",
    "            \n",
    "        for sent_i, sent in enumerate(self.sp_doc.sents):\n",
    "            # Current Points in Sentence\n",
    "            curr_points = [0] * NUM_CATEGORIES\n",
    "\n",
    "            # Sentence Local Info\n",
    "            sent_tokens = [token for token in sent]\n",
    "            sent_cause_tokens = set(sent_tokens).intersection(self.cause.tokens)\n",
    "            sent_change_tokens = set(sent_tokens).intersection(self.change.tokens)\n",
    "            sent_seen_species = {}\n",
    "            sent_num_unique_species = 0\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tSentence: {sent}\")\n",
    "                print(f\"\\tSentence Cause Tokens: {sent_cause_tokens}\")\n",
    "                print(f\"\\tSentence Change Tokens: {sent_change_tokens}\")\n",
    "\n",
    "            \n",
    "            for token in sent_tokens:\n",
    "                # If each category has reached their maximum number of points,\n",
    "                # we can end the loop early.\n",
    "                all_maxed = True\n",
    "                for i in range(NUM_CATEGORIES):\n",
    "                    if i == TRAIT_VARIATION:\n",
    "                        continue\n",
    "                    if curr_points[i] < MPC[i]:\n",
    "                        all_maxed = False\n",
    "\n",
    "                if all_maxed:\n",
    "                    break\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tToken in Sentence: {token}\")\n",
    "                \n",
    "                # Not Topic Points\n",
    "                # if curr_points[NOT_TOPIC] < MPC[NOT_TOPIC]:\n",
    "                #     if token in self.not_topic.tokens:\n",
    "                #         curr_points[NOT_TOPIC] += PIC[NOT_TOPIC]\n",
    "\n",
    "                #         if verbose and VERBOSE_LEVEL >= 3:\n",
    "                #             print(f\"\\t\\t+ Points for Not-Topic\")\n",
    "\n",
    "                        \n",
    "                # Trait Points\n",
    "                # if curr_points[TRAIT] < MPC[TRAIT]:\n",
    "                #     if token in self.trait.tokens:\n",
    "                #         scale = self.valid_trait_token(token, sent_cause_tokens, sent_change_tokens, verbose=verbose)\n",
    "                #         curr_points[TRAIT] += scale * PIC[TRAIT]\n",
    "\n",
    "                #         if verbose and VERBOSE_LEVEL >= 3 and scale:\n",
    "                #             print(f\"\\t\\t+ Points for Trait\")\n",
    "\n",
    "                        \n",
    "                # Not Experiment Points\n",
    "                if token in self.not_experiment.tokens:\n",
    "                    curr_points[EXPERIMENT] -= 2 * PIC[EXPERIMENT]\n",
    "                    banned[EXPERIMENT][sent_i] = 1\n",
    "                    \n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\t- Points for Experiment\")\n",
    "\n",
    "                \n",
    "                # Experiment Points\n",
    "                elif curr_points[EXPERIMENT] < MPC[EXPERIMENT]:\n",
    "                    if token in self.experiment.tokens:\n",
    "                        curr_points[EXPERIMENT] += PIC[EXPERIMENT]\n",
    "\n",
    "                        if verbose and VERBOSE_LEVEL >= 3:\n",
    "                            print(f\"\\t\\t+ Points for Experiment\")\n",
    "\n",
    "                        # seen_exp.add(token.lower_)\n",
    "                        # seen_exp.add(token.lemma_)\n",
    "\n",
    "                        \n",
    "                # Species and/or Interaction Points\n",
    "                # if token in self.species.span_starts:\n",
    "                #     # Update Species\n",
    "                #     seen_species, sent_seen_species, seen_in_sent, sent_num_unique_species, ref = self.update_seen_species(\n",
    "                #         token, \n",
    "                #         seen_species, \n",
    "                #         sent_seen_species, \n",
    "                #         sent_num_unique_species,\n",
    "                #         verbose=verbose\n",
    "                #     )\n",
    "\n",
    "                #     print(f\"Seen Species: {seen_species}\")\n",
    "                #     print(f\"Sent Seen Species: {sent_seen_species}\")\n",
    "                #     print(f\"Seen in Sent: {seen_in_sent}\")\n",
    "                #     print(f\"Sent Num Unique Species: {sent_num_unique_species}\")\n",
    "                #     print(f\"Ref: {ref}\")\n",
    "                    \n",
    "                #     if seen_in_sent:\n",
    "                #         print(f\"\\t\\tAlready Seen Species '{token}' in Sentence (No Interaction)\")\n",
    "                #     else:\n",
    "                #         # Interaction Points\n",
    "                #         if curr_points[INTERACTION] < MPC[INTERACTION]:\n",
    "                #             if sent_num_unique_species == 2:\n",
    "                #                 curr_points[INTERACTION] = 2.0 * PIC[INTERACTION]\n",
    "    \n",
    "                #                 if verbose and VERBOSE_LEVEL >= 3:\n",
    "                #                     print(f\"\\t\\t+ Points for Interaction\")\n",
    "                            \n",
    "                #             elif sent_num_unique_species > 2:\n",
    "                #                 curr_points[INTERACTION] += PIC[INTERACTION]\n",
    "    \n",
    "                #                 if verbose and VERBOSE_LEVEL >= 3:\n",
    "                #                     print(f\"\\t\\t+ Points for Interaction\")\n",
    "\n",
    "\n",
    "                #     if sent_seen_species[ref][\"points\"] <= 0:\n",
    "                #         # Species Points\n",
    "                #         if curr_points[SPECIES] < MPC[SPECIES]:\n",
    "                #             scale = self.valid_species_token(token, sent_cause_tokens, sent_change_tokens)\n",
    "                #             curr_points[SPECIES] += scale * PIC[SPECIES]\n",
    "    \n",
    "                #             if verbose and scale and VERBOSE_LEVEL >= 3:\n",
    "                #                 print(f\"\\t\\t+ Points for Species\")\n",
    "\n",
    "                #             if scale:\n",
    "                #                 sent_seen_species[ref][\"points\"] += scale * PIC[SPECIES]\n",
    "\n",
    "            \n",
    "            # Add Sentence Points to Total Points\n",
    "            for i in range(NUM_CATEGORIES):\n",
    "                if i == EXPERIMENT:\n",
    "                    print(f\"Experiment Points for Sentence: {curr_points[i]}\")\n",
    "                \n",
    "                is_banned = banned[i][sent_i]\n",
    "                if is_banned and i == EXPERIMENT:\n",
    "                    print(f\"Sentence is Banned for Experiment\")\n",
    "                \n",
    "                if curr_points[i] <= 0 and not is_banned:\n",
    "                    num_zero_pt_sents[i] += 1\n",
    "                \n",
    "                if not is_banned:\n",
    "                    points[i] += max(0, min(curr_points[i], MPC[i]))\n",
    "\n",
    "        \n",
    "        # Trait Variation Points\n",
    "        points[TRAIT_VARIATION] = self.valid_trait_variation(verbose=verbose)\n",
    "\n",
    "        return points[TRAIT_VARIATION]\n",
    "\n",
    "        # Bins\n",
    "        # bins = []\n",
    "        # for i in range(NUM_CATEGORIES):\n",
    "        #     bins.append([-math.inf, math.inf])\n",
    "        # bins[SPECIES] = [-math.inf, 0.33, 0.66, math.inf]\n",
    "        # bins[EXPERIMENT] = [-math.inf, 0.7, math.inf]\n",
    "        # print(f\"Bins: {bins}\")\n",
    "        \n",
    "        # # Calculating Score            \n",
    "        # NUM_SENTENCES = len(list(self.sp_doc.sents))\n",
    "        # score = 0\n",
    "\n",
    "        # for i in range(NUM_CATEGORIES):\n",
    "        #     if i != TRAIT_VARIATION:\n",
    "        #         num_non_zero_pt_sents = NUM_SENTENCES - num_zero_pt_sents[i]\n",
    "        #         banned_tax = 0\n",
    "        #         for b in banned[i]:\n",
    "        #             if b:\n",
    "        #                 banned_tax += 1\n",
    "        #         print(f\"Banned Tax: {banned_tax}\")\n",
    "        #         lenient_num_sentences = max(num_non_zero_pt_sents, (1 - LEN[i]) * (NUM_SENTENCES) + banned_tax)\n",
    "    \n",
    "        #         # Calculating FTP\n",
    "        #         points[i] = points[i] / (MPC[i] * lenient_num_sentences)\n",
    "    \n",
    "        #         # Take the Inverse for Not-Topic\n",
    "        #         if i == NOT_TOPIC:\n",
    "        #             points[i] = 1 - points[i]\n",
    "\n",
    "        #     # if i == SPECIES:\n",
    "        #     #     species_points =  0\n",
    "        #     #     species_binned_points = 0\n",
    "        \n",
    "        #     #     # Add onto Score\n",
    "        #     #     binned_points = points[SPECIES]\n",
    "        #     #     start = len(bins[SPECIES]) - 2\n",
    "        #     #     print(start)\n",
    "        #     #     for b in range(start, -1, -1):\n",
    "        #     #         print(bins[SPECIES][b], \"<=\", points[SPECIES], \"<=\", bins[SPECIES][b+1])\n",
    "        #     #         if bins[SPECIES][b] <= points[SPECIES] <= bins[SPECIES][b+1]:\n",
    "        #     #             print(binned_points)\n",
    "        #     #             binned_points = bins[SPECIES][b]\n",
    "        #     #             print(binned_points)\n",
    "        #     #             break\n",
    "        \n",
    "        #     #     species_binned_points = max(0, min(binned_points, 1))\n",
    "        #     #     species_points = max(0, min(points[SPECIES], 1))\n",
    "\n",
    "        #     if i == EXPERIMENT:\n",
    "        #         exp_points =  0\n",
    "        #         exp_binned_points = 0\n",
    "        \n",
    "        #         # Add onto Score\n",
    "        #         binned_points = points[EXPERIMENT]\n",
    "        #         start = len(bins[EXPERIMENT]) - 2\n",
    "        #         print(start)\n",
    "        #         for b in range(start, -1, -1):\n",
    "        #             print(bins[EXPERIMENT][b], \"<=\", points[EXPERIMENT], \"<=\", bins[EXPERIMENT][b+1])\n",
    "        #             if bins[EXPERIMENT][b] <= points[EXPERIMENT] <= bins[EXPERIMENT][b+1]:\n",
    "        #                 print(binned_points)\n",
    "        #                 binned_points = bins[EXPERIMENT][b]\n",
    "        #                 print(binned_points)\n",
    "        #                 break\n",
    "        \n",
    "        #         exp_binned_points = max(0, min(binned_points, 1))\n",
    "        #         exp_points = max(0, min(points[EXPERIMENT], 1))\n",
    "        #             # score += max(0, min(points[i], 1)) * CW[i]\n",
    "\n",
    "        # # Enforcing 3 or More Species            \n",
    "        # # if len(seen_species) < 3:\n",
    "        # #     return 0, 0\n",
    "\n",
    "        # return exp_points, exp_binned_points\n",
    "        # assert 0.0 <= score <= 1.0\n",
    "\n",
    "        # if verbose and VERBOSE_LEVEL >= 1:\n",
    "        #     print(f\"Score, Points: {score}, {points}\")\n",
    "    \n",
    "        # return score, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfb67900-25f1-4d1a-b51f-74d5782b459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(name, save_output=False, version=\"\", df=None, gave_df=None):\n",
    "    # Redirect Print Statements\n",
    "    # https://stackoverflow.com/questions/7152762/how-to-redirect-print-output-to-a-file\n",
    "    if save_output:\n",
    "        initial_stdout = sys.stdout\n",
    "        f = open(f'./Print{name}{\"\" if not version else f\"-{version}\"}.txt', 'w')\n",
    "        sys.stdout = f\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "\n",
    "    # Load Dataset\n",
    "    if not gave_df:\n",
    "        data = load_preprocessed_dataset(name)\n",
    "    else:\n",
    "        data = df\n",
    "\n",
    "    # We'll be running the points algorithm\n",
    "    # on the abstracts of these papers.\n",
    "    texts = list(data['Abstract'].to_numpy())\n",
    "    \n",
    "    # The scores for each paper will be stored here,\n",
    "    # we'll set this as a column of the dataframe.\n",
    "    # scores = []\n",
    "    points = []\n",
    "    # binned_points = []\n",
    "    # trait_points = []\n",
    "    # species_points = []\n",
    "    # experiment_points = []\n",
    "    # interaction_points = []\n",
    "    # neg_topic_points = []\n",
    "    # trait_var_points = []\n",
    "\n",
    "    \n",
    "    \n",
    "    # Scan and Evaluate Documents\n",
    "    main = Main()\n",
    "    for i, doc in enumerate(main.sp_nlp.pipe(texts)):\n",
    "        print(f\"{i+1}/{data.shape[0]} - {data.iloc[i]['Title']}\\n\")\n",
    "        main.update_doc(doc, verbose=save_output)\n",
    "\n",
    "        # Empty string literals cause errors, so it's\n",
    "        # being handled here.\n",
    "        if not main.sp_doc or not main.species.tn_doc:\n",
    "            # scores.append(0)\n",
    "            points.append(0)\n",
    "            # binned_points.append(0)\n",
    "        else:\n",
    "            _points = main.score(verbose=save_output)\n",
    "            # print(_points, _binned_points)\n",
    "\n",
    "            points.append(_points)\n",
    "            # binned_points.append(_binned_points)\n",
    "            \n",
    "            # scores.append(score)\n",
    "            # points.append(_points)\n",
    "            # trait_points.append(_points[0])\n",
    "            # species_points.append(_points[1])\n",
    "            # experiment_points.append(_points[2])\n",
    "            # interaction_points.append(_points[3])\n",
    "            # neg_topic_points.append(_points[4])\n",
    "            # trait_var_points.append(_points[5])\n",
    "\n",
    "        # if not save_output:\n",
    "        #     clear_output(wait=True)\n",
    "\n",
    "    # Reset Standard Output\n",
    "    if save_output:\n",
    "        sys.stdout = initial_stdout\n",
    "        f.close()\n",
    "\n",
    "    data[\"Trait Variation Points\"] = points\n",
    "    # data[\"Experiment Binned Points\"] = binned_points\n",
    "    \n",
    "    # data[\"Score\"] = scores\n",
    "    # data[\"Trait Points\"] = trait_points\n",
    "    # data[\"Species Points\"] = species_points\n",
    "    # data[\"Experiment Points\"] = experiment_points\n",
    "    # data[\"Interaction Points\"] = interaction_points\n",
    "    # data[\"Topic Points\"] = neg_topic_points\n",
    "    # data[\"Trait Variation Points\"] = trait_var_points\n",
    "    \n",
    "    data.sort_values(by='Trait Variation Points', ascending=False, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7571bf1c-4aba-4cf5-bf6f-b0e507f03af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def store_scored_dataset(dataset, name, version=''):\n",
    "#     filename = f\"Scored{name}{'' if not version else f'-{version}'}\"\n",
    "#     dataset.to_pickle(f\"{filename}.pkl\")\n",
    "#     dataset.to_csv(f\"{filename}.csv\", index=False)\n",
    "#     dataset.to_excel(f\"{filename}.xlsx\", index=None, header=True)\n",
    "\n",
    "# scored_data = score_dataset(\"Baseline-1\", save_output=False, version='')\n",
    "# store_scored_dataset(scored_data, \"Baseline-3.1\", version='6')\n",
    "\n",
    "# scored_data = score_dataset(\"Examples\", save_output=False, version='')\n",
    "# store_scored_dataset(scored_data, \"Examples-3.1\", version='6')\n",
    "\n",
    "# # scored_data = score_dataset(\"DFiltered\", save_output=False, version='')\n",
    "# # store_scored_dataset(scored_data, \"DFiltered-3.1\", version='4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21a15564-0d3e-4952-baba-40b5dee8a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Week 16/Datasets/Baseline-1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a9dd2f7-7073-476a-8f62-93037fc59afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intraguild predation of Neoseiulus cucumeris Oudemans (Phytoseiidae) by soil-dwelling predators, Dalotia coriaria Kraatz (Staphylinidae) may limit the utility of open rearing systems in greenhouse thrips management programs. We determined the rate of D. coriaria invasion of N. cucumeris breeder material presented in piles or sachets, bran piles (without mites), and sawdust piles. We also observed mite dispersal from breeder piles and sachets when D. coriaria were not present. Dalotia coriaria invaded breeder and bran piles at higher rates than sawdust piles and sachets. Furthermore, proportions of N. cucumeris in sachets were six- to eight-fold higher compared with breeder piles. When D. coriaria were absent, N. cucumeris dispersed from breeder piles and sachets for up to seven weeks. In earlier weeks, more N. cucumeris dispersed from breeder piles compared with sachets, and in later weeks more N. cucumeris dispersed from sachets compared with breeder piles. Sachets protected N. cucumeris from intraguild predation by D. coriaria resulting in higher populations of mites. Therefore, sachets should be used in greenhouse biocontrol programs that also release D. coriaria. Furthermore, breeder piles that provide \"quick-releases\" or sachets that provide \"slow-releases\" of mites should be considered when incorporating N. cucumeris into greenhouse thrips management programs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/11/2025 22:35:06 - INFO - \t missing_keys: []\n",
      "08/11/2025 22:35:06 - INFO - \t unexpected_keys: []\n",
      "08/11/2025 22:35:06 - INFO - \t mismatched_keys: []\n",
      "08/11/2025 22:35:06 - INFO - \t error_msgs: []\n",
      "08/11/2025 22:35:06 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    }
   ],
   "source": [
    "df.loc[df['Title'].str.contains('Slow-Release')]\n",
    "abstract = df.iloc[24].Abstract\n",
    "print(abstract)\n",
    "main = Main()\n",
    "main.update_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6775811-4e94-4075-8f93-7b7acb8834bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Points for Sentence: 0\n",
      "Experiment Points for Sentence: 0\n",
      "Experiment Points for Sentence: 0.625\n",
      "Experiment Points for Sentence: 0\n",
      "Experiment Points for Sentence: 0.625\n",
      "Experiment Points for Sentence: 0\n",
      "Experiment Points for Sentence: 1.25\n",
      "Experiment Points for Sentence: 0.625\n",
      "Experiment Points for Sentence: 0\n",
      "Experiment Points for Sentence: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Nope\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Nope\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda71f8-1fcc-48df-bb6d-48fe080f1e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5542e043-1c30-4472-9c0e-c1b80330f71f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scored_data = score_dataset(name=\"Hello\", save_output=False, version='', df=df, gave_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e904c70c-ce3a-45c7-a743-369c5396f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scored_data.to_excel(\"TRAIT-VARIATION-3.xlsx\", index=False, header=True)\n",
    "\n",
    "# SPECIES\n",
    "# 2 max one for each sentence\n",
    "# 3 max only two for each sentence\n",
    "# 4 fixed span starts for species\n",
    "# 5 not much changed, added .5 leniency\n",
    "# 6 fixed bug (didnt sort arr) that compromised 4\n",
    "# 7 fixed oversight (species not given points, couldnt be used again in sentence)\n",
    "# 8 added \"adults\", added two more words to cause/change\n",
    "# 9 alternate names have more strict req.\n",
    "# 10 require the verbs to be vbd (removed)\n",
    "\n",
    "# EXP\n",
    "# 1, it doesnt do a good job of separating review from experiment tbh. im going to require verbs\n",
    "# 2. it's unfortunate that good words are unrecognized, but the more review papers are at the bottom\n",
    "# so it is what it is ig, im going to make the negative experiment words more costly (added banned)\n",
    "# 3, not making much of a difference (the banned) so i am goinng to incorporate it in the leniency, also added leniency not sure which version\n",
    "# 6 added bins\n",
    "# 7 added verb noun adj for experiment\n",
    "# 8 tweaked bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d02ef85a-b25a-4987-8a02-5db17414f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_traits(traits):\n",
    "#     merged = {}\n",
    "    \n",
    "#     for trait in traits:\n",
    "#         found_trait = False\n",
    "#         for m_trait in merged:\n",
    "#             # print(trait.i, m_trait.i)\n",
    "            \n",
    "#             if main.has_same_base_nouns(trait, m_trait) or main.is_same_text(trait, m_trait):\n",
    "#                 merged[m_trait] += 1\n",
    "#                 found_trait = True\n",
    "#                 break\n",
    "            \n",
    "#             trait_types = []\n",
    "#             for thing in trait:\n",
    "#                 things = main.trait.mapped_token_data.get(thing)\n",
    "#                 if things:\n",
    "#                     trait_types.extend(things[\"types\"])\n",
    "                \n",
    "#             m_trait_types = []\n",
    "#             for thing in m_trait:\n",
    "#                 things = main.trait.mapped_token_data.get(thing)\n",
    "#                 if things:\n",
    "#                     m_trait_types.extend(things[\"types\"])\n",
    "\n",
    "#             # print(trait_types, m_trait_types)\n",
    "\n",
    "#             if not trait_types or not m_trait_types:\n",
    "#                 continue\n",
    "\n",
    "#             type_intersection = set(trait_types).intersection(m_trait_types)\n",
    "#             if type_intersection.intersection(['Food']):\n",
    "#                 merged[m_trait] += 1\n",
    "#                 found_trait = True\n",
    "#                 break\n",
    "            \n",
    "#         if not found_trait:\n",
    "#             merged[trait] = 1\n",
    "\n",
    "#     return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73367321-7475-40a3-aa76-3f1b35a7ed9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m trait_points \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m traits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m      6\u001b[0m     title \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[i]\u001b[38;5;241m.\u001b[39mTitle\n\u001b[0;32m      7\u001b[0m     abstract \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[i]\u001b[38;5;241m.\u001b[39mAbstract\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "trait_points = []\n",
    "traits = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    title = df.loc[i].Title\n",
    "    abstract = df.loc[i].Abstract\n",
    "    print(title)\n",
    "    print(abstract)\n",
    "\n",
    "    main.update_text(abstract)\n",
    "    merged_traits = merge_traits([main.sp_doc[token.i:token.i+1] for token in main.trait.tokens])\n",
    "\n",
    "    \n",
    "\n",
    "    if not merged_traits:\n",
    "        pts = 0\n",
    "    else:\n",
    "        pts = 0.5\n",
    "        for k, v in merged_traits.items():\n",
    "            if v >= 2:\n",
    "                pts = 1\n",
    "    \n",
    "    trait_points.append(pts)\n",
    "\n",
    "    merged_traits = {k.text: v for k,v in merged_traits.items()}\n",
    "    traits.append(json.dumps(merged_traits))\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "\n",
    "df['Traits'] = traits\n",
    "df['Trait Points'] = trait_points\n",
    "df.to_excel(\"Trait-Points-1.xlsx\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c763d50b-a69e-478d-8e62-f1c75412ea54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
