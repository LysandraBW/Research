{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0608000-6ca5-4dac-a7f8-21d1e6250184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Data\n",
    "# This part is similar to the first part\n",
    "# of this process.\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# The output files have a base of the string below.\n",
    "BASE_FILE_NAME = \"FILTERED_OUT\"\n",
    "MASTER_BASE_FILE_NAME = \"OUT\"\n",
    "\n",
    "# These are the fields that are stored in the CSV\n",
    "# file. Due to the overlap, I should define these constants\n",
    "# elsewhere, in a shared file, but they will work here for now.\n",
    "DOI = \"doi\"\n",
    "TEXT = \"text\"\n",
    "TITLE = \"title\"\n",
    "\n",
    "# We're storing a subset of papers, so we use the number of\n",
    "# that set in order to maintain that reference. I am saying\n",
    "# not a lot with a lot of words.\n",
    "def write_papers(number, papers):\n",
    "    data_file_name = f\"{BASE_FILE_NAME}_DATA_{number}.csv\"\n",
    "    dump_file_name = f\"{BASE_FILE_NAME}_DUMP_{number}.csv\"\n",
    "    \n",
    "    with open(data_file_name, 'w', newline='', encoding='utf-8-sig') as file:\n",
    "        fieldnames = [TITLE, DOI, TEXT]\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for paper in papers.values():\n",
    "            writer.writerow(paper)\n",
    "    \n",
    "    with open(dump_file_name, 'w', newline='', encoding='utf-8') as file:\n",
    "        json.dump(papers, file)\n",
    "\n",
    "# This dictionary will store all the papers\n",
    "# that we find through the various sources.\n",
    "papers = {}\n",
    "# To make sure that we're not doing anything\n",
    "# stupid, there'll be a function that adds the papers.\n",
    "# Use the function.\n",
    "def add_paper(title, doi, text):\n",
    "    number_papers = len(papers.keys())\n",
    "    papers[number_papers] = {TITLE: title, DOI: doi, TEXT: text}\n",
    "    number_papers += 1\n",
    "\n",
    "# We read in a CSV file containing a list of papers to be filtered.\n",
    "# The number indicates the number in the filename.\n",
    "def read_papers(number):\n",
    "    # This is taking too much work for me,\n",
    "    # someone who is trying to get somewhere.\n",
    "    # csv.field_size_limit(sys.maxsize//10)\n",
    "    # papers = []\n",
    "    file_name = f\"{MASTER_BASE_FILE_NAME}_DATA_{number}.csv\"\n",
    "    # with open(file_name, 'r', encoding='utf-8') as file:\n",
    "    #     csv_reader = csv.reader(file)\n",
    "    #     header = next(csv_reader)\n",
    "    #     for row in csv_reader:\n",
    "    #         papers.append(row)\n",
    "    df = pd.read_csv(file_name)\n",
    "    return df\n",
    "    # return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc64ba5-81ee-4e9f-9d01-452ab049d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's no definite names for these patterns as I do not know what\n",
    "# to call them. These patterns are used to extract possessive\n",
    "# relationships from a sentence. I also could not find better names for\n",
    "# the two variables below.\n",
    "OWNER = \"owner\"\n",
    "OWNED = \"owned\"\n",
    "\n",
    "pattern_1 = [\n",
    "    {\n",
    "        \"RIGHT_ID\": OWNED,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"POS\": {\n",
    "                \"IN\": [\"NOUN\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": OWNED,\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": OWNER,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"poss\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "pattern_2 = [\n",
    "     {\n",
    "        \"RIGHT_ID\": OWNED,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"POS\": {\n",
    "                \"IN\": [\"NOUN\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": OWNED,\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"adp\",\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"prep\",\n",
    "            \"POS\": {\n",
    "                \"IN\": [\"ADP\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"adp\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": OWNER,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"pobj\",\n",
    "            \"POS\": {\n",
    "                \"IN\": [\"NOUN\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "pattern_3 = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"verb\",\n",
    "        \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"verb\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": OWNER,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"nsubj\",\n",
    "            \"POS\": {\"IN\": [\"PRON\"]}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"verb\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": OWNED,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"dobj\",\n",
    "            \"POS\": {\"IN\": [\"NOUN\"]}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "pattern_4 = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"verb\",\n",
    "        \"RIGHT_ATTRS\": {\"POS\": {\"IN\": [\"VERB\"]}}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"verb\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": OWNED,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"nsubj\",\n",
    "            \"POS\": {\"IN\": [\"NOUN\"]}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"verb\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"adp\",\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"prep\",\n",
    "            \"POS\": {\"IN\": [\"ADP\"]}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"adp\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": OWNER,\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"pobj\",\n",
    "            \"POS\": {\"IN\": [\"NOUN\"]}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "patterns = {\n",
    "    \"Pattern1\": pattern_1,\n",
    "    \"Pattern2\": pattern_2,\n",
    "    \"Pattern3\": pattern_3,\n",
    "    \"Pattern4\": pattern_4,\n",
    "}\n",
    "\n",
    "def dependency_matcher(sp_nlp):\n",
    "    matcher = DependencyMatcher(sp_nlp.vocab)\n",
    "    for pattern_id, pattern in patterns.items():\n",
    "        matcher.add(pattern_id, [pattern])\n",
    "    return matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80126d41-15ed-438c-8f74-ac9413b68ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import stanza\n",
    "import textacy\n",
    "from fastcoref import FCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher\n",
    "# !pip install https://github.com/nleguillarme/taxonerd/releases/download/v1.5.4/en_ner_eco_md-1.1.0.tar.gz\n",
    "!pip install https://github.com/nleguillarme/taxonerd/releases/download/v1.5.4/en_ner_eco_biobert-1.1.0.tar.gz\n",
    "# !pip install https://github.com/nleguillarme/taxonerd/releases/download/v1.5.4/en_ner_eco_md_weak-1.1.0.tar.gz\n",
    "# !pip install https://github.com/nleguillarme/taxonerd/releases/download/v1.5.4/en_ner_eco_biobert_weak-1.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db677d79-a082-45ee-bb26-bf2b27bd28ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_nlp = spacy.load(\"en_core_web_sm\")\n",
    "st_nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "fcoref = FCoref(enable_progress_bar=False)\n",
    "taxonerd = TaxoNERD()\n",
    "tn_nlp = taxonerd.load(model=\"en_ner_eco_biobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ed716ff-6ec0-45bd-8067-fa9a7c76f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_token(sp_doc):\n",
    "    index_to_token_map = {}\n",
    "    for token in sp_doc:\n",
    "        index_to_token_map[token.idx] = token\n",
    "    return index_to_token_map\n",
    "\n",
    "def index_to_cluster(fc_predictions):\n",
    "    index_to_cluster_map = {}\n",
    "    for prediction in fc_predictions:\n",
    "        clusters = prediction.get_clusters(as_strings=False)\n",
    "        for cluster in clusters:\n",
    "            for token in cluster:\n",
    "                index = token[0]\n",
    "                index_to_cluster_map[index] = cluster\n",
    "    return index_to_cluster_map\n",
    "\n",
    "def index_to_chunk(sp_doc):\n",
    "    index_to_chunk_map = {}\n",
    "    for noun_chunk in sp_doc.noun_chunks:\n",
    "        for token in noun_chunk:\n",
    "            index_to_chunk_map[token.idx] = noun_chunk\n",
    "    return index_to_chunk_map\n",
    "\n",
    "def index_to_what(sp_nlp, sp_doc, what_matches):\n",
    "    index_to_what_map = {}\n",
    "    for match_id, token_ids in what_matches:\n",
    "        pattern_id = sp_nlp.vocab.strings[match_id]\n",
    "        owner = None\n",
    "        owned = None\n",
    "        for i in range(len(token_ids)):\n",
    "            right_id = patterns[pattern_id][i][\"RIGHT_ID\"]\n",
    "            if right_id == OWNER:\n",
    "                owner = sp_doc[token_ids[i]]\n",
    "            if right_id == OWNED:\n",
    "                owned = sp_doc[token_ids[i]]\n",
    "            # print(f\"{right_id}: {sp_doc[token_ids[i]]}\")\n",
    "        if owner.idx not in index_to_what_map:\n",
    "            index_to_what_map[owner.idx] = []\n",
    "        index_to_what_map[owner.idx].append(owned)\n",
    "        if owned.idx not in index_to_what_map:\n",
    "            index_to_what_map[owned.idx] = []\n",
    "        index_to_what_map[owned.idx].append(owner)\n",
    "\n",
    "    return index_to_what_map\n",
    "\n",
    "def species_indices(tn_doc):\n",
    "    indices = []\n",
    "    for species_span in tn_doc.ents:\n",
    "        for species in species_span:\n",
    "            indices.append(species.idx)\n",
    "    return indices\n",
    "\n",
    "def context(sp_doc, tokens, token_map, cluster_map, chunk_map, what_map):\n",
    "    token_indices = [token.idx for token in tokens]\n",
    "    what = []\n",
    "    chunks = []\n",
    "    clusters = []\n",
    "    for token_index in token_indices:\n",
    "        # Clusters\n",
    "        if token_index in cluster_map:\n",
    "            for cluster_token_index in cluster_map[token_index]:\n",
    "                clusters.append(token_map[cluster_token_index[0]])\n",
    "        # Chunks\n",
    "        if token_index in chunk_map:\n",
    "            for token in chunk_map[token_index]:\n",
    "                chunks.append(token)\n",
    "        # What\n",
    "        if token_index in what_map:\n",
    "            for token in what_map[token_index]:\n",
    "                what.append(token)\n",
    "    return ([*clusters, *chunks, *what], {\"clusters\": clusters, \"chunks\": chunks, \"what\": what})\n",
    "\n",
    "def is_species(tokens, context, species_indices):\n",
    "    for token in [*tokens, *context]:\n",
    "        if token.idx in species_indices:\n",
    "            print(f\"\\t\\t\\tToken '{token.text}' is a Species\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_relevant(text):\n",
    "    sp_doc = sp_nlp(text)\n",
    "    \n",
    "    token_map = index_to_token(sp_doc)\n",
    "    # print(token_map)\n",
    "    \n",
    "    chunk_map = index_to_chunk(sp_doc)\n",
    "    # print(chunk_map)\n",
    "\n",
    "    matcher = dependency_matcher(sp_nlp)\n",
    "    matches = matcher(sp_doc)\n",
    "    what_map = index_to_what(sp_nlp, sp_doc, matches)\n",
    "    # print(what_map)\n",
    "    \n",
    "    tn_doc = tn_nlp(text)\n",
    "    species = species_indices(tn_doc)\n",
    "\n",
    "    predictions = fcoref.predict(texts=[text])\n",
    "    cluster_map = index_to_cluster(predictions)\n",
    "\n",
    "    found_instance = False\n",
    "    svo_triples = textacy.extract.subject_verb_object_triples(sp_doc)\n",
    "    for svo_triple in svo_triples:\n",
    "        print(f\"\\tTriple: {svo_triple}\")\n",
    "        sub_context = context(sp_doc, svo_triple.subject, token_map, cluster_map, chunk_map, what_map)\n",
    "        print(f\"\\t\\tSubject Context:\")\n",
    "        print(f\"\\t\\t\\tCluster: {sub_context[1]['clusters']}\")\n",
    "        print(f\"\\t\\t\\tChunks: {sub_context[1]['chunks']}\")\n",
    "        print(f\"\\t\\t\\tWhat: {sub_context[1]['what']}\")\n",
    "        valid_sub = is_species(svo_triple.subject, sub_context[0], species)\n",
    "\n",
    "        obj_context = context(sp_doc, svo_triple.object, token_map, cluster_map, chunk_map, what_map)\n",
    "        print(f\"\\t\\tObject Context:\")\n",
    "        print(f\"\\t\\t\\tCluster: {obj_context[1]['clusters']}\")\n",
    "        print(f\"\\t\\t\\tChunks: {obj_context[1]['chunks']}\")\n",
    "        print(f\"\\t\\t\\tWhat: {obj_context[1]['what']}\")\n",
    "        valid_obj = is_species(svo_triple.object, obj_context[0], species)\n",
    "\n",
    "        if valid_sub and valid_obj:\n",
    "            print(f\"\\tValid Subject and Object\")\n",
    "            found_instance = True\n",
    "        print()\n",
    "    return found_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7accd64d-a993-4f36-80c9-6f3e0c1a758b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
