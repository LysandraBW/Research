{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ee5074-bc96-4fc0-b5be-04a5dd07ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "import fastcoref\n",
    "from fastcoref import FCoref\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9eef12f-0b32-4243-957d-d5c385f9726e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.2\n"
     ]
    }
   ],
   "source": [
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe84c00b-659f-471b-a36c-0e20bf8c4682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeab6b783e7748a09e4567938469ab5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 17:04:51 INFO: Downloaded file to C:\\Users\\lbeln\\stanza_resources\\resources.json\n",
      "03/25/2025 17:04:51 - INFO - \t Downloaded file to C:\\Users\\lbeln\\stanza_resources\\resources.json\n",
      "2025-03-25 17:04:51 INFO: Downloading default packages for language: en (English) ...\n",
      "03/25/2025 17:04:51 - INFO - \t Downloading default packages for language: en (English) ...\n",
      "2025-03-25 17:04:53 INFO: File exists: C:\\Users\\lbeln\\stanza_resources\\en\\default.zip\n",
      "03/25/2025 17:04:53 - INFO - \t File exists: C:\\Users\\lbeln\\stanza_resources\\en\\default.zip\n",
      "2025-03-25 17:04:58 INFO: Finished downloading models and saved to C:\\Users\\lbeln\\stanza_resources\n",
      "03/25/2025 17:04:58 - INFO - \t Finished downloading models and saved to C:\\Users\\lbeln\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "stanza.download(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb0d9cd-c0d9-4236-b49c-63a73b24dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 17:04:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "03/25/2025 17:04:59 - INFO - \t Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e24bd24a394a72a674beebb88db64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 17:04:59 INFO: Downloaded file to C:\\Users\\lbeln\\stanza_resources\\resources.json\n",
      "03/25/2025 17:04:59 - INFO - \t Downloaded file to C:\\Users\\lbeln\\stanza_resources\\resources.json\n",
      "2025-03-25 17:05:01 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "03/25/2025 17:05:01 - INFO - \t Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2025-03-25 17:05:01 INFO: Using device: cpu\n",
      "03/25/2025 17:05:01 - INFO - \t Using device: cpu\n",
      "2025-03-25 17:05:01 INFO: Loading: tokenize\n",
      "03/25/2025 17:05:01 - INFO - \t Loading: tokenize\n",
      "2025-03-25 17:05:03 INFO: Loading: mwt\n",
      "03/25/2025 17:05:03 - INFO - \t Loading: mwt\n",
      "2025-03-25 17:05:03 INFO: Loading: pos\n",
      "03/25/2025 17:05:03 - INFO - \t Loading: pos\n",
      "2025-03-25 17:05:07 INFO: Loading: lemma\n",
      "03/25/2025 17:05:07 - INFO - \t Loading: lemma\n",
      "2025-03-25 17:05:08 INFO: Loading: constituency\n",
      "03/25/2025 17:05:08 - INFO - \t Loading: constituency\n",
      "2025-03-25 17:05:09 INFO: Loading: depparse\n",
      "03/25/2025 17:05:09 - INFO - \t Loading: depparse\n",
      "2025-03-25 17:05:10 INFO: Loading: sentiment\n",
      "03/25/2025 17:05:10 - INFO - \t Loading: sentiment\n",
      "2025-03-25 17:05:10 INFO: Loading: ner\n",
      "03/25/2025 17:05:10 - INFO - \t Loading: ner\n",
      "2025-03-25 17:05:15 INFO: Done loading processors!\n",
      "03/25/2025 17:05:15 - INFO - \t Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b386c50c-4412-47c0-8a6a-ebc5e7f20c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dependents(id, doc):\n",
    "    dependents = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.head == id:\n",
    "                dependents.append(word)\n",
    "    return dependents\n",
    "\n",
    "def find_start_verb(doc):\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.upos == \"VERB\":\n",
    "                return word\n",
    "    return None\n",
    "\n",
    "def find_object(doc, verb):\n",
    "    dependents = find_dependents(verb.id, doc)\n",
    "    for word in dependents:\n",
    "        if word.deprel == \"obj\" or word.deprel == \"nsubj:pass\":\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def find_subject(doc, verb):\n",
    "    dependents = find_dependents(verb.id, doc)\n",
    "    for word in dependents:\n",
    "        if word.deprel == \"nsubj\" or word.deprel == \"obl:agent\":\n",
    "            return word\n",
    "    \n",
    "    for word in dependents:\n",
    "        if word.deprel == \"advcl\":\n",
    "            subject = find_subject(doc, word)\n",
    "            if subject:\n",
    "                return subject\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e1a5af-8f9a-4ee0-9097-e01f83892dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_noun(doc, word):\n",
    "    dependents = find_dependents(word.id, doc)\n",
    "    for dependent in dependents:\n",
    "        if dependent.deprel == \"nmod:poss\":\n",
    "            return find_noun(doc, dependent)\n",
    "    if word.upos == \"NOUN\" or word.upos == \"PRON\":\n",
    "        return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1845811-dd25-40de-ac86-656afae5df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_reference(sentence, a, b, doc=None):\n",
    "    if not doc:\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "    model = FCoref(enable_progress_bar=False)\n",
    "    \n",
    "    noun_a = find_noun(doc, a)\n",
    "    noun_b = find_noun(doc, b)\n",
    "\n",
    "    if not noun_a or not noun_b:\n",
    "        return False\n",
    "\n",
    "    a_pos = (noun_a.start_char, noun_a.end_char)\n",
    "    b_pos = (noun_b.start_char, noun_b.end_char)\n",
    "\n",
    "    a_found = False\n",
    "    b_found = False\n",
    "    \n",
    "    clusters = model.predict(texts=[sentence])[0].get_clusters(as_strings=False)\n",
    "    for cluster in clusters:\n",
    "        # There's a bug here and I just realized it, I forgot to reset a_found and b_found\n",
    "        for c in cluster:\n",
    "            if a_pos[0] >= c[0] and a_pos[0] <= c[1] and a_pos[1] >= c[0] and a_pos[1] <= c[1]:\n",
    "                a_found = True\n",
    "            if b_pos[0] >= c[0] and b_pos[0] <= c[1] and b_pos[1] >= c[0] and b_pos[1] <= c[1]:\n",
    "                b_found = True\n",
    "            if a_found and b_found:\n",
    "                return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43f5626e-a108-496a-8aee-3a3652bbb46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relations(sentence):\n",
    "    relations = []\n",
    "    doc = nlp(sentence)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            print(f\"Word: {word.text}, {word.upos}\")\n",
    "            if word.upos != \"VERB\":\n",
    "                continue\n",
    "            # print(f\"Verb: {word.text}\")\n",
    "            verb = word\n",
    "            object = find_object(doc, verb)\n",
    "            # print(f\"Object: {object.text}\")\n",
    "            subject = find_subject(doc, verb)\n",
    "            # print(f\"Subject: {subject.text}\")\n",
    "            same_ref = same_reference(sentence, object, subject, doc=doc)\n",
    "            # print(f\"Same Reference: {same_ref}\")\n",
    "            rel_obj = is_species_or_trait(sentence, object)\n",
    "            # print(f\"Relevant Object: {rel_obj}\")\n",
    "            rel_sub = is_species_or_trait(sentence, subject)\n",
    "            # print(f\"Relevant Subject: {rel_sub}\")\n",
    "            if verb and object and subject and not same_ref and rel_obj and rel_sub:\n",
    "                relations.append({\"verb\": verb, \"object\": object, \"subject\": subject})\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e0ae5b8-b93c-44cb-a248-1d8b7280699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context(sentence, stanza_word):\n",
    "    snlp = spacy.load(\"en_core_web_sm\")\n",
    "    matcher = Matcher(snlp.vocab)\n",
    "    \n",
    "    pattern = [\n",
    "        {\"POS\": {\"IN\": [\"DET\", \"ADJ\", \"NOUN\", \"PROPN\"]}, \"OP\": \"+\"},\n",
    "        {\"POS\": \"ADP\", \"OP\": \"?\"},\n",
    "        {\"POS\": {\"IN\": [\"DET\", \"ADJ\", \"NOUN\", \"PROPN\"]}, \"OP\": \"+\"}\n",
    "    ]\n",
    "    matcher.add(\"NOUN_PHRASE\", [pattern])\n",
    "\n",
    "    doc = snlp(sentence)\n",
    "    spans = [doc[start:end] for _, start, end in matcher(doc)]\n",
    "    print(spans)\n",
    "    for span in spacy.util.filter_spans(spans):\n",
    "        for word in span:\n",
    "            if stanza_word.start_char == word.idx and stanza_word.end_char == (word.idx + len(word)):\n",
    "                return span\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c17505e9-e46f-46fb-8c39-3a3167607fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_species_or_trait(context_span):\n",
    "    contains_traits = \"\"\n",
    "    contains_species = \"\"\n",
    "\n",
    "    # There is no current way to determine whether\n",
    "    # a species is included or not, I'd have to fine-tune\n",
    "    # a model for that. So, for now, I'm hard-coding it.\n",
    "    for word in context_span:\n",
    "        if word.text[0:2] == \"TR\":\n",
    "            contains_traits = \"TR\"\n",
    "        if word.text[0:2] == \"SP\":\n",
    "            contains_species = \"SP\"\n",
    "\n",
    "    return [contains_traits, contains_species]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83dd22b4-3f22-4969-bbbb-bb495b4aeb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_species_or_trait(sentence, stanza_word):\n",
    "    context = find_context(sentence, stanza_word)\n",
    "    print(f\"Context of '{stanza_word.text}': {context}\")\n",
    "    if not context:\n",
    "        print(f\"Returning False for {stanza_word.text}\")\n",
    "        return stanza_word.text[:2] == \"SP\" or stanza_word.text[:2] == \"TR\"\n",
    "    st_context = find_species_or_trait(context)\n",
    "    return st_context[0] != \"\" or st_context[1] != \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7e7bc3c-609b-4ddb-a1c9-f93ba2211462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Presence of nonlethal tigers reduced the TRAIT of SPECIES_TRAMMEA on SPECIES_SMALL_GREEN_FROGS\"\n",
      "Word: Presence, NOUN\n",
      "Word: of, ADP\n",
      "Word: nonlethal, ADJ\n",
      "Word: tigers, NOUN\n",
      "Word: reduced, VERB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/25/2025 17:55:53 - INFO - \t missing_keys: []\n",
      "03/25/2025 17:55:53 - INFO - \t unexpected_keys: []\n",
      "03/25/2025 17:55:53 - INFO - \t mismatched_keys: []\n",
      "03/25/2025 17:55:53 - INFO - \t error_msgs: []\n",
      "03/25/2025 17:55:53 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "03/25/2025 17:55:53 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adcad2d3d49e4542ae71b52050798b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/25/2025 17:55:54 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Presence of nonlethal, Presence of nonlethal tigers, nonlethal tigers, the TRAIT, TRAIT of SPECIES_TRAMMEA, the TRAIT of SPECIES_TRAMMEA, SPECIES_TRAMMEA on SPECIES_SMALL_GREEN_FROGS]\n",
      "Context of 'TRAIT': the TRAIT of SPECIES_TRAMMEA\n",
      "[Presence of nonlethal, Presence of nonlethal tigers, nonlethal tigers, the TRAIT, TRAIT of SPECIES_TRAMMEA, the TRAIT of SPECIES_TRAMMEA, SPECIES_TRAMMEA on SPECIES_SMALL_GREEN_FROGS]\n",
      "Context of 'Presence': Presence of nonlethal tigers\n",
      "Word: the, DET\n",
      "Word: TRAIT, NOUN\n",
      "Word: of, ADP\n",
      "Word: SPECIES_TRAMMEA, PROPN\n",
      "Word: on, ADP\n",
      "Word: SPECIES_SMALL_GREEN_FROGS, PROPN\n",
      "No Relations\n"
     ]
    }
   ],
   "source": [
    "sentence_basic = \"Presence of nonlethal tigers reduced the TRAIT of SPECIES_TRAMMEA on SPECIES_SMALL_GREEN_FROGS\"\n",
    "print(f\"Sentence: \\\"{sentence_basic}\\\"\")\n",
    "relations = find_relations(sentence_basic)\n",
    "\n",
    "if not relations:\n",
    "    print(\"No Relations\")\n",
    "for r in relations:\n",
    "    print(f\"Subject: \\\"{r['subject'].text}\\\"\")\n",
    "    print(f\"Subject Context: \\\"{find_context(sentence_basic, r['subject'])}\\\"\\n\")\n",
    "    print(f\"Verb: \\\"{r['verb'].text}\\\"\\n\")\n",
    "    print(f\"Object: \\\"{r['object'].text}\\\"\")\n",
    "    print(f\"Object Context: \\\"{find_context(sentence_basic, r['object'])}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11ba433e-0af2-4945-b83d-3553ee390869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( PUNCT\n",
      "2 X\n",
      ") PUNCT\n",
      "Presence PROPN\n",
      "of ADP\n",
      "nonlethal PROPN\n",
      "SPECIES_ANAX PRON\n",
      "reduced VERB\n",
      "the DET\n",
      "TRAIT PROPN\n",
      "of ADP\n",
      "SPECIES_TRAMEA PROPN\n",
      "on ADP\n",
      "SPECIES_SMALL_GREEN_FROGS ADV\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "snlp = spacy.load('en_core_web_sm')\n",
    "doc = snlp(sentence_basic)\n",
    "\n",
    "for word in doc:\n",
    "    print(word, word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319508c0-990f-421d-843b-33f85159b4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
