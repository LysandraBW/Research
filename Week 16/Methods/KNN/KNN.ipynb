{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae54da2-75f8-44ab-a0f4-78cbe2947947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used this notebook as reference\n",
    "# https://www.kaggle.com/code/keitazoumana/scientific-document-similarity-search-with-scibert/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f977f-6b23-48e0-bf93-a1c300076381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer,  AutoModelForSequenceClassification\n",
    "%run -i \"../utils.py\"\n",
    "\n",
    "pretrained_model = 'allenai/scibert_scivocab_uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model, do_lower_case=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model, output_attentions=False, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ff430-b839-431a-bdcc-fc0110aa2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def embed(string, verbose=False):\n",
    "    input_ids = tokenizer.encode(string, add_special_tokens=True)\n",
    "    if verbose:\n",
    "        print(f\"Input IDs: {[input_ids]}\")\n",
    "    padded_input_ids = pad_sequences([input_ids], maxlen=210, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    if verbose:\n",
    "        print(f\"Padded Input IDs: {[padded_input_ids]}\")\n",
    "    \n",
    "    input_ids = padded_input_ids[0]\n",
    "    if verbose:\n",
    "        print(f\"Input IDs: {[input_ids]}\")\n",
    "\n",
    "    # Attention Mask\n",
    "    # It seems that it holds boolean values.\n",
    "    attention_mask = [int(i > 0) for i in input_ids]\n",
    "    \n",
    "    # Convert to Tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    \n",
    "    # Pseudo-Batch\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Run the text through BERT, and collect all of the hidden states produced\n",
    "    # from all 12 layers. \n",
    "    with torch.no_grad():        \n",
    "        logits, encoded_layers = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, return_dict=False)\n",
    "\n",
    "    layer_i = 12 # The last BERT layer before the classifier.\n",
    "    batch_i = 0 # Only one input in the batch.\n",
    "    token_i = 0 # The first token, corresponding to [CLS]\n",
    "        \n",
    "    # Extract the embedding.\n",
    "    embedding = encoded_layers[layer_i][batch_i][token_i]\n",
    "    embedding = embedding.detach().cpu().numpy()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Embedding Shape: {embedding.shape}\")\n",
    "    return (embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a5c81-1520-4b8f-b57c-40994461860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings(data):\n",
    "    data = data.copy()\n",
    "    embeddings = []\n",
    "    for abstract in data.Abstract:\n",
    "        embeddings.append(embed(abstract))\n",
    "    data[\"Embeddings\"] = embeddings\n",
    "    return data\n",
    "\n",
    "def process_embedding(embedding, verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Shape: {embedding.shape}\")\n",
    "    # The embedding needs to be a row vector\n",
    "    # for the cosine similarity calculation.\n",
    "    # I think, I might have misremembered.\n",
    "    embedding = np.array(embedding)\n",
    "    embedding = embedding.reshape(1, -1)\n",
    "    if verbose:\n",
    "        print(f\"Shape: {embedding.shape}\")\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40fef39-caa8-41f6-836f-a07c82b0292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def score_dataset(name, verbose=False):\n",
    "    # Load Dataset\n",
    "    data = load_preprocessed_dataset(name)\n",
    "\n",
    "    if data.shape[0] == 0:\n",
    "        print(\"Nothing to Score\")\n",
    "        return\n",
    "\n",
    "    # Add Embeddings (from Abstract)\n",
    "    data = add_embeddings(data)\n",
    "    \n",
    "    # Load Examples\n",
    "    # These are the abstracts that the papers will be scored against\n",
    "    # (in terms of similarity).\n",
    "    with open(f\"../../Datasets/Examples.pkl\", \"rb\") as f: \n",
    "        examples = pickle.load(f)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Examples Shape: {examples.shape}\")\n",
    "\n",
    "    # Add Embeddings\n",
    "    # Embeddings are vectors that represent real-world objects, like words, images, \n",
    "    # or videos, in a form that machine learning models can easily process.\n",
    "    # (CloudFlare)\n",
    "    # The idea that I have in mind is to compare each paper to the examples.\n",
    "    # So, the examples would make be used here as the neighbors. It's pretty similar\n",
    "    # to the cosine similarity method.\n",
    "    examples = add_embeddings(examples)\n",
    "    example_embeddings = np.stack(examples.Embeddings.to_numpy())\n",
    "    if verbose:\n",
    "        print(f\"Example Embeddings Shape: {example_embeddings.shape}\")\n",
    "    \n",
    "    neighbors = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(example_embeddings)\n",
    "    \n",
    "    data[\"Distances\"] = data[\"Embeddings\"].apply(lambda x: neighbors.kneighbors(process_embedding(x))[0])\n",
    "    data[\"Score\"] = data[\"Distances\"].apply(lambda x: np.min(x))\n",
    "    # The lower the score the better, since we're looking for papers that are closer (in distance) to the example papers\n",
    "    data.sort_values(by='Score', ascending=True, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23640f-aa1d-403d-afe8-844ef4435cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score Datasets\n",
    "# Versions\n",
    "# k\n",
    "version = ''\n",
    "\n",
    "dataset_names = [\"Examples\", \"Baseline-1\", \"SubA\", \"SubAFiltered\", \"SubB\", \"SubBFiltered\", \"C\", \"CFiltered\", \"D\", \"DFiltered\"]\n",
    "for name in dataset_names:\n",
    "    scored_data = score_dataset(name, verbose=False)\n",
    "    store_scored_dataset(scored_data, name, version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c344f6dc-af1e-4f8c-910f-78fdfb9c59e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"../utils.py\"\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "measurements = measure_method_by_threshold(\n",
    "    output_fp=\"./ScoredBaseline-1.csv\",\n",
    "    output_threshold=5,\n",
    "    reverse=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {measurements['accuracy'] * 100:.2f}%\")\n",
    "cm = ConfusionMatrixDisplay(confusion_matrix=measurements['confusion_matrix'])\n",
    "cm.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
