{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbecf5be-df89-4d1a-b75f-d92a5c5fff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trait-Mediated Interaction Modification\n",
    "# Empirical:\n",
    "# You could look for words like \"hypothesis\", \"experiment\", \"found\", and \"discovered\". That may point\n",
    "# towards there being an experiment in the paper. There are also words like \"control group\", \"compared\",\n",
    "# \"findings\", \"results\", \"study\", and more.\n",
    "# Qualitative vs. Quantitative:\n",
    "# To infer whether something is quantitative, you could look for numeric tokens and units.\n",
    "# However, you can only do so much with the abstract. Therefore, this is likely not good enough.\n",
    "# Yet, you could still take advantage of words like \"fewer\" and \"increased\" to show that there is a change.\n",
    "# However, this would be more suited for the above category.\n",
    "# Traits:\n",
    "# There is no NLP tool for traits that I can use or create so I think that I could instead use keywords.\n",
    "# For example, \"snail feeding rates\" is a trait. You may be able to spot this by looking for a word like\n",
    "# \"rate\". You'd expand that word to include \"snail feeding rates\". As \"snail\" is a species you can infer\n",
    "# that \"rates\" is a trait. I would be more decisive and use a dependency parser to ensure that the trait\n",
    "# is a property of the species (like before). However, with all the cases that may exist, I think checking\n",
    "# to see whether a species can be found by traveling back and/or forward without finding certain tokens could\n",
    "# work well enough.\n",
    "# 3 Species or More:\n",
    "# This is simple. However, I think using a dictionary and TaxoNerd would be beneficial (for higher accuracy).\n",
    "# To handle the potential differences in tokenization, character offsets should be used.\n",
    "# Standardization:\n",
    "# There is a lot of variance in the scores. To squash this issue, I think that we could assign each sentence\n",
    "# a value from 0 to 1. We would add these values and divide by the number of sentences. This would result in\n",
    "# a number that is also from 0 to 1. However, there are categories that we would like to inspect. So, we must\n",
    "# create an overall score in the interval from [0, 1] while also scoring each category. Well, for each sentence\n",
    "# we could add a point for each category that is observed. The sentence would receive said score divided by the\n",
    "# number of categories. At the end, we add up all the sentence scores and divide by the number of sentences.\n",
    "# The aggregate score for each category would also be divided by the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "637ccb94-4c7d-49b1-a727-f6e25b021357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from spacy.language import Language\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "%run -i \"../utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b83e8a-083f-46a4-a79e-1877e9d9d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class References:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.predictions = None\n",
    "        # Token's Index to Token's Cluster\n",
    "        self.cluster_map = None\n",
    "\n",
    "    def update(self, text, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.predictions = self.main.fcoref.predict(texts=[text])\n",
    "        self.cluster_map = self.load_cluster_map(self.predictions, verbose=verbose)\n",
    "        \n",
    "    def load_cluster_map(self, predictions, verbose=False):\n",
    "        if verbose:\n",
    "            print(f\"Load Cluster Map:\")\n",
    "        \n",
    "        cluster_map = {}\n",
    "        \n",
    "        for prediction in predictions:\n",
    "            clusters = prediction.get_clusters(as_strings=False)\n",
    "            if verbose:\n",
    "                print(f\"Clusters:\\n{prediction.get_clusters(as_strings=True)}\")\n",
    "\n",
    "            # A cluster contains spans (segments of the text) that are\n",
    "            # reference each other (e.g. ['We', 'our'] or [(0, 2), (5, 8)].\n",
    "            for cluster in clusters:\n",
    "                # It's a cluster of spans,\n",
    "                # but instead it'll be represented into a cluster of tokens.\n",
    "                # This makes it easier to use (coding-wise).\n",
    "                clustered_tokens = []\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Cluster: {cluster}\")\n",
    "                \n",
    "                for span in cluster:\n",
    "                    span_words = self.main.sp_doc.text[span[0]:span[1]].split()\n",
    "                    char_index = span[0]\n",
    "                    for i in range(len(span_words)):\n",
    "                        word = span_words[i]\n",
    "                        clustered_tokens.append(self.main.token_at_char(char_index))\n",
    "                        char_index += len(word) + 1\n",
    "                \n",
    "                for token in clustered_tokens:\n",
    "                    cluster_map[token] = list(filter(lambda t: t != token, clustered_tokens))\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Cluster Map\")\n",
    "            print(cluster_map)\n",
    "            print()\n",
    "        \n",
    "        return cluster_map\n",
    "\n",
    "    def same_reference(self, token_a, token_b, verbose=False, compare_text=True):\n",
    "        if verbose:\n",
    "            print(\"Same Reference:\")\n",
    "            print(f\"Token A: {token_a} v. Token B: {token_b}\")\n",
    "\n",
    "        # Compare Text\n",
    "        if compare_text and token_a.lower_ == token_b.lower_:\n",
    "            if verbose:\n",
    "                print(\"Same String\")\n",
    "            return True\n",
    "\n",
    "        # Check if Token B in Token A Cluster\n",
    "        if token_a.i in self.cluster_map and token_b in self.cluster_map[token_a.i]:\n",
    "            if verbose:\n",
    "                print(f\"Token A Cluster: {self.cluster_map[token_a.i]}\")\n",
    "                print(\"\\tToken B in Token A Cluster\")\n",
    "            return True\n",
    "\n",
    "        # Check if Token A in Token B Cluster\n",
    "        if token_b.i in self.cluster_map and token_a in self.cluster_map[token_b.i]:\n",
    "            if verbose:\n",
    "                print(f\"tToken B Cluster: {self.cluster_map[token_b.i]}\")\n",
    "                print(\"Token A in Token B Cluster\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def same_reference_span(self, span_a, span_b, verbose=False):\n",
    "        # Compare Text\n",
    "        if span_a.text.lower() == span_b.text.lower():\n",
    "            if verbose:\n",
    "                print(\"Same String\")\n",
    "            return True\n",
    "        \n",
    "        for token_a in span_a:\n",
    "            for token_b in span_b:\n",
    "                if self.same_reference(token_a, token_b, compare_text=False, verbose=verbose):\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad706ec8-70e2-4bc2-b1e9-247e1c047796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the 'Gazetteer'\n",
    "@Language.component(\"lower_case_lemmas\")\n",
    "def lower_case_lemmas(doc) :\n",
    "    for token in doc :\n",
    "        token.lemma_ = token.lemma_.lower()\n",
    "    return doc\n",
    "\n",
    "class Species:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\"])\n",
    "        self.tn_nlp.add_pipe(\"lower_case_lemmas\", after=\"lemmatizer\")\n",
    "        self.tn_doc = None\n",
    "        # Contains any spans that have been identified\n",
    "        # as a species.\n",
    "        self.spans = None\n",
    "        # Contains any tokens that have been identified\n",
    "        # as being a part of a species or a species.\n",
    "        # Meaning, if \"brown squirrel\" was in the text,\n",
    "        # this list would contain [\"brown\", \"squirrel\", ...].\n",
    "        self.tokens = None\n",
    "        # To make the lines of code easier later on,\n",
    "        # I've mapped the species token to the span it\n",
    "        # belongs it.\n",
    "        self.token_to_span = None\n",
    "        # Contains pairs of spans that have been identified\n",
    "        # as alternative names of each other.\n",
    "        self.alternative_spans = None\n",
    "        # There are words that TN may not recognize. This\n",
    "        # can be used to help that.\n",
    "        self.gazetteer = [\"juvenile\", \"adult\", \"prey\", \"predator\", \"species\", \"crab\", \"snail\"]\n",
    "        patterns = []\n",
    "        for name in self.gazetteer:\n",
    "            doc = self.tn_nlp(name)\n",
    "            patterns.append({\"label\": \"LIVB\", \"pattern\": [{\"LEMMA\": token.lemma_} for token in doc]})\n",
    "        ruler = self.tn_nlp.add_pipe(\"entity_ruler\")\n",
    "        ruler.add_patterns(patterns)\n",
    "\n",
    "    def update(self, text, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        self.spans, self.tokens, self.token_to_span, self.alternate_spans = self.load_species(verbose=verbose)\n",
    "        \n",
    "    def load_species(self, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # These three contain the literal species that have been\n",
    "        # identified in the text. Tokens that aren't adjectives,\n",
    "        # nouns, or proper nouns will be stripped.\n",
    "        spans = []\n",
    "        tokens = []\n",
    "        token_to_span = {}\n",
    "\n",
    "        # It's useful to know if a different name refers to a\n",
    "        # species we have already seen -- so we pair any alternative\n",
    "        # names. For example, in \"predatory crab (Carcinus maenas)\",\n",
    "        # \"predatory crab\" is an alternative name for \"Carcinus maenas\"\n",
    "        # and vice versa. This is used so that the same species can be\n",
    "        # properly tracked and won't be given redundant points.\n",
    "        alternate_spans = {}\n",
    "\n",
    "        # TaxoNerd\n",
    "        if verbose:\n",
    "            print(\"Recognized TN Entities:\")\n",
    "            print(self.tn_doc.ents)\n",
    "\n",
    "        for species_span in self.tn_doc.ents:\n",
    "            if verbose:\n",
    "                print(f\"Species Span: {species_span}\")\n",
    "   \n",
    "            # Translated to Main Doc\n",
    "            sp_li = self.main.token_at_char(self.tn_doc[species_span.start].idx).i\n",
    "            sp_ri = self.main.token_at_char(self.tn_doc[species_span.end].idx).i\n",
    "            \n",
    "            sp_species_span = self.main.sp_doc[sp_li:sp_ri]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Species Span (SpaCy): {species_span}\")\n",
    "\n",
    "            # Expand Species if Ambiguous (1st Condition) or Possibly Missing Information (2nd Condition)\n",
    "            if sp_species_span.text.lower() in self.gazetteer or (sp_species_span.start > 0 and self.main.sp_doc[sp_species_span.start-1].pos_ in [\"ADJ\"]):\n",
    "                sp_species_span = self.main.expand_unit(\n",
    "                    il_unit=sp_species_span.start, \n",
    "                    ir_unit=sp_species_span.end - 1,\n",
    "                    il_boundary=0,\n",
    "                    ir_boundary=len(self.main.sp_doc),\n",
    "                    direction='LEFT',\n",
    "                    allowed_speech=[\"ADJ\", \"PROPN\"],\n",
    "                    allowed_literals=[\"-\"],\n",
    "                    verbose=verbose\n",
    "                )\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Expanded Species Span (SpaCy): {species_span}\")\n",
    "            \n",
    "            # Contract Species (Remove Outer Punctuations and/or Symbols)\n",
    "            sp_species_span = self.main.contract_unit(\n",
    "                il_unit=sp_species_span.start, \n",
    "                ir_unit=sp_species_span.end - 1, \n",
    "                allowed_speech=[\"ADJ\", \"NOUN\", \"PROPN\"],\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Contracted Species Span (SpaCy): {sp_species_span}\")\n",
    "\n",
    "            # Add Span and Tokens\n",
    "            spans.append(sp_species_span)\n",
    "            for token in sp_species_span:\n",
    "                if verbose:\n",
    "                    print(token, token.pos_)\n",
    "                if token.pos_ not in [\"ADJ\", \"PROPN\", \"NOUN\"]:\n",
    "                    continue\n",
    "                tokens.append(token)\n",
    "                token_to_span[token] = sp_species_span\n",
    "\n",
    "        \n",
    "        # Finding and Storing Alternative Names\n",
    "        if verbose:\n",
    "            print(\"Finding Alternate Names\")\n",
    "        \n",
    "        for i, species_span in enumerate(spans):\n",
    "            if i + 1 >= len(spans):\n",
    "                break\n",
    "            \n",
    "            next_species_span = spans[i+1]\n",
    "            if verbose:\n",
    "                print(f\"SP1: {species_span}, SP2: {next_species_span}, DIST: {next_species_span.start - species_span.end == 1}\")\n",
    "            \n",
    "            # There's one token in between the two species\n",
    "            if next_species_span.start - species_span.end == 1:\n",
    "                before_next = self.main.sp_doc[next_species_span.start - 1]\n",
    "                after_next = self.main.sp_doc[next_species_span.end]\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Token Before SP2: {before_next}, Token After SP2: {after_next}\")\n",
    "                \n",
    "                # The next species span is surrounded by parentheses (generalized).\n",
    "                # This likely indicates that this next species is an alternative name\n",
    "                # for the species before.\n",
    "                if before_next.pos_ in [\"PUNCT\", \"SYM\"] and after_next.pos_ in [\"PUNCT\", \"SYM\"]:\n",
    "                    sp_span_text = species_span.text.lower()\n",
    "                    next_sp_span_text = next_species_span.text.lower()\n",
    "                    \n",
    "                    if sp_span_text not in alternate_spans:\n",
    "                        alternate_spans[sp_span_text] = []\n",
    "                    if next_sp_span_text not in alternate_spans:\n",
    "                        alternate_spans[next_sp_span_text] = []\n",
    "                    \n",
    "                    alternate_spans[sp_span_text].append(next_sp_span_text)\n",
    "                    alternate_spans[next_sp_span_text].append(sp_span_text)\n",
    "                            \n",
    "        if verbose:\n",
    "            print(f\"Spans: {spans}\")\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "            print(f\"Alternate Spans: {alternate_spans}\")\n",
    "        \n",
    "        return (spans, tokens, token_to_span, alternate_spans)\n",
    "\n",
    "    def span_at_token(self, token):\n",
    "        if token in self.token_to_span:\n",
    "            return self.token_to_span[token]\n",
    "        return None\n",
    "    \n",
    "    def is_species(self, token):\n",
    "        return token in self.tokens\n",
    "        \n",
    "    def has_species(self, tokens, verbose=False):\n",
    "        if verbose:\n",
    "            print(f\"Given Tokens: {[t.i for t in tokens]}\")\n",
    "            print(f\"Species Tokens: {[t.i for t in self.tokens]}\")\n",
    "        for token in tokens:\n",
    "            if token in self.tokens:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def same_species(self, sp_1, sp_2, verbose=False):\n",
    "        if verbose:\n",
    "            print(f\"SP 1: {sp_1}\")\n",
    "            print(f\"SP 2: {sp_2}\")\n",
    "            \n",
    "        # Compare\n",
    "        if verbose:\n",
    "            print(f\"Compare Literals: {sp_1.text.lower()}' == '{sp_2.text.lower()}'\")\n",
    "        \n",
    "        if sp_1.text.lower() == sp_2.text.lower():\n",
    "            return True\n",
    "\n",
    "        # Alternate Names\n",
    "        if verbose:\n",
    "            print(\"Check Alternate Names\")\n",
    "\n",
    "        sp_1_text = sp_1.text.lower()\n",
    "        sp_2_text = sp_2.text.lower()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"SP 1 TEXT: {sp_1_text}\")\n",
    "            print(f\"SP 2 TEXT: {sp_2_text}\")\n",
    "        \n",
    "        if sp_1_text in self.alternate_spans:\n",
    "            if verbose:\n",
    "                print(f\"SP 1 Alternate Spans: {self.alternate_spans[sp_1_text]}\")\n",
    "            if sp_2_text in self.alternate_spans[sp_1_text]:\n",
    "                return True\n",
    "\n",
    "        \n",
    "        if sp_2_text in self.alternate_spans:\n",
    "            if verbose:\n",
    "                print(f\"SP 2 Alternate Spans: {self.alternate_spans[sp_2_text]}\")\n",
    "            if sp_1_text in self.alternate_spans[sp_2_text]:\n",
    "                return True\n",
    "\n",
    "        # Singular Version of Phrase (e.g. \"fewer crabs\" becomes \"fewer crab\")\n",
    "        singular_version = lambda tokens : \" \".join([*[token.text for token in tokens[:-1]], tokens[-1].lemma_]).lower()\n",
    "\n",
    "        # Removing Adjectives\n",
    "        # If you had two spans, \"fewer crabs\" and \"crabs\", you'd want them to be\n",
    "        # recognized as the same species. However, you don't want \"red crabs\" and\n",
    "        # \"blue crabs\" to be recognized as the same. So, perhaps we remove adjectives\n",
    "        # until the two species have an equivalent number of adjectives.\n",
    "        # So, \"fewer crabs\" turns into \"crabs\" and is compared with \"crabs\". However,\n",
    "        # \"red crabs\" would remain as \"red crabs\" since \"red crabs\" and \"blue crabs\" have\n",
    "        # the same number of adjectives. I think this could be an okay method.\n",
    "        # This will only be used for species that each have one or less species. Anything else,\n",
    "        # and I feel that they're probably not the same anyway.\n",
    "        sp_1_adjs = []\n",
    "        sp_1_nouns = []\n",
    "        for token in sp_1:\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                sp_1_adjs.append(token)\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_1_nouns.append(token)\n",
    "        \n",
    "        sp_2_adjs = []\n",
    "        sp_2_nouns = []\n",
    "        for token in sp_2:\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                sp_2_adjs.append(token)\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_2_nouns.append(token)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Number of Adjectives in 1: {len(sp_1_adjs)}\")\n",
    "            print(f\"Number of Adjectives in 2: {len(sp_2_adjs)}\")\n",
    "\n",
    "        if sp_1_nouns and sp_2_nouns and ((len(sp_1_adjs) == 1 and len(sp_2_adjs) == 0) or (len(sp_2_adjs) == 1 and len(sp_1_adjs) == 0)):\n",
    "            # \"fewer crabs\" vs. \"crabs\"\n",
    "            sp_singular_nouns_1 = singular_version(sp_1_nouns)\n",
    "            sp_singular_nouns_2 = singular_version(sp_2_nouns)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Compare Singular Nouns: {sp_singular_nouns_1}' == '{sp_singular_nouns_2}'\")\n",
    "            \n",
    "            return sp_singular_nouns_1 == sp_singular_nouns_2\n",
    "            \n",
    "        # Compare Singular Version\n",
    "        # These lines are so that spans like \"predatory crab\" and \"predatory crabs\"\n",
    "        # aren't ruled out as different species. The above check may also be handled\n",
    "        # by this, but the lemma of a token depends on the surrounding context, so it\n",
    "        # also might not.\n",
    "        sp_lemma_1 = singular_version(sp_1)\n",
    "        sp_lemma_2 = singular_version(sp_2)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Compare Singular Spans: {sp_lemma_1}' == '{sp_lemma_2}'\")\n",
    "        \n",
    "        if sp_lemma_1 == sp_lemma_2:\n",
    "            return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c31521a-3d38-4087-b63e-bb974ad3a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    def __init__(self, main, base=[], phrases=[], speech=[], threshold=0.7):\n",
    "        self.main = main\n",
    "        self.base = [b.lower() for b in base]\n",
    "        self.speech = [s.upper() for s in speech]\n",
    "        self.phrases = [p.lower() for p in phrases]\n",
    "        self.threshold = threshold\n",
    "        self.vocab = [self.main.sp_nlp(word) for word in self.base]\n",
    "        self.tokens = []\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tokens = self.match_tokens(verbose=verbose)\n",
    "\n",
    "    def match_tokens(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        matched_tokens = []\n",
    "\n",
    "        # Words\n",
    "        for token in self.main.sp_doc:\n",
    "            if verbose:\n",
    "                print(f\"Potential Keyword: {token, token.pos_} v. Speech: {self.speech}\")\n",
    "            if token.pos_ not in self.speech:\n",
    "                continue\n",
    "            # Comparing Literal Text\n",
    "            if token.lemma_.lower() in self.base or token.lower_ in self.base:\n",
    "                matched_tokens.append(token)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.main.sp_nlp(token.lemma_)\n",
    "            for word in self.vocab:\n",
    "                similarity = word.similarity(lemma)\n",
    "                if verbose:\n",
    "                    print(f\"{lemma} and {word} Similarity: {similarity}\")\n",
    "                if similarity >= self.threshold:\n",
    "                    matched_tokens.append(token)\n",
    "                    break\n",
    "\n",
    "        # Phrases\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "        for phrase in self.phrases:\n",
    "            for char_index in [match.start() for match in re.finditer(phrase, text)]:\n",
    "                matched_tokens.append(self.main.token_at_char(char_index))\n",
    "                \n",
    "        return matched_tokens\n",
    "\n",
    "class ExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"study\", \"hypothesis\", \"experiment\", \"found\", \"discover\", \"compare\", \"finding\", \"result\"],\n",
    "            phrases=[\"control group\", \"independent\", \"dependent\"],\n",
    "            speech=[\"VERB\", \"NOUN\"], \n",
    "            threshold=0.7\n",
    "        )\n",
    "\n",
    "class CauseKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"increase\", \"decrease\", \"change\", \"shift\", \"cause\", \"produce\"], \n",
    "            speech=[\"VERB\"], \n",
    "            threshold=0.6\n",
    "        )\n",
    "\n",
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"few\", \"more\", \"increase\", \"decrease\", \"less\", \"short\", \"long\"], \n",
    "            speech=[\"NOUN\"], \n",
    "            threshold=0.6\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba70792f-d32b-4638-a929-c7499506e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraitKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"behavior\", \"rate\", \"color\", \"mass\", \"size\", \"length\"], \n",
    "            speech=[\"NOUN\", \"ADJ\"], \n",
    "            threshold=0.7\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        if verbose:\n",
    "            print(f\"Unfiltered Tokens: {self.tokens}\")\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            expanded_token = self.main.expand_unit(\n",
    "                il_unit=token.i, \n",
    "                ir_unit=token.i, \n",
    "                il_boundary=0, \n",
    "                ir_boundary=len(self.main.sp_doc), \n",
    "                allowed_speech=[\"ADJ\", \"NOUN\", \"ADP\", \"PART\"],\n",
    "                allowed_literals=[\"-\", \",\"],\n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Token: {token}\")\n",
    "                print(f\"Expanded Token: {expanded_token}\")\n",
    "\n",
    "            if self.main.species.has_species(expanded_token):\n",
    "                if verbose:\n",
    "                    print(f\"\\tContains Species\")\n",
    "                filtered.append(token)\n",
    "        \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fdf8e07-a326-4fea-8615-faa5d549852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main:\n",
    "    def __init__(self):\n",
    "        # Tools\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        self.sp_doc = None\n",
    "\n",
    "        # Maps Character Position to Token in Document\n",
    "        # Used to handle differences between different\n",
    "        # pipelines and tools.\n",
    "        self.index_map = None\n",
    "\n",
    "        # Parsers\n",
    "        self.species = Species(self)\n",
    "        self.traits = TraitKeywords(self)\n",
    "        self.causes = CauseKeywords(self)\n",
    "        self.changes = ChangeKeywords(self)\n",
    "        self.references = References(self)\n",
    "        self.experiment = ExperimentKeywords(self)\n",
    "\n",
    "    def update_doc(self, doc, verbose=False):\n",
    "        self.sp_doc = doc\n",
    "        self.index_map = self.load_index_map()\n",
    "        self.references.update(doc.text, verbose=False)\n",
    "        self.species.update(doc.text, verbose=verbose)\n",
    "        self.traits.update(verbose=False)\n",
    "        self.causes.update(verbose=False)\n",
    "        self.changes.update(verbose=False)\n",
    "        self.experiment.update(verbose=False)\n",
    "\n",
    "    def update_text(self, text, verbose=False):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.update_doc(self.sp_doc, verbose=verbose)\n",
    "        \n",
    "    def token_at_char(self, char_index):\n",
    "        # SpaCy Doc or Indexing Map Not Found\n",
    "        if not self.sp_doc or not self.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Index into Map\n",
    "        if char_index in self.index_map:\n",
    "            return self.index_map[char_index]\n",
    "\n",
    "        # Looking in Tokens\n",
    "        # Depending on the tokenizer, the character being\n",
    "        # used to find a token may not be the first character\n",
    "        # of the token.\n",
    "        for token in self.sp_doc:\n",
    "            if char_index >= token.idx and char_index < token.idx + len(token):\n",
    "                return token\n",
    "\n",
    "        # There must be a token that corresponds to the\n",
    "        # given character index. If there's not, there's\n",
    "        # an issue.\n",
    "        raise Exception(\"Token Not Found\")\n",
    "        \n",
    "    def load_index_map(self):\n",
    "        # SpaCy Doc Not Found\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Map Character Index to Token\n",
    "        index_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            index_map[token.idx] = token\n",
    "        return index_map\n",
    "\n",
    "    def expand_unit(self, *, il_unit, ir_unit, il_boundary, ir_boundary, allowed_speech=[], allowed_literals=[], direction='BOTH', verbose=False):\n",
    "        if verbose:\n",
    "            print(\"LEFT\")\n",
    "            \n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit > il_boundary:\n",
    "                prev_token = self.sp_doc[il_unit-1]\n",
    "                if verbose:\n",
    "                    print(f\"il_unit: {il_unit}, il_boundary: {il_boundary}, prev_token: {prev_token}, prev_token.pos_: {prev_token.pos_}\")\n",
    "                if prev_token.pos_ not in allowed_speech and prev_token.lower_ not in allowed_literals:\n",
    "                    break\n",
    "                il_unit -= 1\n",
    "\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit < ir_boundary:\n",
    "                next_token = self.sp_doc[ir_unit+1]\n",
    "                if verbose:\n",
    "                    print(f\"ir_unit: {ir_unit}, ir_boundary: {ir_boundary}, next_token: {next_token}, next_token.pos_: {next_token.pos_}\")\n",
    "                if next_token.pos_ not in allowed_speech and next_token.lower_ not in allowed_literals:\n",
    "                    break\n",
    "                ir_unit += 1\n",
    "\n",
    "        expanded_unit = self.sp_doc[il_unit:ir_unit+1]\n",
    "        if verbose:\n",
    "            print(f\"Expanded Unit: {expanded_unit}\")\n",
    "        return expanded_unit\n",
    "\n",
    "    def contract_unit(self, *, il_unit, ir_unit, allowed_speech=[], allowed_literals=[], direction='BOTH', verbose=False):\n",
    "        # il_unit_0 = il_unit\n",
    "        # ir_unit_0 = ir_unit\n",
    "\n",
    "        if verbose:\n",
    "            print(\"LEFT\")\n",
    "            \n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit <= ir_unit:\n",
    "                curr_token = self.sp_doc[il_unit]\n",
    "                if verbose:\n",
    "                    print(f\"il_unit: {il_unit}, ir_unit: {ir_unit}, curr_token: {curr_token}, curr_token.pos_: {curr_token.pos_}\")\n",
    "                if curr_token.pos_ in allowed_speech or curr_token.lower_ in allowed_literals:\n",
    "                    break\n",
    "                il_unit += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(\"RIGHT\")\n",
    "        \n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit >= il_unit:\n",
    "                curr_token = self.sp_doc[ir_unit]\n",
    "                if verbose:\n",
    "                    print(f\"il_unit: {il_unit}, ir_unit: {ir_unit}, curr_token: {curr_token}, curr_token.pos_: {curr_token.pos_}\")\n",
    "                if curr_token.pos_ in allowed_speech or curr_token.lower_ in allowed_literals:\n",
    "                    break\n",
    "                ir_unit -= 1\n",
    "\n",
    "        # il_unit = min(il_unit, ir_unit_0)\n",
    "        # ir_unit = max(ir_unit, il_unit_0)\n",
    "        assert il_unit <= ir_unit\n",
    "        contracted_unit = self.sp_doc[il_unit:ir_unit+1]\n",
    "        if verbose:\n",
    "            print(f\"Contracted Unit: {contracted_unit}\")\n",
    "        return contracted_unit\n",
    "\n",
    "    def score(self, verbose=False):\n",
    "        # Categories\n",
    "        SPECIES = 0\n",
    "        TRAIT = 1\n",
    "        EXPERIMENT = 2\n",
    "\n",
    "        # Number Categories\n",
    "        NUM_CATEGORIES = 3\n",
    "        \n",
    "        # Points\n",
    "        # Each sentence can add only one to each\n",
    "        # of these categories. An entry represents\n",
    "        # the points for a category.\n",
    "        points = [0] * NUM_CATEGORIES\n",
    "\n",
    "        # Extracted Information\n",
    "        change_tokens = self.changes.tokens\n",
    "        cause_tokens = self.causes.tokens\n",
    "        trait_tokens = self.traits.tokens\n",
    "        species_tokens = self.species.tokens\n",
    "        experiment_tokens = self.experiment.tokens\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Change Tokens: {self.changes.tokens}\")\n",
    "            print(f\"Cause Tokens: {self.causes.tokens}\")\n",
    "            print(f\"Trait Tokens: {self.traits.tokens}\")\n",
    "            print(f\"Species Tokens: {self.species.tokens}\")\n",
    "            print(f\"Experiment Tokens: {self.experiment.tokens}\")\n",
    "        \n",
    "        # Removing Redundant Species Tokens\n",
    "        seen = set()\n",
    "        species_tokens = [token for token in species_tokens if self.species.span_at_token(token).start not in seen and (seen.add(self.species.span_at_token(token).start) or True)]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Not-Redundant Species Tokens: {species_tokens}\")\n",
    "        \n",
    "        # This is used to ensure that at least three species\n",
    "        # are mentioned.\n",
    "        seen_species = {}\n",
    "\n",
    "        for sent in self.sp_doc.sents:\n",
    "            # This is here so that I don't add more than one point\n",
    "            # for each category in a single sentence.\n",
    "            found = [0] * NUM_CATEGORIES\n",
    "\n",
    "            sent_tokens = [token for token in sent]\n",
    "            sent_cause_tokens = set(sent_tokens).intersection(cause_tokens)\n",
    "            sent_change_tokens = set(sent_tokens).intersection(change_tokens)\n",
    "            sent_seen_species = []\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Sentence: {sent}\")\n",
    "                print(f\"Sentence Tokens: {sent_tokens}\")\n",
    "                print(f\"Sentence Cause Tokens: {sent_cause_tokens}\")\n",
    "                print(f\"Sentence Change Tokens: {sent_change_tokens}\")\n",
    "            \n",
    "            for token in sent_tokens:\n",
    "                # No More Searching Needed\n",
    "                if found[SPECIES] >= 1 and found[TRAIT] >= 1 and found[EXPERIMENT] >= 1:\n",
    "                    break\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Token: '{token}' ({token.pos_})\")\n",
    "\n",
    "                # Trait\n",
    "                if found[TRAIT] < 1 and token in trait_tokens:\n",
    "                    points[TRAIT] += 1\n",
    "                    found[TRAIT] = True\n",
    "\n",
    "                    if verbose:\n",
    "                        print(\"Points Added for Trait\")\n",
    "\n",
    "                # Experiment\n",
    "                if found[EXPERIMENT] < 1 and token in experiment_tokens:\n",
    "                    points[EXPERIMENT] += 1\n",
    "                    found[EXPERIMENT] = True\n",
    "\n",
    "                    if verbose:\n",
    "                        print(\"Points Added for Experiment\")\n",
    "\n",
    "                # Species\n",
    "                if token in species_tokens:\n",
    "                    # Find Species Span\n",
    "                    species_span = self.species.span_at_token(token)\n",
    "                    if not species_span:\n",
    "                        raise Exception(\"Species Span DNE\")\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"Species Span: {species_span}\")\n",
    "    \n",
    "                    # Updating Seen Species\n",
    "                    if verbose:\n",
    "                        print(\"Seen Species:\")\n",
    "                        print(seen_species)\n",
    "                    \n",
    "                    past_visits = 0\n",
    "                    for seen_species_span in seen_species.keys():\n",
    "                        if verbose:\n",
    "                            print(f\"Comparing '{species_span}' and '{seen_species_span}'\")\n",
    "\n",
    "                        if self.species.same_species(species_span, seen_species_span, verbose=verbose):\n",
    "                            past_visits = seen_species[seen_species_span]\n",
    "                            if verbose:\n",
    "                                print(f\"\\t'{species_span}' == '{seen_species_span}'\")\n",
    "                                print(f\"\\tNumber of Visits: {past_visits}\")\n",
    "                            seen_species[seen_species_span] += 1\n",
    "                            break\n",
    "\n",
    "                    if past_visits == 0:\n",
    "                        seen_species[species_span] = 1\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(\"Seen Species Updated:\")\n",
    "                        print(seen_species)\n",
    "\n",
    "                    # We only add points if it's a species that has not been seen\n",
    "                    # in the sentence. This is to avoid redundant points.\n",
    "                    if verbose:\n",
    "                        print(\"Seen Species in Sentence:\")\n",
    "                    redundant_species = False\n",
    "                    for seen_species_span in sent_seen_species:\n",
    "                        if verbose:\n",
    "                            print(f\"Comparing '{species_span}' and '{seen_species_span}'\")\n",
    "                        if self.species.same_species(species_span, seen_species_span, verbose=verbose):\n",
    "                            redundant_species = True\n",
    "                            if verbose:\n",
    "                                print(f\"\\tEqual => Continue\")\n",
    "                            break\n",
    "                    if redundant_species:\n",
    "                        continue\n",
    "                    sent_seen_species.append(species_span)\n",
    "                    \n",
    "                    if found[SPECIES] < 1:\n",
    "                        # To get points in the species category,\n",
    "                        # there must be (1) a species; and (2) a change or cause\n",
    "                        # word nearby.\n",
    "                        distance = 5\n",
    "                        sent_cause_tokens_in_area = [c_token for c_token in sent_cause_tokens if c_token != token and abs(c_token.i - token.i) <= distance]\n",
    "                        sent_change_tokens_in_area = [c_token for c_token in sent_change_tokens if c_token != token and abs(c_token.i - token.i) <= distance]\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"Cause Tokens in Area: {sent_cause_tokens_in_area}\")\n",
    "                            print(f\"Change Tokens in Area: {sent_change_tokens_in_area}\")\n",
    "                        \n",
    "                        if sent_cause_tokens_in_area or sent_change_tokens_in_area:\n",
    "                            points[SPECIES] += 0.5\n",
    "                            found[SPECIES] = True\n",
    "\n",
    "                            if verbose:\n",
    "                                print(\"Points Added for Species\")\n",
    "\n",
    "        # 3 Species Required\n",
    "        if verbose:\n",
    "            print(f\"Seen Species: {seen_species}\")\n",
    "        \n",
    "        if len(seen_species) < 3:\n",
    "            return 0\n",
    "        \n",
    "        # Normalizing Score\n",
    "        NUM_SENTENCES = len(list(self.sp_doc.sents))\n",
    "        \n",
    "        score = (points[TRAIT] + points[SPECIES] + points[EXPERIMENT]) / (NUM_CATEGORIES * NUM_SENTENCES)\n",
    "        assert 0.0 <= score <= 1.0\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Score: {score}\")\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a10821c-4a15-47e6-b3d2-f49b7ef3eec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replicated experiments in artificial ponds demonstrated that an assemblage of aquatic insects competed with tadpoles of the frogs Hyla andersonii and Bufo woodhousei fowleri. We independently manipulated the presence or absence of aquatic insects, and the abundance of an anuran competitor (O or 150 Bufo w. fowleri per experimental pond), using a completely crossed design for two—factor variance analysis, and observed the responses of initially similar cohorts of Hyla andersonii tadpoles to neither, either, or both insect and anuran competitors. Insects and Bufo significantly depressed the mean individual mass at metamorphosis of Hyla froglets and the cumulative biomass of anurans leaving the ponds at metamorphosis. Neither insects nor Bufo affected the survival or larval period of Hyla. Insects also significantly reduced the mean mass of Bufo, showing that both anurans responded to competition from insects. The intensity of competition between natural densities of insects and Hyla tadpoles was comparable to the intensity of competition between Bufo and Hyla, as a density of 150 Bufo/1000 L.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../Datasets/Baseline-1.csv\")\n",
    "df.head(4)\n",
    "text = df.Abstract[3]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7619e40a-251d-46d0-a451-d6e3c8fa30bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/13/2025 12:50:50 - INFO - \t missing_keys: []\n",
      "06/13/2025 12:50:50 - INFO - \t unexpected_keys: []\n",
      "06/13/2025 12:50:50 - INFO - \t mismatched_keys: []\n",
      "06/13/2025 12:50:50 - INFO - \t error_msgs: []\n",
      "06/13/2025 12:50:50 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "06/13/2025 12:51:15 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.77 examples/s]\n",
      "06/13/2025 12:51:15 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    }
   ],
   "source": [
    "main = Main()\n",
    "main.update_text(text, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50556a25-fc86-4a81-b5ff-efb92b2a7004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19444444444444445\n"
     ]
    }
   ],
   "source": [
    "score = main.score(verbose=False)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b30f36d4-6ba2-48ba-b67b-8ba0b9a36790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ceratitis capitata': ['diptera'],\n",
       " 'diptera': ['ceratitis capitata', 'tephritidae'],\n",
       " 'tephritidae': ['diptera']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.species.alternate_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d122dc3-2e12-4f94-b470-d48cddc4564c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
