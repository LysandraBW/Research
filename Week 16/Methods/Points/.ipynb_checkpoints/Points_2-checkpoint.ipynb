{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecf5be-df89-4d1a-b75f-d92a5c5fff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trait-Mediated Interaction Modification\n",
    "# Empirical:\n",
    "# You could look for words like \"hypothesis\", \"experiment\", \"found\", and \"discovered\". That may point\n",
    "# towards there being an experiment in the paper. There are also words like \"control group\", \"compared\",\n",
    "# \"findings\", \"results\", \"study\", and more.\n",
    "# Qualitative vs. Quantitative:\n",
    "# To infer whether something is quantitative, you could look for numeric tokens and units.\n",
    "# However, you can only do so much with the abstract. Therefore, this is likely not good enough.\n",
    "# Yet, you could still take advantage of words like \"fewer\" and \"increased\" to show that there is a change.\n",
    "# However, this would be more suited for the above category.\n",
    "# Traits:\n",
    "# There is no NLP tool for traits that I can use or create so I think that I could instead use keywords.\n",
    "# For example, \"snail feeding rates\" is a trait. You may be able to spot this by looking for a word like\n",
    "# \"rate\". You'd expand that word to include \"snail feeding rates\". As \"snail\" is a species you can infer\n",
    "# that \"rates\" is a trait. I would be more decisive and use a dependency parser to ensure that the trait\n",
    "# is a property of the species (like before). However, with all the cases that may exist, I think checking\n",
    "# to see whether a species can be found by traveling back and/or forward without finding certain tokens could\n",
    "# work well enough.\n",
    "# 3 Species or More:\n",
    "# This is simple. However, I think using a dictionary and TaxoNerd would be beneficial (for higher accuracy).\n",
    "# To handle the potential differences in tokenization, character offsets should be used.\n",
    "# Standardization:\n",
    "# There is a lot of variance in the scores. To squash this issue, I think that we could assign each sentence\n",
    "# a value from 0 to 1. We would add these values and divide by the number of sentences. This would result in\n",
    "# a number that is also from 0 to 1. However, there are categories that we would like to inspect. So, we must\n",
    "# create an overall score in the interval from [0, 1] while also scoring each category. Well, for each sentence\n",
    "# we could add a point for each category that is observed. The sentence would receive said score divided by the\n",
    "# number of categories. At the end, we add up all the sentence scores and divide by the number of sentences.\n",
    "# The aggregate score for each category would also be divided by the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ccb94-4c7d-49b1-a727-f6e25b021357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from spacy.language import Language\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "%run -i \"../utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26736310-0964-4184-a9c1-f6b0b38d7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the Dictionary\n",
    "@Language.component(\"lower_case_lemmas\")\n",
    "def lower_case_lemmas(doc) :\n",
    "    for token in doc :\n",
    "        token.lemma_ = token.lemma_.lower()\n",
    "    return doc\n",
    "\n",
    "class Species:\n",
    "    def __init__(self, main):\n",
    "        # Tools\n",
    "        self.main = main\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"parser\", \"attribute_ruler\"])\n",
    "        self.tn_nlp.add_pipe(\"lower_case_lemmas\", after=\"lemmatizer\")\n",
    "        self.tn_doc = None\n",
    "        \n",
    "        # Contains any spans that have been identified\n",
    "        # as a species.\n",
    "        self.spans = None\n",
    "        \n",
    "        # Contains any tokens that have been identified\n",
    "        # as a species or being a part of a species.\n",
    "        self.tokens = None\n",
    "        \n",
    "        # Used to quickly access the span that a token\n",
    "        # belongs to.\n",
    "        self.token_to_span = None\n",
    "        \n",
    "        # Maps a string to an array of strings wherein\n",
    "        # any string involved in the key-value pair \n",
    "        # has been identified as an alternate name of each other.\n",
    "        self.alternate_names = None\n",
    "        \n",
    "        # Used to increase TaxoNERD's accuracy.\n",
    "        self.dictionary = None\n",
    "        self.load_dictionary()\n",
    "\n",
    "    def load_dictionary(self):\n",
    "        self.dictionary = [\"juvenile\", \"adult\", \"prey\", \"predator\", \"species\"]\n",
    "        # df = pd.read_csv(\"VernacularNames.csv\")\n",
    "        # self.dictionary += df.VernacularName.to_list()\n",
    "\n",
    "        patterns = []\n",
    "        for name in self.dictionary:\n",
    "            doc = self.tn_nlp(name)\n",
    "            patterns.append({\"label\": \"LIVB\", \"pattern\": [{\"LEMMA\": token.lemma_} for token in doc]})\n",
    "        ruler = self.tn_nlp.add_pipe(\"entity_ruler\")\n",
    "        ruler.add_patterns(patterns)\n",
    "        \n",
    "    def update(self, text, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        self.spans, self.tokens, self.token_to_span, self.alternate_names = self.load_species(verbose=verbose)\n",
    "\n",
    "    def load_species(self, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # These three contain the species that have been\n",
    "        # identified in the text. Tokens that aren't adjectives,\n",
    "        # nouns, or proper nouns will be stripped.\n",
    "        spans = []\n",
    "        tokens = []\n",
    "        token_to_span = {}\n",
    "\n",
    "        # It's useful to know if a different name refers to a\n",
    "        # species we have already seen. For example, in \"predatory crab (Carcinus maenas)\",\n",
    "        # \"predatory crab\" is an alternative name for \"Carcinus maenas\"\n",
    "        # and vice versa. This is used so that the species can be\n",
    "        # properly tracked and redundant points are less likely to be given.\n",
    "        alternate_names = {}\n",
    "\n",
    "        # TaxoNerd\n",
    "        if verbose:\n",
    "            print(f\"TN Entities: {self.tn_doc.ents}\\n\")\n",
    "\n",
    "        # TaxoNERD sometimes recognizes one instance of a species and not the other.\n",
    "        # To fix this, I'll use the recognized species to look for all the instances \n",
    "        # in the document that also match those species.\n",
    "        text_lowered = self.main.sp_doc.text.lower()\n",
    "        recognized_species = list(set([span.text.lower() for span in self.tn_doc.ents]))\n",
    "\n",
    "        # Fix: Adding Singular and Plural Versions\n",
    "        # Let's say you're looking for words that contain \"ant\" in the text.\n",
    "        # \"Pants\" and \"antebellum\" would match. To prevent this, we need to make sure\n",
    "        # that the entire word matches; but, with this change, \"ants\" wouldn't match either.\n",
    "        # To fix this issue, we add the singular and plural versions of the recognized\n",
    "        # species so that we \"ant\" and \"ants\" are matched, but \"Pants\" and \"antebellum\"\n",
    "        # aren't.\n",
    "\n",
    "        # This contains strings not Span objects as we're looking throughout the text\n",
    "        # for any matches based on the content and not position.\n",
    "        base_species = []\n",
    "        \n",
    "        for span in self.tn_doc.ents:\n",
    "            base = '' if len(span) == 1 else {span[:-1].text.lower()}\n",
    "            last = span[-1]\n",
    "\n",
    "            # Singular Noun\n",
    "            if last.tag_ in [\"NP\"]:\n",
    "                # 'cactus', 'octopus'\n",
    "                if last.lower_[:2] == \"us\":\n",
    "                    # 'cactus' -> 'cacti'\n",
    "                    last_i = f'{last.lower_[:-2]}i'}\n",
    "                    base_species.append(f'{base} {last_i}')\n",
    "                    # 'octopus' -> 'octopuses'\n",
    "                    last_ses = f'{last.lower_}es'}\n",
    "                    base_species.append(f'{base} {last_ses}')'\n",
    "                # 'bonobo', 'flamingo'\n",
    "                elif last.lower_[:1] == \"o\":\n",
    "                    # 'bonobo' -> 'bonobos'\n",
    "                    last_s = f'{last.lower_[:-2]}s'}\n",
    "                    base_species.append(f'{base} {last_s}')\n",
    "                    # 'flamingo' -> 'flamingoes\n",
    "                    last_es = f'{last.lower_[:-2]}es'}\n",
    "                    base_species.append(f'{base} {last_es}')\n",
    "                # 'butterfly', 'canary'\n",
    "                elif last.lower_[-1] == \"y\" and last.lower_[-2] != \"e\":\n",
    "                    # 'butterfly' -> 'butterflies'\n",
    "                    last_ies = f'{last.lower_[:-1]}ies'\n",
    "                    base_species.append(f'{base} {last_ies}')\n",
    "                # 'chamois', 'fish'\n",
    "                elif last.lower_[:1] in [\"s\", \"h\"]:\n",
    "                    # 'chamois' -> 'chamoises'\n",
    "                    last_es = f'{last.lower_[:-2]}es'}\n",
    "                    base_species.append(f'{base} {last_es}')\n",
    "                    \n",
    "            # Plural Noun\n",
    "            elif last.tag_ in [\"NNS\"]:\n",
    "\n",
    "            \n",
    "            base_species.append(span.text.lower())\n",
    "            \n",
    "            \n",
    "\n",
    "        if verbose:\n",
    "            print(f\"All Recognized Species: {recognized_species}\")\n",
    "        \n",
    "        for species in recognized_species:\n",
    "            if verbose:\n",
    "                print(f\"Recognized Species: {species}\")\n",
    "\n",
    "            # Remove Symbols from Outside\n",
    "            while len(species) > 0:\n",
    "                change_made = False\n",
    "                \n",
    "                if len(species) > 0 and not species[-1].isalnum():\n",
    "                    species = species[:-1]\n",
    "                    change_made = True\n",
    "\n",
    "                if len(species) > 0 and not species[0].isalnum():\n",
    "                    species = species[1:]\n",
    "                    change_made = True\n",
    "                    \n",
    "                if not change_made:\n",
    "                    break\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Recognized Species After Removing Symbols: {species}\")\n",
    "                \n",
    "            # Retrieve the start and end position for each of the substrings\n",
    "            # in the text that matched the species span.\n",
    "            for char_i0, char_i1 in [(match.start(), match.end()) for match in re.finditer(re.escape(species), text_lowered)]:\n",
    "                if verbose:\n",
    "                    print(f\"Match ({char_i0}-{char_i1}): '{text_lowered[char_i0:char_i1]}'\")\n",
    "\n",
    "                if char_i0 > 0 and text_lowered[char_i0-1].isalpha():\n",
    "                    continue\n",
    "\n",
    "                if char_i1 < len(text_lowered) - 1 and text_lowered[char_i1+1].isalpha():\n",
    "                    continue\n",
    "                    \n",
    "                sp_li = self.main.token_at_char(char_i0).i\n",
    "                sp_ri = self.main.token_at_char(char_i1-1).i\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"sp_li: {sp_li}\")\n",
    "                    print(f\"sp_ri: {sp_ri}\")\n",
    "                    \n",
    "                species_span = self.main.sp_doc[sp_li:sp_ri+1]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Species Span: {species_span}\")\n",
    "    \n",
    "                # Expand Species\n",
    "                # a. When it's Potentially Ambiguous\n",
    "                # b. When it's Potentially Missing Information\n",
    "                ambiguous = len(species_span) == 1 and species_span[0].pos_ == \"NOUN\"\n",
    "                missing_info = species_span.start > 0 and self.main.sp_doc[species_span.start-1].pos_ in [\"ADJ\"]\n",
    "                \n",
    "                if ambiguous or missing_info:\n",
    "                    species_span = self.main.expand_unit(\n",
    "                        il_unit=species_span.start, \n",
    "                        ir_unit=species_span.end-1,\n",
    "                        il_boundary=0,\n",
    "                        ir_boundary=len(self.main.sp_doc),\n",
    "                        direction='LEFT',\n",
    "                        allowed_speech=[\"ADJ\", \"PROPN\"],\n",
    "                        allowed_literals=[\"-\"],\n",
    "                        verbose=verbose\n",
    "                    )\n",
    "    \n",
    "                    if verbose:\n",
    "                        print(f\"Expanded Species Span: {species_span}\")\n",
    "                \n",
    "                # Remove Outer Punctuations and Symbols\n",
    "                species_span = self.main.contract_unit(\n",
    "                    il_unit=species_span.start, \n",
    "                    ir_unit=species_span.end-1, \n",
    "                    allowed_speech=[\"ADJ\", \"NOUN\", \"PROPN\"],\n",
    "                    verbose=verbose\n",
    "                )\n",
    "    \n",
    "                if verbose:\n",
    "                    print(f\"Contracted Species Span: {species_span}\")\n",
    "    \n",
    "                # Add Span and Tokens\n",
    "                if verbose:\n",
    "                    print(f\"Adding Species Span and Tokens\")\n",
    "                    \n",
    "\n",
    "                # If each token in the span is a punctuation or symbol,\n",
    "                # then is it even a species? I don't think so. However,\n",
    "                # in case this happens, I don't want this added to the spans.\n",
    "                non_punct_sym_found = False\n",
    "                for token in species_spans:\n",
    "                    if token.pos_ not in [\"PUNCT\", \"SYM\"]:\n",
    "                        non_punct_sym_found = True\n",
    "                        break\n",
    "\n",
    "                if not non_punct_sym_found:\n",
    "                    continue\n",
    "                \n",
    "                spans.append(species_span)\n",
    "                for token in species_span:\n",
    "                    if verbose:\n",
    "                        print(f\"Token in Species ({token.pos_}): {token}\")\n",
    "                    if token.pos_ in [\"PUNCT\", \"SYM\"]:\n",
    "                        continue\n",
    "                    tokens.append(token)\n",
    "                    token_to_span[token] = species_span\n",
    "\n",
    "        # Removing Duplicates and Sorting (for Next Part)\n",
    "        if verbose:\n",
    "            print(f\"Spans Before Removing Duplicates and Sorting: {spans}\")\n",
    "            \n",
    "        mapped_spans = {}\n",
    "        for span in spans:\n",
    "            mapped_spans[span.start] = span\n",
    "\n",
    "        spans = list(mapped_spans.values())\n",
    "        spans.sort(key=lambda span: span.start)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Spans Before Removing Duplicates and Sorting: {spans}\")\n",
    "        \n",
    "        # Finding and Storing Alternative Names\n",
    "        if verbose:\n",
    "            print(\"Finding Alternate Names\")\n",
    "        \n",
    "        for i, species_span in enumerate(spans):\n",
    "            if i + 1 >= len(spans):\n",
    "                break\n",
    "            \n",
    "            next_species_span = spans[i+1]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"SPECIES 1: {species_span}\")\n",
    "                print(f\"SPECIES 2: {next_species_span}\")\n",
    "                print(f\"DIST == 1: {next_species_span.start - species_span.end == 1}\")\n",
    "            \n",
    "            # If there's one token between the species and the next species,\n",
    "            # we check if the next species is surrounded by punctuation.\n",
    "            if next_species_span.start - species_span.end == 1:\n",
    "                # Token Before and After the Next Species\n",
    "                before_next = self.main.sp_doc[next_species_span.start-1]\n",
    "                after_next = self.main.sp_doc[next_species_span.end]\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Token Before SPECIES 2: {before_next} and Token After SPECIES 2: {after_next}\")\n",
    "\n",
    "                # Adding K-V Pair for Names\n",
    "                if before_next.pos_ in [\"PUNCT\", \"SYM\"] and after_next.pos_ in [\"PUNCT\", \"SYM\"]:\n",
    "                    # Instead of using the span objects, the text (string literals)\n",
    "                    # are used. This is because we're focusing on the content (the name)\n",
    "                    # rather than where it appears in the document.\n",
    "                    sp_1_text = species_span.text.lower()\n",
    "                    sp_2_text = next_species_span.text.lower()\n",
    "                    \n",
    "                    if sp_1_text not in alternate_names:\n",
    "                        alternate_names[sp_1_text] = []\n",
    "                    \n",
    "                    if sp_2_text not in alternate_names:\n",
    "                        alternate_names[sp_2_text] = []\n",
    "                    \n",
    "                    alternate_names[sp_1_text].append(sp_2_text)\n",
    "                    alternate_names[sp_2_text].append(sp_1_text)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Spans: {spans}\")\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "            print(f\"Alternate Spans: {alternate_names}\")\n",
    "        \n",
    "        return (spans, tokens, token_to_span, alternate_names)\n",
    "\n",
    "    def span_at_token(self, token):\n",
    "        if token in self.token_to_span:\n",
    "            return self.token_to_span[token]\n",
    "        return None\n",
    "    \n",
    "    def is_species(self, token):\n",
    "        return token in self.tokens\n",
    "        \n",
    "    def has_species(self, tokens, verbose=False):\n",
    "        for token in tokens:\n",
    "            if token in self.tokens:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def same_species(self, sp_1, sp_2, verbose=False):\n",
    "        if verbose:\n",
    "            print(\"Comparing Species\")\n",
    "            print(f\"SPECIES 1: {sp_1}\")\n",
    "            print(f\"SPECIES 2: {sp_2}\")\n",
    "\n",
    "        sp_1_text = sp_1.text.lower()\n",
    "        sp_2_text = sp_2.text.lower()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"SPECIES 1 TEXT: {sp_1_text}\")\n",
    "            print(f\"SPECIES 2 TEXT: {sp_2_text}\")\n",
    "        \n",
    "        # METHOD 1: Check if Texts are Equivalent\n",
    "        equivalent = sp_1.text.lower() == sp_2.text.lower()\n",
    "        \n",
    "        if equivalent:\n",
    "            return True\n",
    "\n",
    "        # METHOD 2: Check Alternate Names\n",
    "        if verbose:\n",
    "            print(\"Check Alternate Names\")\n",
    "        \n",
    "        if sp_1_text in self.alternate_names:\n",
    "            if verbose:\n",
    "                print(f\"SPECIES 1 Alternate Names: {self.alternate_names[sp_1_text]}\")\n",
    "            if sp_2_text in self.alternate_names[sp_1_text]:\n",
    "                return True\n",
    "        \n",
    "        if sp_2_text in self.alternate_names:\n",
    "            if verbose:\n",
    "                print(f\"SPECIES 2 Alternate Names: {self.alternate_names[sp_2_text]}\")\n",
    "            if sp_1_text in self.alternate_names[sp_2_text]:\n",
    "                return True\n",
    "\n",
    "        # Singular Version of Phrase (e.g. \"fewer crabs\" becomes \"fewer crab\")\n",
    "        singular_version = lambda tokens : \" \".join([*[token.text for token in tokens[:-1]], tokens[-1].lemma_]).lower()\n",
    "\n",
    "        # METHOD 3: Check Substrings (More or Less)\n",
    "        # Via this method, pairs like (1) \"dog\" and \"dog red\"; and\n",
    "        # (2) \"red dog\" and \"dog\" should match.\n",
    "\n",
    "        # Common Name at Start\n",
    "        if sp_1[0].lower_ == sp_2[0].lower_ and (sp_1[0].pos_ in [\"NOUN\", \"PROPN\"] or sp_2[0].pos_ in [\"NOUN\", \"PROPN\"]):\n",
    "            if sp_1_text in sp_2_text or sp_2_text in sp_1_text:\n",
    "                if verbose:\n",
    "                    print(f\"{sp_1} and {sp_2} are the same species.\")\n",
    "                return True\n",
    "        # Common Name at End\n",
    "        else:\n",
    "            # Only used when there's 1 adjective in one of the species and\n",
    "            # no adjectives in the other (e.g. \"fewer crabs\" v. \"crabs\").\n",
    "            sp_1_nouns = []\n",
    "            sp_1_num_adjectives = 0\n",
    "            for token in sp_1:\n",
    "                if not sp_1_nouns and token.pos_ == \"ADJ\":\n",
    "                    sp_1_num_adjectives += 1\n",
    "                elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                    sp_1_nouns.append(token)\n",
    "            \n",
    "            sp_2_nouns = []\n",
    "            sp_2_num_adjectives = 0\n",
    "            for token in sp_2:\n",
    "                if not sp_2_nouns and token.pos_ == \"ADJ\":\n",
    "                    sp_2_num_adjectives += 1\n",
    "                elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                    sp_2_nouns.append(token)\n",
    "    \n",
    "            if verbose:\n",
    "                print(f\"Number of Adjectives in 1: {sp_1_num_adjectives}\")\n",
    "                print(f\"Number of Adjectives in 2: {sp_2_num_adjectives}\")\n",
    "    \n",
    "            if sp_1_nouns and sp_2_nouns and (\n",
    "                (sp_1_num_adjectives == 1 and sp_2_num_adjectives == 0) or \n",
    "                (sp_2_num_adjectives == 1 and sp_1_num_adjectives == 0)\n",
    "            ):\n",
    "                sp_singular_nouns_1 = singular_version(sp_1_nouns)\n",
    "                sp_singular_nouns_2 = singular_version(sp_2_nouns)\n",
    "    \n",
    "                if verbose:\n",
    "                    print(f\"Comparing Singular Nouns: '{sp_singular_nouns_1}' == '{sp_singular_nouns_2}'\")\n",
    "                \n",
    "                return sp_singular_nouns_1 == sp_singular_nouns_2\n",
    "\n",
    "        # METHOD 4: Check Singular Version\n",
    "        # This method targets spans like \"predatory crab\" and \"predatory crabs\".\n",
    "        sp_singular_1 = singular_version(sp_1)\n",
    "        sp_singular_2 = singular_version(sp_2)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Comparing Singular Spans: '{sp_singular_1}' == '{sp_singular_2}'\")\n",
    "        \n",
    "        if sp_singular_1 == sp_singular_2:\n",
    "            return True\n",
    "\n",
    "        # At this point, I don't see \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31521a-3d38-4087-b63e-bb974ad3a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    def __init__(self, main, base=[], phrases=[], speech=[], threshold=0.7, sub_base=[]):\n",
    "        self.main = main\n",
    "        # For a token to count towards a base word, it must be the same word.\n",
    "        # For a token to count towards a sub_base word, it must contain the word.\n",
    "        self.base = [b.lower() for b in base]\n",
    "        self.sub_base = [b.lower() for b in sub_base]\n",
    "        self.speech = [s.upper() for s in speech]\n",
    "        self.phrases = [p.lower() for p in phrases]\n",
    "        self.threshold = threshold\n",
    "        self.vocab = [self.main.sp_nlp(word) for word in self.base]\n",
    "        self.tokens = []\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tokens = self.match_tokens(verbose=verbose)\n",
    "\n",
    "    def match_tokens(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        matched_tokens = []\n",
    "\n",
    "        # Check Words\n",
    "        for token in self.main.sp_doc:\n",
    "            if verbose:\n",
    "                print(f\"Potential Keyword: {token, token.pos_} v. Speech: {self.speech}\")\n",
    "            if token.pos_ not in self.speech:\n",
    "                continue\n",
    "\n",
    "            token_lower = token.lower_\n",
    "            token_lemma_lower = token.lemma_.lower()\n",
    "            \n",
    "            # Comparing Literal Text\n",
    "            if token_lemma_lower in self.base or token_lower in self.base:\n",
    "                matched_tokens.append(token)\n",
    "                continue\n",
    "            # Comparing Substrings\n",
    "            for sub_base_word in self.sub_base:\n",
    "                if sub_base_word in token_lemma_lower or sub_base_word in token_lower:\n",
    "                    matched_tokens.append(token)\n",
    "                    break\n",
    "\n",
    "            # Cannot quickly continue onto the next loop (outer)\n",
    "            # because we were already in a loop.\n",
    "            if matched_tokens and matched_tokens[-1] == token:\n",
    "                continue\n",
    "            \n",
    "            # Comparing Similarity\n",
    "            token_doc = self.main.sp_nlp(token_lower)\n",
    "            for word in self.vocab:\n",
    "                similarity = word.similarity(token_doc)\n",
    "                if verbose:\n",
    "                    print(f\"{token_doc} and {word} Similarity: {similarity}\")\n",
    "                if similarity >= self.threshold:\n",
    "                    matched_tokens.append(token)\n",
    "                    break\n",
    "\n",
    "        # Check Phrases\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "        for phrase in self.phrases:\n",
    "            for char_index in [match.start() for match in re.finditer(phrase, text)]:\n",
    "                matched_tokens.append(self.main.token_at_char(char_index))\n",
    "                \n",
    "        return matched_tokens\n",
    "\n",
    "class ExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"study\", \"hypothesis\", \"experiment\", \"found\", \"discover\", \"compare\", \"finding\", \"result\", \"test\", \"examine\", \"model\"],\n",
    "            phrases=[\"control group\", \"independent\", \"dependent\"],\n",
    "            speech=[\"VERB\", \"NOUN\"], \n",
    "            threshold=0.8\n",
    "        )\n",
    "\n",
    "class CauseKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"increase\", \"decrease\", \"change\", \"shift\", \"cause\", \"produce\", \"trigger\", \"suppress\", \"inhibit\", \"encourage\", \"allow\"], \n",
    "            speech=[\"VERB\", \"NOUN\"], \n",
    "            threshold=0.8\n",
    "        )\n",
    "\n",
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"few\", \"more\", \"increase\", \"decrease\", \"less\", \"short\", \"long\", \"greater\"], \n",
    "            speech=[\"NOUN\", \"ADJ\", \"ADV\"], \n",
    "            threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70792f-d32b-4638-a929-c7499506e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraitKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"behavior\", \"rate\", \"color\", \"mass\", \"size\", \"length\", \"pattern\", \"weight\", \"shape\", \"efficiency\", \"trait\", \"ability\", \"capacity\", \"height\", \"width\", \"span\"],\n",
    "            sub_base=[\"mass\", \"span\", \"length\", \"color\", \"rate\"],\n",
    "            speech=[\"NOUN\", \"ADJ\"], \n",
    "            threshold=0.8\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        if verbose:\n",
    "            print(f\"Unfiltered Tokens: {self.tokens}\")\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "        if verbose:\n",
    "            print(f\"Filtered Tokens: {self.tokens}\")\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            expanded_token = self.main.expand_unit(\n",
    "                il_unit=token.i, \n",
    "                ir_unit=token.i, \n",
    "                il_boundary=0, \n",
    "                ir_boundary=len(self.main.sp_doc) - 1, \n",
    "                allowed_speech=[\"ADJ\", \"NOUN\", \"ADP\", \"PART\", \"DET\", \"PROPN\",],\n",
    "                allowed_literals=[\"-\", \",\"],\n",
    "                disallowed_literals=[\"!\", \".\", \"?\"],\n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Token: {token}\")\n",
    "                print(f\"Expanded Token: {expanded_token}\")\n",
    "\n",
    "            if self.main.species.has_species(expanded_token):\n",
    "                if verbose:\n",
    "                    print(f\"\\tContains Species\")\n",
    "                filtered.append(token)\n",
    "        \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf8e07-a326-4fea-8615-faa5d549852d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Main:\n",
    "    def __init__(self):\n",
    "        # Tools\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        self.sp_doc = None\n",
    "\n",
    "        # Maps Character Position to Token in Document\n",
    "        # Used to handle differences between different\n",
    "        # pipelines and tools.\n",
    "        self.index_map = None\n",
    "    \n",
    "        # Parsers\n",
    "        self.species = Species(self)\n",
    "        self.traits = TraitKeywords(self)\n",
    "        self.causes = CauseKeywords(self)\n",
    "        self.changes = ChangeKeywords(self)\n",
    "        self.experiment = ExperimentKeywords(self)\n",
    "\n",
    "    def update_doc(self, doc, verbose=False):\n",
    "        self.sp_doc = doc\n",
    "        self.index_map = self.load_index_map()\n",
    "        self.species.update(doc.text, verbose=False)\n",
    "        self.traits.update(verbose=False)\n",
    "        self.causes.update(verbose=False)\n",
    "        self.changes.update(verbose=False)\n",
    "        self.experiment.update(verbose=False)\n",
    "\n",
    "    def update_text(self, text, verbose=False):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.update_doc(self.sp_doc, verbose=verbose)\n",
    "        \n",
    "    def token_at_char(self, char_index):\n",
    "        # SpaCy Doc or Indexing Map Not Found\n",
    "        if not self.sp_doc or not self.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Index into Map\n",
    "        if char_index in self.index_map:\n",
    "            return self.index_map[char_index]\n",
    "\n",
    "        # Looking in Tokens\n",
    "        # Depending on the tokenizer, the character being\n",
    "        # used to find a token may not be the first character\n",
    "        # of the token.\n",
    "        # This shouldn't be needed anymore as I am pairing\n",
    "        # each character in the document to its token.\n",
    "        # for token in self.sp_doc:\n",
    "        #     if char_index >= token.idx and char_index < token.idx + len(token):\n",
    "        #         return token\n",
    "\n",
    "        # There must be a token that corresponds to the\n",
    "        # given character index. If there's not, there's\n",
    "        # an issue.\n",
    "        raise Exception(\"Token Not Found\")\n",
    "        \n",
    "    def load_index_map(self):\n",
    "        # SpaCy Doc Not Found\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Map Character Index to Token\n",
    "        index_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            # char_i0 is the index of the token's starting character.\n",
    "            # char_i1 is the index of the character after the token's ending character.\n",
    "            char_i0 = token.idx\n",
    "            char_i1 = token.idx + len(token)\n",
    "        \n",
    "            for i in range(char_i0, char_i1):\n",
    "                index_map[i] = token\n",
    "            \n",
    "        return index_map\n",
    "\n",
    "    def expand_unit(self, *, il_unit, ir_unit, il_boundary, ir_boundary, allowed_speech=[], allowed_literals=[], disallowed_literals=[], direction='BOTH', verbose=False):\n",
    "        # Move Left\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit > il_boundary:\n",
    "                prev_token = self.sp_doc[il_unit-1]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"il_unit: {il_unit}, il_boundary: {il_boundary}, prev_token: {prev_token}, prev_token.pos_: {prev_token.pos_}\")\n",
    "\n",
    "                in_disallowed = prev_token.lower_ in disallowed_literals\n",
    "                not_in_allowed = prev_token.pos_ not in allowed_speech and prev_token.lower_ not in allowed_literals\n",
    "                \n",
    "                if in_disallowed or not_in_allowed:\n",
    "                    break\n",
    "                \n",
    "                il_unit -= 1\n",
    "\n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit < ir_boundary:\n",
    "                next_token = self.sp_doc[ir_unit+1]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"ir_unit: {ir_unit}, ir_boundary: {ir_boundary}, next_token: {next_token}, next_token.pos_: {next_token.pos_}\")\n",
    "                \n",
    "                in_disallowed = prev_token.lower_ in disallowed_literals\n",
    "                not_in_allowed = next_token.pos_ not in allowed_speech and next_token.lower_ not in allowed_literals\n",
    "                \n",
    "                if in_disallowed or not_in_allowed:\n",
    "                    break\n",
    "                \n",
    "                ir_unit += 1\n",
    "\n",
    "        assert il_unit >= il_boundary and ir_unit <= ir_boundary\n",
    "        \n",
    "        # Expanded Unit\n",
    "        expanded_unit = self.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Expanded Unit: {expanded_unit}\")\n",
    "        \n",
    "        return expanded_unit\n",
    "\n",
    "    def contract_unit(self, *, il_unit, ir_unit, allowed_speech=[], allowed_literals=[], direction='BOTH', verbose=False):\n",
    "        # Move Left\n",
    "        if verbose:\n",
    "            print(\"LEFT\")\n",
    "            \n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit < ir_unit:\n",
    "                curr_token = self.sp_doc[il_unit]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"il_unit: {il_unit}, ir_unit: {ir_unit}, curr_token: {curr_token}, curr_token.pos_: {curr_token.pos_}\")\n",
    "                    \n",
    "                if curr_token.pos_ in allowed_speech or curr_token.lower_ in allowed_literals:\n",
    "                    break\n",
    "                \n",
    "                il_unit += 1\n",
    "\n",
    "        # Move Right\n",
    "        if verbose:\n",
    "            print(\"RIGHT\")\n",
    "        \n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit > il_unit:\n",
    "                curr_token = self.sp_doc[ir_unit]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"il_unit: {il_unit}, ir_unit: {ir_unit}, curr_token: {curr_token}, curr_token.pos_: {curr_token.pos_}\")\n",
    "                \n",
    "                if curr_token.pos_ in allowed_speech or curr_token.lower_ in allowed_literals:\n",
    "                    break\n",
    "                \n",
    "                ir_unit -= 1\n",
    "\n",
    "        contracted_unit = self.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Contracted Unit: {contracted_unit}\")\n",
    "        \n",
    "        return contracted_unit\n",
    "\n",
    "    def find_unit_context(self, *, il_unit, ir_unit, il_boundary, ir_boundary, verbose=False):\n",
    "        # Check if in Punctuation (e.g. (...))\n",
    "        matching_puncts = {\"[\": \"]\", \"(\": \")\", \"-\": \"-\", \"--\": \"--\"}\n",
    "        \n",
    "        opening_puncts = list(matching_puncts.keys())\n",
    "        closing_puncts = list(matching_puncts.values())\n",
    "        \n",
    "        puncts = [*closing_puncts, *opening_puncts]\n",
    "        \n",
    "        i = il_unit\n",
    "        l_punct = None\n",
    "        while i >= il_boundary:\n",
    "            i_token = self.sp_doc[i]\n",
    "            if i_token.text in puncts:\n",
    "                l_punct = i_token\n",
    "                if verbose:\n",
    "                    print(f\"{i_token} in {puncts}\")\n",
    "                break\n",
    "            i -= 1\n",
    "\n",
    "        i = ir_unit\n",
    "        r_punct = None\n",
    "        while i <= ir_boundary:\n",
    "            i_token = self.sp_doc[i]\n",
    "            if i_token.text in puncts:\n",
    "                r_punct = i_token\n",
    "                if verbose:\n",
    "                    print(f\"{i_token} in {puncts}\")\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        # Return Text inside Punctuation\n",
    "        if verbose:\n",
    "            print(f\"L Punct: {l_punct} v. R Punct: {r_punct}\")\n",
    "        \n",
    "        in_punct = l_punct and r_punct and matching_puncts.get(l_punct.lower_, '') == r_punct.text\n",
    "        if in_punct:\n",
    "            if verbose:\n",
    "                print(f\"Matching Punctuation, Return Inner Text\")\n",
    "            return self.sp_doc[l_punct.i:r_punct.i+1]\n",
    "\n",
    "        exclude = []\n",
    "        \n",
    "        # Expand Left\n",
    "        while il_unit > il_boundary:\n",
    "            prev_token = self.sp_doc[il_unit-1]\n",
    "            if verbose:\n",
    "                print(f\"Expand L:\\nToken: {prev_token} and Position: {prev_token.pos_}\")\n",
    "\n",
    "            # Closing Punctuation\n",
    "            if prev_token.lower_ in closing_puncts:\n",
    "                i = il_unit-1\n",
    "                while i >= il_boundary and matching_puncts.get(self.sp_doc[i].lower_, '') != prev_token.lower_:\n",
    "                    exclude.append(self.sp_doc[i])\n",
    "                    i -= 1\n",
    "                exclude.append(self.sp_doc[i])\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Skipped to Token {i}\")\n",
    "                \n",
    "                il_unit = i\n",
    "                continue\n",
    "            else:\n",
    "                if prev_token.pos_ not in [\"ADJ\", \"NOUN\", \"ADP\", \"ADV\", \"PART\", \"PROPN\", \"VERB\", \"PRON\"]:\n",
    "                    if verbose:\n",
    "                        print(f\"Break\")\n",
    "                    break\n",
    "                else:\n",
    "                    il_unit -= 1\n",
    "\n",
    "        # Expand Right\n",
    "        while ir_unit < ir_boundary:\n",
    "            next_token = self.sp_doc[ir_unit+1]\n",
    "            if verbose:\n",
    "                print(f\"Expand R:\\nToken: {next_token} and Position: {next_token.pos_}\")\n",
    "\n",
    "            # Opening Punctuation\n",
    "            if next_token.lower_ in opening_puncts:\n",
    "                if verbose:\n",
    "                    print(f\"Next Token is an Opening Punctuation\")\n",
    "                \n",
    "                i = ir_unit + 1\n",
    "                if verbose:\n",
    "                    print(f\"i: {i}, ir_boundary: {ir_boundary}, self.sp_doc[i]: '{self.sp_doc[i].lower_}', matching punctuation: '{matching_puncts.get(next_token.lower_, '')}'\")\n",
    "                    print(f\"{i <= ir_boundary} and {self.sp_doc[i].lower_ != matching_puncts.get(next_token.lower_, '')}\")\n",
    "                    \n",
    "                while i <= ir_boundary and self.sp_doc[i].lower_ != matching_puncts.get(next_token.lower_, ''):\n",
    "                    if verbose:\n",
    "                        print(f\"\\t'{self.sp_doc[i]}' != '{matching_puncts.get(next_token.lower_, '')}'\")\n",
    "                    exclude.append(self.sp_doc[i])\n",
    "                    i += 1\n",
    "                exclude.append(self.sp_doc[i])\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Skipped to Token {i}\")\n",
    "                \n",
    "                ir_unit = i\n",
    "                continue\n",
    "            else:\n",
    "                if next_token.pos_ not in [\"ADJ\", \"NOUN\", \"ADP\", \"ADV\", \"PART\", \"PROPN\", \"VERB\", \"PRON\"]:\n",
    "                    if verbose:\n",
    "                        print(f\"Break\")\n",
    "                    break\n",
    "                else:\n",
    "                    ir_unit += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Exclude: {exclude}\")\n",
    "\n",
    "        context = [t for t in self.sp_doc[il_unit:ir_unit+1] if t not in exclude]\n",
    "        if verbose:\n",
    "            print(f\"Context: {context}\")\n",
    "        \n",
    "        return context\n",
    "\n",
    "    def score(self, verbose=False):\n",
    "        NUM_CATEGORIES = 4\n",
    "\n",
    "        # Requires the mention of a trait and a cause or change word.\n",
    "        # The cause or change word indicates some variation.\n",
    "        # Index 0 in Array\n",
    "        TRAIT = 0\n",
    "\n",
    "        # Requires the mention of a species and a cause or change word.\n",
    "        # The cause or change word indicates that the species is being\n",
    "        # affected or is affecting something else.\n",
    "        # Index 1 in Array\n",
    "        SPECIES = 1\n",
    "\n",
    "        # Requires a word that has been defined as \"experiment\"-related.\n",
    "        # Index 2 in Array\n",
    "        EXPERIMENT = 2\n",
    "\n",
    "        # Requires the mention of several species (more or less).\n",
    "        # Index 3 in Array\n",
    "        INTERACTION = 3\n",
    "\n",
    "        # Max # of Points of Category per Sentence (MPC)\n",
    "        # A sentence collects points from its categories.\n",
    "        # For example, a sentence could get a maximum of 2 points from one category\n",
    "        # and a maximum of 1 point from another. The MPC determines the maximum number\n",
    "        # of points a category could contribute to a sentence. To have a range of [0, 1]\n",
    "        # the maximum number of points, across categories, when added should be 1.\n",
    "        MPC = [0] * NUM_CATEGORIES\n",
    "        MPC[TRAIT] = 0.1\n",
    "        MPC[SPECIES] = 0.3\n",
    "        MPC[EXPERIMENT] = 0.3\n",
    "        MPC[INTERACTION] = 0.3\n",
    "\n",
    "        assert np.sum(MPC) == 1\n",
    "        \n",
    "        # Points per Instance of Category (PIC)\n",
    "        # Each token is evaluated to check whether a category\n",
    "        # can be given points. The number of points given, if\n",
    "        # the token is determined to be satisfactory, is the PIC.\n",
    "        # The PIC is less than or equal to the MPC for the corresponding\n",
    "        # category. The idea behind the PIC and MPC is similar to how\n",
    "        # sets work in tennis: you're not immediately awarded the full points\n",
    "        # for the set (MPC) if your opponent fails to return the ball,\n",
    "        # instead you're given a smaller # of points (PIC) that allow you to\n",
    "        # incrementally win the set (category).\n",
    "        PIC = [0] * NUM_CATEGORIES\n",
    "        PIC[TRAIT] = MPC[TRAIT]*1.0\n",
    "        PIC[SPECIES] = MPC[SPECIES]/3.0\n",
    "        PIC[EXPERIMENT] = MPC[EXPERIMENT]/1.0\n",
    "        PIC[INTERACTION] = MPC[INTERACTION]/3.0\n",
    "\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            assert PIC[i] <= MPC[i]\n",
    "\n",
    "        # Category Weights (CW)\n",
    "        # It may be helpful to weigh a certain category's fraction of total\n",
    "        # points more or less than another's. Thus, at the end, we'll take a\n",
    "        # weighted average of the category's FTP. The weights must add up to 1.\n",
    "        CW = [0] * NUM_CATEGORIES\n",
    "        CW[TRAIT] = 0.25\n",
    "        CW[SPECIES] = 0.25\n",
    "        CW[EXPERIMENT] = 0.25\n",
    "        CW[INTERACTION] = 0.25\n",
    "\n",
    "        assert np.sum(MPC) == 1\n",
    "\n",
    "        # Points\n",
    "        points = [0] * NUM_CATEGORIES\n",
    "\n",
    "        # Extracted Information\n",
    "        cause_tokens = self.causes.tokens\n",
    "        change_tokens = self.changes.tokens\n",
    "        trait_tokens = self.traits.tokens\n",
    "        species_tokens = [self.sp_doc[span.start] for span in self.species.spans]\n",
    "        experiment_tokens = self.experiment.tokens\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Cause Tokens: {[(t.i, t) for t in self.causes.tokens]}\")\n",
    "            print(f\"Change Tokens: {[(t.i, t) for t in self.changes.tokens]}\")\n",
    "            print(f\"Trait Tokens: {[(t.i, t) for t in self.traits.tokens]}\")\n",
    "            print(f\"Species Tokens: {[(t.i, t) for t in self.species.tokens]}\")\n",
    "            print(f\"Reduced Species Tokens: {[(t.i, t) for t in species_tokens]}\")\n",
    "            print(f\"Experiment Tokens: {[(t.i, t) for t in self.experiment.tokens]}\")\n",
    "         \n",
    "        # This is used to ensure that at least three species\n",
    "        # are mentioned.\n",
    "        seen_species = {}\n",
    "\n",
    "        for sent in self.sp_doc.sents:\n",
    "            # This contains the number of points\n",
    "            # each category has accumulated in the sentence.\n",
    "            curr_points = [0] * NUM_CATEGORIES\n",
    "\n",
    "            # Contains the tokens in the sentence.\n",
    "            sent_tokens = [token for token in sent]\n",
    "\n",
    "            # This is used for the species (must have a nearby cause and/or\n",
    "            # change word).\n",
    "            sent_cause_tokens = set(sent_tokens).intersection(cause_tokens)\n",
    "            sent_change_tokens = set(sent_tokens).intersection(change_tokens)\n",
    "\n",
    "            # We don't want to visit the same species more than one\n",
    "            # in the same sentence as to avoid redundant points.\n",
    "            sent_seen_species = []\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\n\\nSentence: {sent}\\n\")\n",
    "                print(f\"Sentence Tokens: {[(t.i, t) for t in sent]}\")\n",
    "                print(f\"Sentence Cause Tokens: {sent_cause_tokens}\")\n",
    "                print(f\"Sentence Change Tokens: {sent_change_tokens}\\n\")\n",
    "            \n",
    "            for token in sent_tokens:\n",
    "                # If each category has reached their maximum number of points,\n",
    "                # we can end the loop early.\n",
    "                all_maxed = True\n",
    "                for i in range(NUM_CATEGORIES):\n",
    "                    if curr_points[i] < MPC[i]:\n",
    "                        all_maxed = False\n",
    "\n",
    "                if all_maxed:\n",
    "                    break\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Token ({token.pos_}): '{token}'\")\n",
    "\n",
    "                # TRAIT CATEGORY\n",
    "                if curr_points[TRAIT] < MPC[TRAIT] and token in trait_tokens:\n",
    "                    # To get points in the trait category, there must \n",
    "                    # be (1) a trait; and (2) a change or cause in the token's\n",
    "                    # context.\n",
    "                    token_context = set(self.find_unit_context(il_unit=token.i, ir_unit=token.i, il_boundary=token.sent.start, ir_boundary=token.sent.end-1, verbose=verbose))\n",
    "                    cause_tokens_in_context = set(sent_cause_tokens).intersection(token_context)\n",
    "                    change_tokens_in_context = set(sent_change_tokens).intersection(token_context)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"Token Context: {token_context}\")\n",
    "                        print(f\"Cause Tokens in Context: {cause_tokens_in_context}\")\n",
    "                        print(f\"Change Tokens in Context: {change_tokens_in_context}\")\n",
    "\n",
    "                    if cause_tokens_in_context or change_tokens_in_context:\n",
    "                        curr_points[TRAIT] += PIC[TRAIT]\n",
    "\n",
    "                        if verbose:\n",
    "                            print(f\"Added {PIC[TRAIT]} Points to Trait\")\n",
    "\n",
    "                # EXPERIMENT CATEGORY\n",
    "                if curr_points[EXPERIMENT] < MPC[EXPERIMENT] and token in experiment_tokens:\n",
    "                    curr_points[EXPERIMENT] += PIC[EXPERIMENT]\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"Added {PIC[EXPERIMENT]} Points to Experiment\")\n",
    "\n",
    "                # SPECIES CATEGORY\n",
    "                if token in species_tokens:\n",
    "                    # Find Species Span\n",
    "                    species_span = self.species.span_at_token(token)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"Species Span: {species_span}\")\n",
    "                        print(f\"Seen Species:\\n{seen_species}\")                 \n",
    "\n",
    "                    # Updating Seen Species (in Entire Text)\n",
    "                    past_visits = 0\n",
    "                    for seen_species_span in seen_species.keys():\n",
    "                        if self.species.same_species(species_span, seen_species_span, verbose=verbose):\n",
    "                            past_visits = seen_species[seen_species_span]\n",
    "                            if verbose:\n",
    "                                print(f\"\\t'{species_span}' == '{seen_species_span}'\")\n",
    "                                print(f\"\\tNumber of Visits: {past_visits}\")\n",
    "                            seen_species[seen_species_span] += 1\n",
    "                            break\n",
    "\n",
    "                    if not past_visits:\n",
    "                        seen_species[species_span] = 1\n",
    "                    is_new_species = past_visits == 0\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"Is New Species: {is_new_species}\")\n",
    "                        print(f\"Seen Species Updated:\\n{seen_species}\")\n",
    "                        print(\"Checking Seen Species in Sentence\")\n",
    "\n",
    "                    # Checking Seen Species (in Sentence)\n",
    "                    # We only add points if it's a species that has not been seen\n",
    "                    # in the sentence. This is to avoid redundant points. \n",
    "                    # Also, if it species has not been seen at all (is_new_species),\n",
    "                    # then it cannot be a redundant species (we couldn't have seen it in the sentence\n",
    "                    # either).\n",
    "                    redundant_species = False\n",
    "\n",
    "                    if not is_new_species:\n",
    "                        for seen_species_span in sent_seen_species:\n",
    "                            if self.species.same_species(species_span, seen_species_span, verbose=verbose):\n",
    "                                redundant_species = True\n",
    "                                break\n",
    "                    \n",
    "                    sent_seen_species.append(species_span)\n",
    "                    if redundant_species:\n",
    "                        continue\n",
    "\n",
    "                    # INTERACTION CATEGORY\n",
    "                    # It is helpful to have this category here because (if we've reached here)\n",
    "                    # we're dealing with a new species in the sentence.\n",
    "                    if curr_points[INTERACTION] < MPC[INTERACTION]:\n",
    "                        curr_points[INTERACTION] += PIC[INTERACTION]\n",
    "\n",
    "                        if verbose:\n",
    "                            print(f\"Added {PIC[INTERACTION]} Points to Interaction\")\n",
    "                        \n",
    "                    if curr_points[SPECIES] < MPC[SPECIES]:\n",
    "                        # To get points in the species category, there must be \n",
    "                        # (1) a species; and (2) a change or cause in the phrase\n",
    "                        # (or clause) that the token is a part of.\n",
    "                        token_context = set(self.find_unit_context(il_unit=token.i, ir_unit=token.i, il_boundary=token.sent.start, ir_boundary=token.sent.end-1, verbose=verbose))\n",
    "                        cause_tokens_in_context = set(sent_cause_tokens).intersection(token_context)\n",
    "                        change_tokens_in_context = set(sent_change_tokens).intersection(token_context)\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"Token Context: {token_context}\")\n",
    "                            print(f\"Cause Tokens in Context: {cause_tokens_in_context}\")\n",
    "                            print(f\"Change Tokens in Context: {change_tokens_in_context}\")\n",
    "                        \n",
    "                        if cause_tokens_in_context or change_tokens_in_context:\n",
    "                            curr_points[SPECIES] += PIC[SPECIES]\n",
    "\n",
    "                            if verbose:\n",
    "                                print(f\"Added {PIC[SPECIES]} Points to Species\")\n",
    "         \n",
    "            # SENTENCE DONE\n",
    "            # Add Sentence Points to Total Points\n",
    "            for category in [TRAIT, SPECIES, EXPERIMENT, INTERACTION]:\n",
    "                points[category] += min(curr_points[category], MPC[category])\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Overall Points: {points}\")\n",
    "\n",
    "        # Enforcing 3 or More Species\n",
    "        if verbose:\n",
    "            print(f\"Seen Species: {seen_species}\")\n",
    "            \n",
    "        if len(seen_species) < 3:\n",
    "            return 0\n",
    "        \n",
    "        # Calculating Score\n",
    "        if verbose:\n",
    "            print(f\"Points: {points}\")\n",
    "            \n",
    "        NUM_SENTENCES = len(list(self.sp_doc.sents))\n",
    "\n",
    "        score = 0\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            FTP = points[i] / (MPC[i] * NUM_SENTENCES)\n",
    "            if verbose:\n",
    "                print(f\"Category 1's FTP: {FTP}\")\n",
    "            score += FTP * CW[i]\n",
    "            \n",
    "        assert 0.0 <= score <= 1.0\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Score: {score}\")\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10821c-4a15-47e6-b3d2-f49b7ef3eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../Datasets/Baseline-1.csv\")\n",
    "text = df.Abstract[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619e40a-251d-46d0-a451-d6e3c8fa30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "main = Main()\n",
    "for abstract in [df.Abstract.to_list()[1]]:\n",
    "    print(abstract)\n",
    "    main.update_text(abstract, verbose=False)\n",
    "    score = main.score(verbose=True)\n",
    "    print(score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153d990-7dc2-4241-9cc9-42355bd59c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in main.sp_doc:\n",
    "    if token.lower_ == \"hypothesized\":\n",
    "        print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa858af3-e2c8-4dcc-a7de-4e9f769f310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in main.sp_doc:\n",
    "    if token.pos_ not in [\"NOUN\", \"PROPN\", \"ADJ\"]:\n",
    "        continue\n",
    "    print(f\"Token ({token.pos_}): {token}\\n\\t{'Species' if token in main.species.tokens else 'Not Species'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c7ea427-25ee-4f83-9a97-9e1070c05221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pests NOUN\n",
      "Mediterranean PROPN\n",
      "species NOUN\n",
      "Crematogaster PROPN\n",
      "Tapinoma PROPN\n",
      "Ceratitis PROPN\n",
      "Diptera PROPN\n",
      "Tephritidae PROPN\n",
      "damaging ADJ\n",
      "ant NOUN\n",
      "species NOUN\n",
      "ant NOUN\n",
      "medflies NOUN\n",
      "pupae NOUN\n",
      "ant NOUN\n",
      "species NOUN\n",
      "significantly ADV\n",
      "medflies NOUN\n",
      "ant NOUN\n",
      "pupae NOUN\n",
      "ants NOUN\n",
      "medfly NOUN\n",
      "indirect ADJ\n",
      "ant NOUN\n"
     ]
    }
   ],
   "source": [
    "species_tokens = [main.sp_doc[span.start] for span in main.species.spans]\n",
    "for span in species_tokens:\n",
    "    print(span, span.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb1d2d13-f691-4850-b3ba-7893cd47364f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.species.tn_doc.ents[0][0].tag_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078bf01-5efe-49b5-9ec9-4a8ccf1d47dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
