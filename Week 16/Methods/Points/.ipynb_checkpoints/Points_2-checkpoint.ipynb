{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecf5be-df89-4d1a-b75f-d92a5c5fff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trait-Mediated Interaction Modification\n",
    "# Empirical:\n",
    "# You could look for words like \"hypothesis\", \"experiment\", \"found\", and \"discovered\". That may point\n",
    "# towards there being an experiment in the paper. There are also words like \"control group\", \"compared\",\n",
    "# \"findings\", \"results\", \"study\", and more.\n",
    "# Qualitative vs. Quantitative:\n",
    "# To infer whether something is quantitative, you could look for numeric tokens and units.\n",
    "# However, you can only do so much with the abstract. Therefore, this is likely not good enough.\n",
    "# Yet, you could still take advantage of words like \"fewer\" and \"increased\" to show that there is a change.\n",
    "# However, this would be more suited for the above category.\n",
    "# Traits:\n",
    "# There is no NLP tool for traits that I can use or create so I think that I could instead use keywords.\n",
    "# For example, \"snail feeding rates\" is a trait. You may be able to spot this by looking for a word like\n",
    "# \"rate\". You'd expand that word to include \"snail feeding rates\". As \"snail\" is a species you can infer\n",
    "# that \"rates\" is a trait. I would be more decisive and use a dependency parser to ensure that the trait\n",
    "# is a property of the species (like before). However, with all the cases that may exist, I think checking\n",
    "# to see whether a species can be found by traveling back and/or forward without finding certain tokens could\n",
    "# work well enough.\n",
    "# 3 Species or More:\n",
    "# This is simple. However, I think using a dictionary and TaxoNerd would be beneficial (for higher accuracy).\n",
    "# To handle the potential differences in tokenization, character offsets should be used.\n",
    "# Standardization:\n",
    "# There is a lot of variance in the scores. To squash this issue, I think that we could assign each sentence\n",
    "# a value from 0 to 1. We would add these values and divide by the number of sentences. This would result in\n",
    "# a number that is also from 0 to 1. However, there are categories that we would like to inspect. So, we must\n",
    "# create an overall score in the interval from [0, 1] while also scoring each category. Well, for each sentence\n",
    "# we could add a point for each category that is observed. The sentence would receive said score divided by the\n",
    "# number of categories. At the end, we add up all the sentence scores and divide by the number of sentences.\n",
    "# The aggregate score for each category would also be divided by the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ccb94-4c7d-49b1-a727-f6e25b021357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from spacy.language import Language\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "%run -i \"../utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe27a6-00d1-47e2-b316-09b844dcf25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Help:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        # Zero Plurals\n",
    "        # The singular and plural versions of the words below are the same.\n",
    "        self.zero_plurals = [\"species\", \"deer\", \"fish\", \"moose\", \"sheep\", \"swine\", \"buffalo\"]\n",
    "\n",
    "    def remove_extra_spaces(self, string):\n",
    "        # Remove Duplicate Spaces\n",
    "        string = re.sub(r\"\\s+\", \" \", string)\n",
    "        # Remove Spaces Before Punctuation\n",
    "        string = re.sub(r\"\\s+([?.!,])\", r\"\\1\", string)\n",
    "        # Remove Outside Spaces\n",
    "        return string.strip()\n",
    "\n",
    "    def remove_outer_non_alnum(self, string):\n",
    "        while string:\n",
    "            start_len = len(string)\n",
    "            # Remove Leading Non-Alphanumeric Character\n",
    "            if string and not string[0].isalnum():\n",
    "                string = string[1:]\n",
    "            # Remove Trailing Non-Alphanumeric Character\n",
    "            if string and not string[-1].isalnum():\n",
    "                string = string[:-1]\n",
    "            # No Changes Made\n",
    "            if start_len == len(string):\n",
    "                break\n",
    "        return string\n",
    "\n",
    "    def singularize(self, string):\n",
    "        # The string to singularize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        # If the last word in the string is a zero plural,\n",
    "        # there's no changes to make. For example, \"red-\n",
    "        # sheep\" is already singular.\n",
    "        if words[-1] in self.zero_plurals:\n",
    "            return string\n",
    "        \n",
    "        singulars = []\n",
    "\n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to error), we need to\n",
    "        # handle them all.\n",
    "        singular_forms = self.singular_form(words[-1])\n",
    "        for singular_form in singular_forms:\n",
    "            singular = self.remove_spaces(\" \".join([*words[:-1], singular_form]))\n",
    "            singulars.append(singular)\n",
    "            \n",
    "        return singulars\n",
    "\n",
    "    def singular_form(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Change -ies to -y\n",
    "        if re.fullmatch(r\".*ies$\", string):\n",
    "            versions.append(f'{string[:-3]}y')\n",
    "            return versions\n",
    "\n",
    "        # Change -ves to -f and -fe\n",
    "        if re.fullmatch(r\".*ves$\", string):\n",
    "            versions.append(f'{string[:-3]}f')\n",
    "            versions.append(f'{string[:-3]}fe')\n",
    "            return versions\n",
    "\n",
    "        # Remove -es \n",
    "        if re.fullmatch(r\".*es$\", string):\n",
    "            versions.append(f'{string[:-2]}')\n",
    "            return versions\n",
    "\n",
    "        # Change -i to -us\n",
    "        if re.fullmatch(r\".*i$\", string):\n",
    "            versions.append(f'{string[:-1]}us')\n",
    "            return versions\n",
    "\n",
    "        # Remove -s\n",
    "        if re.fullmatch(r\".*s$\", string):\n",
    "            versions.append(f'{string[:-1]}')\n",
    "            return versions\n",
    "\n",
    "        return versions\n",
    "\n",
    "    def pluralize(self, string):\n",
    "        # The string to pluralize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        if words[-1] in self.zero_plurals:\n",
    "            return string\n",
    "\n",
    "        plurals = []\n",
    "        \n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to error), we need to\n",
    "        # handle them all.\n",
    "        plural_forms = self.plural_form(words[-1])\n",
    "        for plural_form in plural_forms:\n",
    "            plural = self.remove_spaces(\" \".join([*words[:-1], plural_form]))\n",
    "            plurals.append(plural)\n",
    "            \n",
    "        return plurals\n",
    "        \n",
    "    def plural_form(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Words that end with -us often have\n",
    "        # two different plural versions: -es and -i.\n",
    "        # For example, the plural version of cactus \n",
    "        # can be cactuses or cacti.\n",
    "        if re.fullmatch(r\".*us$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            versions.append(f'{string[:-2]}i')\n",
    "            return versions\n",
    "\n",
    "        # The -es ending is added to the words below.\n",
    "        if re.fullmatch(r\".*(s|sh|ch|x|z)$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # Words that end with a consonant followed by 'y'\n",
    "        # are made plural by replacing the 'y' with -ies.\n",
    "        # For example, the plural version of canary is\n",
    "        # canaries.\n",
    "        if re.fullmatch(r\".*([^aeiou])(y)$\", string):\n",
    "            versions.append(f'{string[:-1]}ies')\n",
    "            return versions\n",
    "            \n",
    "        # The plural version of words ending with -f\n",
    "        # and -fe aren't clear. To be safe, I will add\n",
    "        # both versions.\n",
    "        if (re.fullmatch(r\".*(f)(e?)$\", string) and not re.fullmatch(r\".*ff$\", string)):\n",
    "            last_clean = re.sub(r\"(f)(e?)$\", \"\", string)\n",
    "            versions.append(f'{last_clean}fs')\n",
    "            versions.append(f'{last_clean}ves')\n",
    "            return versions\n",
    "\n",
    "        # People add -s or -es to words that end with 'o'.\n",
    "        # To be safe, both versions are added.\n",
    "        if re.fullmatch(r\".*([^aeiou])o$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # Default\n",
    "        versions.append(f'{string}s')\n",
    "        return versions\n",
    "\n",
    "    def expand_unit(self, *, il_unit, ir_unit, il_boundary, ir_boundary, speech=[], literals=[], include=False, direction='BOTH', verbose=False):\n",
    "        # Move Left\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            # The indices are inclusive, therefore, when \n",
    "            # the condition fails, il_unit will be equal\n",
    "            # to il_boundary.\n",
    "            while il_unit > il_boundary:\n",
    "                # We assume that the current token is allowed,\n",
    "                # and look to the token to the left.\n",
    "                l_token = self.sp_doc[il_unit-1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = l_token.pos_ in speech or l_token.lower_ in literals\n",
    "                if include and not in_set:\n",
    "                    break\n",
    "                if not include and in_set:\n",
    "                    break\n",
    "                \n",
    "                # Else, the left token is valid, and\n",
    "                # we continue to expand.\n",
    "                il_unit -= 1\n",
    "\n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            # Likewise, when the condition fails,\n",
    "            # ir_unit will be equal to the ir_boundary.\n",
    "            # The ir_boundary is also inclusive.\n",
    "            while ir_unit < ir_boundary:\n",
    "                # Assuming that the current token is valid,\n",
    "                # we look to the right to see if we can\n",
    "                # expand.\n",
    "                r_token = self.sp_doc[ir_unit+1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = r_token.pos_ in speech or r_token.lower_ in literals\n",
    "                if include and not in_set:\n",
    "                    break\n",
    "                if not include and in_set:\n",
    "                    break\n",
    "\n",
    "                # Else, the token is valid and\n",
    "                # we continue.\n",
    "                ir_unit += 1\n",
    "\n",
    "        assert il_unit >= il_boundary and ir_unit <= ir_boundary\n",
    "        expanded_unit = self.sp_doc[il_unit:ir_unit+1]\n",
    "        return expanded_unit\n",
    "\n",
    "    def contract_unit(self, *, il_unit, ir_unit, speech=[], literals=[], include=False, direction='BOTH', verbose=False):\n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit < ir_unit:\n",
    "                # We must check if the current token\n",
    "                # is not allowed. If it's not allowed,\n",
    "                # we contract (remove).\n",
    "                token = self.sp_doc[il_unit]\n",
    "\n",
    "                # The token is invalid, thus we stop\n",
    "                # contracting.\n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include and not in_set:\n",
    "                    break\n",
    "                if not include and in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid, thus we continue.\n",
    "                il_unit += 1\n",
    "\n",
    "        # Move Left      \n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit > il_unit:\n",
    "                token = self.sp_doc[ir_unit]\n",
    "\n",
    "                # The token is invalid and we\n",
    "                # stop contracting.\n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include and not in_set:\n",
    "                    break\n",
    "                if not include and in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid and we continue.\n",
    "                ir_unit -= 1\n",
    "\n",
    "        assert il_unit <= ir_unit\n",
    "        contracted_unit = self.sp_doc[il_unit:ir_unit+1]\n",
    "        return contracted_unit\n",
    "\n",
    "    def find_unit_context(self, *, il_unit, ir_unit, il_boundary, ir_boundary, verbose=False):\n",
    "        # Caveat: Parentheticals\n",
    "        # The context of a unit inside of parentheses should not\n",
    "        # go farther than the boundaries of those parentheses.\n",
    "        # However, we need to manually determine whether the unit\n",
    "        # is in parentheses (or any set of the matching symbols\n",
    "        # below).\n",
    "        matching_puncts = {\n",
    "            \"[\": \"]\", \n",
    "            \"(\": \")\", \n",
    "            \"-\": \"-\", \n",
    "            \"--\": \"--\"\n",
    "            \"{\": \"}\"\n",
    "        }\n",
    "        \n",
    "        # The opening symbols for group punctuation.\n",
    "        opening_puncts = list(matching_puncts.keys())\n",
    "\n",
    "        # The closing symbols for group punctuation.\n",
    "        closing_puncts = list(matching_puncts.values())\n",
    "\n",
    "        # Both the opening and closing symbols above.\n",
    "        puncts = [*closing_puncts, *opening_puncts]\n",
    "\n",
    "        # Look for Group Punctuation on the Left\n",
    "        i = il_unit\n",
    "        l_punct = None\n",
    "        while i >= il_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.text in puncts:\n",
    "                l_punct = token\n",
    "                break\n",
    "            i -= 1\n",
    "\n",
    "        # Look for Group Punctuation on the Right\n",
    "        i = ir_unit\n",
    "        r_punct = None\n",
    "        while i <= ir_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.text in puncts:\n",
    "                r_punct = token\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        # If there's a group punctuation on the left\n",
    "        # and right, and they match each other (e.g. '(' and ')'),\n",
    "        # we return the text between the punctuations.\n",
    "        parenthetical = l_punct and r_punct and matching_puncts.get(l_punct.lower_, '') == r_punct.text\n",
    "        if parenthetical:\n",
    "            return self.main.sp_doc[l_punct.i:r_punct.i+1]\n",
    "\n",
    "        # As the unit is not a parenthetical, we will expand\n",
    "        # outwards until we run into a stopping token. The exclude\n",
    "        # list contains tokens that should be excluded from the\n",
    "        # context. Currently, it will contain any parentheticals\n",
    "        # that we run into.\n",
    "        exclude = []\n",
    "\n",
    "        # If a token's POS falls into these categories, we will\n",
    "        # continue. If not, we stop expanding.\n",
    "        speech = [\"ADJ\", \"NOUN\", \"ADP\", \"ADV\", \"PART\", \"PROPN\", \"VERB\", \"PRON\"]\n",
    "        \n",
    "        # Expand Left\n",
    "        while il_unit > il_boundary:\n",
    "            # Assuming that the current token is fine,\n",
    "            # we look to the left.\n",
    "            l_token = self.main.sp_doc[il_unit-1]\n",
    "\n",
    "            # If it's a closing punctuation (e.g. ')', ']'),\n",
    "            # we need to skip over whatever is contained in\n",
    "            # that punctuation.\n",
    "            if l_token.lower_ in closing_puncts:\n",
    "                i = il_unit - 1\n",
    "                # We continue until we reach the boundary or we\n",
    "                # find the matching opening punctuation.\n",
    "                token = self.main.sp_doc[i]\n",
    "                while i >= il_boundary and matching_puncts.get(token.lower_, '') != l_token.lower_:\n",
    "                    exclude.append(token)\n",
    "                    i -= 1\n",
    "                exclude.append(token)\n",
    "\n",
    "                # After we've gone past the parenthetical,\n",
    "                # we can jump to the next position.\n",
    "                il_unit = i\n",
    "                continue\n",
    "            # If it's not a closing punctuation, we check\n",
    "            # whether it's a stopping token\n",
    "            else:\n",
    "                if l_token.pos_ not in speech:\n",
    "                    break\n",
    "                else:\n",
    "                    il_unit -= 1\n",
    "\n",
    "        # Expand Right\n",
    "        while ir_unit < ir_boundary:\n",
    "            # We're checking the token to the right\n",
    "            # to see if we can expand or not.\n",
    "            r_token = self.main.sp_doc[ir_unit+1]\n",
    "            \n",
    "            # If the token to the right is an opening\n",
    "            # punctuation (e.g. '(', '['), we must skip\n",
    "            # it, the parenthetical inside, and the\n",
    "            # closing punctuation.\n",
    "            if r_token.lower_ in opening_puncts:\n",
    "                i = ir_unit + 1\n",
    "                token = self.main.sp_doc[i]\n",
    "                while i <= ir_boundary and token.lower_ != matching_puncts.get(r_token.lower_, ''):\n",
    "                    exclude.append(token)\n",
    "                    i += 1\n",
    "                exclude.append(token)\n",
    "\n",
    "                # Skip\n",
    "                ir_unit = i\n",
    "                continue\n",
    "            # If it's not an opening punctuation, we check\n",
    "            # whether we can continue expanding.\n",
    "            else:\n",
    "                if r_token.pos_ not in speech:\n",
    "                    break\n",
    "                else:\n",
    "                    ir_unit += 1\n",
    "\n",
    "        # We remove the excluded tokens\n",
    "        # and return the context.\n",
    "        context = [t for t in self.main.sp_doc[il_unit:ir_unit+1] if t not in exclude]\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26736310-0964-4184-a9c1-f6b0b38d7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the Dictionary\n",
    "@Language.component(\"lower_case_lemmas\")\n",
    "def lower_case_lemmas(doc) :\n",
    "    for token in doc :\n",
    "        token.lemma_ = token.lemma_.lower()\n",
    "    return doc\n",
    "\n",
    "class Species:\n",
    "    def __init__(self, main):\n",
    "        # Tools\n",
    "        self.main = main\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\"])\n",
    "        self.tn_nlp.add_pipe(\"lower_case_lemmas\", after=\"lemmatizer\")\n",
    "        self.tn_doc = None\n",
    "        \n",
    "        # Contains any spans that have been identified\n",
    "        # as a species.\n",
    "        self.spans = None\n",
    "        \n",
    "        # Contains any tokens that have been identified\n",
    "        # as a species or being a part of a species.\n",
    "        self.tokens = None\n",
    "        \n",
    "        # Used to quickly access the span that a token\n",
    "        # belongs to.\n",
    "        self.token_to_span = None\n",
    "        \n",
    "        # Maps a string to an array of strings wherein\n",
    "        # the strings involved in the key-value pair \n",
    "        # have been identified as an alternate name of each other.\n",
    "        self.alternate_names = None\n",
    "        \n",
    "        # Used to increase TaxoNERD's accuracy.\n",
    "        self.dictionary = None\n",
    "        self.load_dictionary()\n",
    "\n",
    "    def load_dictionary(self):\n",
    "        self.dictionary = [\"juvenile\", \"adult\", \"prey\", \"predator\", \"species\"]\n",
    "        # df = pd.read_csv(\"VernacularNames.csv\")\n",
    "        # self.dictionary += df.VernacularName.to_list()\n",
    "\n",
    "        patterns = []\n",
    "        for name in self.dictionary:\n",
    "            doc = self.tn_nlp(name)\n",
    "            patterns.append({\"label\": \"LIVB\", \"pattern\": [{\"LEMMA\": token.lemma_} for token in doc]})\n",
    "        ruler = self.tn_nlp.add_pipe(\"entity_ruler\")\n",
    "        ruler.add_patterns(patterns)\n",
    "        \n",
    "    def update(self, text, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        self.spans, self.tokens, self.token_to_span, self.alternate_names = self.load_species(verbose=verbose)\n",
    "\n",
    "    def load_species(self, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # These three contain the species that have been\n",
    "        # identified in the text. Tokens that aren't adjectives,\n",
    "        # nouns, or proper nouns will be stripped.\n",
    "        spans = []\n",
    "        tokens = []\n",
    "        token_to_span = {}\n",
    "\n",
    "        # It's useful to know if a different name refers to a\n",
    "        # species we have already seen. For example, in\n",
    "        # \"predatory crab (Carcinus maenas)\", \"predatory crab\"\n",
    "        # is an alternative name for \"Carcinus maenas\" and\n",
    "        # vice versa. This is used so that the species can be\n",
    "        # properly tracked and redundant points are less\n",
    "        # likely to be given.\n",
    "        alternate_names = {}\n",
    "\n",
    "        # We convert the spans that TaxoNerd has recognized\n",
    "        # to spans under a different parent document. This is\n",
    "        # because we're largely using said parent document and\n",
    "        # there is more functionality in that parent document.\n",
    "        species_spans = []\n",
    "        for tn_species_span in self.tn_doc.ents:\n",
    "            char_i0 = self.tn_doc[span.start].idx\n",
    "            char_i1 = self.tn_doc[span.end-1].idx\n",
    "\n",
    "            sp_token_i0 = self.main.token_at_char(char_i0).i\n",
    "            sp_token_i1 = self.main.token_at_char(char_i1).i\n",
    "\n",
    "            sp_species_span = self.main.sp_doc[sp_token_i0:sp_token_i1+1]\n",
    "            species_spans.append(sp_species_span)\n",
    "\n",
    "            # Although they have different parent documents,\n",
    "            # they should still have the same text.\n",
    "            assert sp_species_span.text.lower() == tn_species.span.text.lower()\n",
    "\n",
    "        # TaxoNerd sometimes recognizes one instance of a species\n",
    "        # and fails to recognize it elsewhere. To fix this, I'll\n",
    "        # search the text for all the species that TaxoNerd sees.\n",
    "        # This should resolve that issue. To make this more robust,\n",
    "        # I'll include the singular and plural versions of the\n",
    "        # recognized species. Furthermore, the species being used\n",
    "        # to search for other instances of species in the text will\n",
    "        # be called search_species.\n",
    "        search_species = []\n",
    "\n",
    "        for species_span in species_span:\n",
    "            species_text = species_span.text.lower()\n",
    "            species_text = self.main.help.remove_outer_non_alnum(species_text)\n",
    "\n",
    "            search_species.append(species_text)\n",
    "\n",
    "            # Add Singular and/or Plural Version\n",
    "            if species_span[-1].pos_ == \"NOUN\":\n",
    "                # Singular\n",
    "                if species_span[-1].tag_ == \"NN\":\n",
    "                    singular_species = singularize(species_text)\n",
    "                    search_species.extend(singular_species)\n",
    "                # Plural\n",
    "                if species_span[-1].tag_ == \"NNS\":\n",
    "                    plural_species = pluralize(species_text)\n",
    "                    search_species.extend(plural_species)\n",
    "\n",
    "        # Now, we have the species to search for in the text.\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "        search_species = list(set(search_species))\n",
    "        \n",
    "        for species in search_species:\n",
    "            matches = re.finditer(re.escape(species), text)\n",
    "            \n",
    "            for char_i0, char_i1 in [(match.start(), match.end()) for match in matches]:\n",
    "                # The full word must match, not just a substring inside of it.\n",
    "                # So, if the species we're looking for is \"ant\", only \"ant\"\n",
    "                # will match -- not \"pants\" or \"antebellum\". Therefore, the\n",
    "                # characters to the left and right of the matched string must be\n",
    "                # non-alphanumeric. \n",
    "                l_char_is_letter = char_i0 > 0 and text[char_i0-1].isalpha()\n",
    "                r_char_is_letter = char_i1 < len(text) - 1 and text[char_i1+1].isalpha()\n",
    "                \n",
    "                if l_char_is_letter or r_char_is_letter:\n",
    "                    continue\n",
    "                    \n",
    "                sp_li = self.main.token_at_char(char_i0).i\n",
    "                sp_ri = self.main.token_at_char(char_i1-1).i\n",
    "\n",
    "                # This is the matched substring (which would be\n",
    "                # a species) as a span in the parent document.\n",
    "                species_span = self.main.sp_doc[sp_li:sp_ri+1]\n",
    "                \n",
    "                # Expand Species\n",
    "                # Let's say there's a word like \"squirrel\". That's a bit ambiguous. \n",
    "                # Is it a brown squirrel, a bonobo? If the species is possibly missing\n",
    "                # information (like an adjective to the left of it), we should expand\n",
    "                # in order to get a full picture of the species.\n",
    "                unclear_1 = len(species_span) == 1 and species_span[0].pos_ == \"NOUN\"\n",
    "                unclear_2 = species_span.start > 0 and self.main.sp_doc[species_span.start-1].pos_ in [\"ADJ\"]\n",
    "                \n",
    "                if unclear_1 or unclear_2:\n",
    "                    species_span = self.main.help.expand_unit(\n",
    "                        il_unit=species_span.start, \n",
    "                        ir_unit=species_span.end-1,\n",
    "                        il_boundary=0,\n",
    "                        ir_boundary=len(self.main.sp_doc),\n",
    "                        speech=[\"ADJ\", \"PROPN\"],\n",
    "                        literals=[\"-\"],\n",
    "                        direction=\"LEFT\",\n",
    "                        verbose=verbose\n",
    "                    )\n",
    "                \n",
    "                # Remove Outer Symbols\n",
    "                # There are times where a species is identified with a parenthesis\n",
    "                # nearby. Here, we remove that parenthesis (and any other symbols).\n",
    "                species_span = self.main.help.contract_unit(\n",
    "                    il_unit=species_span.start, \n",
    "                    ir_unit=species_span.end-1, \n",
    "                    speech=[\"PUNCT\", \"SYM\"],\n",
    "                    include=False,\n",
    "                    verbose=verbose\n",
    "                )\n",
    "            \n",
    "                # Assuming that a species has letters in it,\n",
    "                # any species identified that somehow does not\n",
    "                # have a letter in it will be removed.\n",
    "                letter_found = False\n",
    "                for token in species_span:\n",
    "                    if token.pos_ not in [\"PUNCT\", \"SYM\"]:\n",
    "                        letter_found = True\n",
    "                        break\n",
    "\n",
    "                if not letter_found:\n",
    "                    continue\n",
    "\n",
    "                # Adding Species\n",
    "                spans.append(species_span)\n",
    "                for token in species_span:\n",
    "                    if token.pos_ in [\"PUNCT\", \"SYM\"]:\n",
    "                        continue\n",
    "                    tokens.append(token)\n",
    "                    token_to_span[token] = species_span\n",
    "\n",
    "        # Removing Duplicates and Sorting \n",
    "        spans = list({span.start: span for span in spans}.values())\n",
    "        spans.sort(key=lambda span: span.start)\n",
    "        \n",
    "        # Finding and Storing Alternative Names\n",
    "        for i, species_span in enumerate(spans):\n",
    "            # There's not a next species to\n",
    "            # evaluate.\n",
    "            if i + 1 >= len(spans):\n",
    "                break\n",
    "            \n",
    "            next_species_span = spans[i+1]\n",
    "            \n",
    "            # If there's one token between the species and the next species,\n",
    "            # we check if the next species is surrounded by punctuation.\n",
    "            if next_species_span.start - species_span.end == 1:\n",
    "                # Token Before and After the Next Species\n",
    "                before_next = self.main.sp_doc[next_species_span.start-1]\n",
    "                after_next = self.main.sp_doc[next_species_span.end]\n",
    "\n",
    "                if before_next.pos_ in [\"PUNCT\", \"SYM\"] and after_next.pos_ in [\"PUNCT\", \"SYM\"]:\n",
    "                    sp_1_text = species_span.text.lower()\n",
    "                    sp_2_text = next_species_span.text.lower()\n",
    "                    \n",
    "                    if sp_1_text not in alternate_names:\n",
    "                        alternate_names[sp_1_text] = []\n",
    "                    \n",
    "                    if sp_2_text not in alternate_names:\n",
    "                        alternate_names[sp_2_text] = []\n",
    "                    \n",
    "                    alternate_names[sp_1_text].append(sp_2_text)\n",
    "                    alternate_names[sp_2_text].append(sp_1_text)\n",
    "            # If there's no token between the species and the next,\n",
    "            # species we assume that they refer to the same species.\n",
    "            elif next_species_span.start - species_span.end == 0:\n",
    "                sp_1_text = species_span.text.lower()\n",
    "                sp_2_text = next_species_span.text.lower()\n",
    "                \n",
    "                if sp_1_text not in alternate_names:\n",
    "                    alternate_names[sp_1_text] = []\n",
    "                \n",
    "                if sp_2_text not in alternate_names:\n",
    "                    alternate_names[sp_2_text] = []\n",
    "\n",
    "                alternate_names[sp_1_text].append(sp_2_text)\n",
    "                alternate_names[sp_2_text].append(sp_1_text)\n",
    "       \n",
    "        return (spans, tokens, token_to_span, alternate_names)\n",
    "\n",
    "    def span_at_token(self, token):\n",
    "        if token in self.token_to_span:\n",
    "            return self.token_to_span[token]\n",
    "        return None\n",
    "    \n",
    "    def is_species(self, token):\n",
    "        return token in self.tokens\n",
    "        \n",
    "    def has_species(self, tokens, verbose=False):\n",
    "        for token in tokens:\n",
    "            if token in self.tokens:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def find_same_species(self, species_A, species_b, verbose=False):\n",
    "        # METHOD 1: Check for Literal Matches\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        \n",
    "        for sp_a in sp_A:\n",
    "            # Verbatim Text\n",
    "            sp_a_text = sp_a.text.lower()\n",
    "\n",
    "            if sp_a_text == sp_b_text:\n",
    "                return sp_a\n",
    "\n",
    "            # Singularized Text\n",
    "            sp_a_singular_texts = sp_a_text if sp_a[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(sp_a_text)\n",
    "            sp_b_singular_texts = sp_b_text if sp_b[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(sp_b_text)\n",
    "\n",
    "            if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "                return sp_a\n",
    "\n",
    "        # METHOD 2: Check Alternate Names\n",
    "        for sp_a in sp_A:\n",
    "            # Species B is an alternate name for Species A\n",
    "            if sp_b_text in self.alternate_names.get(sp_a_text, []):\n",
    "                return sp_a\n",
    "            # Species A is an alternate name for Species B\n",
    "            if sp_a_text in self.alternate_names.get(sp_b_text, []):\n",
    "                return sp_a\n",
    "        \n",
    "        # METHOD 3: Check Nouns\n",
    "        # This is used if one or none of the species being compared\n",
    "        # has 1 adjective.\n",
    "        sp_b_0_text = sp_b[0].lower_\n",
    "        sp_b_is_noun = sp_b[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "        \n",
    "        for sp_a in sp_A:\n",
    "            sp_a_0_text = sp_a[0].lower_\n",
    "            sp_a_is_noun = sp_a[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "            \n",
    "            if sp_a_0_text == sp_b_0_text and (sp_a_is_noun or sp_b_is_noun):\n",
    "                if sp_a_text in sp_b_text or sp_b_text in sp_a_text:\n",
    "                    return True\n",
    "            else:\n",
    "                sp_a_nouns = []\n",
    "                sp_a_num_adjectives = 0\n",
    "                for token in sp_a:\n",
    "                    if not sp_a_nouns and token.pos_ == \"ADJ\":\n",
    "                        sp_a_num_adjectives += 1\n",
    "                    elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                        sp_a_nouns.append(token)\n",
    "                \n",
    "                sp_b_nouns = []\n",
    "                sp_b_num_adjectives = 0\n",
    "                for token in sp_b:\n",
    "                    if not sp_b_nouns and token.pos_ == \"ADJ\":\n",
    "                        sp_b_num_adjectives += 1\n",
    "                    elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                        sp_b_nouns.append(token)\n",
    "        \n",
    "                if sp_a_nouns and sp_b_nouns and (\n",
    "                    (sp_a_num_adjectives == 1 and sp_b_num_adjectives == 0) or \n",
    "                    (sp_b_num_adjectives == 1 and sp_a_num_adjectives == 0)\n",
    "                ):\n",
    "                    sp_a_singular_texts = \" \".join(sp_a_nouns) if sp_a_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(\" \".join(sp_a_nouns))\n",
    "                    sp_b_singular_texts = \" \".join(sp_b_nouns) if sp_b_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(\" \".join(sp_b_nouns))\n",
    "            \n",
    "                    return set(sp_a_singular_texts).intersection(sp_b_singular_texts)\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31521a-3d38-4087-b63e-bb974ad3a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    def __init__(self, main, *, base=[], speech=[], literals=[], threshold=0.7, include_substring=False):\n",
    "        self.main = main\n",
    "        # For a token to count towards a base word, it must be the same word.\n",
    "        self.base = [b.lower() for b in base]\n",
    "        self.speech = [s.upper() for s in speech]\n",
    "        self.literals = [l.lower() for l in literals]\n",
    "        # When comparing two words, SpaCy returns a value\n",
    "        # from 0 to 1, representing how similar the two\n",
    "        # embeddings are. The threshold below determines\n",
    "        # the minimum number of similarity before two words\n",
    "        # are considered as being equivalent.\n",
    "        self.threshold = threshold\n",
    "        self.vocab = [self.main.sp_nlp(word) for word in self.base]\n",
    "        # If this is True, then we will also check\n",
    "        # if the token contains a base word.\n",
    "        self.include_substring = include_substring\n",
    "        # This list contains the matched tokens.\n",
    "        self.tokens = []\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tokens = self.match_tokens(verbose=verbose)\n",
    "\n",
    "    def match_tokens(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        matched_tokens = []\n",
    "\n",
    "        # Check Words\n",
    "        for token in self.main.sp_doc:    \n",
    "            if token.pos_ not in self.speech:\n",
    "                continue\n",
    "\n",
    "            token_lower = token.lower_\n",
    "            token_lemma_lower = token.lemma_.lower()\n",
    "            \n",
    "            # Look for Base Word\n",
    "            if token_lemma_lower in self.base or token_lower in self.base:\n",
    "                matched_tokens.append(token)\n",
    "                continue\n",
    "                \n",
    "            # Look for Base Word in Token \n",
    "            # For example, a word like \"biomass\" would match\n",
    "            # if \"mass\" is a base word.\n",
    "            if self.include_substring:\n",
    "                for base_word in self.base:\n",
    "                    if base_word in token_lemma_lower or sub_base_word in token_lower:\n",
    "                        matched_tokens.append(token)\n",
    "                        break\n",
    "\n",
    "            # Already Matched Token\n",
    "            if matched_tokens and matched_tokens[-1] == token:\n",
    "                continue\n",
    "            \n",
    "            # Comparing Similarity\n",
    "            token_doc = self.main.sp_nlp(token_lower)\n",
    "            for word in self.vocab:\n",
    "                similarity = word.similarity(token_doc)\n",
    "                \n",
    "                if similarity >= self.threshold:\n",
    "                    matched_tokens.append(token)\n",
    "                    break\n",
    "\n",
    "        # Check Literals\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "        for literal in self.literals:\n",
    "            for char_index in [match.start() for match in re.finditer(literal, text)]:\n",
    "                matched_tokens.append(self.main.token_at_char(char_index))\n",
    "                \n",
    "        return matched_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699dce8-5827-4757-becd-a016f9a06cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\n",
    "                \"study\", \n",
    "                \"hypothesis\", \n",
    "                \"experiment\", \n",
    "                \"found\", \n",
    "                \"discover\", \n",
    "                \"compare\", \n",
    "                \"finding\", \n",
    "                \"result\", \n",
    "                \"test\", \n",
    "                \"examine\", \n",
    "                \"model\",\n",
    "                \"measure\",\n",
    "                \"manipulate\",\n",
    "                \"assess\",\n",
    "                \"conduct\",\n",
    "                \"data\",\n",
    "                \"analyze\",\n",
    "                \"sample\",\n",
    "                \"observe\"\n",
    "                \"predict\",\n",
    "            ],\n",
    "            literals=[\n",
    "                \"control group\", \n",
    "                \"independent\", \n",
    "                \"dependent\"\n",
    "            ],\n",
    "            speech=[\"VERB\", \"NOUN\"], \n",
    "            threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ae4dd9-f73e-41d5-926e-96c383d65c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CauseKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\n",
    "                \"increase\", \n",
    "                \"decrease\", \n",
    "                \"change\", \n",
    "                \"shift\", \n",
    "                \"cause\", \n",
    "                \"produce\", \n",
    "                \"trigger\", \n",
    "                \"suppress\", \n",
    "                \"inhibit\",\n",
    "                \"encourage\",\n",
    "                \"allow\",\n",
    "                \"influence\",\n",
    "                \"affect\",\n",
    "                \"alter\",\n",
    "                \"induce\",\n",
    "                \"produce\",\n",
    "                \"result in\",\n",
    "                \"associated with\",\n",
    "                \"correlated with\",\n",
    "                \"contribute\",\n",
    "                \"impact\"\n",
    "            ],\n",
    "            speech=[\"VERB\", \"NOUN\"], \n",
    "            threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb56df8-95d1-4ac9-956a-f7aeddb3ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\n",
    "                \"few\", \n",
    "                \"more\", \n",
    "                \"increase\", \n",
    "                \"decrease\", \n",
    "                \"less\", \n",
    "                \"short\", \n",
    "                \"long\", \n",
    "                \"greater\"\n",
    "                \"shift\",\n",
    "                \"fluctuate\",\n",
    "                \"adapt\",\n",
    "                \"grow\",\n",
    "                \"rise\"\n",
    "                \"surge\",\n",
    "                \"intensify\",\n",
    "                \"amplify\",\n",
    "                \"multiply\",\n",
    "                \"decline\",\n",
    "                \"reduce\",\n",
    "                \"drop\",\n",
    "                \"diminish\",\n",
    "                \"fall\",\n",
    "                \"lessen\"\n",
    "            ],\n",
    "            speech=[\"NOUN\", \"ADJ\", \"ADV\"], \n",
    "            threshold=0.8\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in self.main.sp_doc:\n",
    "            # Already Matched\n",
    "            if token in tokens:\n",
    "                filtered.append(token)\n",
    "            \n",
    "            # Comparative Adjective\n",
    "            # Looking for words like \"bigger\" and \"better\".\n",
    "            elif token.pos_ == \"ADJ\" and token.tag_ == \"JJR\":\n",
    "                filtered.append(token)\n",
    "                continue\n",
    "            \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70792f-d32b-4638-a929-c7499506e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraitKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\n",
    "                \"behavior\", \n",
    "                \"rate\", \n",
    "                \"color\", \n",
    "                \"mass\", \n",
    "                \"size\", \n",
    "                \"length\", \n",
    "                \"pattern\", \n",
    "                \"weight\", \n",
    "                \"shape\", \n",
    "                \"efficiency\", \n",
    "                \"trait\",\n",
    "                \"ability\", \n",
    "                \"capacity\", \n",
    "                \"height\", \n",
    "                \"width\", \n",
    "                \"span\",\n",
    "                \"diet\",\n",
    "                \"feeding\",\n",
    "                \"nest\",\n",
    "                \"substrate\",\n",
    "                \"breeding\",\n",
    "                \"age\",\n",
    "                \"lifespan\",\n",
    "                \"development\",\n",
    "                \"time\",\n",
    "                \"mating\",\n",
    "                \"fur\",\n",
    "                \"feathers\",\n",
    "                \"scales\",\n",
    "                \"skin\",\n",
    "                \"limb\",\n",
    "                \"configuration\",\n",
    "                \"dimorphism\",\n",
    "                \"capability\",\n",
    "                \"appendages\",\n",
    "                \"blood\",\n",
    "                \"regulation\",\n",
    "                \"excretion\",\n",
    "                \"luminescence\",\n",
    "                \"role\",\n",
    "                \"reproduction\",\n",
    "                \"courtship\",\n",
    "                \"pollination\",\n",
    "                \"mechanism\",\n",
    "                \"sensitivity\",\n",
    "                \"resistance\"\n",
    "            ],\n",
    "            include_substring=True,\n",
    "            speech=[\"NOUN\", \"ADJ\"], \n",
    "            threshold=0.8\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            expanded_token = self.main.help.expand_unit(\n",
    "                il_unit=token.i, \n",
    "                ir_unit=token.i, \n",
    "                il_boundary=0, \n",
    "                ir_boundary=len(self.main.sp_doc) - 1, \n",
    "                speech=[\"ADJ\", \"NOUN\", \"ADP\", \"PART\", \"DET\", \"PROPN\",],\n",
    "                literals=[\"-\", \",\"],\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "            if self.main.species.has_species(expanded_token):\n",
    "                filtered.append(token)\n",
    "        \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf8e07-a326-4fea-8615-faa5d549852d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Main:\n",
    "    def __init__(self):\n",
    "        # Tools\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        self.sp_doc = None\n",
    "\n",
    "        # Maps Character Position to Token in Document\n",
    "        # Used to handle differences between different\n",
    "        # pipelines and tools.\n",
    "        self.index_map = None\n",
    "    \n",
    "        # Parsers\n",
    "        self.species = Species(self)\n",
    "        self.traits = TraitKeywords(self)\n",
    "        self.causes = CauseKeywords(self)\n",
    "        self.changes = ChangeKeywords(self)\n",
    "        self.experiment = ExperimentKeywords(self)\n",
    "\n",
    "    def update_doc(self, doc, verbose=False):\n",
    "        self.sp_doc = doc\n",
    "        self.index_map = self.load_index_map()\n",
    "        self.species.update(doc.text, verbose=True)\n",
    "        self.traits.update(verbose=False)\n",
    "        self.causes.update(verbose=False)\n",
    "        self.changes.update(verbose=False)\n",
    "        self.experiment.update(verbose=False)\n",
    "\n",
    "    def update_text(self, text, verbose=False):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.update_doc(self.sp_doc, verbose=verbose)\n",
    "        \n",
    "    def token_at_char(self, char_index):\n",
    "        # SpaCy Doc or Indexing Map Not Found\n",
    "        if not self.sp_doc or not self.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        if char_index in self.index_map:\n",
    "            return self.index_map[char_index]\n",
    "\n",
    "        raise Exception(\"Token Not Found\")\n",
    "        \n",
    "    def load_index_map(self):\n",
    "        # SpaCy Doc Not Found\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Map Character Index to Token\n",
    "        index_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            # char_i0 is the index of the token's starting character.\n",
    "            # char_i1 is the index of the character after the token's ending character.\n",
    "            char_i0 = token.idx\n",
    "            char_i1 = token.idx + len(token)\n",
    "        \n",
    "            for i in range(char_i0, char_i1):\n",
    "                index_map[i] = token\n",
    "            \n",
    "        return index_map\n",
    "\n",
    "    def score(self, verbose=False):\n",
    "        NUM_CATEGORIES = 4\n",
    "\n",
    "        # Requires the mention of a trait and a cause or change word.\n",
    "        # The cause or change word indicates some variation.\n",
    "        # Index 0 in Array\n",
    "        TRAIT = 0\n",
    "\n",
    "        # Requires the mention of a species and a cause or change word.\n",
    "        # The cause or change word indicates that the species is being\n",
    "        # affected or is affecting something else.\n",
    "        # Index 1 in Array\n",
    "        SPECIES = 1\n",
    "\n",
    "        # Requires a word that has been defined as \"experiment\"-related.\n",
    "        # Index 2 in Array\n",
    "        EXPERIMENT = 2\n",
    "\n",
    "        # Requires the mention of several species (more or less).\n",
    "        # Index 3 in Array\n",
    "        INTERACTION = 3\n",
    "\n",
    "        # Max # of Points of Category per Sentence (MPC)\n",
    "        # A sentence collects points from its categories.\n",
    "        # For example, a sentence could get a maximum of 2 points from one category\n",
    "        # and a maximum of 1 point from another. The MPC determines the maximum number\n",
    "        # of points a category could contribute to a sentence. To have a range of [0, 1]\n",
    "        # the maximum number of points, across categories, when added should be 1.\n",
    "        MPC = [0] * NUM_CATEGORIES\n",
    "        MPC[TRAIT] = 0.1\n",
    "        MPC[SPECIES] = 0.3\n",
    "        MPC[EXPERIMENT] = 0.3\n",
    "        MPC[INTERACTION] = 0.3\n",
    "\n",
    "        assert np.sum(MPC) == 1\n",
    "        \n",
    "        # Points per Instance of Category (PIC)\n",
    "        # Each token is evaluated to check whether a category\n",
    "        # can be given points. The number of points given, if\n",
    "        # the token is determined to be satisfactory, is the PIC.\n",
    "        # The PIC is less than or equal to the MPC for the corresponding\n",
    "        # category. The idea behind the PIC and MPC is similar to how\n",
    "        # sets work in tennis: you're not immediately awarded the full points\n",
    "        # for the set (MPC) if your opponent fails to return the ball,\n",
    "        # instead you're given a smaller # of points (PIC) that allow you to\n",
    "        # incrementally win the set (category).\n",
    "        PIC = [0] * NUM_CATEGORIES\n",
    "        PIC[TRAIT] = MPC[TRAIT]*1.0\n",
    "        PIC[SPECIES] = MPC[SPECIES]/3.0\n",
    "        PIC[EXPERIMENT] = MPC[EXPERIMENT]/1.0\n",
    "        PIC[INTERACTION] = MPC[INTERACTION]/3.0\n",
    "\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            assert PIC[i] <= MPC[i]\n",
    "\n",
    "        # Category Weights (CW)\n",
    "        # It may be helpful to weigh a certain category's fraction of total points\n",
    "        # more or less than another's. Thus, at the end, we'll take a\n",
    "        # weighted average of the category's FTP. The weights must add up to 1.\n",
    "        CW = [0] * NUM_CATEGORIES\n",
    "        CW[TRAIT] = 0.25\n",
    "        CW[SPECIES] = 0.25\n",
    "        CW[EXPERIMENT] = 0.25\n",
    "        CW[INTERACTION] = 0.25\n",
    "\n",
    "        assert np.sum(MPC) == 1\n",
    "\n",
    "        # Points\n",
    "        points = [0] * NUM_CATEGORIES\n",
    "\n",
    "        # Extracted Information\n",
    "        cause_tokens = self.causes.tokens\n",
    "        change_tokens = self.changes.tokens\n",
    "        trait_tokens = self.traits.tokens\n",
    "        species_tokens = [self.sp_doc[span.start] for span in self.species.spans]\n",
    "        experiment_tokens = self.experiment.tokens\n",
    "         \n",
    "        # This is used to ensure that at least three species\n",
    "        # are mentioned.\n",
    "        seen_species = {}\n",
    "\n",
    "        for sent in self.sp_doc.sents:\n",
    "            # This contains the number of points\n",
    "            # each category has accumulated in the sentence.\n",
    "            curr_points = [0] * NUM_CATEGORIES\n",
    "\n",
    "            # Contains the tokens in the sentence.\n",
    "            sent_tokens = [token for token in sent]\n",
    "\n",
    "            # This is used for the species (must have a nearby cause and/or\n",
    "            # change word).\n",
    "            sent_cause_tokens = set(sent_tokens).intersection(cause_tokens)\n",
    "            sent_change_tokens = set(sent_tokens).intersection(change_tokens)\n",
    "\n",
    "            # We don't want to visit the same species more than one\n",
    "            # in the same sentence as to avoid redundant points.\n",
    "            sent_seen_species = []\n",
    "            \n",
    "            for token in sent_tokens:\n",
    "                # If each category has reached their maximum number of points,\n",
    "                # we can end the loop early.\n",
    "                all_maxed = True\n",
    "                for i in range(NUM_CATEGORIES):\n",
    "                    if curr_points[i] < MPC[i]:\n",
    "                        all_maxed = False\n",
    "\n",
    "                if all_maxed:\n",
    "                    break\n",
    "\n",
    "                # TRAIT CATEGORY\n",
    "                if curr_points[TRAIT] < MPC[TRAIT] and token in trait_tokens:\n",
    "                    # To get points in the trait category, there must \n",
    "                    # be (1) a trait; and (2) a change or cause in the token's\n",
    "                    # context.\n",
    "                    token_context = set(self.find_unit_context(\n",
    "                        il_unit=token.i, \n",
    "                        ir_unit=token.i, \n",
    "                        il_boundary=token.sent.start, \n",
    "                        ir_boundary=token.sent.end-1, \n",
    "                        verbose=verbose)\n",
    "                    )\n",
    "                    cause_tokens_in_context = set(sent_cause_tokens).intersection(token_context)\n",
    "                    change_tokens_in_context = set(sent_change_tokens).intersection(token_context)\n",
    "\n",
    "                    if cause_tokens_in_context or change_tokens_in_context:\n",
    "                        curr_points[TRAIT] += PIC[TRAIT]\n",
    "\n",
    "                # EXPERIMENT CATEGORY\n",
    "                if curr_points[EXPERIMENT] < MPC[EXPERIMENT] and token in experiment_tokens:\n",
    "                    curr_points[EXPERIMENT] += PIC[EXPERIMENT]\n",
    "\n",
    "                # SPECIES CATEGORY\n",
    "                if token in species_tokens:\n",
    "                    # Find Species Span\n",
    "                    species_span = self.species.span_at_token(token)           \n",
    "\n",
    "                    # Updating Seen Species (in Entire Text)\n",
    "                    past_visits = 0\n",
    "                    for seen_species_span in seen_species.keys():\n",
    "                        if self.species.same_species(species_span, seen_species_span, verbose=verbose):\n",
    "                            past_visits = seen_species[seen_species_span]\n",
    "                            seen_species[seen_species_span] += 1\n",
    "                            break\n",
    "\n",
    "                    if not past_visits:\n",
    "                        seen_species[species_span] = 1\n",
    "\n",
    "                    # Prevents Unneeded Function Call\n",
    "                    is_new_species = past_visits == 0\n",
    "          \n",
    "                    # Checking Seen Species (in Sentence)\n",
    "                    # We only add points if it's a species that has not been seen\n",
    "                    # in the sentence. This is to avoid redundant points. \n",
    "                    # Also, if it species has not been seen at all (is_new_species),\n",
    "                    # then it cannot be a redundant species (we couldn't have seen it in the sentence\n",
    "                    # either).\n",
    "                    redundant_species = False\n",
    "\n",
    "                    if not is_new_species:\n",
    "                        for seen_species_span in sent_seen_species:\n",
    "                            if self.species.same_species(species_span, seen_species_span, verbose=verbose):\n",
    "                                redundant_species = True\n",
    "                                break\n",
    "                    \n",
    "                    sent_seen_species.append(species_span)\n",
    "                    if redundant_species:\n",
    "                        continue\n",
    "\n",
    "                    # INTERACTION CATEGORY\n",
    "                    # It is helpful to have this category here because (if we've reached here)\n",
    "                    # we're dealing with a new species in the sentence.\n",
    "                    if curr_points[INTERACTION] < MPC[INTERACTION]:\n",
    "                        curr_points[INTERACTION] += PIC[INTERACTION]\n",
    "                        \n",
    "                    if curr_points[SPECIES] < MPC[SPECIES]:\n",
    "                        # To get points in the species category, there must be \n",
    "                        # (1) a species; and (2) a change or cause in the phrase\n",
    "                        # (or clause) that the token is a part of.\n",
    "                        token_context = set(self.find_unit_context(\n",
    "                            il_unit=token.i, \n",
    "                            ir_unit=token.i, \n",
    "                            il_boundary=token.sent.start, \n",
    "                            ir_boundary=token.sent.end-1, \n",
    "                            verbose=verbose)\n",
    "                        )\n",
    "                        cause_tokens_in_context = set(sent_cause_tokens).intersection(token_context)\n",
    "                        change_tokens_in_context = set(sent_change_tokens).intersection(token_context)\n",
    "                        \n",
    "                        if cause_tokens_in_context or change_tokens_in_context:\n",
    "                            curr_points[SPECIES] += PIC[SPECIES]\n",
    "         \n",
    "            # SENTENCE DONE\n",
    "            # Add Sentence Points to Total Points\n",
    "            for category in [TRAIT, SPECIES, EXPERIMENT, INTERACTION]:\n",
    "                points[category] += min(curr_points[category], MPC[category])\n",
    "\n",
    "        # Enforcing 3 or More Species            \n",
    "        if len(seen_species) < 3:\n",
    "            return 0\n",
    "        \n",
    "        # Calculating Score            \n",
    "        NUM_SENTENCES = len(list(self.sp_doc.sents))\n",
    "\n",
    "        score = 0\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            FTP = points[i] / (MPC[i] * NUM_SENTENCES)\n",
    "            score += FTP * CW[i]\n",
    "            \n",
    "        assert 0.0 <= score <= 1.0\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10821c-4a15-47e6-b3d2-f49b7ef3eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../Datasets/Baseline-1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619e40a-251d-46d0-a451-d6e3c8fa30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "main = Main()\n",
    "for abstract in [df.Abstract.to_list()[1]]:\n",
    "    print(abstract)\n",
    "    main.update_text(abstract, verbose=True)\n",
    "    score = main.score(verbose=True)\n",
    "    print(score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d078bf01-5efe-49b5-9ec9-4a8ccf1d47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(\"You're brighter than this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54730fc3-724b-4afa-8a04-70f80e7c491b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You PRON PRP\n",
      "'re AUX VBP\n",
      "brighter ADJ JJR\n",
      "than ADP IN\n",
      "this PRON DT\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5666c12-f4ac-4b2f-aeeb-8aab521c8469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
