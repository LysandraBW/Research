{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb531e7d-d086-4971-ad82-4ebc0061de98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "%run -i \"../utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9161e3-67f0-4d3f-8b41-bbbc13a6a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species:\n",
    "    def __init__(self, scanner):\n",
    "        self.scanner = scanner\n",
    "        self.species_spans = None\n",
    "        self.species_indices = None\n",
    "\n",
    "    def update(self):\n",
    "        if not self.scanner.sp_doc or not self.scanner.tn_doc:\n",
    "            return\n",
    "        # print(\"Updating Species Indices and Spans\")\n",
    "        t0 = time.time()\n",
    "        self.species_spans, self.species_indices = self.load_species_spans()\n",
    "        t1 = time.time()\n",
    "        # print(f\"Load Species Indices and Span: {t1-t0}s\")\n",
    "        \n",
    "    def load_species_spans(self):\n",
    "        spans = []\n",
    "        indices = []\n",
    "        for species_span in self.scanner.tn_doc.ents:\n",
    "            l_species_i = species_span[0].i\n",
    "            r_species_i = species_span[-1].i\n",
    "            # print(f\"Species Span: '{species_span}'\")\n",
    "            # print(f\"Species L Index: {l_species_idx}\")\n",
    "            # print(f\"Species R Index: {r_species_idx}\")\n",
    "            # print(f\"Token Map Keys: {self.scanner.tk_map.keys()}\")\n",
    "            # print(f\"TN Text: {self.scanner.tn_doc.text}\")\n",
    "            # print(f\"SP Text: {self.scanner.sp_doc.text}\")\n",
    "            span = self.scanner.sp_doc[l_species_i:r_species_i+1]\n",
    "            spans.append(span)\n",
    "            indices += [token.i for token in span]\n",
    "            \n",
    "        return (spans, indices)\n",
    "\n",
    "    def is_species(self, token):\n",
    "        return token.i in self.species_indices\n",
    "        \n",
    "    def has_species(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.species_indices:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86af113-a5e5-4add-bcc7-aef2db449a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    def __init__(self, scanner, literals, pos_types, threshold=0.7):\n",
    "        self.scanner = scanner\n",
    "        self.literals = literals\n",
    "        self.threshold = threshold\n",
    "        self.pos_types = pos_types\n",
    "        self.keywords = [self.scanner.sp_nlp(literal) for literal in self.literals]\n",
    "        self.keyword_indices = []\n",
    "\n",
    "    def update(self):\n",
    "        if not self.scanner.sp_doc or not self.scanner.sp_nlp:\n",
    "            return\n",
    "        # print(\"Updating Keyword Indices\")\n",
    "        t0 = time.time()\n",
    "        self.keyword_indices = self.load_keyword_indices()\n",
    "        t1 = time.time()\n",
    "        # print(f\"Keyword Indices: {t1-t0}s\")\n",
    "        \n",
    "    def is_keyword(self, token):\n",
    "        return token.i in self.keyword_indices\n",
    "\n",
    "    def has_keyword(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.keyword_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_keyword_indices(self):\n",
    "        indices = []\n",
    "        for token in self.scanner.sp_doc:\n",
    "            if token.pos_ not in self.pos_types or self.do_not_check(token):\n",
    "                continue\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.literals:\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.scanner.sp_nlp(token.lemma_)\n",
    "            for keyword in self.keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > self.threshold:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def find_keyword_indices(self, tokens):\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token.pos_ not in self.pos_types or self.do_not_check(token):\n",
    "                continue\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.literals:\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.scanner.sp_nlp(token.lemma_)\n",
    "            for keyword in self.keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > self.threshold:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def do_not_check(self, token):\n",
    "        return len(token) <= 5 or re.match('^[\\w]+$', token.text) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b9437d-5b6a-4928-8a33-b530f35f4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, scanner):\n",
    "        super().__init__(scanner, {\"increase\", \"decrease\", \"change\", \"shift\", \"cause\", \"produce\"}, [\"NOUN\", \"VERB\"], 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51fbf812-860c-498c-a700-5cd491724bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class References:\n",
    "    def __init__(self, scanner, texts=None):\n",
    "        self.scanner = scanner\n",
    "        self.predictions = None\n",
    "        self.cluster_map = None\n",
    "        self.text_size_in_tokens = 100\n",
    "        if texts:\n",
    "            self.update(texts)\n",
    "\n",
    "    def update(self, text):\n",
    "        if not self.scanner.tn_doc:\n",
    "            return\n",
    "        # print(\"Updating Predictions\")\n",
    "        # t0 = time.time()\n",
    "        \n",
    "        self.predictions = self.scanner.fcoref.predict(texts=[text])\n",
    "        # t1 = time.time()\n",
    "        # print(f\"Predictions: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Updating Cluster Map\")\n",
    "        # t0 = time.time()\n",
    "        self.cluster_map = self.load_cluster_map(self.predictions)\n",
    "        # t1 = time.time()\n",
    "        # print(f\"Cluster Map: {t1-t0}s\")\n",
    "\n",
    "    def load_cluster_map(self, predictions):\n",
    "        cluster_map = {}\n",
    "        for prediction in predictions:\n",
    "            clusters = prediction.get_clusters(as_strings=False)\n",
    "            for cluster in clusters:\n",
    "                print(cluster)\n",
    "                \n",
    "                # It's a cluster of spans,\n",
    "                # but instead it'll be 'converted' into a cluster of tokens.\n",
    "                token_cluster = []\n",
    "            \n",
    "                for span in cluster:\n",
    "                    span_words = self.scanner.tn_doc.text[span[0]:span[1]].split()\n",
    "                    # print(span_words)\n",
    "                    \n",
    "                    word_index = span[0]\n",
    "                    for i in range(len(span_words)):\n",
    "                        word = span_words[i]\n",
    "                        if word_index not in self.scanner.tk_map:\n",
    "                            continue\n",
    "                        token_cluster.append(self.scanner.tk_map[word_index])\n",
    "                        word_index += len(word) + 1\n",
    "\n",
    "                # print(token_cluster)\n",
    "                    \n",
    "                # Mapping\n",
    "                for token in token_cluster:\n",
    "                    cluster_map[token.i] = list(filter(lambda t: t != token, token_cluster))\n",
    "        return cluster_map\n",
    "            \n",
    "    def get_references(self, tokens):\n",
    "        refs = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.cluster_map:\n",
    "                refs += self.cluster_map[index]\n",
    "        return refs\n",
    "\n",
    "    def same_reference(self, token_a, token_b):\n",
    "        if token_a.lemma_.lower() == token_b.lemma_.lower():\n",
    "            return True\n",
    "        if token_a.i in self.cluster_map and token_b in self.cluster_map[token_a.i]:\n",
    "            return True\n",
    "        if token_b.i in self.cluster_map and token_a in self.cluster_map[token_b.i]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def same_reference_span(self, span_a, span_b):\n",
    "        if span_a.text.lower() == span_b.text.lower():\n",
    "            return True\n",
    "        for token_a in span_a:\n",
    "            for token_b in span_b:\n",
    "                if self.same_reference(token_a, token_b):\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0cc244-1a27-4f8f-ae0f-a431da02ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scanner():\n",
    "    def __init__(self):\n",
    "        # Tools\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        self.sp_doc = None\n",
    "        self.tn_doc = None\n",
    "        self.tk_map = None\n",
    "        \n",
    "        # Helpers\n",
    "        self.species = Species(self)\n",
    "        self.references = References(self)\n",
    "        self.changes = ChangeKeywords(self)\n",
    "    \n",
    "    def update(self, doc):\n",
    "        self.sp_doc = doc\n",
    "        self.tn_doc = self.tn_nlp(doc.text)\n",
    "        self.tk_map = self.load_token_map()\n",
    "        self.references.update(doc.text)\n",
    "        self.species.update()\n",
    "\n",
    "    def load_token_map(self):\n",
    "        # Maps Characters to Tokens\n",
    "        tk_map = {}\n",
    "        for token in self.tn_doc:\n",
    "            tk_map[token.idx] = token\n",
    "        return tk_map\n",
    "\n",
    "    def get_full_species(self):\n",
    "        full_species = [*self.species.species_spans]\n",
    "        full_indices = [*self.species.species_indices]\n",
    "        \n",
    "        for k, v in self.references.cluster_map.items():\n",
    "            if k in self.species.species_indices:\n",
    "                for token in v:\n",
    "                    if token.i not in full_indices:\n",
    "                        token_span = self.tn_doc[token.i:token.i+1]\n",
    "                        full_species.append(token_span)\n",
    "                        full_indices.append(token.i)\n",
    "            if self.species.has_species(v):\n",
    "                if k not in full_indices:\n",
    "                    token_span = self.tn_doc[k:k+1]\n",
    "                    full_species.append(token_span)\n",
    "                    full_indices.append(k)\n",
    "        \n",
    "        return (full_species, full_indices)\n",
    "\n",
    "    def get_points(self, verbose=False):\n",
    "        if verbose:\n",
    "            print(\"\\n\\n\\n\")\n",
    "        points = 0\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Cluster Map\")\n",
    "            print(self.references.cluster_map)\n",
    "\n",
    "        # This dictionary contains the species that have been seen already\n",
    "        # and a number indicating the amount of times they have been visited.\n",
    "        # The key is a tuple wherein the first index contains the start of the span\n",
    "        # and the second (last) index contains the end of the span. The span is the\n",
    "        # species. The value is the number of visitations for that species.\n",
    "        visited_species = {}\n",
    "\n",
    "        # This is used to quickly find nearby species.\n",
    "        species_spans, species_indices = self.get_full_species()\n",
    "        if verbose:\n",
    "            print(f\"Species Spans:\")\n",
    "            print(\"\\t\"+\"\\n\\t\".join([f\"'{span.text}' from {span[0].i}-{span[-1].i}\" for span in species_spans]))\n",
    "            print(\"--\")\n",
    "        \n",
    "        for species_span in species_spans:\n",
    "            if verbose:\n",
    "                print(\"\\nLoop\")\n",
    "                print(f\"Sentence of Species: {species_span.sent.text}\")\n",
    "                print(f\"Species: '{species_span.text}' from {species_span[0].i}-{species_span[-1].i}\")\n",
    "                \n",
    "                # This (visited_species) stores a tuple with the start and end token indices, \n",
    "                # which is why the -1 indexing is not being used here.\n",
    "                print(f\"\\tVisited Species:\")\n",
    "                print(\"\\t\\t\"+\"\\n\\t\\t\".join([f\"'{self.sp_doc[sp[0]:sp[1]+1]}' from {sp[0]}-{sp[1]} visited {n} times.\" for sp, n in visited_species.items()]))\n",
    "                print(\"--\")\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Expanding Species\")\n",
    "\n",
    "            # Take the sentence \"blue whale\", for example. TaxoNerd might recognize\n",
    "            # \"whale\", but the adjective \"blue\" is also important in correctly identifying\n",
    "            # the species being referenced. This is why I'm expanding the species.\n",
    "            if species_span[0].text.lower() in [\"species\"]:\n",
    "                i = species_span[0].i\n",
    "                j = i\n",
    "                while j > 0:\n",
    "                    prev_token = self.sp_doc[j-1]\n",
    "                    if prev_token.pos_ not in [\"NOUN\", \"ADJ\", \"PROPN\", \"SYM\"]:\n",
    "                        break\n",
    "                    species_span = self.sp_doc[j-1:i+1]\n",
    "                    if verbose:\n",
    "                        print(f\"\\Expanding Species - Now: {species_span}\")\n",
    "                    j -= 1\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Sentence of (Final) Expanded Species: {species_span.sent.text}\") # Should be the same as before.\n",
    "                print(f\"Final Expanded Species: '{species_span.text}' from {species_span[0].i}-{species_span[-1].i}\")\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Looking for Past References\")\n",
    "            \n",
    "            past_visits = 0\n",
    "            for sp in visited_species.keys():\n",
    "                visited_species_span = self.sp_doc[sp[0]:sp[1]+1]\n",
    "                if verbose:\n",
    "                    print(f\"\\tComparing '{species_span.text}' and '{visited_species_span.text}'\")\n",
    "                \n",
    "                if self.references.same_reference_span(species_span, visited_species_span):\n",
    "                    past_visits = visited_species[sp]\n",
    "                    if verbose:\n",
    "                        print(f\"\\t'{species_span.text}' == '{visited_species_span.text}'\")\n",
    "                        print(f\"\\t\\tNumber of Past Visits: {past_visits}\")\n",
    "                    visited_species[sp] += 1\n",
    "                    break\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Past Visits: {past_visits}\")\n",
    "            if past_visits == 0:\n",
    "                visited_species[(species_span[0].i, species_span[-1].i)] = 1\n",
    "\n",
    "            # The points will begin to inflate at a certain point, so to reduce\n",
    "            # those effects, I'm limiting the number of references. This might not\n",
    "            # matter anymore with the changes I'm making, though.\n",
    "            if past_visits > 50:\n",
    "                if verbose:\n",
    "                    print(f\"Past Visits Exceed 50, Continue\")\n",
    "                continue\n",
    "\n",
    "            # These contains the indices of the tokens that start and end\n",
    "            # the sentence.\n",
    "            li = species_span[0].sent.start\n",
    "            ri = species_span[-1].sent.end\n",
    "\n",
    "            # This contains the indices of the tokens that start and end\n",
    "            # the species span.\n",
    "            sli = species_span[0].i\n",
    "            sri = species_span[-1].i\n",
    "\n",
    "            # These are the indices of the tokens to the left of the species span,\n",
    "            # and the indices of the tokens to the right of the species span.\n",
    "            l_token_indices = set([token.i for token in self.sp_doc[li:sli]])\n",
    "            r_token_indices = set([token.i for token in self.sp_doc[sri+1:ri]])\n",
    "\n",
    "            # Nearby Actions (Modification)\n",
    "            change_indices = set(self.changes.find_keyword_indices(self.sp_doc[li:ri]))\n",
    "            if verbose:\n",
    "                print(f\"Change (Causative) Verbs in Sentence: {[self.sp_doc[i].text for i in change_indices]}\")\n",
    "\n",
    "            # Now that I'm thinking about it, I'm not sure why I make the distinction between the left and right.\n",
    "            # I'll leave it for now in case it becomes useful later on.\n",
    "            l_changes = l_token_indices.intersection(change_indices)\n",
    "            r_changes = r_token_indices.intersection(change_indices)\n",
    "            if verbose:\n",
    "                print(f\"Change (Causative) Verbs in L-Sentence: {[self.sp_doc[i].text for i in l_changes]}\")\n",
    "                print(f\"Change (Causative) Verbs in R-Sentence: {[self.sp_doc[i].text for i in r_changes]}\")\n",
    "\n",
    "            # There must be a change. If there's none,\n",
    "            # we move onto the next species in the paper/abstract.\n",
    "            if not l_changes and not r_changes:\n",
    "                continue\n",
    "                \n",
    "            # Nearby Species (Interaction)\n",
    "            l_species = l_token_indices.intersection(species_indices)\n",
    "            r_species = r_token_indices.intersection(species_indices)\n",
    "            if verbose:\n",
    "                print(f\"Species in L-Sentence: {[self.sp_doc[i].text for i in l_species]}\")\n",
    "                print(f\"Species in R-Sentence: {[self.sp_doc[i].text for i in r_species]}\")\n",
    "\n",
    "            # There must be a species. If there's none,\n",
    "            # we move onto the next species in the paper/abstract.\n",
    "            if not l_species and not r_species:\n",
    "                continue\n",
    "\n",
    "            # Adding Points\n",
    "            points += 10 * (past_visits + 1)\n",
    "            if verbose:\n",
    "                print(f\"Added {10 * (past_visits + 1)} Points for Species, Now {points}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Points: {points}\")\n",
    "        \n",
    "        # Adjustments\n",
    "        number_sentences = len(list(self.sp_doc.sents))\n",
    "        if number_sentences == 0:\n",
    "            points = 0\n",
    "        else:\n",
    "            points /= number_sentences\n",
    "        if verbose:\n",
    "            print(f\"Adjusted Points ({len(list(self.sp_doc.sents))} Sentences): {points}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\tVisited Species:\")\n",
    "            print(\"\\t\\t\"+\"\\n\\t\\t\".join([f\"'{self.sp_doc[sp[0]:sp[1]+1]}' from {sp[0]}-{sp[1]} visited {n} times.\" for sp, n in visited_species.items()]))\n",
    "            print(\"--\")\n",
    "        \n",
    "        if len(visited_species) < 3:\n",
    "            if verbose:\n",
    "                print(\"Less Than 3 Species Visited, 0 Points\")\n",
    "            return 0\n",
    "        if verbose:\n",
    "            print(f\"{points} Points\")\n",
    "        return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcddfc05-9263-4112-a554-8c0bccf03eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(name, save_output=False, version=\"\"):\n",
    "    # Redirect Print Statements\n",
    "    # https://stackoverflow.com/questions/7152762/how-to-redirect-print-output-to-a-file\n",
    "    if save_output:\n",
    "        initial_stdout = sys.stdout\n",
    "        f = open(f'./Print{name}{\"\" if not version else f\"-{version}\"}.txt', 'w')\n",
    "        sys.stdout = f\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "\n",
    "    # Load Dataset\n",
    "    data = load_preprocessed_dataset(name)\n",
    "    print(f\"Data Shape: {data.shape}\")\n",
    "\n",
    "    # We'll be running the points algorithm\n",
    "    # on the abstracts of these papers.\n",
    "    texts = list(data['Abstract'].to_numpy())\n",
    "    number_texts = len(texts)\n",
    "    print(f\"Number of Texts: {number_texts}\")\n",
    "    \n",
    "    # The scores for each paper will be stored here,\n",
    "    # we'll set this as a column of the dataframe.\n",
    "    scores = []\n",
    "    \n",
    "    # Scan and Evaluate Documents\n",
    "    scanner = Scanner()\n",
    "    for i, doc in enumerate(scanner.sp_nlp.pipe(texts)):\n",
    "        print(f\"{i+1}/{number_texts} - {data.iloc[i]['Title']}\")\n",
    "        scanner.update(doc)\n",
    "\n",
    "        # Empty string literals cause errors, so it's\n",
    "        # being handled here.\n",
    "        if not scanner.sp_doc or not scanner.tn_doc:\n",
    "            scores.append(0)\n",
    "        else:\n",
    "            points = scanner.get_points(verbose=save_output)\n",
    "            scores.append(points)\n",
    "\n",
    "        if not save_output:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    # Reset Standard Output\n",
    "    if save_output:\n",
    "        sys.stdout = initial_stdout\n",
    "        f.close()\n",
    "\n",
    "    data[\"Score\"] = scores\n",
    "    data.sort_values(by='Score', ascending=False, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c521b55c-7f65-49a8-9127-c2df1f9a18be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/11/2025 03:10:30 - INFO - \t missing_keys: []\n",
      "06/11/2025 03:10:30 - INFO - \t unexpected_keys: []\n",
      "06/11/2025 03:10:30 - INFO - \t mismatched_keys: []\n",
      "06/11/2025 03:10:30 - INFO - \t error_msgs: []\n",
      "06/11/2025 03:10:30 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "06/11/2025 03:10:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.63 examples/s]\n",
      "06/11/2025 03:10:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:10:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.41 examples/s]\n",
      "06/11/2025 03:10:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:10:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.50 examples/s]\n",
      "06/11/2025 03:10:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:11:03 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.86 examples/s]\n",
      "06/11/2025 03:11:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:11:54 - INFO - \t missing_keys: []\n",
      "06/11/2025 03:11:54 - INFO - \t unexpected_keys: []\n",
      "06/11/2025 03:11:54 - INFO - \t mismatched_keys: []\n",
      "06/11/2025 03:11:54 - INFO - \t error_msgs: []\n",
      "06/11/2025 03:11:54 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "06/11/2025 03:12:00 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.56 examples/s]\n",
      "06/11/2025 03:12:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:12:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.89 examples/s]\n",
      "06/11/2025 03:12:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:12:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.49 examples/s]\n",
      "06/11/2025 03:12:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:12:17 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 22.00 examples/s]\n",
      "06/11/2025 03:12:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:12:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.14 examples/s]\n",
      "06/11/2025 03:12:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:12:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.90 examples/s]\n",
      "06/11/2025 03:12:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:12:28 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.23 examples/s]\n",
      "06/11/2025 03:12:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:12:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 14.42 examples/s]\n",
      "06/11/2025 03:12:34 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:12:41 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.07 examples/s]\n",
      "06/11/2025 03:12:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:12:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.00 examples/s]\n",
      "06/11/2025 03:12:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:12:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.73 examples/s]\n",
      "06/11/2025 03:12:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:13:07 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.19 examples/s]\n",
      "06/11/2025 03:13:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:13:15 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.71 examples/s]\n",
      "06/11/2025 03:13:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:13:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.15 examples/s]\n",
      "06/11/2025 03:13:19 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:13:30 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.35 examples/s]\n",
      "06/11/2025 03:13:30 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:13:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.24 examples/s]\n",
      "06/11/2025 03:13:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:13:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.29 examples/s]\n",
      "06/11/2025 03:13:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:13:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.16 examples/s]\n",
      "06/11/2025 03:13:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:13:58 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.08 examples/s]\n",
      "06/11/2025 03:13:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:14:07 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.63 examples/s]\n",
      "06/11/2025 03:14:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:14:16 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.35 examples/s]\n",
      "06/11/2025 03:14:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:14:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.64 examples/s]\n",
      "06/11/2025 03:14:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:14:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.56 examples/s]\n",
      "06/11/2025 03:14:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:14:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.50 examples/s]\n",
      "06/11/2025 03:14:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:15:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.12 examples/s]\n",
      "06/11/2025 03:15:02 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:15:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.90 examples/s]\n",
      "06/11/2025 03:15:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:15:11 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.83 examples/s]\n",
      "06/11/2025 03:15:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:15:59 - INFO - \t missing_keys: []\n",
      "06/11/2025 03:15:59 - INFO - \t unexpected_keys: []\n",
      "06/11/2025 03:16:00 - INFO - \t mismatched_keys: []\n",
      "06/11/2025 03:16:00 - INFO - \t error_msgs: []\n",
      "06/11/2025 03:16:00 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "06/11/2025 03:16:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 24.39 examples/s]\n",
      "06/11/2025 03:16:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:15 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 14.20 examples/s]\n",
      "06/11/2025 03:16:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:18 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.94 examples/s]\n",
      "06/11/2025 03:16:19 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.74 examples/s]\n",
      "06/11/2025 03:16:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.22 examples/s]\n",
      "06/11/2025 03:16:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:28 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.09 examples/s]\n",
      "06/11/2025 03:16:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:30 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.95 examples/s]\n",
      "06/11/2025 03:16:31 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:33 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.76 examples/s]\n",
      "06/11/2025 03:16:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:35 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 14.48 examples/s]\n",
      "06/11/2025 03:16:36 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:37 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.77 examples/s]\n",
      "06/11/2025 03:16:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.40 examples/s]\n",
      "06/11/2025 03:16:40 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.18 examples/s]\n",
      "06/11/2025 03:16:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.63 examples/s]\n",
      "06/11/2025 03:16:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.42 examples/s]\n",
      "06/11/2025 03:16:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.21 examples/s]\n",
      "06/11/2025 03:16:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.89 examples/s]\n",
      "06/11/2025 03:16:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.17 examples/s]\n",
      "06/11/2025 03:16:55 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:16:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.49 examples/s]\n",
      "06/11/2025 03:16:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.12 examples/s]\n",
      "06/11/2025 03:17:02 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:08 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.03 examples/s]\n",
      "06/11/2025 03:17:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.46 examples/s]\n",
      "06/11/2025 03:17:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:21 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.81 examples/s]\n",
      "06/11/2025 03:17:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.57 examples/s]\n",
      "06/11/2025 03:17:24 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:28 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.51 examples/s]\n",
      "06/11/2025 03:17:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.01 examples/s]\n",
      "06/11/2025 03:17:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:33 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 22.53 examples/s]\n",
      "06/11/2025 03:17:34 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.65 examples/s]\n",
      "06/11/2025 03:17:36 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.95 examples/s]\n",
      "06/11/2025 03:17:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:41 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.45 examples/s]\n",
      "06/11/2025 03:17:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.17 examples/s]\n",
      "06/11/2025 03:17:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:46 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.63 examples/s]\n",
      "06/11/2025 03:17:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.36 examples/s]\n",
      "06/11/2025 03:17:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.82 examples/s]\n",
      "06/11/2025 03:17:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:53 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.08 examples/s]\n",
      "06/11/2025 03:17:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:55 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 33.55 examples/s]\n",
      "06/11/2025 03:17:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:17:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.87 examples/s]\n",
      "06/11/2025 03:17:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:00 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.08 examples/s]\n",
      "06/11/2025 03:18:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.36 examples/s]\n",
      "06/11/2025 03:18:06 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.45 examples/s]\n",
      "06/11/2025 03:18:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:11 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 56.26 examples/s]\n",
      "06/11/2025 03:18:11 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.19 examples/s]\n",
      "06/11/2025 03:18:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:15 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.31 examples/s]\n",
      "06/11/2025 03:18:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:18 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.63 examples/s]\n",
      "06/11/2025 03:18:18 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.74 examples/s]\n",
      "06/11/2025 03:18:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:26 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.13 examples/s]\n",
      "06/11/2025 03:18:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:29 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.95 examples/s]\n",
      "06/11/2025 03:18:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.86 examples/s]\n",
      "06/11/2025 03:18:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.95 examples/s]\n",
      "06/11/2025 03:18:34 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.40 examples/s]\n",
      "06/11/2025 03:18:37 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:40 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.21 examples/s]\n",
      "06/11/2025 03:18:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.54 examples/s]\n",
      "06/11/2025 03:18:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:45 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.71 examples/s]\n",
      "06/11/2025 03:18:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.28 examples/s]\n",
      "06/11/2025 03:18:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 14.51 examples/s]\n",
      "06/11/2025 03:18:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.80 examples/s]\n",
      "06/11/2025 03:18:55 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:18:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.05 examples/s]\n",
      "06/11/2025 03:18:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:00 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.37 examples/s]\n",
      "06/11/2025 03:19:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:02 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.56 examples/s]\n",
      "06/11/2025 03:19:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:04 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.50 examples/s]\n",
      "06/11/2025 03:19:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.21 examples/s]\n",
      "06/11/2025 03:19:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:08 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.33 examples/s]\n",
      "06/11/2025 03:19:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:11 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.72 examples/s]\n",
      "06/11/2025 03:19:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.66 examples/s]\n",
      "06/11/2025 03:19:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:17 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.86 examples/s]\n",
      "06/11/2025 03:19:18 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:20 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.23 examples/s]\n",
      "06/11/2025 03:19:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.14 examples/s]\n",
      "06/11/2025 03:19:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.67 examples/s]\n",
      "06/11/2025 03:19:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:28 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.16 examples/s]\n",
      "06/11/2025 03:19:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.83 examples/s]\n",
      "06/11/2025 03:19:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  2.85 examples/s]\n",
      "06/11/2025 03:19:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.25 examples/s]\n",
      "06/11/2025 03:19:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:46 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.37 examples/s]\n",
      "06/11/2025 03:19:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:19:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.10 examples/s]\n",
      "06/11/2025 03:19:55 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:20:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.75 examples/s]\n",
      "06/11/2025 03:20:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:20:41 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.63 examples/s]\n",
      "06/11/2025 03:20:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:20:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.97 examples/s]\n",
      "06/11/2025 03:20:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:21:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.96 examples/s]\n",
      "06/11/2025 03:21:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:21:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.73 examples/s]\n",
      "06/11/2025 03:21:10 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:21:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.46 examples/s]\n",
      "06/11/2025 03:21:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:21:16 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.76 examples/s]\n",
      "06/11/2025 03:21:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:21:20 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.24 examples/s]\n",
      "06/11/2025 03:21:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:21:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.60 examples/s]\n",
      "06/11/2025 03:21:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:21:29 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.93 examples/s]\n",
      "06/11/2025 03:21:30 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:21:32 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.46 examples/s]\n",
      "06/11/2025 03:21:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:21:53 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  1.32 examples/s]\n",
      "06/11/2025 03:21:55 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:22:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.59 examples/s]\n",
      "06/11/2025 03:22:19 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:22:20 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.02 examples/s]\n",
      "06/11/2025 03:22:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:22:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.57 examples/s]\n",
      "06/11/2025 03:22:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:22:26 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.85 examples/s]\n",
      "06/11/2025 03:22:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:22:27 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.96 examples/s]\n",
      "06/11/2025 03:22:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:23:04 - INFO - \t missing_keys: []\n",
      "06/11/2025 03:23:04 - INFO - \t unexpected_keys: []\n",
      "06/11/2025 03:23:04 - INFO - \t mismatched_keys: []\n",
      "06/11/2025 03:23:04 - INFO - \t error_msgs: []\n",
      "06/11/2025 03:23:04 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "06/11/2025 03:23:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.53 examples/s]\n",
      "06/11/2025 03:23:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:23:10 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.97 examples/s]\n",
      "06/11/2025 03:23:10 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:23:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.90 examples/s]\n",
      "06/11/2025 03:23:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:23:20 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  2.42 examples/s]\n",
      "06/11/2025 03:23:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:08 - INFO - \t missing_keys: []\n",
      "06/11/2025 03:24:08 - INFO - \t unexpected_keys: []\n",
      "06/11/2025 03:24:08 - INFO - \t mismatched_keys: []\n",
      "06/11/2025 03:24:08 - INFO - \t error_msgs: []\n",
      "06/11/2025 03:24:08 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "06/11/2025 03:24:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.94 examples/s]\n",
      "06/11/2025 03:24:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:21 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.77 examples/s]\n",
      "06/11/2025 03:24:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:23 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 14.17 examples/s]\n",
      "06/11/2025 03:24:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.21 examples/s]\n",
      "06/11/2025 03:24:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:26 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 23.18 examples/s]\n",
      "06/11/2025 03:24:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:27 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.88 examples/s]\n",
      "06/11/2025 03:24:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:29 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.38 examples/s]\n",
      "06/11/2025 03:24:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:30 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.32 examples/s]\n",
      "06/11/2025 03:24:31 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:32 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.67 examples/s]\n",
      "06/11/2025 03:24:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.18 examples/s]\n",
      "06/11/2025 03:24:36 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.65 examples/s]\n",
      "06/11/2025 03:24:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.42 examples/s]\n",
      "06/11/2025 03:24:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.61 examples/s]\n",
      "06/11/2025 03:24:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.78 examples/s]\n",
      "06/11/2025 03:24:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.72 examples/s]\n",
      "06/11/2025 03:24:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:55 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 17.00 examples/s]\n",
      "06/11/2025 03:24:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 24.66 examples/s]\n",
      "06/11/2025 03:24:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:24:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.11 examples/s]\n",
      "06/11/2025 03:24:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 14.06 examples/s]\n",
      "06/11/2025 03:25:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:03 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.01 examples/s]\n",
      "06/11/2025 03:25:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.83 examples/s]\n",
      "06/11/2025 03:25:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:10 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 22.60 examples/s]\n",
      "06/11/2025 03:25:10 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:11 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 17.50 examples/s]\n",
      "06/11/2025 03:25:12 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:14 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 17.29 examples/s]\n",
      "06/11/2025 03:25:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:16 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.84 examples/s]\n",
      "06/11/2025 03:25:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.62 examples/s]\n",
      "06/11/2025 03:25:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.29 examples/s]\n",
      "06/11/2025 03:25:24 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.02 examples/s]\n",
      "06/11/2025 03:25:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:27 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.03 examples/s]\n",
      "06/11/2025 03:25:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:29 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.94 examples/s]\n",
      "06/11/2025 03:25:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.87 examples/s]\n",
      "06/11/2025 03:25:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.04 examples/s]\n",
      "06/11/2025 03:25:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:40 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.41 examples/s]\n",
      "06/11/2025 03:25:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.91 examples/s]\n",
      "06/11/2025 03:25:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 174.55 examples/s]\n",
      "06/11/2025 03:25:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 41.31 examples/s]\n",
      "06/11/2025 03:25:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:46 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 29.76 examples/s]\n",
      "06/11/2025 03:25:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.52 examples/s]\n",
      "06/11/2025 03:25:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.99 examples/s]\n",
      "06/11/2025 03:25:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 23.90 examples/s]\n",
      "06/11/2025 03:25:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 24.36 examples/s]\n",
      "06/11/2025 03:25:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:53 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.64 examples/s]\n",
      "06/11/2025 03:25:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.36 examples/s]\n",
      "06/11/2025 03:25:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:58 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 22.48 examples/s]\n",
      "06/11/2025 03:25:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:25:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 30.71 examples/s]\n",
      "06/11/2025 03:25:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:00 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 62.40 examples/s]\n",
      "06/11/2025 03:26:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:02 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 24.48 examples/s]\n",
      "06/11/2025 03:26:02 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:03 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 25.41 examples/s]\n",
      "06/11/2025 03:26:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:08 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 40.42 examples/s]\n",
      "06/11/2025 03:26:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:10 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 26.67 examples/s]\n",
      "06/11/2025 03:26:10 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:11 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.65 examples/s]\n",
      "06/11/2025 03:26:11 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 35.66 examples/s]\n",
      "06/11/2025 03:26:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:14 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 45.06 examples/s]\n",
      "06/11/2025 03:26:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:15 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 23.93 examples/s]\n",
      "06/11/2025 03:26:15 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:16 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 30.80 examples/s]\n",
      "06/11/2025 03:26:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.50 examples/s]\n",
      "06/11/2025 03:26:19 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:21 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 29.40 examples/s]\n",
      "06/11/2025 03:26:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 29.42 examples/s]\n",
      "06/11/2025 03:26:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:23 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 23.64 examples/s]\n",
      "06/11/2025 03:26:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 34.26 examples/s]\n",
      "06/11/2025 03:26:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:27 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.28 examples/s]\n",
      "06/11/2025 03:26:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:29 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 25.65 examples/s]\n",
      "06/11/2025 03:26:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:30 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 32.90 examples/s]\n",
      "06/11/2025 03:26:31 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.30 examples/s]\n",
      "06/11/2025 03:26:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:33 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.89 examples/s]\n",
      "06/11/2025 03:26:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:35 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.36 examples/s]\n",
      "06/11/2025 03:26:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 47.57 examples/s]\n",
      "06/11/2025 03:26:36 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:37 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 39.90 examples/s]\n",
      "06/11/2025 03:26:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 40.01 examples/s]\n",
      "06/11/2025 03:26:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:40 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 27.61 examples/s]\n",
      "06/11/2025 03:26:40 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:41 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 26.96 examples/s]\n",
      "06/11/2025 03:26:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 29.10 examples/s]\n",
      "06/11/2025 03:26:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.83 examples/s]\n",
      "06/11/2025 03:26:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:46 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 207.79 examples/s]\n",
      "06/11/2025 03:26:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.49 examples/s]\n",
      "06/11/2025 03:26:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 33.72 examples/s]\n",
      "06/11/2025 03:26:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 62.68 examples/s]\n",
      "06/11/2025 03:26:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 26.30 examples/s]\n",
      "06/11/2025 03:26:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.91 examples/s]\n",
      "06/11/2025 03:26:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 39.20 examples/s]\n",
      "06/11/2025 03:26:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:55 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.43 examples/s]\n",
      "06/11/2025 03:26:55 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 33.69 examples/s]\n",
      "06/11/2025 03:26:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:58 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 45.53 examples/s]\n",
      "06/11/2025 03:26:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:26:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.80 examples/s]\n",
      "06/11/2025 03:26:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:04 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.41 examples/s]\n",
      "06/11/2025 03:27:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:05 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 27.90 examples/s]\n",
      "06/11/2025 03:27:06 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:07 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.89 examples/s]\n",
      "06/11/2025 03:27:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 30.87 examples/s]\n",
      "06/11/2025 03:27:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:11 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.08 examples/s]\n",
      "06/11/2025 03:27:11 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.26 examples/s]\n",
      "06/11/2025 03:27:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:15 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.11 examples/s]\n",
      "06/11/2025 03:27:15 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:18 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.89 examples/s]\n",
      "06/11/2025 03:27:19 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.13 examples/s]\n",
      "06/11/2025 03:27:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:21 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 23.96 examples/s]\n",
      "06/11/2025 03:27:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 34.59 examples/s]\n",
      "06/11/2025 03:27:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.97 examples/s]\n",
      "06/11/2025 03:27:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:28 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.83 examples/s]\n",
      "06/11/2025 03:27:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:30 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.43 examples/s]\n",
      "06/11/2025 03:27:30 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 35.69 examples/s]\n",
      "06/11/2025 03:27:31 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:32 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.54 examples/s]\n",
      "06/11/2025 03:27:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 26.42 examples/s]\n",
      "06/11/2025 03:27:34 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 35.24 examples/s]\n",
      "06/11/2025 03:27:36 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.15 examples/s]\n",
      "06/11/2025 03:27:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 63.44 examples/s]\n",
      "06/11/2025 03:27:40 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:41 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.11 examples/s]\n",
      "06/11/2025 03:27:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:27:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.84 examples/s]\n",
      "06/11/2025 03:27:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:28:15 - INFO - \t missing_keys: []\n",
      "06/11/2025 03:28:15 - INFO - \t unexpected_keys: []\n",
      "06/11/2025 03:28:15 - INFO - \t mismatched_keys: []\n",
      "06/11/2025 03:28:15 - INFO - \t error_msgs: []\n",
      "06/11/2025 03:28:15 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "06/11/2025 03:28:16 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 24.82 examples/s]\n",
      "06/11/2025 03:28:17 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:28:20 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.80 examples/s]\n",
      "06/11/2025 03:28:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:28:26 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 14.55 examples/s]\n",
      "06/11/2025 03:28:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:28:53 - INFO - \t missing_keys: []\n",
      "06/11/2025 03:28:53 - INFO - \t unexpected_keys: []\n",
      "06/11/2025 03:28:53 - INFO - \t mismatched_keys: []\n",
      "06/11/2025 03:28:53 - INFO - \t error_msgs: []\n",
      "06/11/2025 03:28:53 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "06/11/2025 03:28:55 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 17.03 examples/s]\n",
      "06/11/2025 03:28:55 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:28:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 28.29 examples/s]\n",
      "06/11/2025 03:28:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:28:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 24.84 examples/s]\n",
      "06/11/2025 03:28:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:29:03 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 23.98 examples/s]\n",
      "06/11/2025 03:29:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:29:55 - INFO - \t missing_keys: []\n",
      "06/11/2025 03:29:55 - INFO - \t unexpected_keys: []\n",
      "06/11/2025 03:29:55 - INFO - \t mismatched_keys: []\n",
      "06/11/2025 03:29:55 - INFO - \t error_msgs: []\n",
      "06/11/2025 03:29:55 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "06/11/2025 03:29:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 25.48 examples/s]\n",
      "06/11/2025 03:29:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:29:58 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.77 examples/s]\n",
      "06/11/2025 03:29:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:30:02 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.17 examples/s]\n",
      "06/11/2025 03:30:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:30:27 - INFO - \t missing_keys: []\n",
      "06/11/2025 03:30:27 - INFO - \t unexpected_keys: []\n",
      "06/11/2025 03:30:27 - INFO - \t mismatched_keys: []\n",
      "06/11/2025 03:30:27 - INFO - \t error_msgs: []\n",
      "06/11/2025 03:30:27 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "06/11/2025 03:30:35 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.98 examples/s]\n",
      "06/11/2025 03:30:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:30:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.67 examples/s]\n",
      "06/11/2025 03:30:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:30:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 24.52 examples/s]\n",
      "06/11/2025 03:30:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:30:45 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.24 examples/s]\n",
      "06/11/2025 03:30:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:30:46 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.27 examples/s]\n",
      "06/11/2025 03:30:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:30:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.47 examples/s]\n",
      "06/11/2025 03:30:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:30:53 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.04 examples/s]\n",
      "06/11/2025 03:30:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:30:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.46 examples/s]\n",
      "06/11/2025 03:30:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:30:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.15 examples/s]\n",
      "06/11/2025 03:30:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.28 examples/s]\n",
      "06/11/2025 03:31:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:03 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 41.00 examples/s]\n",
      "06/11/2025 03:31:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:05 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.00 examples/s]\n",
      "06/11/2025 03:31:06 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:07 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 23.26 examples/s]\n",
      "06/11/2025 03:31:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.93 examples/s]\n",
      "06/11/2025 03:31:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:15 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.42 examples/s]\n",
      "06/11/2025 03:31:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:21 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.14 examples/s]\n",
      "06/11/2025 03:31:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:27 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 26.10 examples/s]\n",
      "06/11/2025 03:31:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:29 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.62 examples/s]\n",
      "06/11/2025 03:31:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:32 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 24.03 examples/s]\n",
      "06/11/2025 03:31:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.24 examples/s]\n",
      "06/11/2025 03:31:36 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.20 examples/s]\n",
      "06/11/2025 03:31:40 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 17.93 examples/s]\n",
      "06/11/2025 03:31:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 22.28 examples/s]\n",
      "06/11/2025 03:31:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:46 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 23.71 examples/s]\n",
      "06/11/2025 03:31:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.86 examples/s]\n",
      "06/11/2025 03:31:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.60 examples/s]\n",
      "06/11/2025 03:31:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 23.26 examples/s]\n",
      "06/11/2025 03:31:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:31:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 28.93 examples/s]\n",
      "06/11/2025 03:32:00 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 24.06 examples/s]\n",
      "06/11/2025 03:32:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:03 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 25.54 examples/s]\n",
      "06/11/2025 03:32:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.12 examples/s]\n",
      "06/11/2025 03:32:06 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:08 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.88 examples/s]\n",
      "06/11/2025 03:32:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.65 examples/s]\n",
      "06/11/2025 03:32:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:17 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 39.09 examples/s]\n",
      "06/11/2025 03:32:18 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 17.08 examples/s]\n",
      "06/11/2025 03:32:19 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.30 examples/s]\n",
      "06/11/2025 03:32:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:27 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.83 examples/s]\n",
      "06/11/2025 03:32:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:29 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.78 examples/s]\n",
      "06/11/2025 03:32:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:37 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.03 examples/s]\n",
      "06/11/2025 03:32:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:40 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.54 examples/s]\n",
      "06/11/2025 03:32:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.42 examples/s]\n",
      "06/11/2025 03:32:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.82 examples/s]\n",
      "06/11/2025 03:32:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:45 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 25.48 examples/s]\n",
      "06/11/2025 03:32:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.24 examples/s]\n",
      "06/11/2025 03:32:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.55 examples/s]\n",
      "06/11/2025 03:32:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:32:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.95 examples/s]\n",
      "06/11/2025 03:32:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.03 examples/s]\n",
      "06/11/2025 03:33:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.72 examples/s]\n",
      "06/11/2025 03:33:06 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:15 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 17.86 examples/s]\n",
      "06/11/2025 03:33:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:17 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.59 examples/s]\n",
      "06/11/2025 03:33:18 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:21 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.36 examples/s]\n",
      "06/11/2025 03:33:21 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:23 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.97 examples/s]\n",
      "06/11/2025 03:33:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.61 examples/s]\n",
      "06/11/2025 03:33:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:28 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.62 examples/s]\n",
      "06/11/2025 03:33:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 23.93 examples/s]\n",
      "06/11/2025 03:33:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.94 examples/s]\n",
      "06/11/2025 03:33:34 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:41 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.01 examples/s]\n",
      "06/11/2025 03:33:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.16 examples/s]\n",
      "06/11/2025 03:33:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:45 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 25.52 examples/s]\n",
      "06/11/2025 03:33:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 14.45 examples/s]\n",
      "06/11/2025 03:33:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.25 examples/s]\n",
      "06/11/2025 03:33:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.63 examples/s]\n",
      "06/11/2025 03:33:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:33:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 26.36 examples/s]\n",
      "06/11/2025 03:33:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:34:04 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.20 examples/s]\n",
      "06/11/2025 03:34:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:34:07 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.37 examples/s]\n",
      "06/11/2025 03:34:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:34:10 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.36 examples/s]\n",
      "06/11/2025 03:34:10 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:34:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.43 examples/s]\n",
      "06/11/2025 03:34:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:34:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.65 examples/s]\n",
      "06/11/2025 03:34:24 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:34:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.12 examples/s]\n",
      "06/11/2025 03:34:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:00 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.15 examples/s]\n",
      "06/11/2025 03:35:01 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:10 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.14 examples/s]\n",
      "06/11/2025 03:35:11 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.82 examples/s]\n",
      "06/11/2025 03:35:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.10 examples/s]\n",
      "06/11/2025 03:35:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:22 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.38 examples/s]\n",
      "06/11/2025 03:35:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.52 examples/s]\n",
      "06/11/2025 03:35:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:27 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.35 examples/s]\n",
      "06/11/2025 03:35:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:29 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 35.02 examples/s]\n",
      "06/11/2025 03:35:29 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.45 examples/s]\n",
      "06/11/2025 03:35:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.99 examples/s]\n",
      "06/11/2025 03:35:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:40 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.49 examples/s]\n",
      "06/11/2025 03:35:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.39 examples/s]\n",
      "06/11/2025 03:35:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:45 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.60 examples/s]\n",
      "06/11/2025 03:35:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.48 examples/s]\n",
      "06/11/2025 03:35:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.60 examples/s]\n",
      "06/11/2025 03:35:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.43 examples/s]\n",
      "06/11/2025 03:35:55 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:35:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.32 examples/s]\n",
      "06/11/2025 03:35:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:36:02 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.15 examples/s]\n",
      "06/11/2025 03:36:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:36:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.59 examples/s]\n",
      "06/11/2025 03:36:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:36:18 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  2.83 examples/s]\n",
      "06/11/2025 03:36:19 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:36:33 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.19 examples/s]\n",
      "06/11/2025 03:36:34 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:36:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.55 examples/s]\n",
      "06/11/2025 03:36:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:36:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.77 examples/s]\n",
      "06/11/2025 03:36:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:36:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.39 examples/s]\n",
      "06/11/2025 03:36:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:36:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.03 examples/s]\n",
      "06/11/2025 03:37:00 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:37:08 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.45 examples/s]\n",
      "06/11/2025 03:37:09 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:37:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 17.11 examples/s]\n",
      "06/11/2025 03:37:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:37:15 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.00 examples/s]\n",
      "06/11/2025 03:37:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:37:19 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.37 examples/s]\n",
      "06/11/2025 03:37:20 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:37:21 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 14.88 examples/s]\n",
      "06/11/2025 03:37:22 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:37:23 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 36.55 examples/s]\n",
      "06/11/2025 03:37:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:37:24 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.18 examples/s]\n",
      "06/11/2025 03:37:25 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:37:27 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.08 examples/s]\n",
      "06/11/2025 03:37:28 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:37:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.05 examples/s]\n",
      "06/11/2025 03:37:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:37:37 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.67 examples/s]\n",
      "06/11/2025 03:37:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:37:41 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.75 examples/s]\n",
      "06/11/2025 03:37:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:37:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.07 examples/s]\n",
      "06/11/2025 03:37:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:39:12 - INFO - \t missing_keys: []\n",
      "06/11/2025 03:39:12 - INFO - \t unexpected_keys: []\n",
      "06/11/2025 03:39:12 - INFO - \t mismatched_keys: []\n",
      "06/11/2025 03:39:12 - INFO - \t error_msgs: []\n",
      "06/11/2025 03:39:12 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "06/11/2025 03:39:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.61 examples/s]\n",
      "06/11/2025 03:39:27 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:39:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.08 examples/s]\n",
      "06/11/2025 03:39:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:39:58 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.30 examples/s]\n",
      "06/11/2025 03:39:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:40:07 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.24 examples/s]\n",
      "06/11/2025 03:40:08 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:40:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.21 examples/s]\n",
      "06/11/2025 03:40:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:40:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.89 examples/s]\n",
      "06/11/2025 03:40:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:40:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.48 examples/s]\n",
      "06/11/2025 03:40:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:40:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.01 examples/s]\n",
      "06/11/2025 03:40:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:40:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.57 examples/s]\n",
      "06/11/2025 03:40:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:41:03 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.36 examples/s]\n",
      "06/11/2025 03:41:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:41:25 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.14 examples/s]\n",
      "06/11/2025 03:41:26 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:41:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.29 examples/s]\n",
      "06/11/2025 03:41:35 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:42:02 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.15 examples/s]\n",
      "06/11/2025 03:42:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:42:12 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.75 examples/s]\n",
      "06/11/2025 03:42:13 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:42:32 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.00 examples/s]\n",
      "06/11/2025 03:42:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:42:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.07 examples/s]\n",
      "06/11/2025 03:42:40 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:42:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.83 examples/s]\n",
      "06/11/2025 03:42:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:42:58 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.34 examples/s]\n",
      "06/11/2025 03:42:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:43:05 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.62 examples/s]\n",
      "06/11/2025 03:43:06 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:43:23 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.15 examples/s]\n",
      "06/11/2025 03:43:24 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:43:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.76 examples/s]\n",
      "06/11/2025 03:43:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:43:37 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.03 examples/s]\n",
      "06/11/2025 03:43:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:43:40 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.59 examples/s]\n",
      "06/11/2025 03:43:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:43:45 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.87 examples/s]\n",
      "06/11/2025 03:43:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:43:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.52 examples/s]\n",
      "06/11/2025 03:43:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:43:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.04 examples/s]\n",
      "06/11/2025 03:43:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:44:07 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.68 examples/s]\n",
      "06/11/2025 03:44:07 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:44:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.75 examples/s]\n",
      "06/11/2025 03:44:10 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:44:14 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.04 examples/s]\n",
      "06/11/2025 03:44:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:44:23 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  9.42 examples/s]\n",
      "06/11/2025 03:44:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:44:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.98 examples/s]\n",
      "06/11/2025 03:44:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:44:37 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 16.50 examples/s]\n",
      "06/11/2025 03:44:37 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:44:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  8.13 examples/s]\n",
      "06/11/2025 03:44:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:44:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.31 examples/s]\n",
      "06/11/2025 03:44:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:44:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.51 examples/s]\n",
      "06/11/2025 03:44:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 13.31 examples/s]\n",
      "06/11/2025 03:45:02 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:04 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.50 examples/s]\n",
      "06/11/2025 03:45:04 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:10 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.25 examples/s]\n",
      "06/11/2025 03:45:10 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:13 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 19.62 examples/s]\n",
      "06/11/2025 03:45:14 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:18 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.74 examples/s]\n",
      "06/11/2025 03:45:18 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:29 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 17.33 examples/s]\n",
      "06/11/2025 03:45:30 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:31 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 18.87 examples/s]\n",
      "06/11/2025 03:45:31 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:33 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.64 examples/s]\n",
      "06/11/2025 03:45:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.02 examples/s]\n",
      "06/11/2025 03:45:37 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.62 examples/s]\n",
      "06/11/2025 03:45:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.00 examples/s]\n",
      "06/11/2025 03:45:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 33.73 examples/s]\n",
      "06/11/2025 03:45:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 10.86 examples/s]\n",
      "06/11/2025 03:45:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:45:59 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 11.26 examples/s]\n",
      "06/11/2025 03:45:59 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/11/2025 03:46:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 36.11 examples/s]\n",
      "06/11/2025 03:46:02 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    }
   ],
   "source": [
    "# Score Datasets\n",
    "dataset_names = [\"Examples\", \"Baseline-1\", \"SubA\", \"SubAFiltered\", \"SubB\", \"SubBFiltered\", \"C\", \"CFiltered\", \"D\", \"DFiltered\"]\n",
    "for name in dataset_names:\n",
    "    scored_data = score_dataset(name, save_output=True, version='')\n",
    "    store_scored_dataset(scored_data, name, version='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efa064e6-1e77-42b0-95c1-35496dcb4d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.67%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "measurements = measure_method_by_threshold(\n",
    "    output_fp=\"./ScoredBaseline-1.csv\",\n",
    "    output_threshold=100,\n",
    "    target_threshold=3,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {measurements['accuracy'] * 100:.2f}%\")\n",
    "cm = ConfusionMatrixDisplay(confusion_matrix=measurements['confusion_matrix'])\n",
    "cm.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
