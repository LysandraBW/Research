{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecf5be-df89-4d1a-b75f-d92a5c5fff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trait-Mediated Interaction Modification\n",
    "# Empirical:\n",
    "# You could look for words like \"hypothesis\", \"experiment\", \"found\", and \"discovered\". That may point\n",
    "# towards there being an experiment in the paper. There are also words like \"control group\", \"compared\",\n",
    "# \"findings\", \"results\", \"study\", and more.\n",
    "# Qualitative vs. Quantitative:\n",
    "# To infer whether something is quantitative, you could look for numeric tokens and units.\n",
    "# However, you can only do so much with the abstract. Therefore, this is likely not good enough.\n",
    "# Yet, you could still take advantage of words like \"fewer\" and \"increased\" to show that there is a change.\n",
    "# However, this would be more suited for the above category.\n",
    "# Traits:\n",
    "# There is no NLP tool for traits that I can use or create so I think that I could instead use keywords.\n",
    "# For example, \"snail feeding rates\" is a trait. You may be able to spot this by looking for a word like\n",
    "# \"rate\". You'd expand that word to include \"snail feeding rates\". As \"snail\" is a species you can infer\n",
    "# that \"rates\" is a trait. I would be more decisive and use a dependency parser to ensure that the trait\n",
    "# is a property of the species (like before). However, with all the cases that may exist, I think checking\n",
    "# to see whether a species can be found by traveling back and/or forward without finding certain tokens could\n",
    "# work well enough.\n",
    "# 3 Species or More:\n",
    "# This is simple. However, I think using a dictionary and TaxoNerd would be beneficial (for higher accuracy).\n",
    "# To handle the potential differences in tokenization, character offsets should be used.\n",
    "# Standardization:\n",
    "# There is a lot of variance in the scores. To squash this issue, I think that we could assign each sentence\n",
    "# a value from 0 to 1. We would add these values and divide by the number of sentences. This would result in\n",
    "# a number that is also from 0 to 1. However, there are categories that we would like to inspect. So, we must\n",
    "# create an overall score in the interval from [0, 1] while also scoring each category. Well, for each sentence\n",
    "# we could add a point for each category that is observed. The sentence would receive said score divided by the\n",
    "# number of categories. At the end, we add up all the sentence scores and divide by the number of sentences.\n",
    "# The aggregate score for each category would also be divided by the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ccb94-4c7d-49b1-a727-f6e25b021357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from spacy.language import Language\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "%run -i \"../utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0612988-8cd7-4322-b98a-dbb5dc7d1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Help:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        # Zero Plurals\n",
    "        # The singular and plural versions of the words below are the same.\n",
    "        self.zero_plurals = [\n",
    "            \"species\", \n",
    "            \"deer\", \n",
    "            \"fish\", \n",
    "            \"moose\", \n",
    "            \"sheep\", \n",
    "            \"swine\", \n",
    "            \"buffalo\", \n",
    "            \"trout\", \n",
    "            \"cattle\"\n",
    "        ]\n",
    "        # Irregular Nouns\n",
    "        # There's not a defined conversion method.\n",
    "        self.irregular_nouns = {\n",
    "            \"ox\": \"oxen\",\n",
    "            \"goose\": \"geese\",\n",
    "            \"mouse\": \"mice\",\n",
    "            \"bacterium\": \"bacteria\"\n",
    "        }\n",
    "        self.irregular_nouns_rev = {v: k for k, v in self.irregular_nouns.items()}\n",
    "        self.irregular_singular_nouns = self.irregular_nouns.keys()\n",
    "        self.irregular_plural_nouns = self.irregular_nouns.values()\n",
    "\n",
    "    def remove_extra_spaces(self, string):\n",
    "        # Remove Duplicate Spaces\n",
    "        string = re.sub(r\"\\s+\", \" \", string)\n",
    "        # Remove Spaces Before Punctuation\n",
    "        string = re.sub(r\"\\s+([?.!,])\", r\"\\1\", string)\n",
    "        # Remove Outside Spaces\n",
    "        return string.strip()\n",
    "\n",
    "    def remove_outer_non_alnum(self, string):\n",
    "        while string:\n",
    "            start_len = len(string)\n",
    "            # Remove Leading Non-Alphanumeric Character\n",
    "            if string and not string[0].isalnum():\n",
    "                string = string[1:]\n",
    "            # Remove Trailing Non-Alphanumeric Character\n",
    "            if string and not string[-1].isalnum():\n",
    "                string = string[:-1]\n",
    "            # No Changes Made\n",
    "            if start_len == len(string):\n",
    "                break\n",
    "        return string\n",
    "\n",
    "    def group_text(self, text, flatten=False):\n",
    "        # The parenthetical would be the content inside of a pair of\n",
    "        # matching parentheses, brackets, or braces.\n",
    "        parentheticals = []\n",
    "        \n",
    "        # This contains the text that's not inside of\n",
    "        # parentheses and co.\n",
    "        base_text = []\n",
    "        \n",
    "        # Used for building groups,\n",
    "        # handles a nested structure.\n",
    "        stacks = []\n",
    "        \n",
    "        # These are the characters we recognize\n",
    "        # in terms of grouping.\n",
    "        pairs = {\n",
    "            \"(\": \")\",\n",
    "            \"[\": \"]\",\n",
    "            \"{\": \"}\"\n",
    "        }\n",
    "        open_chars = pairs.keys()\n",
    "        close_chars = pairs.values()\n",
    "        \n",
    "        # This contains the opening characters\n",
    "        # of the groups that are currently open\n",
    "        # (e.g. '(', '['). We use it so that we know\n",
    "        # whether we open or close a group.\n",
    "        opened = []\n",
    "        \n",
    "        for i, char in enumerate(text):\n",
    "            # Opening Character\n",
    "            if char in open_chars:\n",
    "                stacks.append([])\n",
    "                opened.append(char)\n",
    "            # Closing Character\n",
    "            elif opened and char == pairs.get(opened[-1], \"\"):\n",
    "                parentheticals.append(stacks.pop())\n",
    "                opened.pop()\n",
    "            # Add Character to Group\n",
    "            elif opened:\n",
    "                stacks[-1].append(i)\n",
    "            # Add Character to Ungrouped Text\n",
    "            else:\n",
    "                base_text.append(i)\n",
    "        \n",
    "        # If an opening character hasn't been closed,\n",
    "        # we just close all the remaining opened groups.\n",
    "        # This is moreso a problem regarding the text.\n",
    "        while stacks:\n",
    "            parentheticals.append(stacks.pop())\n",
    "            \n",
    "        # Merge\n",
    "        groups = [*parentheticals, base_text]\n",
    "        tuple_groups = []\n",
    "        for group in groups:\n",
    "            if not group:\n",
    "                continue\n",
    "            \n",
    "            tuples = [[group[0], group[0] + 1]]\n",
    "            for index in group[1:]:\n",
    "                if tuples[-1][1] == index:\n",
    "                    tuples[-1][1] = index + 1\n",
    "                else:\n",
    "                    tuples.append([index, index + 1])\n",
    "            tuple_groups.append(tuples)\n",
    "            \n",
    "        if flatten:\n",
    "            flattened_tuple_groups = []\n",
    "            for tuple_group in tuple_groups:\n",
    "                for tuple in tuple_group:\n",
    "                    flattened_tuple_groups.append(tuple)\n",
    "            tuple_groups = flattened_tuple_groups\n",
    "        \n",
    "        return tuple_groups\n",
    "\n",
    "    def singularize(self, string):\n",
    "        string = string.lower()\n",
    "        \n",
    "        # The string to singularize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        if not words:\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is a zero plural\n",
    "        # or a singular irregular noun, there's no changes\n",
    "        # to make. For example, \"red sheep\" and \"ox\" are \n",
    "        # already singular.\n",
    "        if (\n",
    "            words[-1] in self.zero_plurals or \n",
    "            words[-1] in self.irregular_singular_nouns\n",
    "        ):\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is an irregular\n",
    "        # plural noun, we rely on a dictionary with the\n",
    "        # corresponding mapping.\n",
    "        if words[-1] in self.irregular_plural_nouns:\n",
    "            words[-1] = self.irregular_nouns_rev[words[-1]]\n",
    "            return [self.remove_extra_spaces(\" \".join(words))]\n",
    "        \n",
    "        singulars = []\n",
    "\n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to error), we need to\n",
    "        # handle them all.\n",
    "        singular_forms = self.singular_form(words[-1])\n",
    "\n",
    "        if not singular_forms:\n",
    "            return [string]\n",
    "        \n",
    "        for singular_form in singular_forms:\n",
    "            singular = self.remove_extra_spaces(\" \".join([*words[:-1], singular_form]))\n",
    "            singulars.append(singular)\n",
    "            \n",
    "        return singulars\n",
    "\n",
    "    def singular_form(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Change -ies to -y\n",
    "        if re.fullmatch(r\".*ies$\", string):\n",
    "            versions.append(f'{string[:-3]}y')\n",
    "            return versions\n",
    "\n",
    "        # Change -ves to -f and -fe\n",
    "        if re.fullmatch(r\".*ves$\", string):\n",
    "            versions.append(f'{string[:-3]}f')\n",
    "            versions.append(f'{string[:-3]}fe')\n",
    "            return versions\n",
    "\n",
    "        # Remove -es \n",
    "        if re.fullmatch(r\".*es$\", string):\n",
    "            versions.append(f'{string[:-2]}')\n",
    "            return versions\n",
    "\n",
    "        # Change -i to -us\n",
    "        if re.fullmatch(r\".*i$\", string):\n",
    "            versions.append(f'{string[:-1]}us')\n",
    "            return versions\n",
    "\n",
    "        # Remove -s\n",
    "        if re.fullmatch(r\".*s$\", string):\n",
    "            versions.append(f'{string[:-1]}')\n",
    "            return versions\n",
    "\n",
    "        return versions\n",
    "\n",
    "    def pluralize(self, string):\n",
    "        string = string.lower()\n",
    "        \n",
    "        # The string to pluralize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        if not words:\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is a zero plural\n",
    "        # or a plural irregular noun, there's no changes\n",
    "        # to make. For example, \"red sheep\" and \"oxen\" are \n",
    "        # already singular.\n",
    "        if (\n",
    "            words[-1] in self.zero_plurals or \n",
    "            words[-1] in self.irregular_plural_nouns\n",
    "        ):\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is an irregular\n",
    "        # singular noun, we rely on a dictionary with the\n",
    "        # corresponding mapping.\n",
    "        if words[-1] in self.irregular_singular_nouns:\n",
    "            words[-1] = self.irregular_nouns[words[-1]]\n",
    "            return [self.remove_extra_spaces(\" \".join(words))]\n",
    "        \n",
    "        plurals = []\n",
    "        \n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to error), we need to\n",
    "        # handle them all.\n",
    "        plural_forms = self.plural_form(words[-1])\n",
    "\n",
    "        if not plural_forms:\n",
    "            return [string]\n",
    "            \n",
    "        for plural_form in plural_forms:\n",
    "            plural = self.remove_extra_spaces(\" \".join([*words[:-1], plural_form]))\n",
    "            plurals.append(plural)\n",
    "            \n",
    "        return plurals\n",
    "        \n",
    "    def plural_form(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Words that end with -us often have\n",
    "        # two different plural versions: -es and -i.\n",
    "        # For example, the plural version of cactus \n",
    "        # can be cactuses or cacti.\n",
    "        if re.fullmatch(r\".*us$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            versions.append(f'{string[:-2]}i')\n",
    "            return versions\n",
    "\n",
    "        # The -es ending is added to the words below.\n",
    "        if re.fullmatch(r\".*([^l]s|sh|ch|x|z)$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # Words that end with a consonant followed by 'y'\n",
    "        # are made plural by replacing the 'y' with -ies.\n",
    "        # For example, the plural version of canary is\n",
    "        # canaries.\n",
    "        if re.fullmatch(r\".*([^aeiou])(y)$\", string):\n",
    "            versions.append(f'{string[:-1]}ies')\n",
    "            return versions\n",
    "            \n",
    "        # The plural version of words ending with -f\n",
    "        # and -fe aren't clear. To be safe, I will add\n",
    "        # both versions.\n",
    "        if (re.fullmatch(r\".*(f)(e?)$\", string) and not re.fullmatch(r\".*ff$\", string)):\n",
    "            last_clean = re.sub(r\"(f)(e?)$\", \"\", string)\n",
    "            versions.append(f'{last_clean}fs')\n",
    "            versions.append(f'{last_clean}ves')\n",
    "            return versions\n",
    "\n",
    "        # People add -s or -es to words that end with 'o'.\n",
    "        # To be safe, both versions are added.\n",
    "        if re.fullmatch(r\".*([^aeiou])o$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # If there's no -s at the end of the string and\n",
    "        # the other cases didn't run, we add an -s.\n",
    "        if re.fullmatch(r\".*[^s]$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "        \n",
    "        return versions\n",
    "\n",
    "    def expand_unit(self, *, il_unit, ir_unit, il_boundary, ir_boundary, speech=[], literals=[], include=True, direction='BOTH', verbose=False):\n",
    "        assert il_unit <= ir_unit\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            assert il_boundary <= il_unit\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            assert ir_boundary >= ir_unit\n",
    "        \n",
    "        # Move Left\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            # The indices are inclusive, therefore, when \n",
    "            # the condition fails, il_unit will be equal\n",
    "            # to il_boundary.\n",
    "            while il_unit > il_boundary:\n",
    "                # We assume that the current token is allowed,\n",
    "                # and look to the token to the left.\n",
    "                l_token = self.main.sp_doc[il_unit-1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = l_token.pos_ in speech or l_token.lower_ in literals\n",
    "\n",
    "                # Case 1: include=False, in_set=True\n",
    "                # If we're not meant to include the defined tokens, and the\n",
    "                # current token is in that set, we stop expanding.\n",
    "                # Case 2: include=True, in_set=False\n",
    "                # If we're meant to include the defined tokens, and the current\n",
    "                # token is not in that set, we stop expanding.\n",
    "                # Case 3: include=in_set\n",
    "                # If we're meant to include the defined tokens, and the current\n",
    "                # token is in that set, we continue expanding. If we're not meant\n",
    "                # to include the defined tokens, and the current token is not\n",
    "                # in that set, we continue expanding.\n",
    "                if include ^ in_set:\n",
    "                    break\n",
    "                \n",
    "                # Else, the left token is valid, and\n",
    "                # we continue to expand.\n",
    "                il_unit -= 1\n",
    "\n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            # Likewise, when the condition fails,\n",
    "            # ir_unit will be equal to the ir_boundary.\n",
    "            # The ir_boundary is also inclusive.\n",
    "            while ir_unit < ir_boundary:\n",
    "                # Assuming that the current token is valid,\n",
    "                # we look to the right to see if we can\n",
    "                # expand.\n",
    "                r_token = self.main.sp_doc[ir_unit+1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = r_token.pos_ in speech or r_token.lower_ in literals\n",
    "                if include ^ in_set:\n",
    "                    break\n",
    "\n",
    "                # Else, the token is valid and\n",
    "                # we continue.\n",
    "                ir_unit += 1\n",
    "\n",
    "        assert il_unit >= il_boundary and ir_unit <= ir_boundary\n",
    "        expanded_unit = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        return expanded_unit\n",
    "\n",
    "    def contract_unit(self, *, il_unit, ir_unit, speech=[], literals=[], include=True, direction='BOTH', verbose=False):\n",
    "        if il_unit > ir_unit:\n",
    "            print(f\"il_unit of {il_unit} greater than ir_unit of {ir_unit}\")\n",
    "            return None\n",
    "        \n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit < ir_unit:\n",
    "                # We must check if the current token\n",
    "                # is not allowed. If it's not allowed,\n",
    "                # we contract (remove).\n",
    "                token = self.main.sp_doc[il_unit]\n",
    "\n",
    "                # The token is invalid, thus we stop\n",
    "                # contracting.\n",
    "                # include = True means that we want the tokens that match\n",
    "                # the speech and/or literals in the contracted unit.\n",
    "                # include = False means that we don't want the tokens that\n",
    "                # match the speech and/or literals in the contracted unit.\n",
    "                # Case 1: include = True, in_set = True\n",
    "                # We have a token that's meant to be included in the set.\n",
    "                # However, we're contracting, which means we would end up\n",
    "                # removing the token if we continue. Therefore, we break.\n",
    "                # Case 2: include = False, in_set = False\n",
    "                # We have a token that's not in the set which defines the\n",
    "                # tokens that aren't meant to be included. Therefore, we \n",
    "                # have a token that is meant to be included. If we continue,\n",
    "                # we would end up removing this token. Therefore, we break.\n",
    "                # Default:\n",
    "                # If we have a token that's in the set (in_set=True) of\n",
    "                # tokens we're not supposed to include in the contracted \n",
    "                # unit (include=False), we need to remove it. Likewise, if\n",
    "                # we have a token that's not in the set (in_set=False) of\n",
    "                # tokens to include in the contracted unit (include=True),\n",
    "                # we need to remove it.\n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include == in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid, thus we continue.\n",
    "                il_unit += 1\n",
    "\n",
    "        # Move Left      \n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit > il_unit:\n",
    "                token = self.main.sp_doc[ir_unit]\n",
    "\n",
    "                # The token is invalid and we\n",
    "                # stop contracting.\n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include == in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid and we continue.\n",
    "                ir_unit -= 1\n",
    "\n",
    "        assert il_unit <= ir_unit\n",
    "        contracted_unit = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        return contracted_unit\n",
    "\n",
    "    def find_unit_context(self, *, il_unit, ir_unit, il_boundary, ir_boundary, verbose=False):\n",
    "        assert il_unit <= ir_unit\n",
    "        assert il_boundary <= il_unit\n",
    "        assert ir_boundary >= ir_unit\n",
    "        \n",
    "        # Caveat: Parentheticals\n",
    "        # The context of a unit inside of parentheses should not\n",
    "        # go farther than the boundaries of those parentheses.\n",
    "        # However, we need to manually determine whether the unit\n",
    "        # is in parentheses (or any set of the matching symbols\n",
    "        # below).\n",
    "        matching_puncts = {\n",
    "            \"[\": \"]\", \n",
    "            \"(\": \")\", \n",
    "            \"-\": \"-\", \n",
    "            \"--\": \"--\",\n",
    "            \"{\": \"}\",\n",
    "            \",\": \",\"\n",
    "        }\n",
    "        \n",
    "        # The opening symbols for group punctuation.\n",
    "        opening_puncts = list(matching_puncts.keys())\n",
    "\n",
    "        # The closing symbols for group punctuation.\n",
    "        closing_puncts = list(matching_puncts.values())\n",
    "\n",
    "        # Both the opening and closing symbols above.\n",
    "        puncts = [*closing_puncts, *opening_puncts]\n",
    "\n",
    "        # Look for Group Punctuation on the Left\n",
    "        i = il_unit\n",
    "        l_punct = None\n",
    "        while i >= il_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.lower_ in puncts and token.lower_ != \",\":\n",
    "                l_punct = token\n",
    "                break\n",
    "            i -= 1\n",
    "\n",
    "        # Look for Group Punctuation on the Right\n",
    "        i = ir_unit + 1 if l_punct and il_unit == ir_unit else ir_unit\n",
    "        r_punct = None\n",
    "        while i <= ir_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.lower_ in puncts and token.lower_ != \",\":\n",
    "                r_punct = token\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        # If there's a group punctuation on the left\n",
    "        # and right, and they match each other (e.g. '(' and ')'),\n",
    "        # we return the text between the punctuations.\n",
    "        parenthetical = l_punct and r_punct and matching_puncts.get(l_punct.lower_, '') == r_punct.text\n",
    "        if parenthetical:\n",
    "            return self.main.sp_doc[l_punct.i:r_punct.i+1]\n",
    "\n",
    "        # As the unit is not a parenthetical, we will expand\n",
    "        # outwards until we run into a stopping token. The exclude\n",
    "        # list contains tokens that should be excluded from the\n",
    "        # context. Currently, it will contain any parentheticals\n",
    "        # that we run into.\n",
    "        exclude = []\n",
    "\n",
    "        # If a token's POS falls into these categories, we will\n",
    "        # continue. If not, we stop expanding.\n",
    "        speech = [\"ADJ\", \"NOUN\", \"ADP\", \"ADV\", \"PART\", \"PROPN\", \"VERB\", \"PRON\", \"DET\", \"AUX\", \"PART\", \"SCONJ\"]\n",
    "        \n",
    "        # Expand Left\n",
    "        while il_unit > il_boundary:\n",
    "            # Assuming that the current token is fine,\n",
    "            # we look to the left.\n",
    "            l_token = self.main.sp_doc[il_unit-1]\n",
    "\n",
    "            # If it's a closing punctuation (e.g. ')', ']'),\n",
    "            # we need to skip over whatever is contained in\n",
    "            # that punctuation.\n",
    "            if l_token.lower_ in closing_puncts:\n",
    "                i = il_unit - 1\n",
    "                \n",
    "                token = self.main.sp_doc[i]\n",
    "                exclude.append(token)\n",
    "\n",
    "                # We continue until we reach the boundary or\n",
    "                # we find the matching opening punctuation.\n",
    "                opening_punct_found = matching_puncts.get(token.lower_, '') == l_token.lower_\n",
    "                \n",
    "                while i > il_boundary and (not opening_punct_found or (opening_punct_found and l_token == token)):\n",
    "                    i -= 1\n",
    "                    token = self.main.sp_doc[i]\n",
    "                    exclude.append(token)\n",
    "                    opening_punct_found = matching_puncts.get(token.lower_, '') == l_token.lower_\n",
    "                \n",
    "                exclude.append(token)\n",
    "\n",
    "                # After we've gone past the parenthetical,\n",
    "                # we can jump to the next position.\n",
    "                il_unit = i\n",
    "                continue\n",
    "            # If it's not a closing punctuation, we check\n",
    "            # whether it's a stopping token\n",
    "            else:\n",
    "                if l_token.pos_ not in speech:\n",
    "                    break\n",
    "                else:\n",
    "                    il_unit -= 1\n",
    "\n",
    "        # Expand Right\n",
    "        while ir_unit < ir_boundary:\n",
    "            # We're checking the token to the right\n",
    "            # to see if we can expand or not.\n",
    "            r_token = self.main.sp_doc[ir_unit+1]\n",
    "            \n",
    "            # If the token to the right is an opening\n",
    "            # punctuation (e.g. '(', '['), we must skip\n",
    "            # it, the parenthetical inside, and the\n",
    "            # closing punctuation.\n",
    "            if r_token.lower_ in opening_puncts:\n",
    "                i = ir_unit + 1\n",
    "                \n",
    "                token = self.main.sp_doc[i]\n",
    "                exclude.append(token)\n",
    "\n",
    "                closing_punct_found = token.lower_ == matching_puncts.get(r_token.lower_, '')\n",
    "                \n",
    "                while i < ir_boundary and (not closing_punct_found or (closing_punct_found and r_token == token)):\n",
    "                    i += 1\n",
    "                    token = self.main.sp_doc[i]\n",
    "                    exclude.append(token)\n",
    "                    closing_punct_found = token.lower_ == matching_puncts.get(r_token.lower_, '')\n",
    "                \n",
    "                exclude.append(token)\n",
    "\n",
    "                ir_unit = i\n",
    "                continue\n",
    "            # If it's not an opening punctuation, we check\n",
    "            # whether we can continue expanding.\n",
    "            else:\n",
    "                if r_token.pos_ not in speech:\n",
    "                    break\n",
    "                else:\n",
    "                    ir_unit += 1\n",
    "        \n",
    "        # We remove the excluded tokens\n",
    "        # and return the context.\n",
    "        context = [t for t in self.main.sp_doc[il_unit:ir_unit+1] if t not in exclude]\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26736310-0964-4184-a9c1-f6b0b38d7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the Dictionary\n",
    "@Language.component(\"lower_case_lemmas\")\n",
    "def lower_case_lemmas(doc) :\n",
    "    for token in doc :\n",
    "        token.lemma_ = token.lemma_.lower()\n",
    "    return doc\n",
    "\n",
    "class Species:\n",
    "    def __init__(self, main):\n",
    "        # Tools\n",
    "        self.main = main\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\"])\n",
    "        self.tn_nlp.add_pipe(\"lower_case_lemmas\", after=\"lemmatizer\")\n",
    "        self.tn_doc = None\n",
    "        \n",
    "        # Contains any spans that have been identified\n",
    "        # as a species.\n",
    "        self.spans = None\n",
    "        \n",
    "        # Contains any tokens that have been identified\n",
    "        # as a species or being a part of a species.\n",
    "        self.tokens = None\n",
    "        \n",
    "        # Used to quickly access the span that a token\n",
    "        # belongs to.\n",
    "        self.token_to_span = None\n",
    "        \n",
    "        # Maps a string to an array of strings wherein\n",
    "        # the strings involved in the key-value pair \n",
    "        # have been identified as an alternate name of each other.\n",
    "        self.alternate_names = None\n",
    "        \n",
    "        # Used to increase TaxoNERD's accuracy.\n",
    "        self.dictionary = None\n",
    "        self.load_dictionary()\n",
    "\n",
    "    def load_dictionary(self):\n",
    "        self.dictionary = [\"juvenile\", \"adult\", \"prey\", \"predator\", \"species\", \"tree\", \"cat\", \"dog\"]\n",
    "        # df = pd.read_csv(\"VernacularNames.csv\")\n",
    "        # self.dictionary += df.VernacularName.to_list()\n",
    "\n",
    "        patterns = []\n",
    "        for name in self.dictionary:\n",
    "            doc = self.tn_nlp(name)\n",
    "            patterns.append({\"label\": \"LIVB\", \"pattern\": [{\"LEMMA\": token.lemma_} for token in doc]})\n",
    "        ruler = self.tn_nlp.add_pipe(\"entity_ruler\")\n",
    "        ruler.add_patterns(patterns)\n",
    "        \n",
    "    def update(self, text, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        self.spans, self.tokens, self.token_to_span, self.alternate_names = self.load_species(verbose=verbose)\n",
    "\n",
    "    def load_species(self, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # We'll search for species in the text.\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "\n",
    "        # These three contain the species that have been\n",
    "        # identified in the text. Tokens that aren't adjectives,\n",
    "        # nouns, or proper nouns will be stripped.\n",
    "        spans = []\n",
    "        tokens = []\n",
    "        token_to_span = {}\n",
    "\n",
    "        # It's useful to know if a different name refers to a\n",
    "        # species we have already seen. For example, in\n",
    "        # \"predatory crab (Carcinus maenas)\", \"predatory crab\"\n",
    "        # is an alternative name for \"Carcinus maenas\" and\n",
    "        # vice versa. This is used so that the species can be\n",
    "        # properly tracked and redundant points are less\n",
    "        # likely to be given.\n",
    "        alternate_names = {}\n",
    "\n",
    "        # We convert the spans that TaxoNerd has recognized\n",
    "        # to spans under a different parent document. This is\n",
    "        # because we're largely using said parent document and\n",
    "        # there is more functionality in that parent document.\n",
    "        species_spans = []\n",
    "        for tn_species_span in self.tn_doc.ents:\n",
    "            # print(f\"TN Species Span: {tn_species_span}\")\n",
    "            \n",
    "            char_i0 = self.tn_doc[tn_species_span.start].idx\n",
    "            char_i1 = char_i0 + len(tn_species_span.text) - 1\n",
    "\n",
    "            sp_token_i0 = self.main.token_at_char(char_i0).i\n",
    "            sp_token_i1 = self.main.token_at_char(char_i1).i\n",
    "\n",
    "            sp_species_span = self.main.sp_doc[sp_token_i0:sp_token_i1+1]\n",
    "            \n",
    "            # Although they have different parent documents,\n",
    "            # they should still have the same text.\n",
    "            if sp_species_span.text.lower() != tn_species_span.text.lower():\n",
    "                print(sp_species_span.text.lower(), tn_species_span.text.lower())\n",
    "            assert sp_species_span.text.lower() == tn_species_span.text.lower()\n",
    "\n",
    "            # Sometimes, TaxoNerd recognizes two names of a species in one span.\n",
    "            # If they're separated with parentheses, we can handle the case here.\n",
    "            # The naming is difficult, so I'll just call it species_tuples.\n",
    "            species_tuples = self.main.help.group_text(sp_species_span.text, flatten=True)\n",
    "\n",
    "            # print(f\"Species Tuples: {species_tuples}\")\n",
    "            \n",
    "            species_span_chunks = []\n",
    "            for species_tuple in species_tuples:\n",
    "                species_span_chunk_text = sp_species_span.text[species_tuple[0]:species_tuple[1]]\n",
    "                # print(f\"Species Tuple Text: {species_span_chunk_text}\")\n",
    "                \n",
    "                if species_span_chunk_text.isspace():\n",
    "                    continue\n",
    "                \n",
    "                group_char_i0 = char_i0 + species_tuple[0]\n",
    "                group_char_i1 = char_i0 + species_tuple[1] - 1\n",
    "\n",
    "                # Update L Index to Exclude Whitespace Characters\n",
    "                while text[group_char_i0].isspace():\n",
    "                    group_char_i0 += 1\n",
    "\n",
    "                # Update R Index to Exclude Whitespace Characters\n",
    "                while text[group_char_i1].isspace():\n",
    "                    group_char_i1 -= 1\n",
    "\n",
    "                group_token_i0 = self.main.token_at_char(group_char_i0).i\n",
    "                group_token_i1 = self.main.token_at_char(group_char_i1).i\n",
    "\n",
    "                # print(f\"Species Span Chunk Appended: {self.main.sp_doc[group_token_i0:group_token_i1+1]}\")\n",
    "                \n",
    "                species_span_chunks.append(self.main.sp_doc[group_token_i0:group_token_i1+1])\n",
    "\n",
    "            for species_span_chunk in species_span_chunks:\n",
    "                species_spans.append(species_span_chunk)\n",
    "    \n",
    "                # TaxoNERD will recognize the full species (i.e. \"brown squirrels\"),\n",
    "                # and we can use this to find more instances of a species in the text\n",
    "                # by extracting the last noun or proper noun from that span \n",
    "                # (i.e. \"squirrels\"). Now, we can find \"brown squirrels\" and \n",
    "                # \"squirrels\".\n",
    "                reversed_span = [t for t in species_span_chunk]\n",
    "                reversed_span.reverse()\n",
    "                for token in reversed_span:\n",
    "                    # print(f\"Token Reversed Span: {token}\")\n",
    "                    if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                        # print(f\"\\tADDED\")\n",
    "                        species_spans.append(self.main.sp_doc[token.i:token.i+1])\n",
    "                        break\n",
    "\n",
    "        # TaxoNerd sometimes recognizes one instance of a species\n",
    "        # and fails to recognize it elsewhere. To fix this, I'll\n",
    "        # search the text for all the species that TaxoNerd sees.\n",
    "        # This should resolve that issue. To make this more robust,\n",
    "        # I'll include the singular and plural versions of the\n",
    "        # recognized species. Furthermore, the species being used\n",
    "        # to search for other instances of species in the text will\n",
    "        # be called search_species. Using a database I downloaded,\n",
    "        # I've initialized search_species with a set of english\n",
    "        # vernacular names (e.g., \"dog\", \"cat\"). I'm removing it for\n",
    "        # now because there's seemingly a lot of bogus values.\n",
    "        # df = pd.read_csv(\"EnglishVernacularNames-2.csv\")\n",
    "        # search_species = df.Name.to_list()\n",
    "        search_species = [\"juvenile\", \"adult\", \"prey\", \"predator\", \"predators\", \"species\", \"tree\", \"cat\", \"dog\", \"flies\", \"plants\", \"plant\", \"fly\"]\n",
    "\n",
    "        for species_span in species_spans:\n",
    "            species_text = species_span.text.lower()\n",
    "            species_text = self.main.help.remove_extra_spaces(self.main.help.remove_outer_non_alnum(species_text))\n",
    "            \n",
    "            # print(f\"Species Text: {species_text}\")\n",
    "\n",
    "            # not [c for c in species_text if c.isalpha()]\n",
    "            if not species_text or not [c for c in species_text if c.isalpha()]:\n",
    "                # print(f\"Continued\")\n",
    "                continue\n",
    "            \n",
    "            search_species.append(species_text)\n",
    "\n",
    "            # Add Singular and/or Plural Version\n",
    "            if species_span[-1].pos_ == \"NOUN\":\n",
    "                # Plural\n",
    "                if species_span[-1].tag_ == \"NNS\":\n",
    "                    singular_species = self.main.help.singularize(species_text)\n",
    "                    search_species.extend(singular_species)\n",
    "                # Singular\n",
    "                if species_span[-1].tag_ == \"NN\":\n",
    "                    plural_species = self.main.help.pluralize(species_text)\n",
    "                    search_species.extend(plural_species)\n",
    "\n",
    "        # Now, we have the species to search for in the text.\n",
    "        search_species = list(set(search_species))\n",
    "        # print(f\"Search Species: {search_species}\")\n",
    "        \n",
    "        for species in search_species:\n",
    "            matches = re.finditer(re.escape(species), text, re.IGNORECASE)\n",
    "            \n",
    "            for char_i0, char_i1, matched_text in [(match.start(), match.end(), match.group()) for match in matches]:\n",
    "                # The full word must match, not just a substring inside of it.\n",
    "                # So, if the species we're looking for is \"ant\", only \"ant\"\n",
    "                # will match -- not \"pants\" or \"antebellum\". Therefore, the\n",
    "                # characters to the left and right of the matched string must be\n",
    "                # non-alphanumeric.\n",
    "                l_char_is_letter = char_i0 > 0 and text[char_i0-1].isalpha()\n",
    "                r_char_is_letter = char_i1 < len(text) and text[char_i1].isalpha()\n",
    "                \n",
    "                if l_char_is_letter or r_char_is_letter or not matched_text:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    sp_li = self.main.token_at_char(char_i0).i\n",
    "                    sp_ri = self.main.token_at_char(char_i1-1).i\n",
    "                except Exception as e:\n",
    "                    print(f\"Matched Text: '{matched_text}'\")\n",
    "                    print(e)\n",
    "                    continue\n",
    "\n",
    "                # This is the matched substring (which would be\n",
    "                # a species) as a span in the parent document.\n",
    "                species_span = self.main.sp_doc[sp_li:sp_ri+1]\n",
    "                \n",
    "                # Expand Species\n",
    "                # Let's say there's a word like \"squirrel\". That's a bit ambiguous. \n",
    "                # Is it a brown squirrel, a bonobo? If the species is possibly missing\n",
    "                # information (like an adjective to the left of it), we should expand\n",
    "                # in order to get a full picture of the species.\n",
    "                unclear_1 = len(species_span) == 1 and species_span[0].pos_ == \"NOUN\"\n",
    "                unclear_2 = species_span.start > 0 and self.main.sp_doc[species_span.start-1].pos_ in [\"ADJ\"]\n",
    "                \n",
    "                if unclear_1 or unclear_2:\n",
    "                    species_span = self.main.help.expand_unit(\n",
    "                        il_unit=species_span.start, \n",
    "                        ir_unit=species_span.end-1,\n",
    "                        il_boundary=0,\n",
    "                        ir_boundary=len(self.main.sp_doc),\n",
    "                        speech=[\"ADJ\", \"PROPN\"],\n",
    "                        literals=[\"-\"],\n",
    "                        include=True,\n",
    "                        direction=\"LEFT\",\n",
    "                        verbose=verbose\n",
    "                    )\n",
    "                \n",
    "                # Remove Outer Symbols\n",
    "                # There are times where a species is identified with a parenthesis\n",
    "                # nearby. Here, we remove that parenthesis (and any other symbols).\n",
    "                species_span = self.main.help.contract_unit(\n",
    "                    il_unit=species_span.start, \n",
    "                    ir_unit=species_span.end-1, \n",
    "                    speech=[\"PUNCT\", \"SYM\", \"DET\", \"PART\"],\n",
    "                    include=False,\n",
    "                    verbose=verbose\n",
    "                )\n",
    "\n",
    "                if not species_span:\n",
    "                    print(f\"Matched Text: '{matched_text}'\")\n",
    "                    print(char_i0)\n",
    "                    continue\n",
    "            \n",
    "                # A species must have a noun or a\n",
    "                # proper noun. This may help discard\n",
    "                # bogus results.\n",
    "                letter_found = False\n",
    "                for token in species_span:\n",
    "                    if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                        letter_found = True\n",
    "                        break\n",
    "\n",
    "                if not letter_found:\n",
    "                    continue\n",
    "\n",
    "                # Adding Species\n",
    "                spans.append(species_span)\n",
    "                for token in species_span:\n",
    "                    if token in tokens or token.pos_ in [\"PUNCT\", \"SYM\", \"DET\", \"PART\"]:\n",
    "                        continue\n",
    "                    tokens.append(token)\n",
    "                    token_to_span[token] = species_span\n",
    "\n",
    "        # Removing Duplicates and Sorting \n",
    "        spans = list({span.start: span for span in spans}.values())\n",
    "        spans.sort(key=lambda span: span.start)\n",
    "        \n",
    "        # Finding and Storing Alternative Names\n",
    "        for i, species_span in enumerate(spans):\n",
    "            # There's not a next species to\n",
    "            # evaluate.\n",
    "            if i + 1 >= len(spans):\n",
    "                break\n",
    "            \n",
    "            next_species_span = spans[i+1]\n",
    "            \n",
    "            # If there's one token between the species and the next species,\n",
    "            # we check if the next species is surrounded by punctuation.\n",
    "            if next_species_span.start - species_span.end == 1:\n",
    "                # Token Before and After the Next Species\n",
    "                before_next = self.main.sp_doc[next_species_span.start-1]\n",
    "                after_next = self.main.sp_doc[next_species_span.end]\n",
    "\n",
    "                if before_next.pos_ in [\"PUNCT\", \"SYM\"] and after_next.pos_ in [\"PUNCT\", \"SYM\"]:\n",
    "                    sp_1_text = species_span.text.lower()\n",
    "                    sp_2_text = next_species_span.text.lower()\n",
    "                    \n",
    "                    if sp_1_text not in alternate_names:\n",
    "                        alternate_names[sp_1_text] = []\n",
    "                    \n",
    "                    if sp_2_text not in alternate_names:\n",
    "                        alternate_names[sp_2_text] = []\n",
    "                    \n",
    "                    alternate_names[sp_1_text].append(sp_2_text)\n",
    "                    alternate_names[sp_2_text].append(sp_1_text)\n",
    "            # If there's no token between the species and the next,\n",
    "            # species we assume that they refer to the same species.\n",
    "            elif next_species_span.start - species_span.end == 0:\n",
    "                sp_1_text = species_span.text.lower()\n",
    "                sp_2_text = next_species_span.text.lower()\n",
    "                \n",
    "                if sp_1_text not in alternate_names:\n",
    "                    alternate_names[sp_1_text] = []\n",
    "                \n",
    "                if sp_2_text not in alternate_names:\n",
    "                    alternate_names[sp_2_text] = []\n",
    "\n",
    "                alternate_names[sp_1_text].append(sp_2_text)\n",
    "                alternate_names[sp_2_text].append(sp_1_text)\n",
    "       \n",
    "        return (spans, tokens, token_to_span, alternate_names)\n",
    "\n",
    "    def span_at_token(self, token):\n",
    "        if token in self.token_to_span:\n",
    "            return self.token_to_span[token]\n",
    "        return None\n",
    "    \n",
    "    def is_species(self, token):\n",
    "        return token in self.tokens\n",
    "        \n",
    "    def has_species(self, tokens, verbose=False):\n",
    "        for token in tokens:\n",
    "            if token in self.tokens:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def find_same_species(self, sp_A, sp_b, verbose=False):\n",
    "        # METHOD 1: Check for Literal Matches\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        \n",
    "        for sp_a in sp_A:\n",
    "            # Verbatim Text\n",
    "            sp_a_text = sp_a.text.lower()\n",
    "\n",
    "            if sp_a_text == sp_b_text:\n",
    "                return sp_a\n",
    "\n",
    "            # Singularized Text\n",
    "            sp_a_singular_texts = sp_a_text if sp_a[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(sp_a_text)\n",
    "            sp_b_singular_texts = sp_b_text if sp_b[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(sp_b_text)\n",
    "\n",
    "            if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "                return sp_a\n",
    "\n",
    "        # METHOD 2: Check Alternate Names\n",
    "        for sp_a in sp_A:\n",
    "            # Species B is an alternate name for Species A\n",
    "            if sp_b_text in self.alternate_names.get(sp_a_text, []):\n",
    "                return sp_a\n",
    "            # Species A is an alternate name for Species B\n",
    "            if sp_a_text in self.alternate_names.get(sp_b_text, []):\n",
    "                return sp_a\n",
    "        \n",
    "        # METHOD 3: Check Nouns\n",
    "        # This is used if one or none of the species being compared\n",
    "        # has 1 adjective.\n",
    "        sp_b_0_text = sp_b[0].lower_\n",
    "        sp_b_is_noun = sp_b[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "        sp_b_nouns = []\n",
    "        sp_b_num_adjectives = 0\n",
    "        for token in sp_b:\n",
    "            if not sp_b_nouns and token.pos_ == \"ADJ\":\n",
    "                sp_b_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_b_nouns.append(token)\n",
    "        sp_b_nouns_str = [noun.lower_ for noun in sp_b_nouns]\n",
    "        sp_b_singular_texts = \" \".join(sp_b_nouns_str) if sp_b_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(\" \".join(sp_b_nouns_str))\n",
    "        \n",
    "        for sp_a in sp_A:\n",
    "            sp_a_0_text = sp_a[0].lower_\n",
    "            sp_a_is_noun = sp_a[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "            # Case Example: 'Hyla' v. 'Hyla tadpoles'\n",
    "            if sp_a_0_text == sp_b_0_text and (sp_a_is_noun or sp_b_is_noun):\n",
    "                if sp_a_text in sp_b_text or sp_b_text in sp_a_text:\n",
    "                    return sp_a\n",
    "            # Case Example: 'dogs' v. 'red dogs'\n",
    "            else:\n",
    "                sp_a_nouns = []\n",
    "                sp_a_num_adjectives = 0\n",
    "                for token in sp_a:\n",
    "                    if not sp_a_nouns and token.pos_ == \"ADJ\":\n",
    "                        sp_a_num_adjectives += 1\n",
    "                    elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                        sp_a_nouns.append(token)\n",
    "                sp_a_nouns_str = [noun.lower_ for noun in sp_a_nouns]\n",
    "                \n",
    "                if sp_a_nouns and sp_b_nouns and (\n",
    "                    (sp_a_num_adjectives == 1 and sp_b_num_adjectives == 0) or \n",
    "                    (sp_b_num_adjectives == 1 and sp_a_num_adjectives == 0)\n",
    "                ):\n",
    "                    sp_a_singular_texts = \" \".join(sp_a_nouns_str) if sp_a_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(\" \".join(sp_a_nouns_str))\n",
    "                    \n",
    "                    if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "                        return sp_a\n",
    "\n",
    "        # METHOD 3: Last Ditch Effort\n",
    "        # If there's been no matches, we just look for one string inside of\n",
    "        # another.\n",
    "        for sp_a in sp_A:\n",
    "            sp_a_text = sp_a.text.lower()\n",
    "            if sp_b_text in sp_a_text or sp_a_text in sp_b_text:\n",
    "                return sp_a\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31521a-3d38-4087-b63e-bb974ad3a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    def __init__(self, main, *, regexes=[], vocab=[], def_pos=[], def_tag=[], def_threshold=0.7):\n",
    "        self.main = main\n",
    "        # When comparing two words, SpaCy returns a value\n",
    "        # from 0 to 1, representing how similar the two\n",
    "        # embeddings are. The threshold below determines\n",
    "        # the minimum number of similarity before two words\n",
    "        # are considered as being equivalent. This is the\n",
    "        # default values, specific values can be provided for\n",
    "        # each word.\n",
    "        self.def_threshold = def_threshold\n",
    "        # To decrease our \"search space\", we can specify the\n",
    "        # type of tokens we'd want to consider via the\n",
    "        # tag_ and pos_ attributes. The tag and pos keys for a\n",
    "        # vocab word is used for more specificity.\n",
    "        self.def_tag = def_tag\n",
    "        self.def_pos = def_pos\n",
    "        # The words are divided into two categories: vocabulary\n",
    "        # and regex. The vocabulary matches words\n",
    "        # by definition, whereas the regexes match words by\n",
    "        # content.\n",
    "        self.regex = regexes\n",
    "        self.vocab = []\n",
    "        for vocab_word in vocab:\n",
    "            if isinstance(vocab_word, str):\n",
    "                doc = self.main.sp_nlp(vocab_word)\n",
    "                self.vocab.append({\n",
    "                    \"doc\": doc,\n",
    "                    \"lemma\": \" \".join([t.lemma_ for t in doc])\n",
    "                })\n",
    "            else:\n",
    "                doc = self.main.sp_nlp(vocab_word[\"word\"])\n",
    "                self.vocab.append({\n",
    "                    \"doc\": doc,\n",
    "                    \"tag\": vocab_word.get(\"tag\"),\n",
    "                    \"pos\": vocab_word.get(\"pos\"),\n",
    "                    \"threshold\": vocab_word.get(\"threshold\"),\n",
    "                    \"lemma\": \" \".join([t.lemma_ for t in doc])\n",
    "                })\n",
    "        \n",
    "        # Matched Tokens\n",
    "        self.tokens = []\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tokens = self.match_tokens(verbose=verbose)\n",
    "\n",
    "    def match_tokens(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        matched_tokens = []\n",
    "\n",
    "        # Match by Regex\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "        for regex in self.regex:\n",
    "            if verbose:\n",
    "                print(regex, [match.start() for match in re.finditer(regex, text, re.IGNORECASE)])\n",
    "            for char_index in [match.start() for match in re.finditer(regex, text, re.IGNORECASE)]:\n",
    "                adj_char_index = char_index\n",
    "                while text[adj_char_index].isspace():\n",
    "                    adj_char_index += 1\n",
    "                matched_tokens.append(self.main.token_at_char(adj_char_index))\n",
    "\n",
    "        # Match by Vocab\n",
    "        for token in self.main.sp_doc:\n",
    "            if (\n",
    "                (self.def_pos and token.pos_ not in self.def_pos) or \n",
    "                (self.def_tag and token.tag_ not in self.def_tag) or \n",
    "                (token in matched_tokens)\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            token_doc = self.main.sp_nlp(token.lower_)\n",
    "            token_lemma = \" \".join([t.lemma_ for t in token_doc])\n",
    "            \n",
    "            for vocab_word in self.vocab:\n",
    "                # Ensure Correct Tag\n",
    "                if vocab_word.get(\"tag\"):\n",
    "                    if not [t for t in token_doc if t.tag_ in vocab_word.get(\"tag\")]:\n",
    "                        continue\n",
    "                \n",
    "                # Ensure Correct PoS\n",
    "                if vocab_word.get(\"pos\"):\n",
    "                    if not [t for t in token_doc if t.pos_ in vocab_word.get(\"pos\")]:\n",
    "                        continue\n",
    "\n",
    "                # Check Lemma\n",
    "                if token_lemma == vocab_word[\"lemma\"]:\n",
    "                    matched_tokens.append(token)\n",
    "                    break\n",
    "\n",
    "                # Check Similarity\n",
    "                similarity = vocab_word[\"doc\"].similarity(token_doc)\n",
    "                \n",
    "                if similarity >= vocab_word.get(\"threshold\", self.def_threshold):\n",
    "                    matched_tokens.append(token)\n",
    "                    break\n",
    "                \n",
    "        return matched_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699dce8-5827-4757-becd-a016f9a06cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"study\", \n",
    "                \"hypothesis\", \n",
    "                \"experiment\", \n",
    "                \"found\", \n",
    "                \"discover\", \n",
    "                \"compare\", \n",
    "                \"finding\", \n",
    "                \"result\", \n",
    "                \"test\", \n",
    "                \"examine\", \n",
    "                \"model\",\n",
    "                \"measure\",\n",
    "                \"manipulate\",\n",
    "                \"assess\",\n",
    "                \"conduct\",\n",
    "                \"data\",\n",
    "                \"analyze\",\n",
    "                \"sample\",\n",
    "                \"observe\",\n",
    "                \"predict\",\n",
    "                \"suggest\",\n",
    "                \"method\",\n",
    "                \"investigation\",\n",
    "                \"trial\",\n",
    "                \"experimental\",\n",
    "                \"evidence\",\n",
    "                \"demonstrate\",\n",
    "                \"analysis\",\n",
    "                \"show\",\n",
    "                \"compare\",\n",
    "                \"comparable\",\n",
    "                \"control group\", \n",
    "                \"independent\",\n",
    "                \"dependent\",\n",
    "                \"applied\"\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4551e-5af3-4d29-92d1-485f3c1f25b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"theory\",\n",
    "                \"review\",\n",
    "                \"analysis\",\n",
    "                \"meta-analysis\"\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ae4dd9-f73e-41d5-926e-96c383d65c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CauseKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"increase\", \n",
    "                \"decrease\", \n",
    "                \"change\", \n",
    "                \"shift\", \n",
    "                \"cause\", \n",
    "                \"produce\", \n",
    "                \"trigger\", \n",
    "                \"suppress\", \n",
    "                \"inhibit\",\n",
    "                \"encourage\",\n",
    "                \"allow\",\n",
    "                \"influence\",\n",
    "                \"affect\",\n",
    "                \"alter\",\n",
    "                \"induce\",\n",
    "                \"produce\",\n",
    "                \"result in\",\n",
    "                \"associated\",\n",
    "                \"correlated\",\n",
    "                \"contribute\",\n",
    "                \"impact\",\n",
    "                \"deter\",\n",
    "                \"depressed\",\n",
    "                \"when\",\n",
    "                \"because\",\n",
    "                # \"reduce\",\n",
    "                # \"killed\",\n",
    "                # \"supported\"\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"SCONJ\", \"NOUN\"],\n",
    "            # def_tag=[\"VB\", \"VBD\", \"WRB\", \"IN\", \"VBG\"],\n",
    "            # def_threshold=0.75\n",
    "            def_threshold=0.8\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            # I'm not sure what cause words should be filtered out, because\n",
    "            # I haven't seen everything, but this word should be filtered out,\n",
    "            # it's not really reflective the changes that we're looking for. But,\n",
    "            # sometimes it is, so it's up in the air. However, I feel like the\n",
    "            # writer would use more clear language like \"decrease\" or something.\n",
    "            if token.lemma_ in [\"kill\"]:\n",
    "                continue\n",
    "            filtered.append(token)\n",
    "            \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb56df8-95d1-4ac9-956a-f7aeddb3ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"few\", \n",
    "                \"more\", \n",
    "                \"increase\", \n",
    "                \"decrease\", \n",
    "                \"less\", \n",
    "                \"short\", \n",
    "                \"long\", \n",
    "                \"greater\"\n",
    "                \"shift\",\n",
    "                \"fluctuate\",\n",
    "                \"adapt\",\n",
    "                \"grow\",\n",
    "                \"rise\"\n",
    "                \"surge\",\n",
    "                \"intensify\",\n",
    "                \"amplify\",\n",
    "                \"multiply\",\n",
    "                \"decline\",\n",
    "                \"reduce\",\n",
    "                \"drop\",\n",
    "                \"diminish\",\n",
    "                \"fall\",\n",
    "                \"lessen\",\n",
    "                \"doubled\",\n",
    "                \"tripled\",\n",
    "            ],\n",
    "            regexes=[\n",
    "                # Match Examples:\n",
    "                # 1. \"one... as...\"\n",
    "                # 2. \"2x than...\"\n",
    "                r\"(one|two|three|four|five|six|seven|eight|nine|ten|twice|thrice|([0-9]+|[0-9]+.[0-9]+)(x|%))[\\s-]+[^\\s]*[\\s-]+(as|more|than|likely)([\\s-]+|$)\"\n",
    "            ],\n",
    "            def_pos=[\"NOUN\", \"ADJ\", \"ADV\"],\n",
    "            def_threshold=0.75\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose=True)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in self.main.sp_doc:\n",
    "            # Already Matched\n",
    "            if token in tokens:\n",
    "                filtered.append(token)\n",
    "            \n",
    "            # Comparative Adjective\n",
    "            # Looking for words like \"bigger\" and \"better\".\n",
    "            elif token.pos_ == \"ADJ\" and token.tag_ == \"JJR\":\n",
    "                filtered.append(token)\n",
    "                continue\n",
    "            \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70792f-d32b-4638-a929-c7499506e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraitKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            regexes=[\n",
    "                r\"behaviou?r\", \n",
    "                r\"[^A-Za-z]+rate\", \n",
    "                \"colou?r\",\n",
    "                \"biomass\",\n",
    "                r\"[^A-Za-z]+mass\", \n",
    "                r\"[^A-Za-z]+size\", \n",
    "                \"length\", \n",
    "                \"pattern\", \n",
    "                \"weight\",\n",
    "                \"shape\", \n",
    "                \"efficiency\", \n",
    "                \"trait\",\n",
    "                \"phenotype\",\n",
    "                \"demography\",\n",
    "                \"population structure\",\n",
    "                \"ability\", \n",
    "                \"capacity\", \n",
    "                \"height\", \n",
    "                \"width\", \n",
    "                \"[A-Za-z]+span\",\n",
    "                \"diet\",\n",
    "                \"feeding\",\n",
    "                \"nest\",\n",
    "                \"substrate\",\n",
    "                \"breeding\",\n",
    "                r\"[^A-Za-z]+age[^A-Za-z]+\",\n",
    "                \"lifespan\",\n",
    "                \"development\",\n",
    "                \"time\",\n",
    "                \"mating\",\n",
    "                \"[^A-Za-z]+fur\",\n",
    "                \"feathers\",\n",
    "                \"scales\",\n",
    "                \"skin\",\n",
    "                \"limb\",\n",
    "                \"configuration\",\n",
    "                \"dimorphism\",\n",
    "                \"capability\",\n",
    "                \"appendages\",\n",
    "                \"blood\",\n",
    "                \"regulation\",\n",
    "                \"excretion\",\n",
    "                \"luminescence\",\n",
    "                r\"[^A-Za-z]+role\",\n",
    "                \"reproduction\",\n",
    "                \"courtship\",\n",
    "                \"pollination\",\n",
    "                \"mechanism\",\n",
    "                \"sensitivity\",\n",
    "                \"resistance\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=True):\n",
    "        verbose = True\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        print(f\"Tokens to Filter: {tokens}\")\n",
    "        \n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            expanded_token = self.main.help.expand_unit(\n",
    "                il_unit=token.i, \n",
    "                ir_unit=token.i, \n",
    "                il_boundary=0, \n",
    "                ir_boundary=len(self.main.sp_doc) - 1, \n",
    "                speech=[\"PUNCT\"],\n",
    "                include=False,\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "            print(f\"\\tToken: {token}\\n\\tExpanded Token: {expanded_token}\")\n",
    "            if self.main.species.has_species(expanded_token):\n",
    "                filtered.append(token)\n",
    "\n",
    "        print(f\"Filtered Tokens: {filtered}\")\n",
    "        \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf8e07-a326-4fea-8615-faa5d549852d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Main:\n",
    "    def __init__(self):\n",
    "        # Tools\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        self.sp_doc = None\n",
    "\n",
    "        # Maps Character Position to Token in Document\n",
    "        # Used to handle differences between different\n",
    "        # pipelines and tools.\n",
    "        self.index_map = None\n",
    "    \n",
    "        # Parsers\n",
    "        self.species = Species(self)\n",
    "        self.trait = TraitKeywords(self)\n",
    "        self.cause = CauseKeywords(self)\n",
    "        self.change = ChangeKeywords(self)\n",
    "        self.experiment = ExperimentKeywords(self)\n",
    "        self.neg_experiment = NegativeExperimentKeywords(self)\n",
    "\n",
    "        # Helper\n",
    "        self.help = Help(self)\n",
    "\n",
    "    def update_doc(self, doc, verbose=False):\n",
    "        self.sp_doc = doc\n",
    "        self.index_map = self.load_index_map()\n",
    "        self.species.update(doc.text, verbose=True)\n",
    "        self.trait.update(verbose=False)\n",
    "        self.cause.update(verbose=False)\n",
    "        self.change.update(verbose=False)\n",
    "        self.experiment.update(verbose=False)\n",
    "        self.neg_experiment.update(verbose=False)\n",
    "\n",
    "    def update_text(self, text, verbose=False):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.update_doc(self.sp_doc, verbose=verbose)\n",
    "        \n",
    "    def token_at_char(self, char_index):\n",
    "        # SpaCy Doc or Indexing Map Not Found\n",
    "        if not self.sp_doc or not self.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        if char_index in self.index_map:\n",
    "            return self.index_map[char_index]\n",
    "\n",
    "        raise Exception(f\"Token at Index {char_index} Not Found\")\n",
    "        \n",
    "    def load_index_map(self):\n",
    "        # SpaCy Doc Not Found\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Map Character Index to Token\n",
    "        index_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            # char_i0 is the index of the token's starting character.\n",
    "            # char_i1 is the index of the character after the token's ending character.\n",
    "            char_i0 = token.idx\n",
    "            char_i1 = token.idx + len(token)\n",
    "\n",
    "            for i in range(char_i0, char_i1):\n",
    "                index_map[i] = token\n",
    "\n",
    "        return index_map\n",
    "\n",
    "    def score(self, verbose=False):\n",
    "        NUM_CATEGORIES = 4\n",
    "\n",
    "        # Requires the mention of a trait and a cause or change word.\n",
    "        # The cause or change word indicates some variation.\n",
    "        # Index 0 in Array\n",
    "        TRAIT = 0\n",
    "\n",
    "        # Requires the mention of a species and a cause or change word.\n",
    "        # The cause or change word indicates that the species is being\n",
    "        # affected or is affecting something else.\n",
    "        # Index 1 in Array\n",
    "        SPECIES = 1\n",
    "\n",
    "        # Requires a word that has been defined as \"experiment\"-related.\n",
    "        # Index 2 in Array\n",
    "        EXPERIMENT = 2\n",
    "\n",
    "        # Requires the mention of several species (more or less).\n",
    "        # Index 3 in Array\n",
    "        INTERACTION = 3\n",
    "\n",
    "        # Max # of Points of Category per Sentence (MPC)\n",
    "        # A category can collect points from each sentence. However,\n",
    "        # there's a maximum number of points it can collect. This is\n",
    "        # determined by the MPC.\n",
    "        MPC = [0] * NUM_CATEGORIES\n",
    "        MPC[TRAIT] = 1\n",
    "        MPC[SPECIES] = 1\n",
    "        MPC[EXPERIMENT] = 1\n",
    "        MPC[INTERACTION] = 1\n",
    "\n",
    "        # TODO: This is no longer needed as the scores are first calculated vertically,\n",
    "        # rather than horizontally. Previously, we'd add up the points a sentence received\n",
    "        # across its categories. In order to have a final score in the range [0, 1], the\n",
    "        # maximum points a sentence received had to be in the range [0, 1]. However, \n",
    "        # since we no longer add up all the points a sentence received across all\n",
    "        # categories (to apply the weights at the end), they don't need to add up\n",
    "        # to 1. It's hard to explain without an example.\n",
    "        # assert np.sum(MPC) == 1\n",
    "        \n",
    "        # Points per Instance of Category (PIC)\n",
    "        # Each token is evaluated to check whether a category\n",
    "        # can be given points. The number of points given, if\n",
    "        # the token is determined to be satisfactory, is the PIC.\n",
    "        # The PIC is less than or equal to the MPC for the corresponding\n",
    "        # category. The idea behind the PIC and MPC is similar to how\n",
    "        # sets work in tennis: you're not immediately awarded the full points\n",
    "        # for the set (MPC) if your opponent fails to return the ball,\n",
    "        # instead you're given a smaller # of points (PIC) that allow you to\n",
    "        # incrementally win the set (category).\n",
    "        PIC = [0] * NUM_CATEGORIES\n",
    "        PIC[TRAIT] = MPC[TRAIT]*1.0\n",
    "        PIC[SPECIES] = MPC[SPECIES]/3.0\n",
    "        PIC[EXPERIMENT] = MPC[EXPERIMENT]/1.0\n",
    "        PIC[INTERACTION] = MPC[INTERACTION]/3.0\n",
    "\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            assert 0 < PIC[i] <= MPC[i]\n",
    "\n",
    "        # Category Weights (CW)\n",
    "        # It may be helpful to weigh a certain category's fraction of total points\n",
    "        # more or less than another's. Thus, at the end, we'll take a\n",
    "        # weighted average of the category's FTP. The weights must add up to 1.\n",
    "        CW = [0] * NUM_CATEGORIES\n",
    "        CW[TRAIT] = 0.7\n",
    "        CW[SPECIES] = 0.1\n",
    "        CW[EXPERIMENT] = 0.1\n",
    "        CW[INTERACTION] = 0.1\n",
    "\n",
    "        assert round(np.sum(CW)) == 1\n",
    "\n",
    "        # Leniency\n",
    "        # There are certain categories that aren't going to be as frequent as others.\n",
    "        # For example, the trait category. You could try and decrease the influence\n",
    "        # of said category by lowering its MPC and/or increasing the PIC (so that it's\n",
    "        # easier to achieve the FTP). However, this could make it harder to meaningfully\n",
    "        # represent the category. The idea of leniency is to remove (some) sentences that had 0\n",
    "        # points from the scoring. This increases the FTP as, for example, instead of comparing\n",
    "        # 0.5 points to a total of 2.5 points, you can compare 0.5 to 2.0 points, and so on.\n",
    "        # A leniency of 1 means that all sentences that received 0 points will be removed from\n",
    "        # the scoring. A leniency of 0 means that all the sentences are included in the scoring.\n",
    "        LEN = [0] * NUM_CATEGORIES\n",
    "        LEN[TRAIT] = 0\n",
    "        LEN[SPECIES] = 0\n",
    "        LEN[EXPERIMENT] = 0\n",
    "        LEN[INTERACTION] = 0\n",
    "\n",
    "        # Used for Leniency\n",
    "        zero_pt_sents = [0] * NUM_CATEGORIES\n",
    "        \n",
    "        # Points\n",
    "        points = [0] * NUM_CATEGORIES\n",
    "\n",
    "        # Extracted Information\n",
    "        cause_tokens = self.cause.tokens\n",
    "        change_tokens = self.change.tokens\n",
    "        trait_tokens = self.trait.tokens\n",
    "        species_tokens = [self.sp_doc[span.start] for span in self.species.spans]\n",
    "        experiment_tokens = self.experiment.tokens\n",
    "        neg_experiment_tokens = self.neg_experiment.tokens\n",
    "\n",
    "        print(f\"Cause Tokens: {cause_tokens}\")\n",
    "        print(f\"Change Tokens: {change_tokens}\")\n",
    "        print(f\"Experiment Tokens: {experiment_tokens}\")\n",
    "        print(f\"Negative Experiment Tokens: {neg_experiment_tokens}\")\n",
    "        print(f\"Trait Tokens: {trait_tokens}\")\n",
    "        print(f\"Species Tokens: {species_tokens}\")\n",
    "         \n",
    "        # This is used to ensure that at least three species\n",
    "        # are mentioned.\n",
    "        seen_species = {}\n",
    "\n",
    "        for sent in self.sp_doc.sents:\n",
    "            # This contains the number of points\n",
    "            # each category has accumulated in the sentence.\n",
    "            curr_points = [0] * NUM_CATEGORIES\n",
    "\n",
    "            # Contains the tokens in the sentence.\n",
    "            sent_tokens = [token for token in sent]\n",
    "\n",
    "            # This is used for the species (must have a nearby cause and/or\n",
    "            # change word).\n",
    "            sent_cause_tokens = set(sent_tokens).intersection(cause_tokens)\n",
    "            sent_change_tokens = set(sent_tokens).intersection(change_tokens)\n",
    "\n",
    "            # We don't want to visit the same species more than one\n",
    "            # in the same sentence as to avoid redundant points.\n",
    "            sent_seen_species = []\n",
    "\n",
    "            print(f\"Sentence Tokens: {sent_tokens}\")\n",
    "            print(f\"Sentence Cause Tokens: {sent_cause_tokens}\")\n",
    "            print(f\"Sentence Change Tokens: {sent_change_tokens}\")\n",
    "            \n",
    "            for token in sent_tokens:\n",
    "                # If each category has reached their maximum number of points,\n",
    "                # we can end the loop early.\n",
    "                all_maxed = True\n",
    "                for i in range(NUM_CATEGORIES):\n",
    "                    if curr_points[i] < MPC[i]:\n",
    "                        all_maxed = False\n",
    "\n",
    "                if all_maxed:\n",
    "                    break\n",
    "\n",
    "                # TRAIT CATEGORY\n",
    "                if curr_points[TRAIT] < MPC[TRAIT] and token in trait_tokens:\n",
    "                    print(\"TRAIT CATEGORY\")\n",
    "                    # To get points in the trait category, there must \n",
    "                    # be (1) a trait; and (2) a change or cause in the token's\n",
    "                    # context.\n",
    "                    token_context = set(self.help.find_unit_context(\n",
    "                        il_unit=token.i, \n",
    "                        ir_unit=token.i, \n",
    "                        il_boundary=token.sent.start, \n",
    "                        ir_boundary=token.sent.end-1, \n",
    "                        verbose=verbose)\n",
    "                    )\n",
    "                    cause_tokens_in_context = set(sent_cause_tokens).intersection(token_context)\n",
    "                    change_tokens_in_context = set(sent_change_tokens).intersection(token_context)\n",
    "\n",
    "                    print(f\"Token ({token}) Context: {token_context}\")\n",
    "                    print(f\"Cause Tokens in Context: {cause_tokens_in_context}\")\n",
    "                    print(f\"Change Tokens in Context: {change_tokens_in_context}\")\n",
    "\n",
    "                    if cause_tokens_in_context or change_tokens_in_context:\n",
    "                        curr_points[TRAIT] += PIC[TRAIT]\n",
    "                        print(f\"Added Points for Trait via Token '{token}'\")\n",
    "\n",
    "                    print()\n",
    "\n",
    "                # EXPERIMENT CATEGORY\n",
    "                if token in neg_experiment_tokens:\n",
    "                    curr_points[EXPERIMENT] -= PIC[EXPERIMENT]\n",
    "                    print(f\"Deducted Points for Experiment via Token '{token}'\\n\")\n",
    "                elif curr_points[EXPERIMENT] < MPC[EXPERIMENT] and token in experiment_tokens:\n",
    "                    curr_points[EXPERIMENT] += PIC[EXPERIMENT]\n",
    "                    print(f\"Added Points for Experiment via Token '{token}'\\n\")\n",
    "\n",
    "                # SPECIES CATEGORY\n",
    "                if token in species_tokens:\n",
    "                    print(\"SPECIES CATEGORY\")\n",
    "                    # Find Species Span\n",
    "                    species_span = self.species.span_at_token(token)           \n",
    "\n",
    "                    # Updating Seen Species (in Entire Text)\n",
    "                    past_visits = 0\n",
    "\n",
    "                    # Find Previous Instance of Species (if Any)\n",
    "                    print(\"Seen Species Updated\")\n",
    "                    print(seen_species)\n",
    "                    print()\n",
    "                    \n",
    "                    seen_species_span = self.species.find_same_species(seen_species.keys(), species_span)\n",
    "                    if seen_species_span:\n",
    "                        past_visits = seen_species[seen_species_span]\n",
    "                        seen_species[seen_species_span] += 1\n",
    "                    \n",
    "                    if not past_visits:\n",
    "                        seen_species[species_span] = 1\n",
    "\n",
    "                    print(\"Seen Species Updated\")\n",
    "                    print(seen_species)\n",
    "                    print()\n",
    "                    \n",
    "                    # Checking Seen Species (in Sentence)\n",
    "                    # We only add points if it's a species that has not been seen\n",
    "                    # in the sentence. This is to avoid redundant points. \n",
    "                    # Also, if it species has not been seen at all (is_new_species),\n",
    "                    # then it cannot be a redundant species (we couldn't have seen it in the sentence\n",
    "                    # either).\n",
    "                    redundant_species = False\n",
    "\n",
    "                    if not past_visits:\n",
    "                        if self.species.find_same_species(sent_seen_species, species_span):\n",
    "                            redundant_species = True\n",
    "                    \n",
    "                    sent_seen_species.append(species_span)\n",
    "\n",
    "                    print(\"Seen Species in Sentence\")\n",
    "                    print(sent_seen_species)\n",
    "                    print()\n",
    "                    \n",
    "                    if redundant_species:\n",
    "                        continue\n",
    "\n",
    "                    # INTERACTION CATEGORY\n",
    "                    # It is helpful to have this category here because (if we've reached here)\n",
    "                    # we're dealing with a new species in the sentence.\n",
    "                    if curr_points[INTERACTION] < MPC[INTERACTION]:\n",
    "                        curr_points[INTERACTION] += PIC[INTERACTION]\n",
    "                        print(f\"Added Points for Interaction via Token '{token}'\\n\")\n",
    "                        \n",
    "                    if curr_points[SPECIES] < MPC[SPECIES]:\n",
    "                        # To get points in the species category, there must be \n",
    "                        # (1) a species; and (2) a change or cause in the phrase\n",
    "                        # (or clause) that the token is a part of.\n",
    "                        token_context = set(self.help.find_unit_context(\n",
    "                            il_unit=token.i, \n",
    "                            ir_unit=token.i, \n",
    "                            il_boundary=token.sent.start, \n",
    "                            ir_boundary=token.sent.end-1, \n",
    "                            verbose=verbose)\n",
    "                        )\n",
    "                        cause_tokens_in_context = set(sent_cause_tokens).intersection(token_context)\n",
    "                        change_tokens_in_context = set(sent_change_tokens).intersection(token_context)\n",
    "\n",
    "                        print(f\"Token ({token}) Context: {token_context}\")\n",
    "                        print(f\"Cause Tokens in Context: {cause_tokens_in_context}\")\n",
    "                        print(f\"Change Tokens in Context: {change_tokens_in_context}\")\n",
    "                        \n",
    "                        if cause_tokens_in_context or change_tokens_in_context:\n",
    "                            curr_points[SPECIES] += PIC[SPECIES]\n",
    "                            print(f\"Added Points for Species via Token '{token}'\")\n",
    "\n",
    "                        print()\n",
    "         \n",
    "            # SENTENCE DONE\n",
    "            # Add Sentence Points to Total Points\n",
    "            for category in [TRAIT, SPECIES, EXPERIMENT, INTERACTION]:\n",
    "                if round(curr_points[category]) == 0:\n",
    "                    zero_pt_sents[category] += 1\n",
    "                points[category] += max(0, min(curr_points[category], MPC[category]))\n",
    "\n",
    "        # Calculating Score            \n",
    "        NUM_SENTENCES = len(list(self.sp_doc.sents))\n",
    "\n",
    "        score = 0\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            # This is the number of sentences that did not receive\n",
    "            # 0 points (for the category). The number of sentences\n",
    "            # we divide by must minimally account for these sentences.\n",
    "            num_non_zero_pt_sents = NUM_SENTENCES - zero_pt_sents[i]\n",
    "            \n",
    "            # This is the number of sentences to calculate the\n",
    "            # FTP with, with leniency applied.\n",
    "            lenient_num_sentences = max(num_non_zero_pt_sents, (1 - LEN[i]) * NUM_SENTENCES)\n",
    "\n",
    "            # Calculating FTP, Adding Up the Score\n",
    "            points[i] = points[i] / (MPC[i] * lenient_num_sentences)\n",
    "            score += max(0, min(points[i], 1)) * CW[i]\n",
    "\n",
    "        # Enforcing 3 or More Species            \n",
    "        if len(seen_species) < 3:\n",
    "            return 0, points\n",
    "            \n",
    "        assert 0.0 <= score <= 1.0\n",
    "        \n",
    "        return score, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cdd412-e94a-4ead-9b99-a0e8edf3448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(name, save_output=False, version=\"\"):\n",
    "    # Redirect Print Statements\n",
    "    # https://stackoverflow.com/questions/7152762/how-to-redirect-print-output-to-a-file\n",
    "    if save_output:\n",
    "        initial_stdout = sys.stdout\n",
    "        f = open(f'./Print{name}{\"\" if not version else f\"-{version}\"}.txt', 'w')\n",
    "        sys.stdout = f\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "\n",
    "    # Load Dataset\n",
    "    data = load_preprocessed_dataset(name)\n",
    "\n",
    "    # We'll be running the points algorithm\n",
    "    # on the abstracts of these papers.\n",
    "    texts = list(data['Abstract'].to_numpy())\n",
    "    \n",
    "    # The scores for each paper will be stored here,\n",
    "    # we'll set this as a column of the dataframe.\n",
    "    scores = []\n",
    "    points = []\n",
    "    trait_points = []\n",
    "    species_points = []\n",
    "    experiment_points = []\n",
    "    interaction_points = []\n",
    "    \n",
    "    # Scan and Evaluate Documents\n",
    "    main = Main()\n",
    "    for i, doc in enumerate(main.sp_nlp.pipe(texts)):\n",
    "        print(f\"{i+1}/{data.shape[0]} - {data.iloc[i]['Title']}\\n\")\n",
    "        main.update_doc(doc, verbose=save_output)\n",
    "\n",
    "        # Empty string literals cause errors, so it's\n",
    "        # being handled here.\n",
    "        if not main.sp_doc or not main.species.tn_doc:\n",
    "            scores.append(0)\n",
    "        else:\n",
    "            score, _points = main.score(verbose=save_output)\n",
    "            scores.append(score)\n",
    "            points.append(_points)\n",
    "            trait_points.append(_points[0])\n",
    "            species_points.append(_points[1])\n",
    "            experiment_points.append(_points[2])\n",
    "            interaction_points.append(_points[3])\n",
    "\n",
    "        if not save_output:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    # Reset Standard Output\n",
    "    if save_output:\n",
    "        sys.stdout = initial_stdout\n",
    "        f.close()\n",
    "\n",
    "    data[\"Score\"] = scores\n",
    "    data[\"Trait Points\"] = trait_points\n",
    "    data[\"Species Points\"] = species_points\n",
    "    data[\"Experiment Points\"] = experiment_points\n",
    "    data[\"Interaction Points\"] = interaction_points\n",
    "    data.sort_values(by='Score', ascending=False, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced84b80-fd17-4774-940f-64037ee22411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score Datasets\n",
    "# dataset_names = [\"Examples\", \"Baseline-1\", \"SubA\", \"SubAFiltered\", \"SubB\", \"SubBFiltered\", \"C\", \"CFiltered\", \"D\", \"DFiltered\"]\n",
    "# for name in [\"Baseline-1\"]:\n",
    "#     scored_data = score_dataset(name, save_output=False, version='')\n",
    "#     store_scored_dataset(scored_data, \"Baseline-2\", version='8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78034f48-0224-4a95-b0b1-6d90cbedc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_preprocessed_dataset(\"Baseline-1\")\n",
    "main = Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903acfa6-1963-4781-9a34-c241b21ab12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Title'].str.contains('Competition and intraguild')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09abccd9-be79-4cdb-89a7-7e0617880e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 20\n",
    "\n",
    "title = data.iloc[index].Title\n",
    "abstract = data.iloc[index].Abstract\n",
    "\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Abstract: {abstract}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27736d-19f7-4050-a356-fa17767d77c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "main.update_text(str(abstract))\n",
    "print(main.score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be39281-ba1e-42a2-974b-06095bf8f9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
