{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbecf5be-df89-4d1a-b75f-d92a5c5fff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trait-Mediated Interaction Modification\n",
    "# Empirical:\n",
    "# You could look for words like \"hypothesis\", \"experiment\", \"found\", and \"discovered\". That may point\n",
    "# towards there being an experiment in the paper. There are also words like \"control group\", \"compared\",\n",
    "# \"findings\", \"results\", \"study\", and more.\n",
    "# Qualitative vs. Quantitative:\n",
    "# To infer whether something is quantitative, you could look for numeric tokens and units.\n",
    "# However, you can only do so much with the abstract. Therefore, this is likely not good enough.\n",
    "# Yet, you could still take advantage of words like \"fewer\" and \"increased\" to show that there is a change.\n",
    "# However, this would be more suited for the above category.\n",
    "# Traits:\n",
    "# There is no NLP tool for traits that I can use or create so I think that I could instead use keywords.\n",
    "# For example, \"snail feeding rates\" is a trait. You may be able to spot this by looking for a word like\n",
    "# \"rate\". You'd expand that word to include \"snail feeding rates\". As \"snail\" is a species you can infer\n",
    "# that \"rates\" is a trait. I would be more decisive and use a dependency parser to ensure that the trait\n",
    "# is a property of the species (like before). However, with all the cases that may exist, I think checking\n",
    "# to see whether a species can be found by traveling back and/or forward without finding certain tokens could\n",
    "# work well enough.\n",
    "# 3 Species or More:\n",
    "# This is simple. However, I think using a dictionary and TaxoNerd would be beneficial (for higher accuracy).\n",
    "# To handle the potential differences in tokenization, character offsets should be used.\n",
    "# Standardization:\n",
    "# There is a lot of variance in the scores. To squash this issue, I think that we could assign each sentence\n",
    "# a value from 0 to 1. We would add these values and divide by the number of sentences. This would result in\n",
    "# a number that is also from 0 to 1. However, there are categories that we would like to inspect. So, we must\n",
    "# create an overall score in the interval from [0, 1] while also scoring each category. Well, for each sentence\n",
    "# we could add a point for each category that is observed. The sentence would receive said score divided by the\n",
    "# number of categories. At the end, we add up all the sentence scores and divide by the number of sentences.\n",
    "# The aggregate score for each category would also be divided by the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "637ccb94-4c7d-49b1-a727-f6e25b021357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from spacy.language import Language\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "%run -i \"../utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe27a6-00d1-47e2-b316-09b844dcf25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "class Help:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.ignore = [\"species\", \"deer\", \"fish\", \"moose\", \"sheep\", \"swine\", \"buffalo\"]\n",
    "\n",
    "    def remove_spaces(self, string):\n",
    "        cleaned_str = string\n",
    "        cleaned_str = re.sub(\"\\s+\", \" \", cleaned_str) # Remove Duplicate Spaces\n",
    "        cleaned_str = re.sub(r\"\\s+([?.!,])\", r\"\\1\", cleaned_str) # Remove Spaces Before Punctuation\n",
    "        return cleaned_str.strip()\n",
    "\n",
    "    def remove_non_alnum(self, string):\n",
    "        while len(string) > 0:\n",
    "            start_len = len(string)\n",
    "            if len(string) > 0 and not string[-1].isalnum():\n",
    "                string = string[:-1]\n",
    "            if len(string) > 0 and not string[0].isalnum():\n",
    "                string = string[1:]\n",
    "            # No Changes Made\n",
    "            if start_len == len(string):\n",
    "                break\n",
    "        return string\n",
    "    \n",
    "    def singularize(self, string):\n",
    "        parts = re.split(r\" \", string)\n",
    "\n",
    "        if parts[-1] in self.ignore:\n",
    "            return string\n",
    "\n",
    "        singulars = []\n",
    "        for singular_end in self.singularize_string(parts[-1]):\n",
    "            singular = self.remove_spaces(\" \".join([*parts[:-1], singular_end]))\n",
    "            singulars.append(singular)\n",
    "            \n",
    "        return singulars\n",
    "        \n",
    "    def singularize_string(self, string):\n",
    "        versions = []\n",
    "        \n",
    "        if re.fullmatch(r\".*us$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            versions.append(f'{string[:-2]}i')\n",
    "        elif re.fullmatch(r\".*(s|sh|ch|x|z)$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "        elif re.fullmatch(r\".*([^aeiou])(y)$\", string):\n",
    "            versions.append(f'{string[:-1]}ies')\n",
    "        # The rules are murky where there's -f.\n",
    "        # To be safe, I'll add both versions (one of \n",
    "        # which is incorrect).\n",
    "        elif (\n",
    "            re.fullmatch(r\".*(f)(e?)$\", string) and\n",
    "            not re.fullmatch(r\".*ff$\", string)\n",
    "        ):\n",
    "            last_clean = re.sub(r\"(f)(e?)$\", \"\", string)\n",
    "            versions.append(f'{last_clean}fs')\n",
    "            versions.append(f'{last_clean}ves')\n",
    "        # Some people would just add -s to 'bonobo', some would add\n",
    "        # -es. To be safe, I'll add both versions.\n",
    "        elif re.fullmatch(r\".*([^aeiou])o$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "            versions.append(f'{string}es')\n",
    "        else:\n",
    "            versions.append(f'{string}s')\n",
    "        \n",
    "        return versions\n",
    "\n",
    "    def pluralize(self, string):\n",
    "        parts = re.split(r\" \", string)\n",
    "\n",
    "        if parts[-1] in self.ignore:\n",
    "            return string\n",
    "\n",
    "        plurals = []\n",
    "        for plural_end in self.singularize_string(parts[-1]):\n",
    "            plural = self.remove_spaces(\" \".join([*parts[:-1], plural_end]))\n",
    "            plurals.append(plural)\n",
    "            \n",
    "        return plurals\n",
    "\n",
    "    def pluralize_string(self, string):\n",
    "        versions = []\n",
    "\n",
    "        if re.fullmatch(r\".*ies$\", string):\n",
    "            # 'butterflies' -> 'butterfly'\n",
    "            versions.append(f'{string[:-3]}y')\n",
    "        elif re.fullmatch(r\".*ves$\", string):\n",
    "            # 'loaves' -> 'loaf' and 'lives' -> 'life'\n",
    "            versions.append(f'{string[:-3]}f')\n",
    "            versions.append(f'{string[:-3]}fe')\n",
    "        elif re.fullmatch(r\".*es$\", string):\n",
    "            versions.append(f'{string[:-2]}')\n",
    "        elif re.fullmatch(r\".*i$\", string):\n",
    "            versions.append(f'{string[:-1]}us')\n",
    "        elif re.fullmatch(r\".*s$\", string):\n",
    "            versions.append(f'{string[:-1]}')\n",
    "\n",
    "        return versions\n",
    "\n",
    "    def expand_unit(self, *, il_unit, ir_unit, il_boundary, ir_boundary, allowed_speech=[], allowed_literals=[], disallowed_literals=[], direction='BOTH', verbose=False):\n",
    "        # Move Left\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit > il_boundary:\n",
    "                prev_token = self.sp_doc[il_unit-1]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"il_unit: {il_unit}, il_boundary: {il_boundary}, prev_token: {prev_token}, prev_token.pos_: {prev_token.pos_}\")\n",
    "\n",
    "                in_disallowed = prev_token.lower_ in disallowed_literals\n",
    "                not_in_allowed = prev_token.pos_ not in allowed_speech and prev_token.lower_ not in allowed_literals\n",
    "                \n",
    "                if in_disallowed or not_in_allowed:\n",
    "                    break\n",
    "                \n",
    "                il_unit -= 1\n",
    "\n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit < ir_boundary:\n",
    "                next_token = self.sp_doc[ir_unit+1]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"ir_unit: {ir_unit}, ir_boundary: {ir_boundary}, next_token: {next_token}, next_token.pos_: {next_token.pos_}\")\n",
    "                \n",
    "                in_disallowed = prev_token.lower_ in disallowed_literals\n",
    "                not_in_allowed = next_token.pos_ not in allowed_speech and next_token.lower_ not in allowed_literals\n",
    "                \n",
    "                if in_disallowed or not_in_allowed:\n",
    "                    break\n",
    "                \n",
    "                ir_unit += 1\n",
    "\n",
    "        assert il_unit >= il_boundary and ir_unit <= ir_boundary\n",
    "        \n",
    "        # Expanded Unit\n",
    "        expanded_unit = self.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Expanded Unit: {expanded_unit}\")\n",
    "        \n",
    "        return expanded_unit\n",
    "\n",
    "    def contract_unit(self, *, il_unit, ir_unit, speech=[], literals=[], must_not_be_in_set=False, direction='BOTH', verbose=False):\n",
    "        # Move Left\n",
    "        if verbose:\n",
    "            print(\"LEFT\")\n",
    "            \n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit < ir_unit:\n",
    "                curr_token = self.sp_doc[il_unit]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"il_unit: {il_unit}, ir_unit: {ir_unit}, curr_token: {curr_token}, curr_token.pos_: {curr_token.pos_}\")\n",
    "\n",
    "                in_set = curr_token.pos_ in speech or curr_token.lower_ in literals\n",
    "                if (in_set and must_not_be_in_set) or (not in_set and not must_not_be_in_set):\n",
    "                    break\n",
    "            \n",
    "                il_unit += 1\n",
    "\n",
    "        # Move Right\n",
    "        if verbose:\n",
    "            print(\"RIGHT\")\n",
    "        \n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit > il_unit:\n",
    "                curr_token = self.sp_doc[ir_unit]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"il_unit: {il_unit}, ir_unit: {ir_unit}, curr_token: {curr_token}, curr_token.pos_: {curr_token.pos_}\")\n",
    "                \n",
    "                if curr_token.pos_ in allowed_speech or curr_token.lower_ in allowed_literals:\n",
    "                    break\n",
    "                \n",
    "                ir_unit -= 1\n",
    "\n",
    "        contracted_unit = self.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Contracted Unit: {contracted_unit}\")\n",
    "        \n",
    "        return contracted_unit\n",
    "\n",
    "    def find_unit_context(self, *, il_unit, ir_unit, il_boundary, ir_boundary, verbose=False):\n",
    "        # Check if in Punctuation (e.g. (...))\n",
    "        matching_puncts = {\"[\": \"]\", \"(\": \")\", \"-\": \"-\", \"--\": \"--\"}\n",
    "        \n",
    "        opening_puncts = list(matching_puncts.keys())\n",
    "        closing_puncts = list(matching_puncts.values())\n",
    "        \n",
    "        puncts = [*closing_puncts, *opening_puncts]\n",
    "        \n",
    "        i = il_unit\n",
    "        l_punct = None\n",
    "        while i >= il_boundary:\n",
    "            i_token = self.main.sp_doc[i]\n",
    "            if i_token.text in puncts:\n",
    "                l_punct = i_token\n",
    "                if verbose:\n",
    "                    print(f\"{i_token} in {puncts}\")\n",
    "                break\n",
    "            i -= 1\n",
    "\n",
    "        i = ir_unit\n",
    "        r_punct = None\n",
    "        while i <= ir_boundary:\n",
    "            i_token = self.main.sp_doc[i]\n",
    "            if i_token.text in puncts:\n",
    "                r_punct = i_token\n",
    "                if verbose:\n",
    "                    print(f\"{i_token} in {puncts}\")\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        # Return Text inside Punctuation\n",
    "        if verbose:\n",
    "            print(f\"L Punct: {l_punct} v. R Punct: {r_punct}\")\n",
    "        \n",
    "        in_punct = l_punct and r_punct and matching_puncts.get(l_punct.lower_, '') == r_punct.text\n",
    "        if in_punct:\n",
    "            if verbose:\n",
    "                print(f\"Matching Punctuation, Return Inner Text\")\n",
    "            return self.main.sp_doc[l_punct.i:r_punct.i+1]\n",
    "\n",
    "        exclude = []\n",
    "        \n",
    "        # Expand Left\n",
    "        while il_unit > il_boundary:\n",
    "            prev_token = self.main.sp_doc[il_unit-1]\n",
    "            if verbose:\n",
    "                print(f\"Expand L:\\nToken: {prev_token} and Position: {prev_token.pos_}\")\n",
    "\n",
    "            # Closing Punctuation\n",
    "            if prev_token.lower_ in closing_puncts:\n",
    "                i = il_unit-1\n",
    "                while i >= il_boundary and matching_puncts.get(self.main.sp_doc[i].lower_, '') != prev_token.lower_:\n",
    "                    exclude.append(self.main.sp_doc[i])\n",
    "                    i -= 1\n",
    "                exclude.append(self.main.sp_doc[i])\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Skipped to Token {i}\")\n",
    "                \n",
    "                il_unit = i\n",
    "                continue\n",
    "            else:\n",
    "                if prev_token.pos_ not in [\"ADJ\", \"NOUN\", \"ADP\", \"ADV\", \"PART\", \"PROPN\", \"VERB\", \"PRON\"]:\n",
    "                    if verbose:\n",
    "                        print(f\"Break\")\n",
    "                    break\n",
    "                else:\n",
    "                    il_unit -= 1\n",
    "\n",
    "        # Expand Right\n",
    "        while ir_unit < ir_boundary:\n",
    "            next_token = self.main.sp_doc[ir_unit+1]\n",
    "            if verbose:\n",
    "                print(f\"Expand R:\\nToken: {next_token} and Position: {next_token.pos_}\")\n",
    "\n",
    "            # Opening Punctuation\n",
    "            if next_token.lower_ in opening_puncts:\n",
    "                if verbose:\n",
    "                    print(f\"Next Token is an Opening Punctuation\")\n",
    "                \n",
    "                i = ir_unit + 1\n",
    "                if verbose:\n",
    "                    print(f\"i: {i}, ir_boundary: {ir_boundary}, self.sp_doc[i]: '{self.main.sp_doc[i].lower_}', matching punctuation: '{matching_puncts.get(next_token.lower_, '')}'\")\n",
    "                    print(f\"{i <= ir_boundary} and {self.main.sp_doc[i].lower_ != matching_puncts.get(next_token.lower_, '')}\")\n",
    "                    \n",
    "                while i <= ir_boundary and self.main.sp_doc[i].lower_ != matching_puncts.get(next_token.lower_, ''):\n",
    "                    if verbose:\n",
    "                        print(f\"\\t'{self.main.sp_doc[i]}' != '{matching_puncts.get(next_token.lower_, '')}'\")\n",
    "                    exclude.append(self.main.sp_doc[i])\n",
    "                    i += 1\n",
    "                exclude.append(self.main.sp_doc[i])\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Skipped to Token {i}\")\n",
    "                \n",
    "                ir_unit = i\n",
    "                continue\n",
    "            else:\n",
    "                if next_token.pos_ not in [\"ADJ\", \"NOUN\", \"ADP\", \"ADV\", \"PART\", \"PROPN\", \"VERB\", \"PRON\"]:\n",
    "                    if verbose:\n",
    "                        print(f\"Break\")\n",
    "                    break\n",
    "                else:\n",
    "                    ir_unit += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Exclude: {exclude}\")\n",
    "\n",
    "        context = [t for t in self.main.sp_doc[il_unit:ir_unit+1] if t not in exclude]\n",
    "        if verbose:\n",
    "            print(f\"Context: {context}\")\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26736310-0964-4184-a9c1-f6b0b38d7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the Dictionary\n",
    "@Language.component(\"lower_case_lemmas\")\n",
    "def lower_case_lemmas(doc) :\n",
    "    for token in doc :\n",
    "        token.lemma_ = token.lemma_.lower()\n",
    "    return doc\n",
    "\n",
    "class Species:\n",
    "    def __init__(self, main):\n",
    "        # Tools\n",
    "        self.main = main\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\"])\n",
    "        self.tn_nlp.add_pipe(\"lower_case_lemmas\", after=\"lemmatizer\")\n",
    "        self.tn_doc = None\n",
    "        \n",
    "        # Contains any spans that have been identified\n",
    "        # as a species.\n",
    "        self.spans = None\n",
    "        \n",
    "        # Contains any tokens that have been identified\n",
    "        # as a species or being a part of a species.\n",
    "        self.tokens = None\n",
    "        \n",
    "        # Used to quickly access the span that a token\n",
    "        # belongs to.\n",
    "        self.token_to_span = None\n",
    "        \n",
    "        # Maps a string to an array of strings wherein\n",
    "        # any string involved in the key-value pair \n",
    "        # has been identified as an alternate name of each other.\n",
    "        self.alternate_names = None\n",
    "        \n",
    "        # Used to increase TaxoNERD's accuracy.\n",
    "        self.dictionary = None\n",
    "        self.load_dictionary()\n",
    "\n",
    "    def load_dictionary(self):\n",
    "        self.dictionary = [\"juvenile\", \"adult\", \"prey\", \"predator\", \"species\"]\n",
    "        # df = pd.read_csv(\"VernacularNames.csv\")\n",
    "        # self.dictionary += df.VernacularName.to_list()\n",
    "\n",
    "        patterns = []\n",
    "        for name in self.dictionary:\n",
    "            doc = self.tn_nlp(name)\n",
    "            patterns.append({\"label\": \"LIVB\", \"pattern\": [{\"LEMMA\": token.lemma_} for token in doc]})\n",
    "        ruler = self.tn_nlp.add_pipe(\"entity_ruler\")\n",
    "        ruler.add_patterns(patterns)\n",
    "        \n",
    "    def update(self, text, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        self.spans, self.tokens, self.token_to_span, self.alternate_names = self.load_species(verbose=verbose)\n",
    "\n",
    "    def load_species(self, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # These three contain the species that have been\n",
    "        # identified in the text. Tokens that aren't adjectives,\n",
    "        # nouns, or proper nouns will be stripped.\n",
    "        spans = []\n",
    "        tokens = []\n",
    "        token_to_span = {}\n",
    "\n",
    "        # It's useful to know if a different name refers to a\n",
    "        # species we have already seen. For example, in \"predatory crab (Carcinus maenas)\",\n",
    "        # \"predatory crab\" is an alternative name for \"Carcinus maenas\"\n",
    "        # and vice versa. This is used so that the species can be\n",
    "        # properly tracked and redundant points are less likely to be given.\n",
    "        alternate_names = {}\n",
    "\n",
    "        # TaxoNerd\n",
    "        if verbose:\n",
    "            print(f\"TN Entities ({len(self.tn_doc.ents)}): {self.tn_doc.ents}\\n\")\n",
    "\n",
    "        # The TN pipeline does not appear to have .pos_ or .tag_. Since,\n",
    "        # has it, I'll just immediately convert the spans.\n",
    "        sp_doc_species = []\n",
    "        for span in self.tn_doc.ents:\n",
    "            char_i0 = self.tn_doc[span.start].idx\n",
    "            char_i1 = self.tn_doc[span.end-1].idx\n",
    "\n",
    "            sp_token_i0 = self.main.token_at_char(char_i0).i\n",
    "            sp_token_i1 = self.main.token_at_char(char_i1).i\n",
    "\n",
    "            sp_doc_species.append(self.main.sp_doc[sp_token_i0:sp_token_i1+1])\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"SP Entities ({len(sp_doc_species)}): {sp_doc_species}\")\n",
    "\n",
    "        # Fix: Adding Singular and Plural Versions\n",
    "        # Let's say you're looking for words that contain \"ant\" in the text.\n",
    "        # \"Pants\" and \"antebellum\" would match. To prevent this, we need to make sure\n",
    "        # that the entire word matches; but, with this change, \"ants\" wouldn't match either.\n",
    "        # To fix this issue, we add the singular and plural versions of the recognized\n",
    "        # species so that we \"ant\" and \"ants\" are matched, but \"Pants\" and \"antebellum\"\n",
    "        # aren't.\n",
    "\n",
    "        # This contains strings (not span objects) as we're looking throughout the text\n",
    "        # for any matches based on the content and not position.\n",
    "        base_species = []\n",
    "\n",
    "        for span in sp_doc_species:\n",
    "            string = span.text.lower()\n",
    "            \n",
    "            # Remove Non-Alphanumeric Characters on Outside\n",
    "            while len(string) > 0:\n",
    "                start_len = len(string)\n",
    "                if len(string) > 0 and not string[-1].isalnum():\n",
    "                    string = string[:-1]\n",
    "                if len(string) > 0 and not string[0].isalnum():\n",
    "                    string = string[1:]\n",
    "                # No Changes Made\n",
    "                if start_len == len(string):\n",
    "                    break\n",
    "\n",
    "            if span[-1].pos_ in [\"NOUN\"]:\n",
    "                # Singular Noun => Add Plural Versions\n",
    "                if span[-1].tag_ in [\"NN\"]:\n",
    "                    if re.fullmatch(r\".*us$\", string):\n",
    "                        print(1)\n",
    "                        # 'octopus' -> 'octopuses'\n",
    "                        base_species.append(f'{string}es')\n",
    "                        # 'cactus' -> 'cacti'\n",
    "                        base_species.append(f'{string[:-2]}o')\n",
    "                    elif re.fullmatch(r\".*(s|sh|ch|x|z)$\", string):\n",
    "                        # 'beach' -> 'beaches'\n",
    "                        print(2)\n",
    "                        base_species.append(f'{string}es')\n",
    "                    elif re.fullmatch(r\".*([^aeiou])(y)$\", string):\n",
    "                        # 'canary' -> 'canaries'\n",
    "                        print(3)\n",
    "                        base_species.append(f'{string[:-1]}ies')\n",
    "                    # The rules are murky where there's -f.\n",
    "                    # To be safe, I'll add both versions (one of which is incorrect).\n",
    "                    elif re.fullmatch(r\".*(f)(e?)$\", string) and not re.fullmatch(r\".*ff$\", string):\n",
    "                        # 'chief' -> 'chiefs' and 'loaf' -> 'loaves'\n",
    "                        print(4)\n",
    "                        last_clean = re.sub(r\"(f)(e?)$\", \"\", string)\n",
    "                        base.species.append(f'{last_clean}fs')\n",
    "                        base.species.append(f'{last_clean}ves')\n",
    "                    # Some people would just add -s to 'bonobo', some would add\n",
    "                    # -es. To be safe, I'll add both versions.\n",
    "                    elif re.fullmatch(r\".*([^aeiou])o$\", string):\n",
    "                        base.species.append(f'{string}s')\n",
    "                        base.species.append(f'{string}es')\n",
    "                    else:\n",
    "                        print(5)\n",
    "                        base_species.append(f'{string}s')\n",
    "                # Plural Noun => Add Singular Versions\n",
    "                elif span[-1].tag_ in [\"NNS\"]:\n",
    "                    if re.fullmatch(r\".*ies$\", string):\n",
    "                        # 'butterflies' -> 'butterfly'\n",
    "                        print(6)\n",
    "                        base_species.append(f'{string[:-3]}y')\n",
    "                    elif re.fullmatch(r\".*ves$\", string):\n",
    "                        # 'loaves' -> 'loaf' and 'lives' -> 'life'\n",
    "                        print(7)\n",
    "                        base_species.append(f'{string[:-3]}f')\n",
    "                        base_species.append(f'{string[:-3]}fe')\n",
    "                    elif re.fullmatch(r\".*es$\", string):\n",
    "                        print(8)\n",
    "                        base_species.append(f'{string[:-2]}')\n",
    "                    elif re.fullmatch(r\".*i$\", string):\n",
    "                        print(9)\n",
    "                        base_species.append(f'{string[:-1]}us')\n",
    "                    elif re.fullmatch(r\".*s$\", string):\n",
    "                        print(10)\n",
    "                        base_species.append(f'{string[:-1]}')\n",
    "\n",
    "            # Default\n",
    "            base_species.append(span.text.lower())\n",
    "\n",
    "        # Now, we'll find all the words in the text that\n",
    "        # have been recognized as a species (and a \n",
    "        # singular or plural version of such).\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "        base_species = list(set(base_species))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Base Species: {base_species}\")\n",
    "        \n",
    "        for species in base_species:\n",
    "            if verbose:\n",
    "                print(f\"Searching '{species}'\")\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Search Species w/o Symbols: {species}\")\n",
    "\n",
    "            # Fix: Bootstrapping\n",
    "            # TaxoNERD sometimes identifies one instance of a species and not the other.\n",
    "            # For example, \"ant\" will be latered three times, and not the other four.\n",
    "            # To fix this, I'll use the recognized species to look for all the instances \n",
    "            # in the document that also match those species.\n",
    "            # Retrieve the start and end position for each of the substrings\n",
    "            # in the text that matched the species span.\n",
    "            matches = re.finditer(re.escape(species), text)\n",
    "            for char_i0, char_i1 in [(match.start(), match.end()) for match in matches]:\n",
    "                if verbose:\n",
    "                    print(f\"Match ({char_i0}-{char_i1}): '{text[char_i0:char_i1]}'\")\n",
    "\n",
    "                # As previously mentioned, the full word must match, not just a substring inside of it.\n",
    "                # So, if the species we're looking for is \"ant\", only \"ant\" will match -- not \"pants\"\n",
    "                # or \"antebellum\".\n",
    "                letter_to_l = char_i0 > 0 and text[char_i0-1].isalpha()\n",
    "                letter_to_r = char_i1 < len(text) - 1 and text[char_i1+1].isalpha()\n",
    "                \n",
    "                if letter_to_l or letter_to_r:\n",
    "                    if verbose:\n",
    "                        print(f\"Letter on L/R -> Skip\")\n",
    "                    continue\n",
    "                    \n",
    "                sp_li = self.main.token_at_char(char_i0).i\n",
    "                sp_ri = self.main.token_at_char(char_i1-1).i\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"sp_li: {sp_li}\")\n",
    "                    print(f\"sp_ri: {sp_ri}\")\n",
    "                    \n",
    "                species_span = self.main.sp_doc[sp_li:sp_ri+1]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Species Span: {species_span}\")\n",
    "    \n",
    "                # Expand Species\n",
    "                # Let's say there's a word like \"squirrel\". That's a bit ambiguous. Is it a brown squirrel, a zebra?\n",
    "                # If the species is possibly missing information (like an adjective to the left of it), we should expand\n",
    "                # in order to get a full picture of the species.\n",
    "                ambiguous = len(species_span) == 1 and species_span[0].pos_ == \"NOUN\"\n",
    "                missing_info = species_span.start > 0 and self.main.sp_doc[species_span.start-1].pos_ in [\"ADJ\"]\n",
    "                \n",
    "                if ambiguous or missing_info:\n",
    "                    species_span = self.main.expand_unit(\n",
    "                        il_unit=species_span.start, \n",
    "                        ir_unit=species_span.end-1,\n",
    "                        il_boundary=0,\n",
    "                        ir_boundary=len(self.main.sp_doc),\n",
    "                        direction='LEFT',\n",
    "                        allowed_speech=[\"ADJ\", \"PROPN\"],\n",
    "                        allowed_literals=[\"-\"],\n",
    "                        verbose=verbose\n",
    "                    )\n",
    "    \n",
    "                    if verbose:\n",
    "                        print(f\"Expanded Species Span: {species_span}\")\n",
    "\n",
    "                # Remove Outer Symbols\n",
    "                # There are times where a species is identified with a parenthesis nearby.\n",
    "                # Here, we remove that parenthesis (and any other symbols).\n",
    "                # This should no longer be needed if the prior removing of symbols is working\n",
    "                # properly.\n",
    "                # species_span = self.main.contract_unit(\n",
    "                #     il_unit=species_span.start, \n",
    "                #     ir_unit=species_span.end-1, \n",
    "                #     speech=[\"PUNCT\", \"SYM\"],\n",
    "                #     must_not_be_in_set=True,\n",
    "                #     verbose=verbose\n",
    "                # )\n",
    "    \n",
    "                # if verbose:\n",
    "                #     print(f\"Contracted Species Span: {species_span}\")\n",
    "\n",
    "                # Nothing Burger Species\n",
    "                # If each token in the span is a punctuation or symbol,\n",
    "                # then is it even a species? I don't think so. However,\n",
    "                # in case this happens, this can't be added to the spans\n",
    "                # or else it will cause an error later on.\n",
    "                non_punct_sym_found = False\n",
    "                for token in species_span:\n",
    "                    if token.pos_ not in [\"PUNCT\", \"SYM\"]:\n",
    "                        non_punct_sym_found = True\n",
    "                        break\n",
    "\n",
    "                if not non_punct_sym_found:\n",
    "                    continue\n",
    "                \n",
    "                spans.append(species_span)\n",
    "                for token in species_span:\n",
    "                    if token.pos_ in [\"PUNCT\", \"SYM\"]:\n",
    "                        continue\n",
    "                    tokens.append(token)\n",
    "                    token_to_span[token] = species_span\n",
    "\n",
    "        # Removing Duplicates and Sorting (for Next Part)\n",
    "        if verbose:\n",
    "            print(f\"Spans Before Removing Duplicates and Sorting: {spans}\")\n",
    "            \n",
    "        mapped_spans = {}\n",
    "        for span in spans:\n",
    "            mapped_spans[span.start] = span\n",
    "\n",
    "        spans = list(mapped_spans.values())\n",
    "        spans.sort(key=lambda span: span.start)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Spans Before Removing Duplicates and Sorting: {spans}\")\n",
    "        \n",
    "        # Finding and Storing Alternative Names\n",
    "        if verbose:\n",
    "            print(\"Finding Alternate Names\")\n",
    "        \n",
    "        for i, species_span in enumerate(spans):\n",
    "            if i + 1 >= len(spans):\n",
    "                break\n",
    "            \n",
    "            next_species_span = spans[i+1]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"SPECIES 1: {species_span}\")\n",
    "                print(f\"SPECIES 2: {next_species_span}\")\n",
    "                print(f\"DIST == 1: {next_species_span.start - species_span.end == 1}\")\n",
    "            \n",
    "            # If there's one token between the species and the next species,\n",
    "            # we check if the next species is surrounded by punctuation.\n",
    "            if next_species_span.start - species_span.end == 1:\n",
    "                # Token Before and After the Next Species\n",
    "                before_next = self.main.sp_doc[next_species_span.start-1]\n",
    "                after_next = self.main.sp_doc[next_species_span.end]\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Token Before SPECIES 2: {before_next} and Token After SPECIES 2: {after_next}\")\n",
    "\n",
    "                # Adding K-V Pair for Names\n",
    "                if before_next.pos_ in [\"PUNCT\", \"SYM\"] and after_next.pos_ in [\"PUNCT\", \"SYM\"]:\n",
    "                    # Instead of using the span objects, the text (string literals)\n",
    "                    # are used. This is because we're focusing on the content (the name)\n",
    "                    # rather than where it appears in the document.\n",
    "                    sp_1_text = species_span.text.lower()\n",
    "                    sp_2_text = next_species_span.text.lower()\n",
    "                    \n",
    "                    if sp_1_text not in alternate_names:\n",
    "                        alternate_names[sp_1_text] = []\n",
    "                    \n",
    "                    if sp_2_text not in alternate_names:\n",
    "                        alternate_names[sp_2_text] = []\n",
    "                    \n",
    "                    alternate_names[sp_1_text].append(sp_2_text)\n",
    "                    alternate_names[sp_2_text].append(sp_1_text)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Spans: {spans}\")\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "            print(f\"Alternate Spans: {alternate_names}\")\n",
    "        \n",
    "        return (spans, tokens, token_to_span, alternate_names)\n",
    "\n",
    "    def span_at_token(self, token):\n",
    "        if token in self.token_to_span:\n",
    "            return self.token_to_span[token]\n",
    "        return None\n",
    "    \n",
    "    def is_species(self, token):\n",
    "        return token in self.tokens\n",
    "        \n",
    "    def has_species(self, tokens, verbose=False):\n",
    "        for token in tokens:\n",
    "            if token in self.tokens:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def find_same_species(self, species_A, species_b, verbose=False):\n",
    "        # METHOD 1: Check for Literal Matches\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        \n",
    "        for sp_a in sp_A:\n",
    "            # Verbatim Text\n",
    "            sp_a_text = sp_a.text.lower()\n",
    "\n",
    "            if sp_a_text == sp_b_text:\n",
    "                return sp_a\n",
    "\n",
    "            # Singularized Text\n",
    "            sp_a_singular_texts = self.main.help.singularize(sp_a_text)\n",
    "            sp_b_singular_texts = self.main.help.singularize(sp_b_text)\n",
    "\n",
    "            if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "                return sp_a\n",
    "\n",
    "        # METHOD 2: Check Alternate Names\n",
    "        for sp_a in sp_A:\n",
    "            if sp_b_text in self.alternate_names.get(sp_a_text, []):\n",
    "                return sp_a\n",
    "            if sp_a_text in self.alternate_names\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Comparing Species\")\n",
    "            print(f\"SPECIES 1: {sp_1}\")\n",
    "            print(f\"SPECIES 2: {sp_2}\")\n",
    "\n",
    "        sp_1_text = sp_1.text.lower()\n",
    "        sp_2_text = sp_2.text.lower()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"SPECIES 1 TEXT: {sp_1_text}\")\n",
    "            print(f\"SPECIES 2 TEXT: {sp_2_text}\")\n",
    "        \n",
    "        # METHOD 1: Check if Texts are Equivalent\n",
    "        equivalent = sp_1.text.lower() == sp_2.text.lower()\n",
    "        \n",
    "        if equivalent:\n",
    "            return True\n",
    "\n",
    "        # METHOD 2: Check Alternate Names\n",
    "        if verbose:\n",
    "            print(\"Check Alternate Names\")\n",
    "        \n",
    "        if sp_1_text in self.alternate_names:\n",
    "            if verbose:\n",
    "                print(f\"SPECIES 1 Alternate Names: {self.alternate_names[sp_1_text]}\")\n",
    "            if sp_2_text in self.alternate_names[sp_1_text]:\n",
    "                return True\n",
    "        \n",
    "        if sp_2_text in self.alternate_names:\n",
    "            if verbose:\n",
    "                print(f\"SPECIES 2 Alternate Names: {self.alternate_names[sp_2_text]}\")\n",
    "            if sp_1_text in self.alternate_names[sp_2_text]:\n",
    "                return True\n",
    "\n",
    "        # Singular Version of Phrase (e.g. \"fewer crabs\" becomes \"fewer crab\")\n",
    "        singular_version = lambda tokens : \" \".join([*[token.text for token in tokens[:-1]], tokens[-1].lemma_]).lower()\n",
    "\n",
    "        # METHOD 3: Check Substrings (More or Less)\n",
    "        # Via this method, pairs like (1) \"dog\" and \"dog red\"; and\n",
    "        # (2) \"red dog\" and \"dog\" should match.\n",
    "\n",
    "        # Common Name at Start\n",
    "        if sp_1[0].lower_ == sp_2[0].lower_ and (sp_1[0].pos_ in [\"NOUN\", \"PROPN\"] or sp_2[0].pos_ in [\"NOUN\", \"PROPN\"]):\n",
    "            if sp_1_text in sp_2_text or sp_2_text in sp_1_text:\n",
    "                if verbose:\n",
    "                    print(f\"{sp_1} and {sp_2} are the same species.\")\n",
    "                return True\n",
    "        # Common Name at End\n",
    "        else:\n",
    "            # Only used when there's 1 adjective in one of the species and\n",
    "            # no adjectives in the other (e.g. \"fewer crabs\" v. \"crabs\").\n",
    "            sp_1_nouns = []\n",
    "            sp_1_num_adjectives = 0\n",
    "            for token in sp_1:\n",
    "                if not sp_1_nouns and token.pos_ == \"ADJ\":\n",
    "                    sp_1_num_adjectives += 1\n",
    "                elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                    sp_1_nouns.append(token)\n",
    "            \n",
    "            sp_2_nouns = []\n",
    "            sp_2_num_adjectives = 0\n",
    "            for token in sp_2:\n",
    "                if not sp_2_nouns and token.pos_ == \"ADJ\":\n",
    "                    sp_2_num_adjectives += 1\n",
    "                elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                    sp_2_nouns.append(token)\n",
    "    \n",
    "            if verbose:\n",
    "                print(f\"Number of Adjectives in 1: {sp_1_num_adjectives}\")\n",
    "                print(f\"Number of Adjectives in 2: {sp_2_num_adjectives}\")\n",
    "    \n",
    "            if sp_1_nouns and sp_2_nouns and (\n",
    "                (sp_1_num_adjectives == 1 and sp_2_num_adjectives == 0) or \n",
    "                (sp_2_num_adjectives == 1 and sp_1_num_adjectives == 0)\n",
    "            ):\n",
    "                sp_singular_nouns_1 = singular_version(sp_1_nouns)\n",
    "                sp_singular_nouns_2 = singular_version(sp_2_nouns)\n",
    "    \n",
    "                if verbose:\n",
    "                    print(f\"Comparing Singular Nouns: '{sp_singular_nouns_1}' == '{sp_singular_nouns_2}'\")\n",
    "                \n",
    "                return sp_singular_nouns_1 == sp_singular_nouns_2\n",
    "\n",
    "        # METHOD 4: Check Singular Version\n",
    "        # This method targets spans like \"predatory crab\" and \"predatory crabs\".\n",
    "        sp_singular_1 = singular_version(sp_1)\n",
    "        sp_singular_2 = singular_version(sp_2)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Comparing Singular Spans: '{sp_singular_1}' == '{sp_singular_2}'\")\n",
    "        \n",
    "        if sp_singular_1 == sp_singular_2:\n",
    "            return True\n",
    "\n",
    "        # At this point, I don't see \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c31521a-3d38-4087-b63e-bb974ad3a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    def __init__(self, main, base=[], phrases=[], speech=[], threshold=0.7, sub_base=[]):\n",
    "        self.main = main\n",
    "        # For a token to count towards a base word, it must be the same word.\n",
    "        # For a token to count towards a sub_base word, it must contain the word.\n",
    "        self.base = [b.lower() for b in base]\n",
    "        self.sub_base = [b.lower() for b in sub_base]\n",
    "        self.speech = [s.upper() for s in speech]\n",
    "        self.phrases = [p.lower() for p in phrases]\n",
    "        self.threshold = threshold\n",
    "        self.vocab = [self.main.sp_nlp(word) for word in self.base]\n",
    "        self.tokens = []\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tokens = self.match_tokens(verbose=verbose)\n",
    "\n",
    "    def match_tokens(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        matched_tokens = []\n",
    "\n",
    "        # Check Words\n",
    "        for token in self.main.sp_doc:\n",
    "            if verbose:\n",
    "                print(f\"Potential Keyword: {token, token.pos_} v. Speech: {self.speech}\")\n",
    "            if token.pos_ not in self.speech:\n",
    "                continue\n",
    "\n",
    "            token_lower = token.lower_\n",
    "            token_lemma_lower = token.lemma_.lower()\n",
    "            \n",
    "            # Comparing Literal Text\n",
    "            if token_lemma_lower in self.base or token_lower in self.base:\n",
    "                matched_tokens.append(token)\n",
    "                continue\n",
    "            # Comparing Substrings\n",
    "            for sub_base_word in self.sub_base:\n",
    "                if sub_base_word in token_lemma_lower or sub_base_word in token_lower:\n",
    "                    matched_tokens.append(token)\n",
    "                    break\n",
    "\n",
    "            # Cannot quickly continue onto the next loop (outer)\n",
    "            # because we were already in a loop.\n",
    "            if matched_tokens and matched_tokens[-1] == token:\n",
    "                continue\n",
    "            \n",
    "            # Comparing Similarity\n",
    "            token_doc = self.main.sp_nlp(token_lower)\n",
    "            for word in self.vocab:\n",
    "                similarity = word.similarity(token_doc)\n",
    "                if verbose:\n",
    "                    print(f\"{token_doc} and {word} Similarity: {similarity}\")\n",
    "                if similarity >= self.threshold:\n",
    "                    matched_tokens.append(token)\n",
    "                    break\n",
    "\n",
    "        # Check Phrases\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "        for phrase in self.phrases:\n",
    "            for char_index in [match.start() for match in re.finditer(phrase, text)]:\n",
    "                matched_tokens.append(self.main.token_at_char(char_index))\n",
    "                \n",
    "        return matched_tokens\n",
    "\n",
    "class ExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"study\", \"hypothesis\", \"experiment\", \"found\", \"discover\", \"compare\", \"finding\", \"result\", \"test\", \"examine\", \"model\"],\n",
    "            phrases=[\"control group\", \"independent\", \"dependent\"],\n",
    "            speech=[\"VERB\", \"NOUN\"], \n",
    "            threshold=0.8\n",
    "        )\n",
    "\n",
    "class CauseKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"increase\", \"decrease\", \"change\", \"shift\", \"cause\", \"produce\", \"trigger\", \"suppress\", \"inhibit\", \"encourage\", \"allow\"], \n",
    "            speech=[\"VERB\", \"NOUN\"], \n",
    "            threshold=0.8\n",
    "        )\n",
    "\n",
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"few\", \"more\", \"increase\", \"decrease\", \"less\", \"short\", \"long\", \"greater\"], \n",
    "            speech=[\"NOUN\", \"ADJ\", \"ADV\"], \n",
    "            threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba70792f-d32b-4638-a929-c7499506e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraitKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"behavior\", \"rate\", \"color\", \"mass\", \"size\", \"length\", \"pattern\", \"weight\", \"shape\", \"efficiency\", \"trait\", \"ability\", \"capacity\", \"height\", \"width\", \"span\"],\n",
    "            sub_base=[\"mass\", \"span\", \"length\", \"color\", \"rate\"],\n",
    "            speech=[\"NOUN\", \"ADJ\"], \n",
    "            threshold=0.8\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        if verbose:\n",
    "            print(f\"Unfiltered Tokens: {self.tokens}\")\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "        if verbose:\n",
    "            print(f\"Filtered Tokens: {self.tokens}\")\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            expanded_token = self.main.expand_unit(\n",
    "                il_unit=token.i, \n",
    "                ir_unit=token.i, \n",
    "                il_boundary=0, \n",
    "                ir_boundary=len(self.main.sp_doc) - 1, \n",
    "                allowed_speech=[\"ADJ\", \"NOUN\", \"ADP\", \"PART\", \"DET\", \"PROPN\",],\n",
    "                allowed_literals=[\"-\", \",\"],\n",
    "                disallowed_literals=[\"!\", \".\", \"?\"],\n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Token: {token}\")\n",
    "                print(f\"Expanded Token: {expanded_token}\")\n",
    "\n",
    "            if self.main.species.has_species(expanded_token):\n",
    "                if verbose:\n",
    "                    print(f\"\\tContains Species\")\n",
    "                filtered.append(token)\n",
    "        \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fdf8e07-a326-4fea-8615-faa5d549852d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Main:\n",
    "    def __init__(self):\n",
    "        # Tools\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        self.sp_doc = None\n",
    "\n",
    "        # Maps Character Position to Token in Document\n",
    "        # Used to handle differences between different\n",
    "        # pipelines and tools.\n",
    "        self.index_map = None\n",
    "    \n",
    "        # Parsers\n",
    "        self.species = Species(self)\n",
    "        self.traits = TraitKeywords(self)\n",
    "        self.causes = CauseKeywords(self)\n",
    "        self.changes = ChangeKeywords(self)\n",
    "        self.experiment = ExperimentKeywords(self)\n",
    "\n",
    "    def update_doc(self, doc, verbose=False):\n",
    "        self.sp_doc = doc\n",
    "        self.index_map = self.load_index_map()\n",
    "        self.species.update(doc.text, verbose=True)\n",
    "        self.traits.update(verbose=False)\n",
    "        self.causes.update(verbose=False)\n",
    "        self.changes.update(verbose=False)\n",
    "        self.experiment.update(verbose=False)\n",
    "\n",
    "    def update_text(self, text, verbose=False):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.update_doc(self.sp_doc, verbose=verbose)\n",
    "        \n",
    "    def token_at_char(self, char_index):\n",
    "        # SpaCy Doc or Indexing Map Not Found\n",
    "        if not self.sp_doc or not self.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Index into Map\n",
    "        if char_index in self.index_map:\n",
    "            return self.index_map[char_index]\n",
    "\n",
    "        # Looking in Tokens\n",
    "        # Depending on the tokenizer, the character being\n",
    "        # used to find a token may not be the first character\n",
    "        # of the token.\n",
    "        # This shouldn't be needed anymore as I am pairing\n",
    "        # each character in the document to its token.\n",
    "        # for token in self.sp_doc:\n",
    "        #     if char_index >= token.idx and char_index < token.idx + len(token):\n",
    "        #         return token\n",
    "\n",
    "        # There must be a token that corresponds to the\n",
    "        # given character index. If there's not, there's\n",
    "        # an issue.\n",
    "        raise Exception(\"Token Not Found\")\n",
    "        \n",
    "    def load_index_map(self):\n",
    "        # SpaCy Doc Not Found\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Map Character Index to Token\n",
    "        index_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            # char_i0 is the index of the token's starting character.\n",
    "            # char_i1 is the index of the character after the token's ending character.\n",
    "            char_i0 = token.idx\n",
    "            char_i1 = token.idx + len(token)\n",
    "        \n",
    "            for i in range(char_i0, char_i1):\n",
    "                index_map[i] = token\n",
    "            \n",
    "        return index_map\n",
    "\n",
    "    def score(self, verbose=False):\n",
    "        NUM_CATEGORIES = 4\n",
    "\n",
    "        # Requires the mention of a trait and a cause or change word.\n",
    "        # The cause or change word indicates some variation.\n",
    "        # Index 0 in Array\n",
    "        TRAIT = 0\n",
    "\n",
    "        # Requires the mention of a species and a cause or change word.\n",
    "        # The cause or change word indicates that the species is being\n",
    "        # affected or is affecting something else.\n",
    "        # Index 1 in Array\n",
    "        SPECIES = 1\n",
    "\n",
    "        # Requires a word that has been defined as \"experiment\"-related.\n",
    "        # Index 2 in Array\n",
    "        EXPERIMENT = 2\n",
    "\n",
    "        # Requires the mention of several species (more or less).\n",
    "        # Index 3 in Array\n",
    "        INTERACTION = 3\n",
    "\n",
    "        # Max # of Points of Category per Sentence (MPC)\n",
    "        # A sentence collects points from its categories.\n",
    "        # For example, a sentence could get a maximum of 2 points from one category\n",
    "        # and a maximum of 1 point from another. The MPC determines the maximum number\n",
    "        # of points a category could contribute to a sentence. To have a range of [0, 1]\n",
    "        # the maximum number of points, across categories, when added should be 1.\n",
    "        MPC = [0] * NUM_CATEGORIES\n",
    "        MPC[TRAIT] = 0.1\n",
    "        MPC[SPECIES] = 0.3\n",
    "        MPC[EXPERIMENT] = 0.3\n",
    "        MPC[INTERACTION] = 0.3\n",
    "\n",
    "        assert np.sum(MPC) == 1\n",
    "        \n",
    "        # Points per Instance of Category (PIC)\n",
    "        # Each token is evaluated to check whether a category\n",
    "        # can be given points. The number of points given, if\n",
    "        # the token is determined to be satisfactory, is the PIC.\n",
    "        # The PIC is less than or equal to the MPC for the corresponding\n",
    "        # category. The idea behind the PIC and MPC is similar to how\n",
    "        # sets work in tennis: you're not immediately awarded the full points\n",
    "        # for the set (MPC) if your opponent fails to return the ball,\n",
    "        # instead you're given a smaller # of points (PIC) that allow you to\n",
    "        # incrementally win the set (category).\n",
    "        PIC = [0] * NUM_CATEGORIES\n",
    "        PIC[TRAIT] = MPC[TRAIT]*1.0\n",
    "        PIC[SPECIES] = MPC[SPECIES]/3.0\n",
    "        PIC[EXPERIMENT] = MPC[EXPERIMENT]/1.0\n",
    "        PIC[INTERACTION] = MPC[INTERACTION]/3.0\n",
    "\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            assert PIC[i] <= MPC[i]\n",
    "\n",
    "        # Category Weights (CW)\n",
    "        # It may be helpful to weigh a certain category's fraction of total\n",
    "        # points more or less than another's. Thus, at the end, we'll take a\n",
    "        # weighted average of the category's FTP. The weights must add up to 1.\n",
    "        CW = [0] * NUM_CATEGORIES\n",
    "        CW[TRAIT] = 0.25\n",
    "        CW[SPECIES] = 0.25\n",
    "        CW[EXPERIMENT] = 0.25\n",
    "        CW[INTERACTION] = 0.25\n",
    "\n",
    "        assert np.sum(MPC) == 1\n",
    "\n",
    "        # Points\n",
    "        points = [0] * NUM_CATEGORIES\n",
    "\n",
    "        # Extracted Information\n",
    "        cause_tokens = self.causes.tokens\n",
    "        change_tokens = self.changes.tokens\n",
    "        trait_tokens = self.traits.tokens\n",
    "        species_tokens = [self.sp_doc[span.start] for span in self.species.spans]\n",
    "        experiment_tokens = self.experiment.tokens\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Cause Tokens: {[(t.i, t) for t in self.causes.tokens]}\")\n",
    "            print(f\"Change Tokens: {[(t.i, t) for t in self.changes.tokens]}\")\n",
    "            print(f\"Trait Tokens: {[(t.i, t) for t in self.traits.tokens]}\")\n",
    "            print(f\"Species Tokens: {[(t.i, t) for t in self.species.tokens]}\")\n",
    "            print(f\"Reduced Species Tokens: {[(t.i, t) for t in species_tokens]}\")\n",
    "            print(f\"Experiment Tokens: {[(t.i, t) for t in self.experiment.tokens]}\")\n",
    "         \n",
    "        # This is used to ensure that at least three species\n",
    "        # are mentioned.\n",
    "        seen_species = {}\n",
    "\n",
    "        for sent in self.sp_doc.sents:\n",
    "            # This contains the number of points\n",
    "            # each category has accumulated in the sentence.\n",
    "            curr_points = [0] * NUM_CATEGORIES\n",
    "\n",
    "            # Contains the tokens in the sentence.\n",
    "            sent_tokens = [token for token in sent]\n",
    "\n",
    "            # This is used for the species (must have a nearby cause and/or\n",
    "            # change word).\n",
    "            sent_cause_tokens = set(sent_tokens).intersection(cause_tokens)\n",
    "            sent_change_tokens = set(sent_tokens).intersection(change_tokens)\n",
    "\n",
    "            # We don't want to visit the same species more than one\n",
    "            # in the same sentence as to avoid redundant points.\n",
    "            sent_seen_species = []\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\n\\nSentence: {sent}\\n\")\n",
    "                print(f\"Sentence Tokens: {[(t.i, t) for t in sent]}\")\n",
    "                print(f\"Sentence Cause Tokens: {sent_cause_tokens}\")\n",
    "                print(f\"Sentence Change Tokens: {sent_change_tokens}\\n\")\n",
    "            \n",
    "            for token in sent_tokens:\n",
    "                # If each category has reached their maximum number of points,\n",
    "                # we can end the loop early.\n",
    "                all_maxed = True\n",
    "                for i in range(NUM_CATEGORIES):\n",
    "                    if curr_points[i] < MPC[i]:\n",
    "                        all_maxed = False\n",
    "\n",
    "                if all_maxed:\n",
    "                    break\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Token ({token.pos_}): '{token}'\")\n",
    "\n",
    "                # TRAIT CATEGORY\n",
    "                if curr_points[TRAIT] < MPC[TRAIT] and token in trait_tokens:\n",
    "                    # To get points in the trait category, there must \n",
    "                    # be (1) a trait; and (2) a change or cause in the token's\n",
    "                    # context.\n",
    "                    token_context = set(self.find_unit_context(il_unit=token.i, ir_unit=token.i, il_boundary=token.sent.start, ir_boundary=token.sent.end-1, verbose=verbose))\n",
    "                    cause_tokens_in_context = set(sent_cause_tokens).intersection(token_context)\n",
    "                    change_tokens_in_context = set(sent_change_tokens).intersection(token_context)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"Token Context: {token_context}\")\n",
    "                        print(f\"Cause Tokens in Context: {cause_tokens_in_context}\")\n",
    "                        print(f\"Change Tokens in Context: {change_tokens_in_context}\")\n",
    "\n",
    "                    if cause_tokens_in_context or change_tokens_in_context:\n",
    "                        curr_points[TRAIT] += PIC[TRAIT]\n",
    "\n",
    "                        if verbose:\n",
    "                            print(f\"Added {PIC[TRAIT]} Points to Trait\")\n",
    "\n",
    "                # EXPERIMENT CATEGORY\n",
    "                if curr_points[EXPERIMENT] < MPC[EXPERIMENT] and token in experiment_tokens:\n",
    "                    curr_points[EXPERIMENT] += PIC[EXPERIMENT]\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"Added {PIC[EXPERIMENT]} Points to Experiment\")\n",
    "\n",
    "                # SPECIES CATEGORY\n",
    "                if token in species_tokens:\n",
    "                    # Find Species Span\n",
    "                    species_span = self.species.span_at_token(token)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"Species Span: {species_span}\")\n",
    "                        print(f\"Seen Species:\\n{seen_species}\")                 \n",
    "\n",
    "                    # Updating Seen Species (in Entire Text)\n",
    "                    past_visits = 0\n",
    "                    for seen_species_span in seen_species.keys():\n",
    "                        if self.species.same_species(species_span, seen_species_span, verbose=verbose):\n",
    "                            past_visits = seen_species[seen_species_span]\n",
    "                            if verbose:\n",
    "                                print(f\"\\t'{species_span}' == '{seen_species_span}'\")\n",
    "                                print(f\"\\tNumber of Visits: {past_visits}\")\n",
    "                            seen_species[seen_species_span] += 1\n",
    "                            break\n",
    "\n",
    "                    if not past_visits:\n",
    "                        seen_species[species_span] = 1\n",
    "                    is_new_species = past_visits == 0\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"Is New Species: {is_new_species}\")\n",
    "                        print(f\"Seen Species Updated:\\n{seen_species}\")\n",
    "                        print(\"Checking Seen Species in Sentence\")\n",
    "\n",
    "                    # Checking Seen Species (in Sentence)\n",
    "                    # We only add points if it's a species that has not been seen\n",
    "                    # in the sentence. This is to avoid redundant points. \n",
    "                    # Also, if it species has not been seen at all (is_new_species),\n",
    "                    # then it cannot be a redundant species (we couldn't have seen it in the sentence\n",
    "                    # either).\n",
    "                    redundant_species = False\n",
    "\n",
    "                    if not is_new_species:\n",
    "                        for seen_species_span in sent_seen_species:\n",
    "                            if self.species.same_species(species_span, seen_species_span, verbose=verbose):\n",
    "                                redundant_species = True\n",
    "                                break\n",
    "                    \n",
    "                    sent_seen_species.append(species_span)\n",
    "                    if redundant_species:\n",
    "                        continue\n",
    "\n",
    "                    # INTERACTION CATEGORY\n",
    "                    # It is helpful to have this category here because (if we've reached here)\n",
    "                    # we're dealing with a new species in the sentence.\n",
    "                    if curr_points[INTERACTION] < MPC[INTERACTION]:\n",
    "                        curr_points[INTERACTION] += PIC[INTERACTION]\n",
    "\n",
    "                        if verbose:\n",
    "                            print(f\"Added {PIC[INTERACTION]} Points to Interaction\")\n",
    "                        \n",
    "                    if curr_points[SPECIES] < MPC[SPECIES]:\n",
    "                        # To get points in the species category, there must be \n",
    "                        # (1) a species; and (2) a change or cause in the phrase\n",
    "                        # (or clause) that the token is a part of.\n",
    "                        token_context = set(self.find_unit_context(il_unit=token.i, ir_unit=token.i, il_boundary=token.sent.start, ir_boundary=token.sent.end-1, verbose=verbose))\n",
    "                        cause_tokens_in_context = set(sent_cause_tokens).intersection(token_context)\n",
    "                        change_tokens_in_context = set(sent_change_tokens).intersection(token_context)\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"Token Context: {token_context}\")\n",
    "                            print(f\"Cause Tokens in Context: {cause_tokens_in_context}\")\n",
    "                            print(f\"Change Tokens in Context: {change_tokens_in_context}\")\n",
    "                        \n",
    "                        if cause_tokens_in_context or change_tokens_in_context:\n",
    "                            curr_points[SPECIES] += PIC[SPECIES]\n",
    "\n",
    "                            if verbose:\n",
    "                                print(f\"Added {PIC[SPECIES]} Points to Species\")\n",
    "         \n",
    "            # SENTENCE DONE\n",
    "            # Add Sentence Points to Total Points\n",
    "            for category in [TRAIT, SPECIES, EXPERIMENT, INTERACTION]:\n",
    "                points[category] += min(curr_points[category], MPC[category])\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Overall Points: {points}\")\n",
    "\n",
    "        # Enforcing 3 or More Species\n",
    "        if verbose:\n",
    "            print(f\"Seen Species: {seen_species}\")\n",
    "            \n",
    "        if len(seen_species) < 3:\n",
    "            return 0\n",
    "        \n",
    "        # Calculating Score\n",
    "        if verbose:\n",
    "            print(f\"Points: {points}\")\n",
    "            \n",
    "        NUM_SENTENCES = len(list(self.sp_doc.sents))\n",
    "\n",
    "        score = 0\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            FTP = points[i] / (MPC[i] * NUM_SENTENCES)\n",
    "            if verbose:\n",
    "                print(f\"Category 1's FTP: {FTP}\")\n",
    "            score += FTP * CW[i]\n",
    "            \n",
    "        assert 0.0 <= score <= 1.0\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Score: {score}\")\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a10821c-4a15-47e6-b3d2-f49b7ef3eec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In simple, linear food chains, top predators can have positive indirect effects on basal resources by causing changes in the traits (e.g. behaviour, feeding rates) of intermediate consumers. Although less is known about trait-mediated indirect interactions (TMIIs) in more complex food webs, it has been suggested that such complexity dampens trophic cascades. We examined TMIIs between a predatory crab ( Carcinus maenas ) and two ecologically important basal resources, fucoid algae ( Ascophyllum nodosum ) and barnacles ( Semibalanus balanoides ), which are consumed by herbivorous ( Littorina littorea ) and carnivorous ( Nucella lapillus ) snails, respectively. Because crab predation risk suppresses snail feeding rates, we hypothesized that crabs would also shape direct and indirect interactions among the multiple consumers and resources. We found that the magnitude of TMIIs between the crab and each resource depended on the suite of intermediate consumers present in the food web. Carnivorous snails ( Nucella ) transmitted TMIIs between crabs and barnacles. However, crabalgae TMIIs were transmitted by both herbivorous ( Littorina ) and carnivorous ( Nucella ) snails, and these TMIIs were additive. By causing Nucella to consume fewer barnacles, crab predation risk allowed fucoids that had settled on or between barnacles to remain in the community. Hence, positive interactions between barnacles and algae caused crabalgae TMIIs to be strongest when both consumers were present. Studies of TMIIs in more realistic, reticulate food webs will be necessary for a more complete understanding of how predation risk shapes community dynamics.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../Datasets/Baseline-1.csv\")\n",
    "text = df.Abstract[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7619e40a-251d-46d0-a451-d6e3c8fa30bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/16/2025 07:36:05 - INFO - \t missing_keys: []\n",
      "06/16/2025 07:36:05 - INFO - \t unexpected_keys: []\n",
      "06/16/2025 07:36:05 - INFO - \t mismatched_keys: []\n",
      "06/16/2025 07:36:05 - INFO - \t error_msgs: []\n",
      "06/16/2025 07:36:05 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative methods to achieve sustainable agricultural production while reducing the use of chemical pesticides, such as biological control, are increasingly needed. The exploitation of trait-mediated indirect interactions (TMIIs), in which pests modify their behavior in response to some cues (e.g., pheromones and other semiochemicals) to avoid predation risk, may be a possible strategy. In this study, we tested the effect of TMIIs of two Mediterranean ant species, Crematogaster scutellaris and Tapinoma nigerrimum, on the oviposition behaviour of Ceratitis capitata (Diptera: Tephritidae), one of the world's most economically damaging pests, which attacks fruits. For each ant species, we performed choice experiments using ant-scented and control plums, counting the time spent by medflies on fruits and the number of pupae emerging from them. Results of both ant species tests showed a significantly shorter time spent by ovipositing medflies on ant-exposed plums and a lower number of pupae, when compared to the control group. Our findings highlighted that the semiochemicals released by ants on plums triggered an avoidance behaviour by medfly females, leading to lower oviposition rates. This study contributes to the understanding of indirect ant-pest interactions in Mediterranean agricultural settings and points out the potential of utilising ant-borne semiochemicals in sustainable IPM strategies.\n",
      "TN Entities (16): (pests, species, Crematogaster scutellaris, Tapinoma nigerrimum, Ceratitis capitata, Diptera, Tephritidae, ant, species, medflies, ant, species, medflies, pupae, ants, medfly)\n",
      "\n",
      "SP Entities (16): [pests, species, Crematogaster scutellaris, Tapinoma nigerrimum, Ceratitis capitata, Diptera, Tephritidae, ant, species, medflies, ant, species, medflies, pupae, ants, medfly]\n",
      "10\n",
      "6\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "5\n",
      "6\n",
      "6\n",
      "10\n",
      "3\n",
      "Base Species: ['pest', 'ceratitis capitatas', 'ceratitis capitata', 'tapinoma nigerrimum', 'tephritidae', 'ants', 'medfly', 'ant', 'diptera', 'medflies', 'pupae', 'tapinoma nigerrimums', 'pests', 'species', 'specy', 'crematogaster scutellaris']\n",
      "Searching 'pest'\n",
      "Search Species w/o Symbols: pest\n",
      "Match (102-106): 'pest'\n",
      "Letter on L/R -> Skip\n",
      "Match (242-246): 'pest'\n",
      "sp_li: 38\n",
      "sp_ri: 38\n",
      "Species Span: pests\n",
      "il_unit: 38, il_boundary: 0, prev_token: which, prev_token.pos_: PRON\n",
      "Expanded Unit: pests\n",
      "Expanded Species Span: pests\n",
      "Match (643-647): 'pest'\n",
      "sp_li: 109\n",
      "sp_ri: 109\n",
      "Species Span: pests\n",
      "il_unit: 109, il_boundary: 0, prev_token: damaging, prev_token.pos_: ADJ\n",
      "il_unit: 108, il_boundary: 0, prev_token: economically, prev_token.pos_: ADV\n",
      "Expanded Unit: damaging pests\n",
      "Expanded Species Span: damaging pests\n",
      "Match (1262-1266): 'pest'\n",
      "Letter on L/R -> Skip\n",
      "Searching 'ceratitis capitatas'\n",
      "Search Species w/o Symbols: ceratitis capitatas\n",
      "Searching 'ceratitis capitata'\n",
      "Search Species w/o Symbols: ceratitis capitata\n",
      "Match (554-572): 'ceratitis capitata'\n",
      "sp_li: 93\n",
      "sp_ri: 94\n",
      "Species Span: Ceratitis capitata\n",
      "Searching 'tapinoma nigerrimum'\n",
      "Search Species w/o Symbols: tapinoma nigerrimum\n",
      "Match (501-520): 'tapinoma nigerrimum'\n",
      "sp_li: 85\n",
      "sp_ri: 86\n",
      "Species Span: Tapinoma nigerrimum\n",
      "Searching 'tephritidae'\n",
      "Search Species w/o Symbols: tephritidae\n",
      "Match (583-594): 'tephritidae'\n",
      "sp_li: 98\n",
      "sp_ri: 98\n",
      "Species Span: Tephritidae\n",
      "Searching 'ants'\n",
      "Search Species w/o Symbols: ants\n",
      "Match (1100-1104): 'ants'\n",
      "Letter on L/R -> Skip\n",
      "Searching 'medfly'\n",
      "Search Species w/o Symbols: medfly\n",
      "Match (1150-1156): 'medfly'\n",
      "Letter on L/R -> Skip\n",
      "Searching 'ant'\n",
      "Search Species w/o Symbols: ant\n",
      "Match (458-461): 'ant'\n",
      "Letter on L/R -> Skip\n",
      "Match (681-684): 'ant'\n",
      "Letter on L/R -> Skip\n",
      "Match (732-735): 'ant'\n",
      "Letter on L/R -> Skip\n",
      "Match (869-872): 'ant'\n",
      "Letter on L/R -> Skip\n",
      "Match (904-907): 'ant'\n",
      "Letter on L/R -> Skip\n",
      "Match (956-959): 'ant'\n",
      "Letter on L/R -> Skip\n",
      "Match (1100-1103): 'ant'\n",
      "sp_li: 191\n",
      "sp_ri: 191\n",
      "Species Span: ants\n",
      "il_unit: 191, il_boundary: 0, prev_token: by, prev_token.pos_: ADP\n",
      "Expanded Unit: ants\n",
      "Expanded Species Span: ants\n",
      "Match (1258-1261): 'ant'\n",
      "Letter on L/R -> Skip\n",
      "Match (1361-1364): 'ant'\n",
      "Letter on L/R -> Skip\n",
      "Searching 'diptera'\n",
      "Search Species w/o Symbols: diptera\n",
      "Match (574-581): 'diptera'\n",
      "sp_li: 96\n",
      "sp_ri: 96\n",
      "Species Span: Diptera\n",
      "Searching 'medflies'\n",
      "Search Species w/o Symbols: medflies\n",
      "Match (790-798): 'medflies'\n",
      "Letter on L/R -> Skip\n",
      "Match (944-952): 'medflies'\n",
      "Letter on L/R -> Skip\n",
      "Searching 'pupae'\n",
      "Search Species w/o Symbols: pupae\n",
      "Match (827-832): 'pupae'\n",
      "Letter on L/R -> Skip\n",
      "Match (996-1001): 'pupae'\n",
      "sp_li: 174\n",
      "sp_ri: 174\n",
      "Species Span: pupae\n",
      "il_unit: 174, il_boundary: 0, prev_token: of, prev_token.pos_: ADP\n",
      "Expanded Unit: pupae\n",
      "Expanded Species Span: pupae\n",
      "Searching 'tapinoma nigerrimums'\n",
      "Search Species w/o Symbols: tapinoma nigerrimums\n",
      "Searching 'pests'\n",
      "Search Species w/o Symbols: pests\n",
      "Match (242-247): 'pests'\n",
      "Letter on L/R -> Skip\n",
      "Match (643-648): 'pests'\n",
      "sp_li: 109\n",
      "sp_ri: 109\n",
      "Species Span: pests\n",
      "il_unit: 109, il_boundary: 0, prev_token: damaging, prev_token.pos_: ADJ\n",
      "il_unit: 108, il_boundary: 0, prev_token: economically, prev_token.pos_: ADV\n",
      "Expanded Unit: damaging pests\n",
      "Expanded Species Span: damaging pests\n",
      "Searching 'species'\n",
      "Search Species w/o Symbols: species\n",
      "Match (462-469): 'species'\n",
      "sp_li: 80\n",
      "sp_ri: 80\n",
      "Species Span: species\n",
      "il_unit: 80, il_boundary: 0, prev_token: ant, prev_token.pos_: NOUN\n",
      "Expanded Unit: species\n",
      "Expanded Species Span: species\n",
      "Match (685-692): 'species'\n",
      "sp_li: 118\n",
      "sp_ri: 118\n",
      "Species Span: species\n",
      "il_unit: 118, il_boundary: 0, prev_token: ant, prev_token.pos_: NOUN\n",
      "Expanded Unit: species\n",
      "Expanded Species Span: species\n",
      "Match (873-880): 'species'\n",
      "Letter on L/R -> Skip\n",
      "Searching 'specy'\n",
      "Search Species w/o Symbols: specy\n",
      "Searching 'crematogaster scutellaris'\n",
      "Search Species w/o Symbols: crematogaster scutellaris\n",
      "Match (471-496): 'crematogaster scutellaris'\n",
      "Letter on L/R -> Skip\n",
      "Spans Before Removing Duplicates and Sorting: [pests, damaging pests, Ceratitis capitata, Tapinoma nigerrimum, Tephritidae, ants, Diptera, pupae, damaging pests, species, species]\n",
      "Spans Before Removing Duplicates and Sorting: [pests, species, Tapinoma nigerrimum, Ceratitis capitata, Diptera, Tephritidae, damaging pests, species, pupae, ants]\n",
      "Finding Alternate Names\n",
      "SPECIES 1: pests\n",
      "SPECIES 2: species\n",
      "DIST == 1: False\n",
      "SPECIES 1: species\n",
      "SPECIES 2: Tapinoma nigerrimum\n",
      "DIST == 1: False\n",
      "SPECIES 1: Tapinoma nigerrimum\n",
      "SPECIES 2: Ceratitis capitata\n",
      "DIST == 1: False\n",
      "SPECIES 1: Ceratitis capitata\n",
      "SPECIES 2: Diptera\n",
      "DIST == 1: True\n",
      "Token Before SPECIES 2: ( and Token After SPECIES 2: :\n",
      "SPECIES 1: Diptera\n",
      "SPECIES 2: Tephritidae\n",
      "DIST == 1: True\n",
      "Token Before SPECIES 2: : and Token After SPECIES 2: )\n",
      "SPECIES 1: Tephritidae\n",
      "SPECIES 2: damaging pests\n",
      "DIST == 1: False\n",
      "SPECIES 1: damaging pests\n",
      "SPECIES 2: species\n",
      "DIST == 1: False\n",
      "SPECIES 1: species\n",
      "SPECIES 2: pupae\n",
      "DIST == 1: False\n",
      "SPECIES 1: pupae\n",
      "SPECIES 2: ants\n",
      "DIST == 1: False\n",
      "Spans: [pests, species, Tapinoma nigerrimum, Ceratitis capitata, Diptera, Tephritidae, damaging pests, species, pupae, ants]\n",
      "Tokens: [pests, damaging, pests, Ceratitis, capitata, Tapinoma, nigerrimum, Tephritidae, ants, Diptera, pupae, damaging, pests, species, species]\n",
      "Alternate Spans: {'ceratitis capitata': ['diptera'], 'diptera': ['ceratitis capitata', 'tephritidae'], 'tephritidae': ['diptera']}\n",
      "Cause Tokens: [(194, triggered)]\n",
      "Change Tokens: [(158, shorter)]\n",
      "Trait Tokens: [(91, behaviour)]\n",
      "Species Tokens: [(38, pests), (108, damaging), (109, pests), (93, Ceratitis), (94, capitata), (85, Tapinoma), (86, nigerrimum), (98, Tephritidae), (191, ants), (96, Diptera), (174, pupae), (108, damaging), (109, pests), (80, species), (118, species)]\n",
      "Reduced Species Tokens: [(38, pests), (80, species), (85, Tapinoma), (93, Ceratitis), (96, Diptera), (98, Tephritidae), (108, damaging), (118, species), (174, pupae), (191, ants)]\n",
      "Experiment Tokens: [(68, study), (71, tested), (123, experiments), (149, Results), (154, tests), (177, compared), (184, findings), (209, study), (180, control)]\n",
      "\n",
      "\n",
      "Sentence: Alternative methods to achieve sustainable agricultural production while reducing the use of chemical pesticides, such as biological control, are increasingly needed.\n",
      "\n",
      "Sentence Tokens: [(0, Alternative), (1, methods), (2, to), (3, achieve), (4, sustainable), (5, agricultural), (6, production), (7, while), (8, reducing), (9, the), (10, use), (11, of), (12, chemical), (13, pesticides), (14, ,), (15, such), (16, as), (17, biological), (18, control), (19, ,), (20, are), (21, increasingly), (22, needed), (23, .)]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: set()\n",
      "\n",
      "Token (ADJ): 'Alternative'\n",
      "Token (NOUN): 'methods'\n",
      "Token (PART): 'to'\n",
      "Token (VERB): 'achieve'\n",
      "Token (ADJ): 'sustainable'\n",
      "Token (ADJ): 'agricultural'\n",
      "Token (NOUN): 'production'\n",
      "Token (SCONJ): 'while'\n",
      "Token (VERB): 'reducing'\n",
      "Token (DET): 'the'\n",
      "Token (NOUN): 'use'\n",
      "Token (ADP): 'of'\n",
      "Token (ADJ): 'chemical'\n",
      "Token (NOUN): 'pesticides'\n",
      "Token (PUNCT): ','\n",
      "Token (ADJ): 'such'\n",
      "Token (ADP): 'as'\n",
      "Token (ADJ): 'biological'\n",
      "Token (NOUN): 'control'\n",
      "Token (PUNCT): ','\n",
      "Token (AUX): 'are'\n",
      "Token (ADV): 'increasingly'\n",
      "Token (VERB): 'needed'\n",
      "Token (PUNCT): '.'\n",
      "Overall Points: [0, 0, 0, 0]\n",
      "\n",
      "\n",
      "Sentence: The exploitation of trait-mediated indirect interactions (TMIIs), in which pests modify their behavior in response to some cues (e.g., pheromones and other semiochemicals) to avoid predation risk, may be a possible strategy.\n",
      "\n",
      "Sentence Tokens: [(24, The), (25, exploitation), (26, of), (27, trait), (28, -), (29, mediated), (30, indirect), (31, interactions), (32, (), (33, TMIIs), (34, )), (35, ,), (36, in), (37, which), (38, pests), (39, modify), (40, their), (41, behavior), (42, in), (43, response), (44, to), (45, some), (46, cues), (47, (), (48, e.g.), (49, ,), (50, pheromones), (51, and), (52, other), (53, semiochemicals), (54, )), (55, to), (56, avoid), (57, predation), (58, risk), (59, ,), (60, may), (61, be), (62, a), (63, possible), (64, strategy), (65, .)]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: set()\n",
      "\n",
      "Token (DET): 'The'\n",
      "Token (NOUN): 'exploitation'\n",
      "Token (ADP): 'of'\n",
      "Token (NOUN): 'trait'\n",
      "Token (PUNCT): '-'\n",
      "Token (VERB): 'mediated'\n",
      "Token (ADJ): 'indirect'\n",
      "Token (NOUN): 'interactions'\n",
      "Token (PUNCT): '('\n",
      "Token (PROPN): 'TMIIs'\n",
      "Token (PUNCT): ')'\n",
      "Token (PUNCT): ','\n",
      "Token (ADP): 'in'\n",
      "Token (PRON): 'which'\n",
      "Token (NOUN): 'pests'\n",
      "Species Span: pests\n",
      "Seen Species:\n",
      "{}\n",
      "Is New Species: True\n",
      "Seen Species Updated:\n",
      "{pests: 1}\n",
      "Checking Seen Species in Sentence\n",
      "Added 0.09999999999999999 Points to Interaction\n",
      ") in [']', ')', '-', '--', '[', '(', '-', '--']\n",
      "( in [']', ')', '-', '--', '[', '(', '-', '--']\n",
      "L Punct: ) v. R Punct: (\n",
      "Expand L:\n",
      "Token: which and Position: PRON\n",
      "Expand L:\n",
      "Token: in and Position: ADP\n",
      "Expand L:\n",
      "Token: , and Position: PUNCT\n",
      "Break\n",
      "Expand R:\n",
      "Token: modify and Position: VERB\n",
      "Expand R:\n",
      "Token: their and Position: PRON\n",
      "Expand R:\n",
      "Token: behavior and Position: NOUN\n",
      "Expand R:\n",
      "Token: in and Position: ADP\n",
      "Expand R:\n",
      "Token: response and Position: NOUN\n",
      "Expand R:\n",
      "Token: to and Position: ADP\n",
      "Expand R:\n",
      "Token: some and Position: DET\n",
      "Break\n",
      "Exclude: []\n",
      "Context: [in, which, pests, modify, their, behavior, in, response, to]\n",
      "Token Context: {which, their, response, modify, in, in, behavior, pests, to}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "Token (VERB): 'modify'\n",
      "Token (PRON): 'their'\n",
      "Token (NOUN): 'behavior'\n",
      "Token (ADP): 'in'\n",
      "Token (NOUN): 'response'\n",
      "Token (ADP): 'to'\n",
      "Token (DET): 'some'\n",
      "Token (NOUN): 'cues'\n",
      "Token (PUNCT): '('\n",
      "Token (ADV): 'e.g.'\n",
      "Token (PUNCT): ','\n",
      "Token (NOUN): 'pheromones'\n",
      "Token (CCONJ): 'and'\n",
      "Token (ADJ): 'other'\n",
      "Token (NOUN): 'semiochemicals'\n",
      "Token (PUNCT): ')'\n",
      "Token (PART): 'to'\n",
      "Token (VERB): 'avoid'\n",
      "Token (NOUN): 'predation'\n",
      "Token (NOUN): 'risk'\n",
      "Token (PUNCT): ','\n",
      "Token (AUX): 'may'\n",
      "Token (AUX): 'be'\n",
      "Token (DET): 'a'\n",
      "Token (ADJ): 'possible'\n",
      "Token (NOUN): 'strategy'\n",
      "Token (PUNCT): '.'\n",
      "Overall Points: [0, 0, 0, 0.09999999999999999]\n",
      "\n",
      "\n",
      "Sentence: In this study, we tested the effect of TMIIs of two Mediterranean ant species, Crematogaster scutellaris and Tapinoma nigerrimum, on the oviposition behaviour of Ceratitis capitata (Diptera: Tephritidae), one of the world's most economically damaging pests, which attacks fruits.\n",
      "\n",
      "Sentence Tokens: [(66, In), (67, this), (68, study), (69, ,), (70, we), (71, tested), (72, the), (73, effect), (74, of), (75, TMIIs), (76, of), (77, two), (78, Mediterranean), (79, ant), (80, species), (81, ,), (82, Crematogaster), (83, scutellaris), (84, and), (85, Tapinoma), (86, nigerrimum), (87, ,), (88, on), (89, the), (90, oviposition), (91, behaviour), (92, of), (93, Ceratitis), (94, capitata), (95, (), (96, Diptera), (97, :), (98, Tephritidae), (99, )), (100, ,), (101, one), (102, of), (103, the), (104, world), (105, 's), (106, most), (107, economically), (108, damaging), (109, pests), (110, ,), (111, which), (112, attacks), (113, fruits), (114, .)]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: set()\n",
      "\n",
      "Token (ADP): 'In'\n",
      "Token (DET): 'this'\n",
      "Token (NOUN): 'study'\n",
      "Added 0.3 Points to Experiment\n",
      "Token (PUNCT): ','\n",
      "Token (PRON): 'we'\n",
      "Token (VERB): 'tested'\n",
      "Token (DET): 'the'\n",
      "Token (NOUN): 'effect'\n",
      "Token (ADP): 'of'\n",
      "Token (PROPN): 'TMIIs'\n",
      "Token (ADP): 'of'\n",
      "Token (NUM): 'two'\n",
      "Token (PROPN): 'Mediterranean'\n",
      "Token (NOUN): 'ant'\n",
      "Token (NOUN): 'species'\n",
      "Species Span: species\n",
      "Seen Species:\n",
      "{pests: 1}\n",
      "Comparing Species\n",
      "SPECIES 1: species\n",
      "SPECIES 2: pests\n",
      "SPECIES 1 TEXT: species\n",
      "SPECIES 2 TEXT: pests\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'specie' == 'pest'\n",
      "Is New Species: True\n",
      "Seen Species Updated:\n",
      "{pests: 1, species: 1}\n",
      "Checking Seen Species in Sentence\n",
      "Added 0.09999999999999999 Points to Interaction\n",
      "( in [']', ')', '-', '--', '[', '(', '-', '--']\n",
      "L Punct: None v. R Punct: (\n",
      "Expand L:\n",
      "Token: ant and Position: NOUN\n",
      "Expand L:\n",
      "Token: Mediterranean and Position: PROPN\n",
      "Expand L:\n",
      "Token: two and Position: NUM\n",
      "Break\n",
      "Expand R:\n",
      "Token: , and Position: PUNCT\n",
      "Break\n",
      "Exclude: []\n",
      "Context: [Mediterranean, ant, species]\n",
      "Token Context: {species, Mediterranean, ant}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "Token (PUNCT): ','\n",
      "Token (PROPN): 'Crematogaster'\n",
      "Token (PROPN): 'scutellaris'\n",
      "Token (CCONJ): 'and'\n",
      "Token (PROPN): 'Tapinoma'\n",
      "Species Span: Tapinoma nigerrimum\n",
      "Seen Species:\n",
      "{pests: 1, species: 1}\n",
      "Comparing Species\n",
      "SPECIES 1: Tapinoma nigerrimum\n",
      "SPECIES 2: pests\n",
      "SPECIES 1 TEXT: tapinoma nigerrimum\n",
      "SPECIES 2 TEXT: pests\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'tapinoma nigerrimum' == 'pest'\n",
      "Comparing Species\n",
      "SPECIES 1: Tapinoma nigerrimum\n",
      "SPECIES 2: species\n",
      "SPECIES 1 TEXT: tapinoma nigerrimum\n",
      "SPECIES 2 TEXT: species\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'tapinoma nigerrimum' == 'specie'\n",
      "Is New Species: True\n",
      "Seen Species Updated:\n",
      "{pests: 1, species: 1, Tapinoma nigerrimum: 1}\n",
      "Checking Seen Species in Sentence\n",
      "Added 0.09999999999999999 Points to Interaction\n",
      "( in [']', ')', '-', '--', '[', '(', '-', '--']\n",
      "L Punct: None v. R Punct: (\n",
      "Expand L:\n",
      "Token: and and Position: CCONJ\n",
      "Break\n",
      "Expand R:\n",
      "Token: nigerrimum and Position: NOUN\n",
      "Expand R:\n",
      "Token: , and Position: PUNCT\n",
      "Break\n",
      "Exclude: []\n",
      "Context: [Tapinoma, nigerrimum]\n",
      "Token Context: {nigerrimum, Tapinoma}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "Token (NOUN): 'nigerrimum'\n",
      "Token (PUNCT): ','\n",
      "Token (ADP): 'on'\n",
      "Token (DET): 'the'\n",
      "Token (NOUN): 'oviposition'\n",
      "Token (NOUN): 'behaviour'\n",
      "( in [']', ')', '-', '--', '[', '(', '-', '--']\n",
      "L Punct: None v. R Punct: (\n",
      "Expand L:\n",
      "Token: oviposition and Position: NOUN\n",
      "Expand L:\n",
      "Token: the and Position: DET\n",
      "Break\n",
      "Expand R:\n",
      "Token: of and Position: ADP\n",
      "Expand R:\n",
      "Token: Ceratitis and Position: PROPN\n",
      "Expand R:\n",
      "Token: capitata and Position: NOUN\n",
      "Expand R:\n",
      "Token: ( and Position: PUNCT\n",
      "Next Token is an Opening Punctuation\n",
      "i: 95, ir_boundary: 114, self.sp_doc[i]: '(', matching punctuation: ')'\n",
      "True and True\n",
      "\t'(' != ')'\n",
      "\t'Diptera' != ')'\n",
      "\t':' != ')'\n",
      "\t'Tephritidae' != ')'\n",
      "Skipped to Token 99\n",
      "Expand R:\n",
      "Token: , and Position: PUNCT\n",
      "Break\n",
      "Exclude: [(, Diptera, :, Tephritidae, )]\n",
      "Context: [oviposition, behaviour, of, Ceratitis, capitata]\n",
      "Token Context: {of, behaviour, capitata, oviposition, Ceratitis}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "Token (ADP): 'of'\n",
      "Token (PROPN): 'Ceratitis'\n",
      "Species Span: Ceratitis capitata\n",
      "Seen Species:\n",
      "{pests: 1, species: 1, Tapinoma nigerrimum: 1}\n",
      "Comparing Species\n",
      "SPECIES 1: Ceratitis capitata\n",
      "SPECIES 2: pests\n",
      "SPECIES 1 TEXT: ceratitis capitata\n",
      "SPECIES 2 TEXT: pests\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['diptera']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'ceratitis capitata' == 'pest'\n",
      "Comparing Species\n",
      "SPECIES 1: Ceratitis capitata\n",
      "SPECIES 2: species\n",
      "SPECIES 1 TEXT: ceratitis capitata\n",
      "SPECIES 2 TEXT: species\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['diptera']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'ceratitis capitata' == 'specie'\n",
      "Comparing Species\n",
      "SPECIES 1: Ceratitis capitata\n",
      "SPECIES 2: Tapinoma nigerrimum\n",
      "SPECIES 1 TEXT: ceratitis capitata\n",
      "SPECIES 2 TEXT: tapinoma nigerrimum\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['diptera']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'ceratitis capitata' == 'tapinoma nigerrimum'\n",
      "Is New Species: True\n",
      "Seen Species Updated:\n",
      "{pests: 1, species: 1, Tapinoma nigerrimum: 1, Ceratitis capitata: 1}\n",
      "Checking Seen Species in Sentence\n",
      "Added 0.09999999999999999 Points to Interaction\n",
      "( in [']', ')', '-', '--', '[', '(', '-', '--']\n",
      "L Punct: None v. R Punct: (\n",
      "Expand L:\n",
      "Token: of and Position: ADP\n",
      "Expand L:\n",
      "Token: behaviour and Position: NOUN\n",
      "Expand L:\n",
      "Token: oviposition and Position: NOUN\n",
      "Expand L:\n",
      "Token: the and Position: DET\n",
      "Break\n",
      "Expand R:\n",
      "Token: capitata and Position: NOUN\n",
      "Expand R:\n",
      "Token: ( and Position: PUNCT\n",
      "Next Token is an Opening Punctuation\n",
      "i: 95, ir_boundary: 114, self.sp_doc[i]: '(', matching punctuation: ')'\n",
      "True and True\n",
      "\t'(' != ')'\n",
      "\t'Diptera' != ')'\n",
      "\t':' != ')'\n",
      "\t'Tephritidae' != ')'\n",
      "Skipped to Token 99\n",
      "Expand R:\n",
      "Token: , and Position: PUNCT\n",
      "Break\n",
      "Exclude: [(, Diptera, :, Tephritidae, )]\n",
      "Context: [oviposition, behaviour, of, Ceratitis, capitata]\n",
      "Token Context: {of, behaviour, capitata, oviposition, Ceratitis}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "Token (NOUN): 'capitata'\n",
      "Token (PUNCT): '('\n",
      "Token (PROPN): 'Diptera'\n",
      "Species Span: Diptera\n",
      "Seen Species:\n",
      "{pests: 1, species: 1, Tapinoma nigerrimum: 1, Ceratitis capitata: 1}\n",
      "Comparing Species\n",
      "SPECIES 1: Diptera\n",
      "SPECIES 2: pests\n",
      "SPECIES 1 TEXT: diptera\n",
      "SPECIES 2 TEXT: pests\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['ceratitis capitata', 'tephritidae']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'diptera' == 'pest'\n",
      "Comparing Species\n",
      "SPECIES 1: Diptera\n",
      "SPECIES 2: species\n",
      "SPECIES 1 TEXT: diptera\n",
      "SPECIES 2 TEXT: species\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['ceratitis capitata', 'tephritidae']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'diptera' == 'specie'\n",
      "Comparing Species\n",
      "SPECIES 1: Diptera\n",
      "SPECIES 2: Tapinoma nigerrimum\n",
      "SPECIES 1 TEXT: diptera\n",
      "SPECIES 2 TEXT: tapinoma nigerrimum\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['ceratitis capitata', 'tephritidae']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'diptera' == 'tapinoma nigerrimum'\n",
      "Comparing Species\n",
      "SPECIES 1: Diptera\n",
      "SPECIES 2: Ceratitis capitata\n",
      "SPECIES 1 TEXT: diptera\n",
      "SPECIES 2 TEXT: ceratitis capitata\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['ceratitis capitata', 'tephritidae']\n",
      "\t'Diptera' == 'Ceratitis capitata'\n",
      "\tNumber of Visits: 1\n",
      "Is New Species: False\n",
      "Seen Species Updated:\n",
      "{pests: 1, species: 1, Tapinoma nigerrimum: 1, Ceratitis capitata: 2}\n",
      "Checking Seen Species in Sentence\n",
      "Comparing Species\n",
      "SPECIES 1: Diptera\n",
      "SPECIES 2: species\n",
      "SPECIES 1 TEXT: diptera\n",
      "SPECIES 2 TEXT: species\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['ceratitis capitata', 'tephritidae']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'diptera' == 'specie'\n",
      "Comparing Species\n",
      "SPECIES 1: Diptera\n",
      "SPECIES 2: Tapinoma nigerrimum\n",
      "SPECIES 1 TEXT: diptera\n",
      "SPECIES 2 TEXT: tapinoma nigerrimum\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['ceratitis capitata', 'tephritidae']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'diptera' == 'tapinoma nigerrimum'\n",
      "Comparing Species\n",
      "SPECIES 1: Diptera\n",
      "SPECIES 2: Ceratitis capitata\n",
      "SPECIES 1 TEXT: diptera\n",
      "SPECIES 2 TEXT: ceratitis capitata\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['ceratitis capitata', 'tephritidae']\n",
      "Token (PUNCT): ':'\n",
      "Token (PROPN): 'Tephritidae'\n",
      "Species Span: Tephritidae\n",
      "Seen Species:\n",
      "{pests: 1, species: 1, Tapinoma nigerrimum: 1, Ceratitis capitata: 2}\n",
      "Comparing Species\n",
      "SPECIES 1: Tephritidae\n",
      "SPECIES 2: pests\n",
      "SPECIES 1 TEXT: tephritidae\n",
      "SPECIES 2 TEXT: pests\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['diptera']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'tephritidae' == 'pest'\n",
      "Comparing Species\n",
      "SPECIES 1: Tephritidae\n",
      "SPECIES 2: species\n",
      "SPECIES 1 TEXT: tephritidae\n",
      "SPECIES 2 TEXT: species\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['diptera']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'tephritidae' == 'specie'\n",
      "Comparing Species\n",
      "SPECIES 1: Tephritidae\n",
      "SPECIES 2: Tapinoma nigerrimum\n",
      "SPECIES 1 TEXT: tephritidae\n",
      "SPECIES 2 TEXT: tapinoma nigerrimum\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['diptera']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'tephritidae' == 'tapinoma nigerrimum'\n",
      "Comparing Species\n",
      "SPECIES 1: Tephritidae\n",
      "SPECIES 2: Ceratitis capitata\n",
      "SPECIES 1 TEXT: tephritidae\n",
      "SPECIES 2 TEXT: ceratitis capitata\n",
      "Check Alternate Names\n",
      "SPECIES 1 Alternate Names: ['diptera']\n",
      "SPECIES 2 Alternate Names: ['diptera']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'tephritidae' == 'ceratitis capitata'\n",
      "Is New Species: True\n",
      "Seen Species Updated:\n",
      "{pests: 1, species: 1, Tapinoma nigerrimum: 1, Ceratitis capitata: 2, Tephritidae: 1}\n",
      "Checking Seen Species in Sentence\n",
      "( in [']', ')', '-', '--', '[', '(', '-', '--']\n",
      ") in [']', ')', '-', '--', '[', '(', '-', '--']\n",
      "L Punct: ( v. R Punct: )\n",
      "Matching Punctuation, Return Inner Text\n",
      "Token Context: {Tephritidae, (, :, Diptera, )}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "Token (PUNCT): ')'\n",
      "Token (PUNCT): ','\n",
      "Token (NUM): 'one'\n",
      "Token (ADP): 'of'\n",
      "Token (DET): 'the'\n",
      "Token (NOUN): 'world'\n",
      "Token (PART): ''s'\n",
      "Token (ADV): 'most'\n",
      "Token (ADV): 'economically'\n",
      "Token (ADJ): 'damaging'\n",
      "Species Span: damaging pests\n",
      "Seen Species:\n",
      "{pests: 1, species: 1, Tapinoma nigerrimum: 1, Ceratitis capitata: 2, Tephritidae: 1}\n",
      "Comparing Species\n",
      "SPECIES 1: damaging pests\n",
      "SPECIES 2: pests\n",
      "SPECIES 1 TEXT: damaging pests\n",
      "SPECIES 2 TEXT: pests\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 1\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Nouns: 'pest' == 'pest'\n",
      "\t'damaging pests' == 'pests'\n",
      "\tNumber of Visits: 1\n",
      "Is New Species: False\n",
      "Seen Species Updated:\n",
      "{pests: 2, species: 1, Tapinoma nigerrimum: 1, Ceratitis capitata: 2, Tephritidae: 1}\n",
      "Checking Seen Species in Sentence\n",
      "Comparing Species\n",
      "SPECIES 1: damaging pests\n",
      "SPECIES 2: species\n",
      "SPECIES 1 TEXT: damaging pests\n",
      "SPECIES 2 TEXT: species\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 1\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Nouns: 'pest' == 'specie'\n",
      "Comparing Species\n",
      "SPECIES 1: damaging pests\n",
      "SPECIES 2: Tapinoma nigerrimum\n",
      "SPECIES 1 TEXT: damaging pests\n",
      "SPECIES 2 TEXT: tapinoma nigerrimum\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 1\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Nouns: 'pest' == 'tapinoma nigerrimum'\n",
      "Comparing Species\n",
      "SPECIES 1: damaging pests\n",
      "SPECIES 2: Ceratitis capitata\n",
      "SPECIES 1 TEXT: damaging pests\n",
      "SPECIES 2 TEXT: ceratitis capitata\n",
      "Check Alternate Names\n",
      "SPECIES 2 Alternate Names: ['diptera']\n",
      "Number of Adjectives in 1: 1\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Nouns: 'pest' == 'ceratitis capitata'\n",
      "Comparing Species\n",
      "SPECIES 1: damaging pests\n",
      "SPECIES 2: Diptera\n",
      "SPECIES 1 TEXT: damaging pests\n",
      "SPECIES 2 TEXT: diptera\n",
      "Check Alternate Names\n",
      "SPECIES 2 Alternate Names: ['ceratitis capitata', 'tephritidae']\n",
      "Number of Adjectives in 1: 1\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Nouns: 'pest' == 'diptera'\n",
      "Comparing Species\n",
      "SPECIES 1: damaging pests\n",
      "SPECIES 2: Tephritidae\n",
      "SPECIES 1 TEXT: damaging pests\n",
      "SPECIES 2 TEXT: tephritidae\n",
      "Check Alternate Names\n",
      "SPECIES 2 Alternate Names: ['diptera']\n",
      "Number of Adjectives in 1: 1\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Nouns: 'pest' == 'tephritidae'\n",
      ") in [']', ')', '-', '--', '[', '(', '-', '--']\n",
      "L Punct: ) v. R Punct: None\n",
      "Expand L:\n",
      "Token: economically and Position: ADV\n",
      "Expand L:\n",
      "Token: most and Position: ADV\n",
      "Expand L:\n",
      "Token: 's and Position: PART\n",
      "Expand L:\n",
      "Token: world and Position: NOUN\n",
      "Expand L:\n",
      "Token: the and Position: DET\n",
      "Break\n",
      "Expand R:\n",
      "Token: pests and Position: NOUN\n",
      "Expand R:\n",
      "Token: , and Position: PUNCT\n",
      "Break\n",
      "Exclude: []\n",
      "Context: [world, 's, most, economically, damaging, pests]\n",
      "Token Context: {damaging, world, economically, most, pests, 's}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "Token (NOUN): 'pests'\n",
      "Token (PUNCT): ','\n",
      "Token (PRON): 'which'\n",
      "Token (VERB): 'attacks'\n",
      "Token (NOUN): 'fruits'\n",
      "Token (PUNCT): '.'\n",
      "Overall Points: [0, 0, 0.3, 0.39999999999999997]\n",
      "\n",
      "\n",
      "Sentence: For each ant species, we performed choice experiments using ant-scented and control plums, counting the time spent by medflies on fruits and the number of pupae emerging from them.\n",
      "\n",
      "Sentence Tokens: [(115, For), (116, each), (117, ant), (118, species), (119, ,), (120, we), (121, performed), (122, choice), (123, experiments), (124, using), (125, ant), (126, -), (127, scented), (128, and), (129, control), (130, plums), (131, ,), (132, counting), (133, the), (134, time), (135, spent), (136, by), (137, medflies), (138, on), (139, fruits), (140, and), (141, the), (142, number), (143, of), (144, pupae), (145, emerging), (146, from), (147, them), (148, .)]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: set()\n",
      "\n",
      "Token (ADP): 'For'\n",
      "Token (DET): 'each'\n",
      "Token (NOUN): 'ant'\n",
      "Token (NOUN): 'species'\n",
      "Species Span: species\n",
      "Seen Species:\n",
      "{pests: 2, species: 1, Tapinoma nigerrimum: 1, Ceratitis capitata: 2, Tephritidae: 1}\n",
      "Comparing Species\n",
      "SPECIES 1: species\n",
      "SPECIES 2: pests\n",
      "SPECIES 1 TEXT: species\n",
      "SPECIES 2 TEXT: pests\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'specie' == 'pest'\n",
      "Comparing Species\n",
      "SPECIES 1: species\n",
      "SPECIES 2: species\n",
      "SPECIES 1 TEXT: species\n",
      "SPECIES 2 TEXT: species\n",
      "\t'species' == 'species'\n",
      "\tNumber of Visits: 1\n",
      "Is New Species: False\n",
      "Seen Species Updated:\n",
      "{pests: 2, species: 2, Tapinoma nigerrimum: 1, Ceratitis capitata: 2, Tephritidae: 1}\n",
      "Checking Seen Species in Sentence\n",
      "Added 0.09999999999999999 Points to Interaction\n",
      "- in [']', ')', '-', '--', '[', '(', '-', '--']\n",
      "L Punct: None v. R Punct: -\n",
      "Expand L:\n",
      "Token: ant and Position: NOUN\n",
      "Expand L:\n",
      "Token: each and Position: DET\n",
      "Break\n",
      "Expand R:\n",
      "Token: , and Position: PUNCT\n",
      "Break\n",
      "Exclude: []\n",
      "Context: [ant, species]\n",
      "Token Context: {ant, species}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "Token (PUNCT): ','\n",
      "Token (PRON): 'we'\n",
      "Token (VERB): 'performed'\n",
      "Token (NOUN): 'choice'\n",
      "Token (NOUN): 'experiments'\n",
      "Added 0.3 Points to Experiment\n",
      "Token (VERB): 'using'\n",
      "Token (NOUN): 'ant'\n",
      "Token (PUNCT): '-'\n",
      "Token (ADJ): 'scented'\n",
      "Token (CCONJ): 'and'\n",
      "Token (NOUN): 'control'\n",
      "Token (NOUN): 'plums'\n",
      "Token (PUNCT): ','\n",
      "Token (VERB): 'counting'\n",
      "Token (DET): 'the'\n",
      "Token (NOUN): 'time'\n",
      "Token (VERB): 'spent'\n",
      "Token (ADP): 'by'\n",
      "Token (NOUN): 'medflies'\n",
      "Token (ADP): 'on'\n",
      "Token (NOUN): 'fruits'\n",
      "Token (CCONJ): 'and'\n",
      "Token (DET): 'the'\n",
      "Token (NOUN): 'number'\n",
      "Token (ADP): 'of'\n",
      "Token (NOUN): 'pupae'\n",
      "Token (VERB): 'emerging'\n",
      "Token (ADP): 'from'\n",
      "Token (PRON): 'them'\n",
      "Token (PUNCT): '.'\n",
      "Overall Points: [0, 0, 0.6, 0.49999999999999994]\n",
      "\n",
      "\n",
      "Sentence: Results of both ant species tests showed a significantly shorter time spent by ovipositing medflies on ant-exposed plums and a lower number of pupae, when compared to the control group.\n",
      "\n",
      "Sentence Tokens: [(149, Results), (150, of), (151, both), (152, ant), (153, species), (154, tests), (155, showed), (156, a), (157, significantly), (158, shorter), (159, time), (160, spent), (161, by), (162, ovipositing), (163, medflies), (164, on), (165, ant), (166, -), (167, exposed), (168, plums), (169, and), (170, a), (171, lower), (172, number), (173, of), (174, pupae), (175, ,), (176, when), (177, compared), (178, to), (179, the), (180, control), (181, group), (182, .)]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: {shorter}\n",
      "\n",
      "Token (NOUN): 'Results'\n",
      "Added 0.3 Points to Experiment\n",
      "Token (ADP): 'of'\n",
      "Token (CCONJ): 'both'\n",
      "Token (NOUN): 'ant'\n",
      "Token (NOUN): 'species'\n",
      "Token (NOUN): 'tests'\n",
      "Token (VERB): 'showed'\n",
      "Token (DET): 'a'\n",
      "Token (ADV): 'significantly'\n",
      "Token (ADJ): 'shorter'\n",
      "Token (NOUN): 'time'\n",
      "Token (VERB): 'spent'\n",
      "Token (ADP): 'by'\n",
      "Token (VERB): 'ovipositing'\n",
      "Token (NOUN): 'medflies'\n",
      "Token (ADP): 'on'\n",
      "Token (NOUN): 'ant'\n",
      "Token (PUNCT): '-'\n",
      "Token (VERB): 'exposed'\n",
      "Token (NOUN): 'plums'\n",
      "Token (CCONJ): 'and'\n",
      "Token (DET): 'a'\n",
      "Token (ADJ): 'lower'\n",
      "Token (NOUN): 'number'\n",
      "Token (ADP): 'of'\n",
      "Token (NOUN): 'pupae'\n",
      "Species Span: pupae\n",
      "Seen Species:\n",
      "{pests: 2, species: 2, Tapinoma nigerrimum: 1, Ceratitis capitata: 2, Tephritidae: 1}\n",
      "Comparing Species\n",
      "SPECIES 1: pupae\n",
      "SPECIES 2: pests\n",
      "SPECIES 1 TEXT: pupae\n",
      "SPECIES 2 TEXT: pests\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'pupa' == 'pest'\n",
      "Comparing Species\n",
      "SPECIES 1: pupae\n",
      "SPECIES 2: species\n",
      "SPECIES 1 TEXT: pupae\n",
      "SPECIES 2 TEXT: species\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'pupa' == 'specie'\n",
      "Comparing Species\n",
      "SPECIES 1: pupae\n",
      "SPECIES 2: Tapinoma nigerrimum\n",
      "SPECIES 1 TEXT: pupae\n",
      "SPECIES 2 TEXT: tapinoma nigerrimum\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'pupa' == 'tapinoma nigerrimum'\n",
      "Comparing Species\n",
      "SPECIES 1: pupae\n",
      "SPECIES 2: Ceratitis capitata\n",
      "SPECIES 1 TEXT: pupae\n",
      "SPECIES 2 TEXT: ceratitis capitata\n",
      "Check Alternate Names\n",
      "SPECIES 2 Alternate Names: ['diptera']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'pupa' == 'ceratitis capitata'\n",
      "Comparing Species\n",
      "SPECIES 1: pupae\n",
      "SPECIES 2: Tephritidae\n",
      "SPECIES 1 TEXT: pupae\n",
      "SPECIES 2 TEXT: tephritidae\n",
      "Check Alternate Names\n",
      "SPECIES 2 Alternate Names: ['diptera']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'pupa' == 'tephritidae'\n",
      "Is New Species: True\n",
      "Seen Species Updated:\n",
      "{pests: 2, species: 2, Tapinoma nigerrimum: 1, Ceratitis capitata: 2, Tephritidae: 1, pupae: 1}\n",
      "Checking Seen Species in Sentence\n",
      "Added 0.09999999999999999 Points to Interaction\n",
      "- in [']', ')', '-', '--', '[', '(', '-', '--']\n",
      "L Punct: - v. R Punct: None\n",
      "Expand L:\n",
      "Token: of and Position: ADP\n",
      "Expand L:\n",
      "Token: number and Position: NOUN\n",
      "Expand L:\n",
      "Token: lower and Position: ADJ\n",
      "Expand L:\n",
      "Token: a and Position: DET\n",
      "Break\n",
      "Expand R:\n",
      "Token: , and Position: PUNCT\n",
      "Break\n",
      "Exclude: []\n",
      "Context: [lower, number, of, pupae]\n",
      "Token Context: {lower, number, of, pupae}\n",
      "Cause Tokens in Context: set()\n",
      "Change Tokens in Context: set()\n",
      "Token (PUNCT): ','\n",
      "Token (SCONJ): 'when'\n",
      "Token (VERB): 'compared'\n",
      "Token (ADP): 'to'\n",
      "Token (DET): 'the'\n",
      "Token (NOUN): 'control'\n",
      "Token (NOUN): 'group'\n",
      "Token (PUNCT): '.'\n",
      "Overall Points: [0, 0, 0.8999999999999999, 0.6]\n",
      "\n",
      "\n",
      "Sentence: Our findings highlighted that the semiochemicals released by ants on plums triggered an avoidance behaviour by medfly females, leading to lower oviposition rates.\n",
      "\n",
      "Sentence Tokens: [(183, Our), (184, findings), (185, highlighted), (186, that), (187, the), (188, semiochemicals), (189, released), (190, by), (191, ants), (192, on), (193, plums), (194, triggered), (195, an), (196, avoidance), (197, behaviour), (198, by), (199, medfly), (200, females), (201, ,), (202, leading), (203, to), (204, lower), (205, oviposition), (206, rates), (207, .)]\n",
      "Sentence Cause Tokens: {triggered}\n",
      "Sentence Change Tokens: set()\n",
      "\n",
      "Token (PRON): 'Our'\n",
      "Token (NOUN): 'findings'\n",
      "Added 0.3 Points to Experiment\n",
      "Token (VERB): 'highlighted'\n",
      "Token (SCONJ): 'that'\n",
      "Token (DET): 'the'\n",
      "Token (NOUN): 'semiochemicals'\n",
      "Token (VERB): 'released'\n",
      "Token (ADP): 'by'\n",
      "Token (NOUN): 'ants'\n",
      "Species Span: ants\n",
      "Seen Species:\n",
      "{pests: 2, species: 2, Tapinoma nigerrimum: 1, Ceratitis capitata: 2, Tephritidae: 1, pupae: 1}\n",
      "Comparing Species\n",
      "SPECIES 1: ants\n",
      "SPECIES 2: pests\n",
      "SPECIES 1 TEXT: ants\n",
      "SPECIES 2 TEXT: pests\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'ant' == 'pest'\n",
      "Comparing Species\n",
      "SPECIES 1: ants\n",
      "SPECIES 2: species\n",
      "SPECIES 1 TEXT: ants\n",
      "SPECIES 2 TEXT: species\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'ant' == 'specie'\n",
      "Comparing Species\n",
      "SPECIES 1: ants\n",
      "SPECIES 2: Tapinoma nigerrimum\n",
      "SPECIES 1 TEXT: ants\n",
      "SPECIES 2 TEXT: tapinoma nigerrimum\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'ant' == 'tapinoma nigerrimum'\n",
      "Comparing Species\n",
      "SPECIES 1: ants\n",
      "SPECIES 2: Ceratitis capitata\n",
      "SPECIES 1 TEXT: ants\n",
      "SPECIES 2 TEXT: ceratitis capitata\n",
      "Check Alternate Names\n",
      "SPECIES 2 Alternate Names: ['diptera']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'ant' == 'ceratitis capitata'\n",
      "Comparing Species\n",
      "SPECIES 1: ants\n",
      "SPECIES 2: Tephritidae\n",
      "SPECIES 1 TEXT: ants\n",
      "SPECIES 2 TEXT: tephritidae\n",
      "Check Alternate Names\n",
      "SPECIES 2 Alternate Names: ['diptera']\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'ant' == 'tephritidae'\n",
      "Comparing Species\n",
      "SPECIES 1: ants\n",
      "SPECIES 2: pupae\n",
      "SPECIES 1 TEXT: ants\n",
      "SPECIES 2 TEXT: pupae\n",
      "Check Alternate Names\n",
      "Number of Adjectives in 1: 0\n",
      "Number of Adjectives in 2: 0\n",
      "Comparing Singular Spans: 'ant' == 'pupa'\n",
      "Is New Species: True\n",
      "Seen Species Updated:\n",
      "{pests: 2, species: 2, Tapinoma nigerrimum: 1, Ceratitis capitata: 2, Tephritidae: 1, pupae: 1, ants: 1}\n",
      "Checking Seen Species in Sentence\n",
      "Added 0.09999999999999999 Points to Interaction\n",
      "L Punct: None v. R Punct: None\n",
      "Expand L:\n",
      "Token: by and Position: ADP\n",
      "Expand L:\n",
      "Token: released and Position: VERB\n",
      "Expand L:\n",
      "Token: semiochemicals and Position: NOUN\n",
      "Expand L:\n",
      "Token: the and Position: DET\n",
      "Break\n",
      "Expand R:\n",
      "Token: on and Position: ADP\n",
      "Expand R:\n",
      "Token: plums and Position: NOUN\n",
      "Expand R:\n",
      "Token: triggered and Position: VERB\n",
      "Expand R:\n",
      "Token: an and Position: DET\n",
      "Break\n",
      "Exclude: []\n",
      "Context: [semiochemicals, released, by, ants, on, plums, triggered]\n",
      "Token Context: {semiochemicals, ants, triggered, plums, by, released, on}\n",
      "Cause Tokens in Context: {triggered}\n",
      "Change Tokens in Context: set()\n",
      "Added 0.09999999999999999 Points to Species\n",
      "Token (ADP): 'on'\n",
      "Token (NOUN): 'plums'\n",
      "Token (VERB): 'triggered'\n",
      "Token (DET): 'an'\n",
      "Token (NOUN): 'avoidance'\n",
      "Token (NOUN): 'behaviour'\n",
      "Token (ADP): 'by'\n",
      "Token (NOUN): 'medfly'\n",
      "Token (NOUN): 'females'\n",
      "Token (PUNCT): ','\n",
      "Token (VERB): 'leading'\n",
      "Token (ADP): 'to'\n",
      "Token (ADJ): 'lower'\n",
      "Token (NOUN): 'oviposition'\n",
      "Token (NOUN): 'rates'\n",
      "Token (PUNCT): '.'\n",
      "Overall Points: [0, 0.09999999999999999, 1.2, 0.7]\n",
      "\n",
      "\n",
      "Sentence: This study contributes to the understanding of indirect ant-pest interactions in Mediterranean agricultural settings and points out the potential of utilising ant-borne semiochemicals in sustainable IPM strategies.\n",
      "\n",
      "Sentence Tokens: [(208, This), (209, study), (210, contributes), (211, to), (212, the), (213, understanding), (214, of), (215, indirect), (216, ant), (217, -), (218, pest), (219, interactions), (220, in), (221, Mediterranean), (222, agricultural), (223, settings), (224, and), (225, points), (226, out), (227, the), (228, potential), (229, of), (230, utilising), (231, ant), (232, -), (233, borne), (234, semiochemicals), (235, in), (236, sustainable), (237, IPM), (238, strategies), (239, .)]\n",
      "Sentence Cause Tokens: set()\n",
      "Sentence Change Tokens: set()\n",
      "\n",
      "Token (DET): 'This'\n",
      "Token (NOUN): 'study'\n",
      "Added 0.3 Points to Experiment\n",
      "Token (VERB): 'contributes'\n",
      "Token (ADP): 'to'\n",
      "Token (DET): 'the'\n",
      "Token (NOUN): 'understanding'\n",
      "Token (ADP): 'of'\n",
      "Token (ADJ): 'indirect'\n",
      "Token (NOUN): 'ant'\n",
      "Token (PUNCT): '-'\n",
      "Token (NOUN): 'pest'\n",
      "Token (NOUN): 'interactions'\n",
      "Token (ADP): 'in'\n",
      "Token (PROPN): 'Mediterranean'\n",
      "Token (ADJ): 'agricultural'\n",
      "Token (NOUN): 'settings'\n",
      "Token (CCONJ): 'and'\n",
      "Token (VERB): 'points'\n",
      "Token (ADP): 'out'\n",
      "Token (DET): 'the'\n",
      "Token (NOUN): 'potential'\n",
      "Token (ADP): 'of'\n",
      "Token (VERB): 'utilising'\n",
      "Token (NOUN): 'ant'\n",
      "Token (PUNCT): '-'\n",
      "Token (VERB): 'borne'\n",
      "Token (NOUN): 'semiochemicals'\n",
      "Token (ADP): 'in'\n",
      "Token (ADJ): 'sustainable'\n",
      "Token (PROPN): 'IPM'\n",
      "Token (NOUN): 'strategies'\n",
      "Token (PUNCT): '.'\n",
      "Overall Points: [0, 0.09999999999999999, 1.5, 0.7]\n",
      "Seen Species: {pests: 2, species: 2, Tapinoma nigerrimum: 1, Ceratitis capitata: 2, Tephritidae: 1, pupae: 1, ants: 1}\n",
      "Points: [0, 0.09999999999999999, 1.5, 0.7]\n",
      "Category 1's FTP: 0.0\n",
      "Category 1's FTP: 0.047619047619047616\n",
      "Category 1's FTP: 0.7142857142857143\n",
      "Category 1's FTP: 0.3333333333333333\n",
      "Score: 0.2738095238095238\n",
      "0.2738095238095238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main = Main()\n",
    "for abstract in [df.Abstract.to_list()[1]]:\n",
    "    print(abstract)\n",
    "    main.update_text(abstract, verbose=True)\n",
    "    score = main.score(verbose=True)\n",
    "    print(score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7153d990-7dc2-4241-9cc9-42355bd59c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in main.sp_doc:\n",
    "    if token.lower_ == \"hypothesized\":\n",
    "        print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa858af3-e2c8-4dcc-a7de-4e9f769f310b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token (ADJ): Alternative\n",
      "\tNot Species\n",
      "Token (NOUN): methods\n",
      "\tNot Species\n",
      "Token (ADJ): sustainable\n",
      "\tNot Species\n",
      "Token (ADJ): agricultural\n",
      "\tNot Species\n",
      "Token (NOUN): production\n",
      "\tNot Species\n",
      "Token (NOUN): use\n",
      "\tNot Species\n",
      "Token (ADJ): chemical\n",
      "\tNot Species\n",
      "Token (NOUN): pesticides\n",
      "\tNot Species\n",
      "Token (ADJ): such\n",
      "\tNot Species\n",
      "Token (ADJ): biological\n",
      "\tNot Species\n",
      "Token (NOUN): control\n",
      "\tNot Species\n",
      "Token (NOUN): exploitation\n",
      "\tNot Species\n",
      "Token (NOUN): trait\n",
      "\tNot Species\n",
      "Token (ADJ): indirect\n",
      "\tNot Species\n",
      "Token (NOUN): interactions\n",
      "\tNot Species\n",
      "Token (PROPN): TMIIs\n",
      "\tNot Species\n",
      "Token (NOUN): pests\n",
      "\tSpecies\n",
      "Token (NOUN): behavior\n",
      "\tNot Species\n",
      "Token (NOUN): response\n",
      "\tNot Species\n",
      "Token (NOUN): cues\n",
      "\tNot Species\n",
      "Token (NOUN): pheromones\n",
      "\tNot Species\n",
      "Token (ADJ): other\n",
      "\tNot Species\n",
      "Token (NOUN): semiochemicals\n",
      "\tNot Species\n",
      "Token (NOUN): predation\n",
      "\tNot Species\n",
      "Token (NOUN): risk\n",
      "\tNot Species\n",
      "Token (ADJ): possible\n",
      "\tNot Species\n",
      "Token (NOUN): strategy\n",
      "\tNot Species\n",
      "Token (NOUN): study\n",
      "\tNot Species\n",
      "Token (NOUN): effect\n",
      "\tNot Species\n",
      "Token (PROPN): TMIIs\n",
      "\tNot Species\n",
      "Token (PROPN): Mediterranean\n",
      "\tNot Species\n",
      "Token (NOUN): ant\n",
      "\tNot Species\n",
      "Token (NOUN): species\n",
      "\tSpecies\n",
      "Token (PROPN): Crematogaster\n",
      "\tNot Species\n",
      "Token (PROPN): scutellaris\n",
      "\tNot Species\n",
      "Token (PROPN): Tapinoma\n",
      "\tSpecies\n",
      "Token (NOUN): nigerrimum\n",
      "\tSpecies\n",
      "Token (NOUN): oviposition\n",
      "\tNot Species\n",
      "Token (NOUN): behaviour\n",
      "\tNot Species\n",
      "Token (PROPN): Ceratitis\n",
      "\tSpecies\n",
      "Token (NOUN): capitata\n",
      "\tSpecies\n",
      "Token (PROPN): Diptera\n",
      "\tSpecies\n",
      "Token (PROPN): Tephritidae\n",
      "\tSpecies\n",
      "Token (NOUN): world\n",
      "\tNot Species\n",
      "Token (ADJ): damaging\n",
      "\tSpecies\n",
      "Token (NOUN): pests\n",
      "\tSpecies\n",
      "Token (NOUN): fruits\n",
      "\tNot Species\n",
      "Token (NOUN): ant\n",
      "\tNot Species\n",
      "Token (NOUN): species\n",
      "\tSpecies\n",
      "Token (NOUN): choice\n",
      "\tNot Species\n",
      "Token (NOUN): experiments\n",
      "\tNot Species\n",
      "Token (NOUN): ant\n",
      "\tNot Species\n",
      "Token (ADJ): scented\n",
      "\tNot Species\n",
      "Token (NOUN): control\n",
      "\tNot Species\n",
      "Token (NOUN): plums\n",
      "\tNot Species\n",
      "Token (NOUN): time\n",
      "\tNot Species\n",
      "Token (NOUN): medflies\n",
      "\tNot Species\n",
      "Token (NOUN): fruits\n",
      "\tNot Species\n",
      "Token (NOUN): number\n",
      "\tNot Species\n",
      "Token (NOUN): pupae\n",
      "\tNot Species\n",
      "Token (NOUN): Results\n",
      "\tNot Species\n",
      "Token (NOUN): ant\n",
      "\tNot Species\n",
      "Token (NOUN): species\n",
      "\tNot Species\n",
      "Token (NOUN): tests\n",
      "\tNot Species\n",
      "Token (ADJ): shorter\n",
      "\tNot Species\n",
      "Token (NOUN): time\n",
      "\tNot Species\n",
      "Token (NOUN): medflies\n",
      "\tNot Species\n",
      "Token (NOUN): ant\n",
      "\tNot Species\n",
      "Token (NOUN): plums\n",
      "\tNot Species\n",
      "Token (ADJ): lower\n",
      "\tNot Species\n",
      "Token (NOUN): number\n",
      "\tNot Species\n",
      "Token (NOUN): pupae\n",
      "\tSpecies\n",
      "Token (NOUN): control\n",
      "\tNot Species\n",
      "Token (NOUN): group\n",
      "\tNot Species\n",
      "Token (NOUN): findings\n",
      "\tNot Species\n",
      "Token (NOUN): semiochemicals\n",
      "\tNot Species\n",
      "Token (NOUN): ants\n",
      "\tSpecies\n",
      "Token (NOUN): plums\n",
      "\tNot Species\n",
      "Token (NOUN): avoidance\n",
      "\tNot Species\n",
      "Token (NOUN): behaviour\n",
      "\tNot Species\n",
      "Token (NOUN): medfly\n",
      "\tNot Species\n",
      "Token (NOUN): females\n",
      "\tNot Species\n",
      "Token (ADJ): lower\n",
      "\tNot Species\n",
      "Token (NOUN): oviposition\n",
      "\tNot Species\n",
      "Token (NOUN): rates\n",
      "\tNot Species\n",
      "Token (NOUN): study\n",
      "\tNot Species\n",
      "Token (NOUN): understanding\n",
      "\tNot Species\n",
      "Token (ADJ): indirect\n",
      "\tNot Species\n",
      "Token (NOUN): ant\n",
      "\tNot Species\n",
      "Token (NOUN): pest\n",
      "\tNot Species\n",
      "Token (NOUN): interactions\n",
      "\tNot Species\n",
      "Token (PROPN): Mediterranean\n",
      "\tNot Species\n",
      "Token (ADJ): agricultural\n",
      "\tNot Species\n",
      "Token (NOUN): settings\n",
      "\tNot Species\n",
      "Token (NOUN): potential\n",
      "\tNot Species\n",
      "Token (NOUN): ant\n",
      "\tNot Species\n",
      "Token (NOUN): semiochemicals\n",
      "\tNot Species\n",
      "Token (ADJ): sustainable\n",
      "\tNot Species\n",
      "Token (PROPN): IPM\n",
      "\tNot Species\n",
      "Token (NOUN): strategies\n",
      "\tNot Species\n"
     ]
    }
   ],
   "source": [
    "for token in main.sp_doc:\n",
    "    if token.pos_ not in [\"NOUN\", \"PROPN\", \"ADJ\"]:\n",
    "        continue\n",
    "    print(f\"Token ({token.pos_}): {token}\\n\\t{'Species' if token in main.species.tokens else 'Not Species'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c7ea427-25ee-4f83-9a97-9e1070c05221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pests NOUN\n",
      "species NOUN\n",
      "Tapinoma PROPN\n",
      "Ceratitis PROPN\n",
      "Diptera PROPN\n",
      "Tephritidae PROPN\n",
      "damaging ADJ\n",
      "species NOUN\n",
      "pupae NOUN\n",
      "ants NOUN\n"
     ]
    }
   ],
   "source": [
    "species_tokens = [main.sp_doc[span.start] for span in main.species.spans]\n",
    "for span in species_tokens:\n",
    "    print(span, span.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb1d2d13-f691-4850-b3ba-7893cd47364f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.species.tn_doc.ents[0][0].tag_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078bf01-5efe-49b5-9ec9-4a8ccf1d47dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
