{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecf5be-df89-4d1a-b75f-d92a5c5fff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trait-Mediated Interaction Modification\n",
    "# Empirical:\n",
    "# You could look for words like \"hypothesis\", \"experiment\", \"found\", and \"discovered\". That may point\n",
    "# towards there being an experiment in the paper. There are also words like \"control group\", \"compared\",\n",
    "# \"findings\", \"results\", \"study\", and more.\n",
    "# Qualitative vs. Quantitative:\n",
    "# To infer whether something is quantitative, you could look for numeric tokens and units.\n",
    "# However, you can only do so much with the abstract. Therefore, this is likely not good enough.\n",
    "# Yet, you could still take advantage of words like \"fewer\" and \"increased\" to show that there is a change.\n",
    "# However, this would be more suited for the above category.\n",
    "# Traits:\n",
    "# There is no NLP tool for traits that I can use or create so I think that I could instead use keywords.\n",
    "# For example, \"snail feeding rates\" is a trait. You may be able to spot this by looking for a word like\n",
    "# \"rate\". You'd expand that word to include \"snail feeding rates\". As \"snail\" is a species you can infer\n",
    "# that \"rates\" is a trait. I would be more decisive and use a dependency parser to ensure that the trait\n",
    "# is a property of the species (like before). However, with all the cases that may exist, I think checking\n",
    "# to see whether a species can be found by traveling back and/or forward without finding certain tokens could\n",
    "# work well enough.\n",
    "# 3 Species or More:\n",
    "# This is simple. However, I think using a dictionary and TaxoNerd would be beneficial (for higher accuracy).\n",
    "# To handle the potential differences in tokenization, character offsets should be used.\n",
    "# Standardization:\n",
    "# There is a lot of variance in the scores. To squash this issue, I think that we could assign each sentence\n",
    "# a value from 0 to 1. We would add these values and divide by the number of sentences. This would result in\n",
    "# a number that is also from 0 to 1. However, there are categories that we would like to inspect. So, we must\n",
    "# create an overall score in the interval from [0, 1] while also scoring each category. Well, for each sentence\n",
    "# we could add a point for each category that is observed. The sentence would receive said score divided by the\n",
    "# number of categories. At the end, we add up all the sentence scores and divide by the number of sentences.\n",
    "# The aggregate score for each category would also be divided by the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ccb94-4c7d-49b1-a727-f6e25b021357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from spacy.language import Language\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "%run -i \"../utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad706ec8-70e2-4bc2-b1e9-247e1c047796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the Dictionary\n",
    "@Language.component(\"lower_case_lemmas\")\n",
    "def lower_case_lemmas(doc) :\n",
    "    for token in doc :\n",
    "        token.lemma_ = token.lemma_.lower()\n",
    "    return doc\n",
    "\n",
    "class Species:\n",
    "    def __init__(self, main):\n",
    "        # Tools\n",
    "        self.main = main\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\"])\n",
    "        self.tn_nlp.add_pipe(\"lower_case_lemmas\", after=\"lemmatizer\")\n",
    "        self.tn_doc = None\n",
    "        \n",
    "        # Contains any spans that have been identified\n",
    "        # as a species.\n",
    "        self.spans = None\n",
    "        \n",
    "        # Contains any tokens that have been identified\n",
    "        # as a species or being a part of a species.\n",
    "        self.tokens = None\n",
    "        \n",
    "        # Used to quickly access the span that a token\n",
    "        # belongs to.\n",
    "        self.token_to_span = None\n",
    "        \n",
    "        # Maps a string to an array of strings wherein\n",
    "        # any string involved in the key-value pair \n",
    "        # has been identified as an alternate name of each other.\n",
    "        self.alternate_names = None\n",
    "        \n",
    "        # Used to increase TaxoNERD's accuracy.\n",
    "        self.dictionary = None\n",
    "        self.load_dictionary()\n",
    "\n",
    "    def load_dictionary(self):\n",
    "        self.dictionary = [\"juvenile\", \"adult\", \"prey\", \"predator\", \"species\"]\n",
    "        # df = pd.read_csv(\"VernacularNames.csv\")\n",
    "        # self.dictionary += df.VernacularName.to_list()\n",
    "\n",
    "        patterns = []\n",
    "        for name in self.dictionary:\n",
    "            doc = self.tn_nlp(name)\n",
    "            patterns.append({\"label\": \"LIVB\", \"pattern\": [{\"LEMMA\": token.lemma_} for token in doc]})\n",
    "        ruler = self.tn_nlp.add_pipe(\"entity_ruler\")\n",
    "        ruler.add_patterns(patterns)\n",
    "        \n",
    "    def update(self, text, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        self.spans, self.tokens, self.token_to_span, self.alternate_names = self.load_species(verbose=verbose)\n",
    "\n",
    "    def load_species(self, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # These three contain the species that have been\n",
    "        # identified in the text. Tokens that aren't adjectives,\n",
    "        # nouns, or proper nouns will be stripped.\n",
    "        spans = []\n",
    "        tokens = []\n",
    "        token_to_span = {}\n",
    "\n",
    "        # It's useful to know if a different name refers to a\n",
    "        # species we have already seen. For example, in \"predatory crab (Carcinus maenas)\",\n",
    "        # \"predatory crab\" is an alternative name for \"Carcinus maenas\"\n",
    "        # and vice versa. This is used so that the species can be\n",
    "        # properly tracked and redundant points are less likely to be given.\n",
    "        alternate_names = {}\n",
    "\n",
    "        # TaxoNerd\n",
    "        if verbose:\n",
    "            print(f\"TN Entities: {self.tn_doc.ents}\\n\")\n",
    "\n",
    "        # TaxoNERD sometimes recognizes one instance of a species and not the other.\n",
    "        # To fix this, I'll use the recognized species to look for all the instances \n",
    "        # in the document that also match those species.\n",
    "        # text = self.main.sp_doc.text.lower()\n",
    "        # for phrase in self.phrases:\n",
    "        #     for char_index in [match.start() for match in re.finditer(phrase, text)]:\n",
    "        #         matched_tokens.append(self.main.token_at_char(char_index))\n",
    "\n",
    "        text_lowered = self.main.sp_doc.text.lower()\n",
    "        species_spans = list(set([span.text.lower() for span in self.tn_doc.ents]))\n",
    "        \n",
    "        for species_span in species_spans:\n",
    "            # Retrieve the start and end position for each of the substrings\n",
    "            # in the text that matched the species span.\n",
    "            for char_start_i, char_end_i in [(match.start(), match.end()) for match in re.finditer(species_span, text_lowered)]:\n",
    "                \n",
    "                # TODO\n",
    "                \n",
    "            if verbose:\n",
    "                print(f\"Species Span (TN): {species_span}\")\n",
    "   \n",
    "            # TN Span Translated to SP Span\n",
    "            sp_li = self.main.token_at_char(self.tn_doc[species_span.start].idx).i\n",
    "            sp_ri = self.main.token_at_char(self.tn_doc[species_span.end].idx).i\n",
    "            species_span = self.main.sp_doc[sp_li:sp_ri]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Species Span (SP): {species_span}\")\n",
    "\n",
    "            # Expand Species\n",
    "            # a. When it's Potentially Ambiguous\n",
    "            # b. When it's Potentially Missing Information\n",
    "            ambiguous = len(species_span) == 1 and species_span[0].pos_ == \"NOUN\"\n",
    "            missing_info = species_span.start > 0 and self.main.sp_doc[species_span.start-1].pos_ in [\"ADJ\"]\n",
    "            \n",
    "            if ambiguous or missing_info:\n",
    "                species_span = self.main.expand_unit(\n",
    "                    il_unit=species_span.start, \n",
    "                    ir_unit=species_span.end-1,\n",
    "                    il_boundary=0,\n",
    "                    ir_boundary=len(self.main.sp_doc),\n",
    "                    direction='LEFT',\n",
    "                    allowed_speech=[\"ADJ\", \"PROPN\"],\n",
    "                    allowed_literals=[\"-\"],\n",
    "                    verbose=verbose\n",
    "                )\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Expanded Species Span: {species_span}\")\n",
    "            \n",
    "            # Remove Outer Punctuations and Symbols\n",
    "            species_span = self.main.contract_unit(\n",
    "                il_unit=species_span.start, \n",
    "                ir_unit=species_span.end-1, \n",
    "                allowed_speech=[\"ADJ\", \"NOUN\", \"PROPN\"],\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Contracted Species Span: {species_span}\")\n",
    "\n",
    "            # Add Span and Tokens\n",
    "            if verbose:\n",
    "                print(f\"Adding Species Span and Tokens\")\n",
    "                \n",
    "            spans.append(species_span)\n",
    "            for token in species_span:\n",
    "                if verbose:\n",
    "                    print(f\"Token in Species ({token.pos_}): {token}\")\n",
    "                if token.pos_ not in [\"ADJ\", \"PROPN\", \"NOUN\"]:\n",
    "                    continue\n",
    "                tokens.append(token)\n",
    "                token_to_span[token] = species_span\n",
    "\n",
    "        # Finding and Storing Alternative Names\n",
    "        if verbose:\n",
    "            print(\"Finding Alternate Names\")\n",
    "        \n",
    "        for i, species_span in enumerate(spans):\n",
    "            if i + 1 >= len(spans):\n",
    "                break\n",
    "            \n",
    "            next_species_span = spans[i+1]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"SPECIES 1: {species_span}\")\n",
    "                print(f\"SPECIES 2: {next_species_span}\")\n",
    "                print(f\"DIST == 1: {next_species_span.start - species_span.end == 1}\")\n",
    "            \n",
    "            # If there's one token between the species and the next species,\n",
    "            # we check if the next species is surrounded by punctuation.\n",
    "            if next_species_span.start - species_span.end == 1:\n",
    "                # Token Before and After the Next Species\n",
    "                before_next = self.main.sp_doc[next_species_span.start-1]\n",
    "                after_next = self.main.sp_doc[next_species_span.end]\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Token Before SPECIES 2: {before_next} and Token After SPECIES 2: {after_next}\")\n",
    "\n",
    "                # Adding K-V Pair for Names\n",
    "                if before_next.pos_ in [\"PUNCT\", \"SYM\"] and after_next.pos_ in [\"PUNCT\", \"SYM\"]:\n",
    "                    # Instead of using the span objects, the text (string literals)\n",
    "                    # are used. This is because we're focusing on the content (the name)\n",
    "                    # rather than where it appears in the document.\n",
    "                    sp_1_text = species_span.text.lower()\n",
    "                    sp_2_text = next_species_span.text.lower()\n",
    "                    \n",
    "                    if sp_1_text not in alternate_names:\n",
    "                        alternate_names[sp_1_text] = []\n",
    "                    \n",
    "                    if sp_2_text not in alternate_names:\n",
    "                        alternate_names[sp_2_text] = []\n",
    "                    \n",
    "                    alternate_names[sp_1_text].append(sp_2_text)\n",
    "                    alternate_names[sp_2_text].append(sp_1_text)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Spans: {spans}\")\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "            print(f\"Alternate Spans: {alternate_names}\")\n",
    "        \n",
    "        return (spans, tokens, token_to_span, alternate_names)\n",
    "\n",
    "    def span_at_token(self, token):\n",
    "        if token in self.token_to_span:\n",
    "            return self.token_to_span[token]\n",
    "        return None\n",
    "    \n",
    "    def is_species(self, token):\n",
    "        return token in self.tokens\n",
    "        \n",
    "    def has_species(self, tokens, verbose=False):\n",
    "        for token in tokens:\n",
    "            if token in self.tokens:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def same_species(self, sp_1, sp_2, verbose=False):\n",
    "        if verbose:\n",
    "            print(\"Comparing Species\")\n",
    "            print(f\"SPECIES 1: {sp_1}\")\n",
    "            print(f\"SPECIES 2: {sp_2}\")\n",
    "\n",
    "        sp_1_text = sp_1.text.lower()\n",
    "        sp_2_text = sp_2.text.lower()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"SPECIES 1 TEXT: {sp_1_text}\")\n",
    "            print(f\"SPECIES 2 TEXT: {sp_2_text}\")\n",
    "        \n",
    "        # METHOD 1: Check if Texts are Equivalent\n",
    "        equivalent = sp_1.text.lower() == sp_2.text.lower()\n",
    "        \n",
    "        if equivalent:\n",
    "            return True\n",
    "\n",
    "        # METHOD 2: Check Alternate Names\n",
    "        if verbose:\n",
    "            print(\"Check Alternate Names\")\n",
    "        \n",
    "        if sp_1_text in self.alternate_names:\n",
    "            if verbose:\n",
    "                print(f\"SPECIES 1 Alternate Names: {self.alternate_names[sp_1_text]}\")\n",
    "            if sp_2_text in self.alternate_names[sp_1_text]:\n",
    "                return True\n",
    "        \n",
    "        if sp_2_text in self.alternate_names:\n",
    "            if verbose:\n",
    "                print(f\"SPECIES 2 Alternate Names: {self.alternate_names[sp_2_text]}\")\n",
    "            if sp_1_text in self.alternate_names[sp_2_text]:\n",
    "                return True\n",
    "\n",
    "        # Singular Version of Phrase (e.g. \"fewer crabs\" becomes \"fewer crab\")\n",
    "        singular_version = lambda tokens : \" \".join([*[token.text for token in tokens[:-1]], tokens[-1].lemma_]).lower()\n",
    "\n",
    "        # METHOD 3: Check Base Nouns\n",
    "        # Only used when there's 1 adjective in one of the species and\n",
    "        # no adjectives in the other (e.g. \"fewer crabs\" v. \"crabs\").\n",
    "        sp_1_nouns = []\n",
    "        sp_1_num_adjectives = 0\n",
    "        for token in sp_1:\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                sp_1_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_1_nouns.append(token)\n",
    "        \n",
    "        sp_2_nouns = []\n",
    "        sp_2_num_adjectives = 0\n",
    "        for token in sp_2:\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                sp_2_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_2_nouns.append(token)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Number of Adjectives in 1: {sp_1_num_adjectives}\")\n",
    "            print(f\"Number of Adjectives in 2: {sp_2_num_adjectives}\")\n",
    "\n",
    "        if sp_1_nouns and sp_2_nouns and (\n",
    "            (sp_1_num_adjectives == 1 and sp_2_num_adjectives == 0) or \n",
    "            (sp_2_num_adjectives == 1 and sp_1_num_adjectives == 0)\n",
    "        ):\n",
    "            sp_singular_nouns_1 = singular_version(sp_1_nouns)\n",
    "            sp_singular_nouns_2 = singular_version(sp_2_nouns)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Comparing Singular Nouns: '{sp_singular_nouns_1}' == '{sp_singular_nouns_2}'\")\n",
    "            \n",
    "            return sp_singular_nouns_1 == sp_singular_nouns_2\n",
    "\n",
    "        # METHOD 4: Check Singular Version\n",
    "        # This method targets spans like \"predatory crab\" and \"predatory crabs\".\n",
    "        sp_singular_1 = singular_version(sp_1)\n",
    "        sp_singular_2 = singular_version(sp_2)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Comparing Singular Spans: '{sp_singular_1}' == '{sp_singular_2}'\")\n",
    "        \n",
    "        if sp_singular_1 == sp_singular_2:\n",
    "            return True\n",
    "\n",
    "        # At this point, I don't see \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31521a-3d38-4087-b63e-bb974ad3a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    def __init__(self, main, base=[], phrases=[], speech=[], threshold=0.7):\n",
    "        self.main = main\n",
    "        self.base = [b.lower() for b in base]\n",
    "        self.speech = [s.upper() for s in speech]\n",
    "        self.phrases = [p.lower() for p in phrases]\n",
    "        self.threshold = threshold\n",
    "        self.vocab = [self.main.sp_nlp(word) for word in self.base]\n",
    "        self.tokens = []\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tokens = self.match_tokens(verbose=verbose)\n",
    "\n",
    "    def match_tokens(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        matched_tokens = []\n",
    "\n",
    "        # Check Words\n",
    "        for token in self.main.sp_doc:\n",
    "            if verbose:\n",
    "                print(f\"Potential Keyword: {token, token.pos_} v. Speech: {self.speech}\")\n",
    "            if token.pos_ not in self.speech:\n",
    "                continue\n",
    "            # Comparing Literal Text\n",
    "            if token.lemma_.lower() in self.base or token.lower_ in self.base:\n",
    "                matched_tokens.append(token)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.main.sp_nlp(token.lemma_)\n",
    "            for word in self.vocab:\n",
    "                similarity = word.similarity(lemma)\n",
    "                if verbose:\n",
    "                    print(f\"{lemma} and {word} Similarity: {similarity}\")\n",
    "                if similarity >= self.threshold:\n",
    "                    matched_tokens.append(token)\n",
    "                    break\n",
    "\n",
    "        # Check Phrases\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "        for phrase in self.phrases:\n",
    "            for char_index in [match.start() for match in re.finditer(phrase, text)]:\n",
    "                matched_tokens.append(self.main.token_at_char(char_index))\n",
    "                \n",
    "        return matched_tokens\n",
    "\n",
    "class ExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"study\", \"hypothesis\", \"experiment\", \"found\", \"discover\", \"compare\", \"finding\", \"result\"],\n",
    "            phrases=[\"control group\", \"independent\", \"dependent\"],\n",
    "            speech=[\"VERB\", \"NOUN\"], \n",
    "            threshold=0.7\n",
    "        )\n",
    "\n",
    "class CauseKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"increase\", \"decrease\", \"change\", \"shift\", \"cause\", \"produce\"], \n",
    "            speech=[\"VERB\"], \n",
    "            threshold=0.6\n",
    "        )\n",
    "\n",
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"few\", \"more\", \"increase\", \"decrease\", \"less\", \"short\", \"long\"], \n",
    "            speech=[\"NOUN\"], \n",
    "            threshold=0.6\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70792f-d32b-4638-a929-c7499506e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraitKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\"behavior\", \"rate\", \"color\", \"mass\", \"size\", \"length\"], \n",
    "            speech=[\"NOUN\", \"ADJ\"], \n",
    "            threshold=0.7\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        if verbose:\n",
    "            print(f\"Unfiltered Tokens: {self.tokens}\")\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            expanded_token = self.main.expand_unit(\n",
    "                il_unit=token.i, \n",
    "                ir_unit=token.i, \n",
    "                il_boundary=0, \n",
    "                ir_boundary=len(self.main.sp_doc), \n",
    "                allowed_speech=[\"ADJ\", \"NOUN\", \"ADP\", \"PART\", \"DET\", \"PROPN\",],\n",
    "                allowed_literals=[\"-\", \",\"],\n",
    "                disallowed_literals=[\"!\", \".\", \"?\"],\n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Token: {token}\")\n",
    "                print(f\"Expanded Token: {expanded_token}\")\n",
    "\n",
    "            if self.main.species.has_species(expanded_token):\n",
    "                if verbose:\n",
    "                    print(f\"\\tContains Species\")\n",
    "                filtered.append(token)\n",
    "        \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf8e07-a326-4fea-8615-faa5d549852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main:\n",
    "    def __init__(self):\n",
    "        # Tools\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        self.sp_doc = None\n",
    "\n",
    "        # Maps Character Position to Token in Document\n",
    "        # Used to handle differences between different\n",
    "        # pipelines and tools.\n",
    "        self.index_map = None\n",
    "    \n",
    "        # Parsers\n",
    "        self.species = Species(self)\n",
    "        self.traits = TraitKeywords(self)\n",
    "        self.causes = CauseKeywords(self)\n",
    "        self.changes = ChangeKeywords(self)\n",
    "        self.experiment = ExperimentKeywords(self)\n",
    "\n",
    "    def update_doc(self, doc, verbose=False):\n",
    "        self.sp_doc = doc\n",
    "        self.index_map = self.load_index_map()\n",
    "        self.species.update(doc.text, verbose=verbose)\n",
    "        self.traits.update(verbose=True)\n",
    "        self.causes.update(verbose=False)\n",
    "        self.changes.update(verbose=False)\n",
    "        self.experiment.update(verbose=False)\n",
    "\n",
    "    def update_text(self, text, verbose=False):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.update_doc(self.sp_doc, verbose=verbose)\n",
    "        \n",
    "    def token_at_char(self, char_index):\n",
    "        # SpaCy Doc or Indexing Map Not Found\n",
    "        if not self.sp_doc or not self.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Index into Map\n",
    "        if char_index in self.index_map:\n",
    "            return self.index_map[char_index]\n",
    "\n",
    "        # Looking in Tokens\n",
    "        # Depending on the tokenizer, the character being\n",
    "        # used to find a token may not be the first character\n",
    "        # of the token.\n",
    "        # This shouldn't be needed anymore as I am pairing\n",
    "        # each character in the document to its token.\n",
    "        # for token in self.sp_doc:\n",
    "        #     if char_index >= token.idx and char_index < token.idx + len(token):\n",
    "        #         return token\n",
    "\n",
    "        # There must be a token that corresponds to the\n",
    "        # given character index. If there's not, there's\n",
    "        # an issue.\n",
    "        raise Exception(\"Token Not Found\")\n",
    "        \n",
    "    def load_index_map(self):\n",
    "        # SpaCy Doc Not Found\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Map Character Index to Token\n",
    "        index_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            # char_i0 is the index of the token's starting character.\n",
    "            # char_i1 is the index of the character after the token's ending character.\n",
    "            char_i0 = token.idx\n",
    "            char_i1 = token.idx + len(token)\n",
    "        \n",
    "            for i in range(char_i0, char_i1):\n",
    "                index_map[i] = token\n",
    "            \n",
    "        return index_map\n",
    "\n",
    "    def expand_unit(self, *, il_unit, ir_unit, il_boundary, ir_boundary, allowed_speech=[], allowed_literals=[], disallowed_literals=[], direction='BOTH', verbose=False):\n",
    "        # Move Left\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit > il_boundary:\n",
    "                prev_token = self.sp_doc[il_unit-1]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"il_unit: {il_unit}, il_boundary: {il_boundary}, prev_token: {prev_token}, prev_token.pos_: {prev_token.pos_}\")\n",
    "\n",
    "                in_disallowed = prev_token.lower_ in disallowed_literals\n",
    "                not_in_allowed = prev_token.pos_ not in allowed_speech and prev_token.lower_ not in allowed_literals\n",
    "                \n",
    "                if in_disallowed or not_in_allowed:\n",
    "                    break\n",
    "                \n",
    "                il_unit -= 1\n",
    "\n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit < ir_boundary:\n",
    "                next_token = self.sp_doc[ir_unit+1]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"ir_unit: {ir_unit}, ir_boundary: {ir_boundary}, next_token: {next_token}, next_token.pos_: {next_token.pos_}\")\n",
    "                \n",
    "                in_disallowed = prev_token.lower_ in disallowed_literals\n",
    "                not_in_allowed = next_token.pos_ not in allowed_speech and next_token.lower_ not in allowed_literals\n",
    "                \n",
    "                if in_disallowed or not_in_allowed:\n",
    "                    break\n",
    "                \n",
    "                ir_unit += 1\n",
    "\n",
    "        # Expanded Unit\n",
    "        expanded_unit = self.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Expanded Unit: {expanded_unit}\")\n",
    "        \n",
    "        return expanded_unit\n",
    "\n",
    "    def contract_unit(self, *, il_unit, ir_unit, allowed_speech=[], allowed_literals=[], direction='BOTH', verbose=False):\n",
    "        # Move Left\n",
    "        if verbose:\n",
    "            print(\"LEFT\")\n",
    "            \n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit < ir_unit:\n",
    "                curr_token = self.sp_doc[il_unit]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"il_unit: {il_unit}, ir_unit: {ir_unit}, curr_token: {curr_token}, curr_token.pos_: {curr_token.pos_}\")\n",
    "                    \n",
    "                if curr_token.pos_ in allowed_speech or curr_token.lower_ in allowed_literals:\n",
    "                    break\n",
    "                \n",
    "                il_unit += 1\n",
    "\n",
    "        # Move Right\n",
    "        if verbose:\n",
    "            print(\"RIGHT\")\n",
    "        \n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit > il_unit:\n",
    "                curr_token = self.sp_doc[ir_unit]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"il_unit: {il_unit}, ir_unit: {ir_unit}, curr_token: {curr_token}, curr_token.pos_: {curr_token.pos_}\")\n",
    "                \n",
    "                if curr_token.pos_ in allowed_speech or curr_token.lower_ in allowed_literals:\n",
    "                    break\n",
    "                \n",
    "                ir_unit -= 1\n",
    "\n",
    "        contracted_unit = self.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Contracted Unit: {contracted_unit}\")\n",
    "        \n",
    "        return contracted_unit\n",
    "\n",
    "    def score(self, verbose=False):\n",
    "        # Categories\n",
    "        TRAIT = 0\n",
    "        SPECIES = 1\n",
    "        EXPERIMENT = 2\n",
    "        INTERACTION = 3\n",
    "\n",
    "        # Number Categories\n",
    "        NUM_CATEGORIES = 4\n",
    "\n",
    "        # Max # of Points of Category per Sentence (MPC)\n",
    "        MPC = [0] * NUM_CATEGORIES\n",
    "        MPC[TRAIT] = 0.6\n",
    "        MPC[SPECIES] = 0.8\n",
    "        MPC[EXPERIMENT] = 0.8\n",
    "        MPC[INTERACTION] = 0.8\n",
    "        \n",
    "        # Points per Instance of Category (PIC)\n",
    "        PIC = [0] * NUM_CATEGORIES\n",
    "        PIC[TRAIT] = 0.6\n",
    "        PIC[SPECIES] = MPC[SPECIES]/3\n",
    "        PIC[EXPERIMENT] = 0.5\n",
    "        PIC[INTERACTION] = MPC[INTERACTION]/3\n",
    "        \n",
    "        # Points\n",
    "        points = [0] * NUM_CATEGORIES\n",
    "\n",
    "        # Extracted Information\n",
    "        experiment_tokens = self.experiment.tokens\n",
    "        trait_tokens = self.traits.tokens\n",
    "        cause_tokens = self.causes.tokens\n",
    "        change_tokens = self.changes.tokens\n",
    "        species_tokens = [self.sp_doc[span.start] for span in self.species.spans]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Experiment Tokens: {self.experiment.tokens}\")\n",
    "            print(f\"Trait Tokens: {self.traits.tokens}\")\n",
    "            print(f\"Cause Tokens: {self.causes.tokens}\")\n",
    "            print(f\"Change Tokens: {self.changes.tokens}\")\n",
    "            print(f\"Species Tokens: {self.species.tokens}\")\n",
    "         \n",
    "        # This is used to ensure that at least three species\n",
    "        # are mentioned.\n",
    "        seen_species = {}\n",
    "\n",
    "        for sent in self.sp_doc.sents:\n",
    "            # This contains the number of points\n",
    "            # each category has accumulated in the sentence.\n",
    "            curr_points = [0] * NUM_CATEGORIES\n",
    "\n",
    "            # Contains the tokens in the sentence.\n",
    "            sent_tokens = [token for token in sent]\n",
    "\n",
    "            # This is used for the species (must have a nearby cause and/or\n",
    "            # change word).\n",
    "            sent_cause_tokens = set(sent_tokens).intersection(cause_tokens)\n",
    "            sent_change_tokens = set(sent_tokens).intersection(change_tokens)\n",
    "\n",
    "            # We don't want to visit the same species more than one\n",
    "            # in the same sentence as to avoid redundant points.\n",
    "            sent_seen_species = []\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Sentence: {sent}\")\n",
    "                print(f\"Sentence Tokens: {sent_tokens}\")\n",
    "                print(f\"Sentence Cause Tokens: {sent_cause_tokens}\")\n",
    "                print(f\"Sentence Change Tokens: {sent_change_tokens}\")\n",
    "            \n",
    "            for token in sent_tokens:\n",
    "                # If each category has maxed out their points,\n",
    "                # we can end the loop early.\n",
    "                if curr_points[SPECIES] >= MPC[SPECIES] and curr_points[TRAIT] >= MPC[TRAIT] and curr_points[EXPERIMENT] >= MPC[EXPERIMENT]:\n",
    "                    break\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Token ({token.pos_}): '{token}'\")\n",
    "\n",
    "                # TRAIT CATEGORY\n",
    "                if curr_points[TRAIT] < MPC[TRAIT] and token in trait_tokens:\n",
    "                    # To get points in the trait category,\n",
    "                    # there must be (1) a trait; and (2) a change or cause\n",
    "                    # word nearby.\n",
    "                    distance = 5\n",
    "                    sent_cause_tokens_in_area = [c_token for c_token in sent_cause_tokens if c_token != token and abs(c_token.i - token.i) <= distance]\n",
    "                    sent_change_tokens_in_area = [c_token for c_token in sent_change_tokens if c_token != token and abs(c_token.i - token.i) <= distance]\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"Cause Tokens in Area: {sent_cause_tokens_in_area}\")\n",
    "                        print(f\"Change Tokens in Area: {sent_change_tokens_in_area}\")\n",
    "\n",
    "                    if sent_cause_tokens_in_area or sent_change_tokens_in_area:\n",
    "                        curr_points[TRAIT] += PIC[TRAIT]\n",
    "\n",
    "                        if verbose:\n",
    "                            print(f\"Added Trait {curr_points[TRAIT]} Points\")\n",
    "\n",
    "                # EXPERIMENT CATEGORY\n",
    "                if curr_points[EXPERIMENT] < MPC[EXPERIMENT] and token in experiment_tokens:\n",
    "                    curr_points[EXPERIMENT] += PIC[EXPERIMENT]\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"Added Experiment {curr_points[EXPERIMENT]} Points\")\n",
    "\n",
    "                # SPECIES CATEGORY\n",
    "                if token in species_tokens:\n",
    "                    # Find Species Span\n",
    "                    species_span = self.species.span_at_token(token)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"Species Span: {species_span}\")\n",
    "                        print(f\"Seen Species:\\n{seen_species}\")                 \n",
    "\n",
    "                    # Updating Seen Speeces\n",
    "                    num_visits = 0\n",
    "                    for seen_species_span in seen_species.keys():\n",
    "                        if self.species.same_species(species_span, seen_species_span, verbose=verbose):\n",
    "                            num_visits = seen_species[seen_species_span]\n",
    "                            if verbose:\n",
    "                                print(f\"\\t'{species_span}' == '{seen_species_span}'\")\n",
    "                                print(f\"\\tNumber of Visits: {num_visits}\")\n",
    "                            seen_species[seen_species_span] += 1\n",
    "                            break\n",
    "\n",
    "                    if not num_visits:\n",
    "                        seen_species[species_span] = 1\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"Seen Species Updated:\\n{seen_species}\")\n",
    "                        print(\"Checking Seen Species in Sentence\")\n",
    "\n",
    "                    # We only add points if it's a species that has not been seen\n",
    "                    # in the sentence. This is to avoid redundant points.        \n",
    "                    redundant_species = False\n",
    "                    \n",
    "                    for seen_species_span in sent_seen_species:\n",
    "                        if self.species.same_species(species_span, seen_species_span, verbose=verbose):\n",
    "                            redundant_species = True\n",
    "                            break\n",
    "                    \n",
    "                    sent_seen_species.append(species_span) \n",
    "                    if redundant_species:\n",
    "                        continue\n",
    "                    \n",
    "                    if curr_points[SPECIES] < MPC[SPECIES]:\n",
    "                        # To get points in the species category,\n",
    "                        # there must be (1) a species; and (2) a change or cause\n",
    "                        # word nearby.\n",
    "                        distance = 5\n",
    "                        sent_cause_tokens_in_area = [c_token for c_token in sent_cause_tokens if c_token != token and abs(c_token.i - token.i) <= distance]\n",
    "                        sent_change_tokens_in_area = [c_token for c_token in sent_change_tokens if c_token != token and abs(c_token.i - token.i) <= distance]\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"Cause Tokens in Area: {sent_cause_tokens_in_area}\")\n",
    "                            print(f\"Change Tokens in Area: {sent_change_tokens_in_area}\")\n",
    "                        \n",
    "                        if sent_cause_tokens_in_area or sent_change_tokens_in_area:\n",
    "                            curr_points[SPECIES] += PIC[SPECIES]\n",
    "\n",
    "                            if verbose:\n",
    "                                print(f\"Added Species {curr_points[SPECIES]} Points\")\n",
    "\n",
    "            # INTERACTION CATEGORY\n",
    "            # Reducing the species found in the sentence\n",
    "            # to make an educated guess about the interactions\n",
    "            # happening.\n",
    "            i = 0\n",
    "            while len(sent_seen_species) > 1:\n",
    "                changes_made = False\n",
    "                \n",
    "                j = 1\n",
    "                while j < len(sent_seen_species):\n",
    "                    if self.species.same_species(sent_seen_species[i], sent_seen_species[j]):\n",
    "                        sent_seen_species.pop(j)\n",
    "                        changes_made = True\n",
    "                        continue\n",
    "                    j += 1\n",
    "\n",
    "                if not changes_made:\n",
    "                    break\n",
    "\n",
    "                i += 1\n",
    "    \n",
    "            curr_points[INTERACTION] = PIC[INTERACTION] * len(sent_seen_species)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Reduced Species Seen in Sentence: {sent_seen_species}\")\n",
    "                print(f\"Added {curr_points[INTERACTION]} Points to Interaction\")\n",
    "                print(f\"Overall Points Before: {points}\")\n",
    "            \n",
    "            # SENTENCE DONE\n",
    "            # Add Points from Sentence to Total\n",
    "            for category in [TRAIT, SPECIES, EXPERIMENT, INTERACTION]:\n",
    "                points[category] += min(curr_points[category], MPC[category])\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Overall Points: {points}\")\n",
    "\n",
    "        # Enforcing 3 or More Species\n",
    "        if verbose:\n",
    "            print(f\"Seen Species: {seen_species}\")\n",
    "            \n",
    "        if len(seen_species) < 3:\n",
    "            return 0\n",
    "        \n",
    "        # Adding Up Points and Normalizing Score\n",
    "        NUM_SENTENCES = len(list(self.sp_doc.sents))\n",
    "        \n",
    "        score = (points[TRAIT] + points[SPECIES] + points[EXPERIMENT]) / (NUM_CATEGORIES * NUM_SENTENCES)\n",
    "        assert 0.0 <= score <= 1.0\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Score: {score}\")\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10821c-4a15-47e6-b3d2-f49b7ef3eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../Datasets/Baseline-1.csv\")\n",
    "text = df.Abstract[3]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619e40a-251d-46d0-a451-d6e3c8fa30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "main = Main()\n",
    "main.update_text(text, verbose=True)\n",
    "score = main.score(verbose=True)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa858af3-e2c8-4dcc-a7de-4e9f769f310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in main.species.tokens:\n",
    "    print(main.sp_doc[token.i-10:token.i+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce28077-7771-4735-aa76-f312135c73be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
