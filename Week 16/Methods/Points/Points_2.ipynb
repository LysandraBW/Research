{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecf5be-df89-4d1a-b75f-d92a5c5fff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trait-Mediated Interaction Modification\n",
    "# Empirical:\n",
    "# You could look for words like \"hypothesis\", \"experiment\", \"found\", and \"discovered\". That may point\n",
    "# towards there being an experiment in the paper. There are also words like \"control group\", \"compared\",\n",
    "# \"findings\", \"results\", \"study\", and more.\n",
    "# Qualitative vs. Quantitative:\n",
    "# To infer whether something is quantitative, you could look for numeric tokens and units.\n",
    "# However, you can only do so much with the abstract. Therefore, this is likely not good enough.\n",
    "# Yet, you could still take advantage of words like \"fewer\" and \"increased\" to show that there is a change.\n",
    "# However, this would be more suited for the above category.\n",
    "# Traits:\n",
    "# There is no NLP tool for traits that I can use or create so I think that I could instead use keywords.\n",
    "# For example, \"snail feeding rates\" is a trait. You may be able to spot this by looking for a word like\n",
    "# \"rate\". You'd expand that word to include \"snail feeding rates\". As \"snail\" is a species you can infer\n",
    "# that \"rates\" is a trait. I would be more decisive and use a dependency parser to ensure that the trait\n",
    "# is a property of the species (like before). However, with all the cases that may exist, I think checking\n",
    "# to see whether a species can be found by traveling back and/or forward without finding certain tokens could\n",
    "# work well enough.\n",
    "# 3 Species or More:\n",
    "# This is simple. However, I think using a dictionary and TaxoNerd would be beneficial (for higher accuracy).\n",
    "# To handle the potential differences in tokenization, character offsets should be used.\n",
    "# Standardization:\n",
    "# There is a lot of variance in the scores. To squash this issue, I think that we could assign each sentence\n",
    "# a value from 0 to 1. We would add these values and divide by the number of sentences. This would result in\n",
    "# a number that is also from 0 to 1. However, there are categories that we would like to inspect. So, we must\n",
    "# create an overall score in the interval from [0, 1] while also scoring each category. Well, for each sentence\n",
    "# we could add a point for each category that is observed. The sentence would receive said score divided by the\n",
    "# number of categories. At the end, we add up all the sentence scores and divide by the number of sentences.\n",
    "# The aggregate score for each category would also be divided by the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ccb94-4c7d-49b1-a727-f6e25b021357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from spacy.language import Language\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "%run -i \"../utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0612988-8cd7-4322-b98a-dbb5dc7d1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Help:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        # Zero Plurals\n",
    "        # The singular and plural versions of the words below are the same.\n",
    "        self.zero_plurals = [\n",
    "            \"species\", \n",
    "            \"deer\", \n",
    "            \"fish\", \n",
    "            \"moose\", \n",
    "            \"sheep\", \n",
    "            \"swine\", \n",
    "            \"buffalo\", \n",
    "            \"trout\", \n",
    "            \"cattle\"\n",
    "        ]\n",
    "        # Irregular Nouns\n",
    "        # There's not a defined conversion method.\n",
    "        self.irregular_nouns = {\n",
    "            \"ox\": \"oxen\",\n",
    "            \"goose\": \"geese\",\n",
    "            \"mouse\": \"mice\",\n",
    "            \"bacterium\": \"bacteria\"\n",
    "        }\n",
    "        self.irregular_nouns_rev = {v: k for k, v in self.irregular_nouns.items()}\n",
    "        self.irregular_singular_nouns = self.irregular_nouns.keys()\n",
    "        self.irregular_plural_nouns = self.irregular_nouns.values()\n",
    "\n",
    "    def remove_extra_spaces(self, string):\n",
    "        # Remove Duplicate Spaces\n",
    "        string = re.sub(r\"\\s+\", \" \", string)\n",
    "        # Remove Spaces Before Punctuation\n",
    "        string = re.sub(r\"\\s+([?.!,])\", r\"\\1\", string)\n",
    "        # Remove Outside Spaces\n",
    "        return string.strip()\n",
    "\n",
    "    def remove_outer_non_alnum(self, string):\n",
    "        while string:\n",
    "            start_len = len(string)\n",
    "            # Remove Leading Non-Alphanumeric Character\n",
    "            if string and not string[0].isalnum():\n",
    "                string = string[1:]\n",
    "            # Remove Trailing Non-Alphanumeric Character\n",
    "            if string and not string[-1].isalnum():\n",
    "                string = string[:-1]\n",
    "            # No Changes Made\n",
    "            if start_len == len(string):\n",
    "                break\n",
    "        return string\n",
    "\n",
    "    def group_text(self, text, flatten=False):\n",
    "        # The parenthetical would be the content inside of a pair of\n",
    "        # matching parentheses, brackets, or braces.\n",
    "        parentheticals = []\n",
    "        \n",
    "        # This contains the text that's not inside of\n",
    "        # parentheses and co.\n",
    "        base_text = []\n",
    "        \n",
    "        # Used for building groups,\n",
    "        # handles a nested structure.\n",
    "        stacks = []\n",
    "        \n",
    "        # These are the characters we recognize\n",
    "        # in terms of grouping.\n",
    "        pairs = {\n",
    "            \"(\": \")\",\n",
    "            \"[\": \"]\",\n",
    "            \"{\": \"}\"\n",
    "        }\n",
    "        open_chars = pairs.keys()\n",
    "        close_chars = pairs.values()\n",
    "        \n",
    "        # This contains the opening characters\n",
    "        # of the groups that are currently open\n",
    "        # (e.g. '(', '['). We use it so that we know\n",
    "        # whether we open or close a group.\n",
    "        opened = []\n",
    "        \n",
    "        for i, char in enumerate(text):\n",
    "            # Opening Character\n",
    "            if char in open_chars:\n",
    "                stacks.append([])\n",
    "                opened.append(char)\n",
    "            # Closing Character\n",
    "            elif opened and char == pairs.get(opened[-1], \"\"):\n",
    "                parentheticals.append(stacks.pop())\n",
    "                opened.pop()\n",
    "            # Add Character to Group\n",
    "            elif opened:\n",
    "                stacks[-1].append(i)\n",
    "            # Add Character to Ungrouped Text\n",
    "            else:\n",
    "                base_text.append(i)\n",
    "        \n",
    "        # If an opening character hasn't been closed,\n",
    "        # we just close all the remaining opened groups.\n",
    "        # This is moreso a problem regarding the text.\n",
    "        while stacks:\n",
    "            parentheticals.append(stacks.pop())\n",
    "            \n",
    "        # Merge\n",
    "        groups = [*parentheticals, base_text]\n",
    "        tuple_groups = []\n",
    "        for group in groups:\n",
    "            if not group:\n",
    "                continue\n",
    "            \n",
    "            tuples = [[group[0], group[0] + 1]]\n",
    "            for index in group[1:]:\n",
    "                if tuples[-1][1] == index:\n",
    "                    tuples[-1][1] = index + 1\n",
    "                else:\n",
    "                    tuples.append([index, index + 1])\n",
    "            tuple_groups.append(tuples)\n",
    "            \n",
    "        if flatten:\n",
    "            flattened_tuple_groups = []\n",
    "            for tuple_group in tuple_groups:\n",
    "                for tuple in tuple_group:\n",
    "                    flattened_tuple_groups.append(tuple)\n",
    "            tuple_groups = flattened_tuple_groups\n",
    "        \n",
    "        return tuple_groups\n",
    "\n",
    "    def singularize(self, string):\n",
    "        string = string.lower()\n",
    "        \n",
    "        # The string to singularize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        if not words:\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is a zero plural\n",
    "        # or a singular irregular noun, there's no changes\n",
    "        # to make. For example, \"red sheep\" and \"ox\" are \n",
    "        # already singular.\n",
    "        if (\n",
    "            words[-1] in self.zero_plurals or \n",
    "            words[-1] in self.irregular_singular_nouns\n",
    "        ):\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is an irregular\n",
    "        # plural noun, we rely on a dictionary with the\n",
    "        # corresponding mapping.\n",
    "        if words[-1] in self.irregular_plural_nouns:\n",
    "            words[-1] = self.irregular_nouns_rev[words[-1]]\n",
    "            return [self.remove_extra_spaces(\" \".join(words))]\n",
    "        \n",
    "        singulars = []\n",
    "\n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to error), we need to\n",
    "        # handle them all.\n",
    "        singular_forms = self.singular_form(words[-1])\n",
    "\n",
    "        if not singular_forms:\n",
    "            return [string]\n",
    "        \n",
    "        for singular_form in singular_forms:\n",
    "            singular = self.remove_extra_spaces(\" \".join([*words[:-1], singular_form]))\n",
    "            singulars.append(singular)\n",
    "            \n",
    "        return singulars\n",
    "\n",
    "    def singular_form(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Change -ies to -y\n",
    "        if re.fullmatch(r\".*ies$\", string):\n",
    "            versions.append(f'{string[:-3]}y')\n",
    "            return versions\n",
    "\n",
    "        # Change -ves to -f and -fe\n",
    "        if re.fullmatch(r\".*ves$\", string):\n",
    "            versions.append(f'{string[:-3]}f')\n",
    "            versions.append(f'{string[:-3]}fe')\n",
    "            return versions\n",
    "\n",
    "        # Remove -es \n",
    "        if re.fullmatch(r\".*es$\", string):\n",
    "            versions.append(f'{string[:-2]}')\n",
    "            return versions\n",
    "\n",
    "        # Change -i to -us\n",
    "        if re.fullmatch(r\".*i$\", string):\n",
    "            versions.append(f'{string[:-1]}us')\n",
    "            return versions\n",
    "\n",
    "        # Remove -s\n",
    "        if re.fullmatch(r\".*s$\", string):\n",
    "            versions.append(f'{string[:-1]}')\n",
    "            return versions\n",
    "\n",
    "        return versions\n",
    "\n",
    "    def pluralize(self, string):\n",
    "        string = string.lower()\n",
    "        \n",
    "        # The string to pluralize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        if not words:\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is a zero plural\n",
    "        # or a plural irregular noun, there's no changes\n",
    "        # to make. For example, \"red sheep\" and \"oxen\" are \n",
    "        # already singular.\n",
    "        if (\n",
    "            words[-1] in self.zero_plurals or \n",
    "            words[-1] in self.irregular_plural_nouns\n",
    "        ):\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is an irregular\n",
    "        # singular noun, we rely on a dictionary with the\n",
    "        # corresponding mapping.\n",
    "        if words[-1] in self.irregular_singular_nouns:\n",
    "            words[-1] = self.irregular_nouns[words[-1]]\n",
    "            return [self.remove_extra_spaces(\" \".join(words))]\n",
    "        \n",
    "        plurals = []\n",
    "        \n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to error), we need to\n",
    "        # handle them all.\n",
    "        plural_forms = self.plural_form(words[-1])\n",
    "\n",
    "        if not plural_forms:\n",
    "            return [string]\n",
    "            \n",
    "        for plural_form in plural_forms:\n",
    "            plural = self.remove_extra_spaces(\" \".join([*words[:-1], plural_form]))\n",
    "            plurals.append(plural)\n",
    "            \n",
    "        return plurals\n",
    "        \n",
    "    def plural_form(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Words that end with -us often have\n",
    "        # two different plural versions: -es and -i.\n",
    "        # For example, the plural version of cactus \n",
    "        # can be cactuses or cacti.\n",
    "        if re.fullmatch(r\".*us$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            versions.append(f'{string[:-2]}i')\n",
    "            return versions\n",
    "\n",
    "        # The -es ending is added to the words below.\n",
    "        if re.fullmatch(r\".*([^l]s|sh|ch|x|z)$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # Words that end with a consonant followed by 'y'\n",
    "        # are made plural by replacing the 'y' with -ies.\n",
    "        # For example, the plural version of canary is\n",
    "        # canaries.\n",
    "        if re.fullmatch(r\".*([^aeiou])(y)$\", string):\n",
    "            versions.append(f'{string[:-1]}ies')\n",
    "            return versions\n",
    "            \n",
    "        # The plural version of words ending with -f\n",
    "        # and -fe aren't clear. To be safe, I will add\n",
    "        # both versions.\n",
    "        if (re.fullmatch(r\".*(f)(e?)$\", string) and not re.fullmatch(r\".*ff$\", string)):\n",
    "            last_clean = re.sub(r\"(f)(e?)$\", \"\", string)\n",
    "            versions.append(f'{last_clean}fs')\n",
    "            versions.append(f'{last_clean}ves')\n",
    "            return versions\n",
    "\n",
    "        # People add -s or -es to words that end with 'o'.\n",
    "        # To be safe, both versions are added.\n",
    "        if re.fullmatch(r\".*([^aeiou])o$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # If there's no -s at the end of the string and\n",
    "        # the other cases didn't run, we add an -s.\n",
    "        if re.fullmatch(r\".*[^s]$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "        \n",
    "        return versions\n",
    "\n",
    "    def expand_unit(self, *, il_unit, ir_unit, il_boundary, ir_boundary, speech=[], literals=[], include=True, direction='BOTH', verbose=False):\n",
    "        assert il_unit <= ir_unit\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            assert il_boundary <= il_unit\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            assert ir_boundary >= ir_unit\n",
    "        \n",
    "        # Move Left\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            # The indices are inclusive, therefore, when \n",
    "            # the condition fails, il_unit will be equal\n",
    "            # to il_boundary.\n",
    "            while il_unit > il_boundary:\n",
    "                # We assume that the current token is allowed,\n",
    "                # and look to the token to the left.\n",
    "                l_token = self.main.sp_doc[il_unit-1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = l_token.pos_ in speech or l_token.lower_ in literals\n",
    "\n",
    "                # Case 1: include=False, in_set=True\n",
    "                # If we're not meant to include the defined tokens, and the\n",
    "                # current token is in that set, we stop expanding.\n",
    "                # Case 2: include=True, in_set=False\n",
    "                # If we're meant to include the defined tokens, and the current\n",
    "                # token is not in that set, we stop expanding.\n",
    "                # Case 3: include=in_set\n",
    "                # If we're meant to include the defined tokens, and the current\n",
    "                # token is in that set, we continue expanding. If we're not meant\n",
    "                # to include the defined tokens, and the current token is not\n",
    "                # in that set, we continue expanding.\n",
    "                if include ^ in_set:\n",
    "                    break\n",
    "                \n",
    "                # Else, the left token is valid, and\n",
    "                # we continue to expand.\n",
    "                il_unit -= 1\n",
    "\n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            # Likewise, when the condition fails,\n",
    "            # ir_unit will be equal to the ir_boundary.\n",
    "            # The ir_boundary is also inclusive.\n",
    "            while ir_unit < ir_boundary:\n",
    "                # Assuming that the current token is valid,\n",
    "                # we look to the right to see if we can\n",
    "                # expand.\n",
    "                r_token = self.main.sp_doc[ir_unit+1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = r_token.pos_ in speech or r_token.lower_ in literals\n",
    "                if include ^ in_set:\n",
    "                    break\n",
    "\n",
    "                # Else, the token is valid and\n",
    "                # we continue.\n",
    "                ir_unit += 1\n",
    "\n",
    "        assert il_unit >= il_boundary and ir_unit <= ir_boundary\n",
    "        expanded_unit = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        return expanded_unit\n",
    "\n",
    "    def contract_unit(self, *, il_unit, ir_unit, speech=[], literals=[], include=True, direction='BOTH', verbose=False):\n",
    "        assert il_unit <= ir_unit\n",
    "        \n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit < ir_unit:\n",
    "                # We must check if the current token\n",
    "                # is not allowed. If it's not allowed,\n",
    "                # we contract (remove).\n",
    "                token = self.main.sp_doc[il_unit]\n",
    "\n",
    "                # The token is invalid, thus we stop\n",
    "                # contracting.\n",
    "                # include = True means that we want the tokens that match\n",
    "                # the speech and/or literals in the contracted unit.\n",
    "                # include = False means that we don't want the tokens that\n",
    "                # match the speech and/or literals in the contracted unit.\n",
    "                # Case 1: include = True, in_set = True\n",
    "                # We have a token that's meant to be included in the set.\n",
    "                # However, we're contracting, which means we would end up\n",
    "                # removing the token if we continue. Therefore, we break.\n",
    "                # Case 2: include = False, in_set = False\n",
    "                # We have a token that's not in the set which defines the\n",
    "                # tokens that aren't meant to be included. Therefore, we \n",
    "                # have a token that is meant to be included. If we continue,\n",
    "                # we would end up removing this token. Therefore, we break.\n",
    "                # Default:\n",
    "                # If we have a token that's in the set (in_set=True) of\n",
    "                # tokens we're not supposed to include in the contracted \n",
    "                # unit (include=False), we need to remove it. Likewise, if\n",
    "                # we have a token that's not in the set (in_set=False) of\n",
    "                # tokens to include in the contracted unit (include=True),\n",
    "                # we need to remove it.\n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include == in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid, thus we continue.\n",
    "                il_unit += 1\n",
    "\n",
    "        # Move Left      \n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit > il_unit:\n",
    "                token = self.main.sp_doc[ir_unit]\n",
    "\n",
    "                # The token is invalid and we\n",
    "                # stop contracting.\n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include == in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid and we continue.\n",
    "                ir_unit -= 1\n",
    "\n",
    "        assert il_unit <= ir_unit\n",
    "        contracted_unit = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        return contracted_unit\n",
    "\n",
    "    def find_unit_context(self, *, il_unit, ir_unit, il_boundary, ir_boundary, verbose=False):\n",
    "        assert il_unit <= ir_unit\n",
    "        assert il_boundary <= il_unit\n",
    "        assert ir_boundary >= ir_unit\n",
    "        \n",
    "        # Caveat: Parentheticals\n",
    "        # The context of a unit inside of parentheses should not\n",
    "        # go farther than the boundaries of those parentheses.\n",
    "        # However, we need to manually determine whether the unit\n",
    "        # is in parentheses (or any set of the matching symbols\n",
    "        # below).\n",
    "        matching_puncts = {\n",
    "            \"[\": \"]\", \n",
    "            \"(\": \")\", \n",
    "            \"-\": \"-\", \n",
    "            \"--\": \"--\",\n",
    "            \"{\": \"}\"\n",
    "        }\n",
    "        \n",
    "        # The opening symbols for group punctuation.\n",
    "        opening_puncts = list(matching_puncts.keys())\n",
    "\n",
    "        # The closing symbols for group punctuation.\n",
    "        closing_puncts = list(matching_puncts.values())\n",
    "\n",
    "        # Both the opening and closing symbols above.\n",
    "        puncts = [*closing_puncts, *opening_puncts]\n",
    "\n",
    "        # Look for Group Punctuation on the Left\n",
    "        i = il_unit\n",
    "        l_punct = None\n",
    "        while i >= il_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.lower_ in puncts:\n",
    "                l_punct = token\n",
    "                break\n",
    "            i -= 1\n",
    "\n",
    "        # Look for Group Punctuation on the Right\n",
    "        i = ir_unit + 1 if l_punct and il_unit == ir_unit else ir_unit\n",
    "        r_punct = None\n",
    "        while i <= ir_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.lower_ in puncts:\n",
    "                r_punct = token\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        # If there's a group punctuation on the left\n",
    "        # and right, and they match each other (e.g. '(' and ')'),\n",
    "        # we return the text between the punctuations.\n",
    "        parenthetical = l_punct and r_punct and matching_puncts.get(l_punct.lower_, '') == r_punct.text\n",
    "        if parenthetical:\n",
    "            return self.main.sp_doc[l_punct.i:r_punct.i+1]\n",
    "\n",
    "        # As the unit is not a parenthetical, we will expand\n",
    "        # outwards until we run into a stopping token. The exclude\n",
    "        # list contains tokens that should be excluded from the\n",
    "        # context. Currently, it will contain any parentheticals\n",
    "        # that we run into.\n",
    "        exclude = []\n",
    "\n",
    "        # If a token's POS falls into these categories, we will\n",
    "        # continue. If not, we stop expanding.\n",
    "        speech = [\"ADJ\", \"NOUN\", \"ADP\", \"ADV\", \"PART\", \"PROPN\", \"VERB\", \"PRON\", \"DET\", \"AUX\", \"PART\"]\n",
    "        \n",
    "        # Expand Left\n",
    "        while il_unit > il_boundary:\n",
    "            # Assuming that the current token is fine,\n",
    "            # we look to the left.\n",
    "            l_token = self.main.sp_doc[il_unit-1]\n",
    "\n",
    "            # If it's a closing punctuation (e.g. ')', ']'),\n",
    "            # we need to skip over whatever is contained in\n",
    "            # that punctuation.\n",
    "            if l_token.lower_ in closing_puncts:\n",
    "                i = il_unit - 1\n",
    "                # We continue until we reach the boundary or we\n",
    "                # find the matching opening punctuation.\n",
    "                token = self.main.sp_doc[i]\n",
    "                while i >= il_boundary and matching_puncts.get(token.lower_, '') != l_token.lower_:\n",
    "                    token = self.main.sp_doc[i]\n",
    "                    exclude.append(token)\n",
    "                    i -= 1\n",
    "                exclude.append(token)\n",
    "\n",
    "                # After we've gone past the parenthetical,\n",
    "                # we can jump to the next position.\n",
    "                il_unit = i\n",
    "                continue\n",
    "            # If it's not a closing punctuation, we check\n",
    "            # whether it's a stopping token\n",
    "            else:\n",
    "                if l_token.pos_ not in speech:\n",
    "                    break\n",
    "                else:\n",
    "                    il_unit -= 1\n",
    "\n",
    "        # Expand Right\n",
    "        while ir_unit < ir_boundary:\n",
    "            # We're checking the token to the right\n",
    "            # to see if we can expand or not.\n",
    "            r_token = self.main.sp_doc[ir_unit+1]\n",
    "            \n",
    "            # If the token to the right is an opening\n",
    "            # punctuation (e.g. '(', '['), we must skip\n",
    "            # it, the parenthetical inside, and the\n",
    "            # closing punctuation.\n",
    "            if r_token.lower_ in opening_puncts:\n",
    "                i = ir_unit + 1\n",
    "                token = self.main.sp_doc[i]\n",
    "                while i <= ir_boundary and token.lower_ != matching_puncts.get(r_token.lower_, ''):\n",
    "                    token = self.main.sp_doc[i]\n",
    "                    exclude.append(token)\n",
    "                    i += 1\n",
    "                exclude.append(token)\n",
    "\n",
    "                # Skip\n",
    "                ir_unit = i\n",
    "                continue\n",
    "            # If it's not an opening punctuation, we check\n",
    "            # whether we can continue expanding.\n",
    "            else:\n",
    "                if r_token.pos_ not in speech:\n",
    "                    break\n",
    "                else:\n",
    "                    ir_unit += 1\n",
    "\n",
    "        # We remove the excluded tokens\n",
    "        # and return the context.\n",
    "        context = [t for t in self.main.sp_doc[il_unit:ir_unit+1] if t not in exclude]\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26736310-0964-4184-a9c1-f6b0b38d7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the Dictionary\n",
    "@Language.component(\"lower_case_lemmas\")\n",
    "def lower_case_lemmas(doc) :\n",
    "    for token in doc :\n",
    "        token.lemma_ = token.lemma_.lower()\n",
    "    return doc\n",
    "\n",
    "class Species:\n",
    "    def __init__(self, main):\n",
    "        # Tools\n",
    "        self.main = main\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\"])\n",
    "        self.tn_nlp.add_pipe(\"lower_case_lemmas\", after=\"lemmatizer\")\n",
    "        self.tn_doc = None\n",
    "        \n",
    "        # Contains any spans that have been identified\n",
    "        # as a species.\n",
    "        self.spans = None\n",
    "        \n",
    "        # Contains any tokens that have been identified\n",
    "        # as a species or being a part of a species.\n",
    "        self.tokens = None\n",
    "        \n",
    "        # Used to quickly access the span that a token\n",
    "        # belongs to.\n",
    "        self.token_to_span = None\n",
    "        \n",
    "        # Maps a string to an array of strings wherein\n",
    "        # the strings involved in the key-value pair \n",
    "        # have been identified as an alternate name of each other.\n",
    "        self.alternate_names = None\n",
    "        \n",
    "        # Used to increase TaxoNERD's accuracy.\n",
    "        self.dictionary = None\n",
    "        self.load_dictionary()\n",
    "\n",
    "    def load_dictionary(self):\n",
    "        self.dictionary = [\"juvenile\", \"adult\", \"prey\", \"predator\", \"species\", \"tree\", \"cat\", \"dog\"]\n",
    "        # df = pd.read_csv(\"VernacularNames.csv\")\n",
    "        # self.dictionary += df.VernacularName.to_list()\n",
    "\n",
    "        patterns = []\n",
    "        for name in self.dictionary:\n",
    "            doc = self.tn_nlp(name)\n",
    "            patterns.append({\"label\": \"LIVB\", \"pattern\": [{\"LEMMA\": token.lemma_} for token in doc]})\n",
    "        ruler = self.tn_nlp.add_pipe(\"entity_ruler\")\n",
    "        ruler.add_patterns(patterns)\n",
    "        \n",
    "    def update(self, text, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        self.spans, self.tokens, self.token_to_span, self.alternate_names = self.load_species(verbose=verbose)\n",
    "\n",
    "    def load_species(self, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # We'll search for species in the text.\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "\n",
    "        # These three contain the species that have been\n",
    "        # identified in the text. Tokens that aren't adjectives,\n",
    "        # nouns, or proper nouns will be stripped.\n",
    "        spans = []\n",
    "        tokens = []\n",
    "        token_to_span = {}\n",
    "\n",
    "        # It's useful to know if a different name refers to a\n",
    "        # species we have already seen. For example, in\n",
    "        # \"predatory crab (Carcinus maenas)\", \"predatory crab\"\n",
    "        # is an alternative name for \"Carcinus maenas\" and\n",
    "        # vice versa. This is used so that the species can be\n",
    "        # properly tracked and redundant points are less\n",
    "        # likely to be given.\n",
    "        alternate_names = {}\n",
    "\n",
    "        # We convert the spans that TaxoNerd has recognized\n",
    "        # to spans under a different parent document. This is\n",
    "        # because we're largely using said parent document and\n",
    "        # there is more functionality in that parent document.\n",
    "        species_spans = []\n",
    "        for tn_species_span in self.tn_doc.ents:\n",
    "            char_i0 = self.tn_doc[tn_species_span.start].idx\n",
    "            char_i1 = char_i0 + len(tn_species_span.text) - 1\n",
    "\n",
    "            sp_token_i0 = self.main.token_at_char(char_i0).i\n",
    "            sp_token_i1 = self.main.token_at_char(char_i1).i\n",
    "\n",
    "            sp_species_span = self.main.sp_doc[sp_token_i0:sp_token_i1+1]\n",
    "            \n",
    "            # Although they have different parent documents,\n",
    "            # they should still have the same text.\n",
    "            if sp_species_span.text.lower() != tn_species_span.text.lower():\n",
    "                print(sp_species_span.text.lower(), tn_species_span.text.lower())\n",
    "            assert sp_species_span.text.lower() == tn_species_span.text.lower()\n",
    "\n",
    "            # Sometimes, TaxoNerd recognizes two names of a species in one span.\n",
    "            # If they're separated with parentheses, we can handle the case here.\n",
    "            # The naming is difficult, so I'll just call it species_tuples.\n",
    "            species_tuples = self.main.help.group_text(sp_species_span.text, flatten=True)\n",
    "\n",
    "            species_span_chunks = []\n",
    "            for species_tuple in species_tuples:\n",
    "                species_span_chunk_text = sp_species_span.text[species_tuple[0]:species_tuple[1]]\n",
    "                if species_span_chunk_text.isspace():\n",
    "                    continue\n",
    "                \n",
    "                group_char_i0 = char_i0 + species_tuple[0]\n",
    "                group_char_i1 = char_i0 + species_tuple[1] - 1\n",
    "\n",
    "                # Update L Index to Exclude Whitespace Characters\n",
    "                while text[group_char_i0].isspace():\n",
    "                    group_char_i0 += 1\n",
    "\n",
    "                # Update R Index to Exclude Whitespace Characters\n",
    "                while text[group_char_i1].isspace():\n",
    "                    group_char_i1 -= 1\n",
    "\n",
    "                group_token_i0 = self.main.token_at_char(group_char_i0).i\n",
    "                group_token_i1 = self.main.token_at_char(group_char_i1).i\n",
    "\n",
    "                species_span_chunks.append(self.main.sp_doc[group_token_i0:group_token_i1+1])\n",
    "\n",
    "            for species_span_chunk in species_span_chunks:\n",
    "                species_spans.append(species_span_chunk)\n",
    "    \n",
    "                # TaxoNERD will recognize the full species (i.e. \"brown squirrels\"),\n",
    "                # and we can use this to find more instances of a species in the text\n",
    "                # by extracting the last noun or proper noun from that span \n",
    "                # (i.e. \"squirrels\"). Now, we can find \"brown squirrels\" and \n",
    "                # \"squirrels\".\n",
    "                reversed_span = [t for t in species_span_chunk]\n",
    "                reversed_span.reverse()\n",
    "                for token in reversed_span:\n",
    "                    if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                        species_spans.append(self.main.sp_doc[token.i:token.i+1])\n",
    "                        break\n",
    "\n",
    "        # TaxoNerd sometimes recognizes one instance of a species\n",
    "        # and fails to recognize it elsewhere. To fix this, I'll\n",
    "        # search the text for all the species that TaxoNerd sees.\n",
    "        # This should resolve that issue. To make this more robust,\n",
    "        # I'll include the singular and plural versions of the\n",
    "        # recognized species. Furthermore, the species being used\n",
    "        # to search for other instances of species in the text will\n",
    "        # be called search_species. Using a database I downloaded,\n",
    "        # I've initialized search_species with a set of english\n",
    "        # vernacular names (e.g., \"dog\", \"cat\"). I'm removing it for\n",
    "        # now because there's seeminly a lot of bogus values.\n",
    "        # df = pd.read_csv(\"EnglishVernacularNames-2.csv\")\n",
    "        # search_species = df.Name.to_list()\n",
    "        search_species = [\"juvenile\", \"adult\", \"prey\", \"predator\", \"predators\", \"species\", \"tree\", \"cat\", \"dog\", \"flies\", \"plants\", \"plant\", \"fly\"]\n",
    "\n",
    "        for species_span in species_spans:\n",
    "            species_text = species_span.text.lower()\n",
    "            species_text = self.main.help.remove_outer_non_alnum(species_text)\n",
    "\n",
    "            search_species.append(species_text)\n",
    "\n",
    "            # Add Singular and/or Plural Version\n",
    "            if species_span[-1].pos_ == \"NOUN\":\n",
    "                # Plural\n",
    "                if species_span[-1].tag_ == \"NNS\":\n",
    "                    singular_species = self.main.help.singularize(species_text)\n",
    "                    search_species.extend(singular_species)\n",
    "                # Singular\n",
    "                if species_span[-1].tag_ == \"NN\":\n",
    "                    plural_species = self.main.help.pluralize(species_text)\n",
    "                    search_species.extend(plural_species)\n",
    "\n",
    "        # Now, we have the species to search for in the text.\n",
    "        search_species = list(set(search_species))\n",
    "        \n",
    "        for species in search_species:\n",
    "            matches = re.finditer(re.escape(species), text)\n",
    "            \n",
    "            for char_i0, char_i1 in [(match.start(), match.end()) for match in matches]:\n",
    "                # The full word must match, not just a substring inside of it.\n",
    "                # So, if the species we're looking for is \"ant\", only \"ant\"\n",
    "                # will match -- not \"pants\" or \"antebellum\". Therefore, the\n",
    "                # characters to the left and right of the matched string must be\n",
    "                # non-alphanumeric. \n",
    "                l_char_is_letter = char_i0 > 0 and text[char_i0-1].isalpha()\n",
    "                r_char_is_letter = char_i1 < len(text) and text[char_i1].isalpha()\n",
    "                \n",
    "                if l_char_is_letter or r_char_is_letter:\n",
    "                    continue\n",
    "                    \n",
    "                sp_li = self.main.token_at_char(char_i0).i\n",
    "                sp_ri = self.main.token_at_char(char_i1-1).i\n",
    "\n",
    "                # This is the matched substring (which would be\n",
    "                # a species) as a span in the parent document.\n",
    "                species_span = self.main.sp_doc[sp_li:sp_ri+1]\n",
    "                \n",
    "                # Expand Species\n",
    "                # Let's say there's a word like \"squirrel\". That's a bit ambiguous. \n",
    "                # Is it a brown squirrel, a bonobo? If the species is possibly missing\n",
    "                # information (like an adjective to the left of it), we should expand\n",
    "                # in order to get a full picture of the species.\n",
    "                unclear_1 = len(species_span) == 1 and species_span[0].pos_ == \"NOUN\"\n",
    "                unclear_2 = species_span.start > 0 and self.main.sp_doc[species_span.start-1].pos_ in [\"ADJ\"]\n",
    "                \n",
    "                if unclear_1 or unclear_2:\n",
    "                    species_span = self.main.help.expand_unit(\n",
    "                        il_unit=species_span.start, \n",
    "                        ir_unit=species_span.end-1,\n",
    "                        il_boundary=0,\n",
    "                        ir_boundary=len(self.main.sp_doc),\n",
    "                        speech=[\"ADJ\", \"PROPN\"],\n",
    "                        literals=[\"-\"],\n",
    "                        include=True,\n",
    "                        direction=\"LEFT\",\n",
    "                        verbose=verbose\n",
    "                    )\n",
    "                \n",
    "                # Remove Outer Symbols\n",
    "                # There are times where a species is identified with a parenthesis\n",
    "                # nearby. Here, we remove that parenthesis (and any other symbols).\n",
    "                species_span = self.main.help.contract_unit(\n",
    "                    il_unit=species_span.start, \n",
    "                    ir_unit=species_span.end-1, \n",
    "                    speech=[\"PUNCT\", \"SYM\", \"DET\", \"PART\"],\n",
    "                    include=False,\n",
    "                    verbose=verbose\n",
    "                )\n",
    "            \n",
    "                # A species must have a noun or a\n",
    "                # proper noun. This may help discard\n",
    "                # bogus results.\n",
    "                letter_found = False\n",
    "                for token in species_span:\n",
    "                    if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                        letter_found = True\n",
    "                        break\n",
    "\n",
    "                if not letter_found:\n",
    "                    continue\n",
    "\n",
    "                # Adding Species\n",
    "                spans.append(species_span)\n",
    "                for token in species_span:\n",
    "                    if token in tokens or token.pos_ in [\"PUNCT\", \"SYM\", \"DET\", \"PART\"]:\n",
    "                        continue\n",
    "                    tokens.append(token)\n",
    "                    token_to_span[token] = species_span\n",
    "\n",
    "        # Removing Duplicates and Sorting \n",
    "        spans = list({span.start: span for span in spans}.values())\n",
    "        spans.sort(key=lambda span: span.start)\n",
    "        \n",
    "        # Finding and Storing Alternative Names\n",
    "        for i, species_span in enumerate(spans):\n",
    "            # There's not a next species to\n",
    "            # evaluate.\n",
    "            if i + 1 >= len(spans):\n",
    "                break\n",
    "            \n",
    "            next_species_span = spans[i+1]\n",
    "            \n",
    "            # If there's one token between the species and the next species,\n",
    "            # we check if the next species is surrounded by punctuation.\n",
    "            if next_species_span.start - species_span.end == 1:\n",
    "                # Token Before and After the Next Species\n",
    "                before_next = self.main.sp_doc[next_species_span.start-1]\n",
    "                after_next = self.main.sp_doc[next_species_span.end]\n",
    "\n",
    "                if before_next.pos_ in [\"PUNCT\", \"SYM\"] and after_next.pos_ in [\"PUNCT\", \"SYM\"]:\n",
    "                    sp_1_text = species_span.text.lower()\n",
    "                    sp_2_text = next_species_span.text.lower()\n",
    "                    \n",
    "                    if sp_1_text not in alternate_names:\n",
    "                        alternate_names[sp_1_text] = []\n",
    "                    \n",
    "                    if sp_2_text not in alternate_names:\n",
    "                        alternate_names[sp_2_text] = []\n",
    "                    \n",
    "                    alternate_names[sp_1_text].append(sp_2_text)\n",
    "                    alternate_names[sp_2_text].append(sp_1_text)\n",
    "            # If there's no token between the species and the next,\n",
    "            # species we assume that they refer to the same species.\n",
    "            elif next_species_span.start - species_span.end == 0:\n",
    "                sp_1_text = species_span.text.lower()\n",
    "                sp_2_text = next_species_span.text.lower()\n",
    "                \n",
    "                if sp_1_text not in alternate_names:\n",
    "                    alternate_names[sp_1_text] = []\n",
    "                \n",
    "                if sp_2_text not in alternate_names:\n",
    "                    alternate_names[sp_2_text] = []\n",
    "\n",
    "                alternate_names[sp_1_text].append(sp_2_text)\n",
    "                alternate_names[sp_2_text].append(sp_1_text)\n",
    "       \n",
    "        return (spans, tokens, token_to_span, alternate_names)\n",
    "\n",
    "    def span_at_token(self, token):\n",
    "        if token in self.token_to_span:\n",
    "            return self.token_to_span[token]\n",
    "        return None\n",
    "    \n",
    "    def is_species(self, token):\n",
    "        return token in self.tokens\n",
    "        \n",
    "    def has_species(self, tokens, verbose=False):\n",
    "        for token in tokens:\n",
    "            if token in self.tokens:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def find_same_species(self, sp_A, sp_b, verbose=False):\n",
    "        # METHOD 1: Check for Literal Matches\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        \n",
    "        for sp_a in sp_A:\n",
    "            # Verbatim Text\n",
    "            sp_a_text = sp_a.text.lower()\n",
    "\n",
    "            if sp_a_text == sp_b_text:\n",
    "                return sp_a\n",
    "\n",
    "            # Singularized Text\n",
    "            sp_a_singular_texts = sp_a_text if sp_a[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(sp_a_text)\n",
    "            sp_b_singular_texts = sp_b_text if sp_b[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(sp_b_text)\n",
    "\n",
    "            if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "                return sp_a\n",
    "\n",
    "        # METHOD 2: Check Alternate Names\n",
    "        for sp_a in sp_A:\n",
    "            # Species B is an alternate name for Species A\n",
    "            if sp_b_text in self.alternate_names.get(sp_a_text, []):\n",
    "                return sp_a\n",
    "            # Species A is an alternate name for Species B\n",
    "            if sp_a_text in self.alternate_names.get(sp_b_text, []):\n",
    "                return sp_a\n",
    "        \n",
    "        # METHOD 3: Check Nouns\n",
    "        # This is used if one or none of the species being compared\n",
    "        # has 1 adjective.\n",
    "        sp_b_0_text = sp_b[0].lower_\n",
    "        sp_b_is_noun = sp_b[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "        sp_b_nouns = []\n",
    "        sp_b_num_adjectives = 0\n",
    "        for token in sp_b:\n",
    "            if not sp_b_nouns and token.pos_ == \"ADJ\":\n",
    "                sp_b_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_b_nouns.append(token)\n",
    "        sp_b_nouns_str = [noun.lower_ for noun in sp_b_nouns]\n",
    "        sp_b_singular_texts = \" \".join(sp_b_nouns_str) if sp_b_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(\" \".join(sp_b_nouns_str))\n",
    "        \n",
    "        for sp_a in sp_A:\n",
    "            sp_a_0_text = sp_a[0].lower_\n",
    "            sp_a_is_noun = sp_a[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "            # Case Example: 'Hyla' v. 'Hyla tadpoles'\n",
    "            if sp_a_0_text == sp_b_0_text and (sp_a_is_noun or sp_b_is_noun):\n",
    "                if sp_a_text in sp_b_text or sp_b_text in sp_a_text:\n",
    "                    return sp_a\n",
    "            # Case Example: 'dogs' v. 'red dogs'\n",
    "            else:\n",
    "                sp_a_nouns = []\n",
    "                sp_a_num_adjectives = 0\n",
    "                for token in sp_a:\n",
    "                    if not sp_a_nouns and token.pos_ == \"ADJ\":\n",
    "                        sp_a_num_adjectives += 1\n",
    "                    elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                        sp_a_nouns.append(token)\n",
    "                sp_a_nouns_str = [noun.lower_ for noun in sp_a_nouns]\n",
    "                \n",
    "                if sp_a_nouns and sp_b_nouns and (\n",
    "                    (sp_a_num_adjectives == 1 and sp_b_num_adjectives == 0) or \n",
    "                    (sp_b_num_adjectives == 1 and sp_a_num_adjectives == 0)\n",
    "                ):\n",
    "                    sp_a_singular_texts = \" \".join(sp_a_nouns_str) if sp_a_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.help.singularize(\" \".join(sp_a_nouns_str))\n",
    "                    \n",
    "                    if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "                        return sp_a\n",
    "\n",
    "        # METHOD 3: Last Ditch Effort\n",
    "        # If there's been no matches, we just look for one string inside of\n",
    "        # another.\n",
    "        for sp_a in sp_A:\n",
    "            sp_a_text = sp_a.text.lower()\n",
    "            if sp_b_text in sp_a_text or sp_a_text in sp_b_text:\n",
    "                return sp_a\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31521a-3d38-4087-b63e-bb974ad3a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    def __init__(self, main, *, base=[], speech=[], literals=[], threshold=0.7, include_substring=False):\n",
    "        self.main = main\n",
    "        # For a token to count towards a base word, it must be the same word.\n",
    "        self.base = [b.lower() for b in base]\n",
    "        self.speech = [s.upper() for s in speech]\n",
    "        self.literals = [l.lower() for l in literals]\n",
    "        # When comparing two words, SpaCy returns a value\n",
    "        # from 0 to 1, representing how similar the two\n",
    "        # embeddings are. The threshold below determines\n",
    "        # the minimum number of similarity before two words\n",
    "        # are considered as being equivalent.\n",
    "        self.threshold = threshold\n",
    "        self.vocab = [self.main.sp_nlp(word) for word in self.base]\n",
    "        # If this is True, then we will also check\n",
    "        # if the token contains a base word.\n",
    "        self.include_substring = include_substring\n",
    "        # This list contains the matched tokens.\n",
    "        self.tokens = []\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tokens = self.match_tokens(verbose=verbose)\n",
    "\n",
    "    def match_tokens(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        matched_tokens = []\n",
    "\n",
    "        # Check Words\n",
    "        for token in self.main.sp_doc:    \n",
    "            if token.pos_ not in self.speech:\n",
    "                continue\n",
    "\n",
    "            token_lower = token.lower_\n",
    "            token_lemma_lower = token.lemma_.lower()\n",
    "\n",
    "            # Look for Base Word\n",
    "            if token_lemma_lower in self.base or token_lower in self.base:\n",
    "                matched_tokens.append(token)\n",
    "                continue\n",
    "\n",
    "            # Look for Base Word in Token \n",
    "            # For example, a word like \"biomass\" would match\n",
    "            # if \"mass\" is a base word.\n",
    "            if self.include_substring:\n",
    "                for base_word in self.base:\n",
    "                    if base_word in token_lemma_lower or base_word in token_lower:\n",
    "                        matched_tokens.append(token)\n",
    "                        break\n",
    "            \n",
    "\n",
    "            # Already Matched Token\n",
    "            if matched_tokens and matched_tokens[-1] == token:\n",
    "                continue\n",
    "            \n",
    "            # Comparing Similarity\n",
    "            token_doc = self.main.sp_nlp(token_lower)\n",
    "            for word in self.vocab:\n",
    "                similarity = word.similarity(token_doc)\n",
    "\n",
    "                if similarity >= self.threshold:\n",
    "                    matched_tokens.append(token)\n",
    "                    break\n",
    "\n",
    "        # Check Literals\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "        for literal in self.literals:\n",
    "            for char_index in [match.start() for match in re.finditer(literal, text)]:\n",
    "                adj_char_index = char_index\n",
    "                while text[adj_char_index].isspace():\n",
    "                    adj_char_index += 1\n",
    "                matched_tokens.append(self.main.token_at_char(adj_char_index))\n",
    "                \n",
    "        return matched_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699dce8-5827-4757-becd-a016f9a06cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\n",
    "                \"study\", \n",
    "                \"hypothesis\", \n",
    "                \"experiment\", \n",
    "                \"found\", \n",
    "                \"discover\", \n",
    "                \"compare\", \n",
    "                \"finding\", \n",
    "                \"result\", \n",
    "                \"test\", \n",
    "                \"examine\", \n",
    "                \"model\",\n",
    "                \"measure\",\n",
    "                \"manipulate\",\n",
    "                \"assess\",\n",
    "                \"conduct\",\n",
    "                \"data\",\n",
    "                \"analyze\",\n",
    "                \"sample\",\n",
    "                \"observe\",\n",
    "                \"predict\",\n",
    "                \"suggest\",\n",
    "                \"method\",\n",
    "                \"investigation\",\n",
    "                \"trial\",\n",
    "                \"experimental\",\n",
    "                \"evidence\",\n",
    "                \"demonstrate\",\n",
    "                \"analysis\",\n",
    "                \"show\",\n",
    "                \"compare\",\n",
    "                \"comparable\"\n",
    "            ],\n",
    "            literals=[\n",
    "                \"control group\", \n",
    "                \"independent\", \n",
    "                \"dependent\"\n",
    "            ],\n",
    "            speech=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            threshold=0.7\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ae4dd9-f73e-41d5-926e-96c383d65c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CauseKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\n",
    "                \"increase\", \n",
    "                \"decrease\", \n",
    "                \"change\", \n",
    "                \"shift\", \n",
    "                \"cause\", \n",
    "                \"produce\", \n",
    "                \"trigger\", \n",
    "                \"suppress\", \n",
    "                \"inhibit\",\n",
    "                \"encourage\",\n",
    "                \"allow\",\n",
    "                \"influence\",\n",
    "                \"affect\",\n",
    "                \"alter\",\n",
    "                \"induce\",\n",
    "                \"produce\",\n",
    "                \"result in\",\n",
    "                \"associated with\",\n",
    "                \"correlated with\",\n",
    "                \"contribute\",\n",
    "                \"impact\",\n",
    "                \"deter\",\n",
    "                \"depressed\"\n",
    "            ],\n",
    "            speech=[\"VERB\", \"NOUN\"], \n",
    "            threshold=0.7\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb56df8-95d1-4ac9-956a-f7aeddb3ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            base=[\n",
    "                \"few\", \n",
    "                \"more\", \n",
    "                \"increase\", \n",
    "                \"decrease\", \n",
    "                \"less\", \n",
    "                \"short\", \n",
    "                \"long\", \n",
    "                \"greater\"\n",
    "                \"shift\",\n",
    "                \"fluctuate\",\n",
    "                \"adapt\",\n",
    "                \"grow\",\n",
    "                \"rise\"\n",
    "                \"surge\",\n",
    "                \"intensify\",\n",
    "                \"amplify\",\n",
    "                \"multiply\",\n",
    "                \"decline\",\n",
    "                \"reduce\",\n",
    "                \"drop\",\n",
    "                \"diminish\",\n",
    "                \"fall\",\n",
    "                \"lessen\"\n",
    "            ],\n",
    "            speech=[\"NOUN\", \"ADJ\", \"ADV\"], \n",
    "            threshold=0.7\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in self.main.sp_doc:\n",
    "            # Already Matched\n",
    "            if token in tokens:\n",
    "                filtered.append(token)\n",
    "            \n",
    "            # Comparative Adjective\n",
    "            # Looking for words like \"bigger\" and \"better\".\n",
    "            elif token.pos_ == \"ADJ\" and token.tag_ == \"JJR\":\n",
    "                filtered.append(token)\n",
    "                continue\n",
    "            \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70792f-d32b-4638-a929-c7499506e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraitKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            literals=[\n",
    "                \"behavior\", \n",
    "                r\"[^A-Za-z]+rate\", \n",
    "                \"color\", \n",
    "                r\"[^A-Za-z]+mass\", \n",
    "                \"size\", \n",
    "                \"length\", \n",
    "                \"pattern\", \n",
    "                \"weight\",\n",
    "                \"shape\", \n",
    "                \"efficiency\", \n",
    "                \"trait\",\n",
    "                \"ability\", \n",
    "                \"capacity\", \n",
    "                \"height\", \n",
    "                \"width\", \n",
    "                \"span\",\n",
    "                \"diet\",\n",
    "                \"feeding\",\n",
    "                \"nest\",\n",
    "                \"substrate\",\n",
    "                \"breeding\",\n",
    "                r\"[^A-Za-z]+age[^A-Za-z]+\",\n",
    "                \"lifespan\",\n",
    "                \"development\",\n",
    "                \"time\",\n",
    "                \"mating\",\n",
    "                \"fur\",\n",
    "                \"feathers\",\n",
    "                \"scales\",\n",
    "                \"skin\",\n",
    "                \"limb\",\n",
    "                \"configuration\",\n",
    "                \"dimorphism\",\n",
    "                \"capability\",\n",
    "                \"appendages\",\n",
    "                \"blood\",\n",
    "                \"regulation\",\n",
    "                \"excretion\",\n",
    "                \"luminescence\",\n",
    "                r\"[^A-Za-z]+role\",\n",
    "                \"reproduction\",\n",
    "                \"courtship\",\n",
    "                \"pollination\",\n",
    "                \"mechanism\",\n",
    "                \"sensitivity\",\n",
    "                \"resistance\"\n",
    "            ],\n",
    "            include_substring=True,\n",
    "            speech=[\"NOUN\", \"ADJ\"], \n",
    "            threshold=0.8\n",
    "        )\n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        print(\"filter_tokens\")\n",
    "        print(tokens)\n",
    "        print()\n",
    "        \n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            expanded_token = self.main.help.expand_unit(\n",
    "                il_unit=token.i, \n",
    "                ir_unit=token.i, \n",
    "                il_boundary=0, \n",
    "                ir_boundary=len(self.main.sp_doc) - 1, \n",
    "                speech=[\"PUNCT\"],\n",
    "                include=False,\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "            print(token)\n",
    "            print(expanded_token)\n",
    "            print(self.main.species.has_species(expanded_token))\n",
    "            print()\n",
    "            \n",
    "            if self.main.species.has_species(expanded_token):\n",
    "                filtered.append(token)\n",
    "        \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf8e07-a326-4fea-8615-faa5d549852d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Main:\n",
    "    def __init__(self):\n",
    "        # Tools\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        self.sp_doc = None\n",
    "\n",
    "        # Maps Character Position to Token in Document\n",
    "        # Used to handle differences between different\n",
    "        # pipelines and tools.\n",
    "        self.index_map = None\n",
    "    \n",
    "        # Parsers\n",
    "        self.species = Species(self)\n",
    "        self.traits = TraitKeywords(self)\n",
    "        self.causes = CauseKeywords(self)\n",
    "        self.changes = ChangeKeywords(self)\n",
    "        self.experiment = ExperimentKeywords(self)\n",
    "\n",
    "        # Helper\n",
    "        self.help = Help(self)\n",
    "\n",
    "    def update_doc(self, doc, verbose=False):\n",
    "        self.sp_doc = doc\n",
    "        self.index_map = self.load_index_map()\n",
    "        self.species.update(doc.text, verbose=True)\n",
    "        self.traits.update(verbose=False)\n",
    "        self.causes.update(verbose=False)\n",
    "        self.changes.update(verbose=False)\n",
    "        self.experiment.update(verbose=False)\n",
    "\n",
    "    def update_text(self, text, verbose=False):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.update_doc(self.sp_doc, verbose=verbose)\n",
    "        \n",
    "    def token_at_char(self, char_index):\n",
    "        # SpaCy Doc or Indexing Map Not Found\n",
    "        if not self.sp_doc or not self.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        if char_index in self.index_map:\n",
    "            return self.index_map[char_index]\n",
    "\n",
    "        raise Exception(\"Token Not Found\")\n",
    "        \n",
    "    def load_index_map(self):\n",
    "        # SpaCy Doc Not Found\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Map Character Index to Token\n",
    "        index_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            # char_i0 is the index of the token's starting character.\n",
    "            # char_i1 is the index of the character after the token's ending character.\n",
    "            char_i0 = token.idx\n",
    "            char_i1 = token.idx + len(token)\n",
    "        \n",
    "            for i in range(char_i0, char_i1):\n",
    "                index_map[i] = token\n",
    "            \n",
    "        return index_map\n",
    "\n",
    "    def score(self, verbose=False):\n",
    "        NUM_CATEGORIES = 4\n",
    "\n",
    "        # Requires the mention of a trait and a cause or change word.\n",
    "        # The cause or change word indicates some variation.\n",
    "        # Index 0 in Array\n",
    "        TRAIT = 0\n",
    "\n",
    "        # Requires the mention of a species and a cause or change word.\n",
    "        # The cause or change word indicates that the species is being\n",
    "        # affected or is affecting something else.\n",
    "        # Index 1 in Array\n",
    "        SPECIES = 1\n",
    "\n",
    "        # Requires a word that has been defined as \"experiment\"-related.\n",
    "        # Index 2 in Array\n",
    "        EXPERIMENT = 2\n",
    "\n",
    "        # Requires the mention of several species (more or less).\n",
    "        # Index 3 in Array\n",
    "        INTERACTION = 3\n",
    "\n",
    "        # Max # of Points of Category per Sentence (MPC)\n",
    "        # A sentence collects points from its categories.\n",
    "        # For example, a sentence could get a maximum of 2 points from one category\n",
    "        # and a maximum of 1 point from another. The MPC determines the maximum number\n",
    "        # of points a category could contribute to a sentence. To have a range of [0, 1]\n",
    "        # the maximum number of points, across categories, when added should be 1.\n",
    "        MPC = [0] * NUM_CATEGORIES\n",
    "        MPC[TRAIT] = 0.1\n",
    "        MPC[SPECIES] = 0.3\n",
    "        MPC[EXPERIMENT] = 0.3\n",
    "        MPC[INTERACTION] = 0.3\n",
    "\n",
    "        assert np.sum(MPC) == 1\n",
    "        \n",
    "        # Points per Instance of Category (PIC)\n",
    "        # Each token is evaluated to check whether a category\n",
    "        # can be given points. The number of points given, if\n",
    "        # the token is determined to be satisfactory, is the PIC.\n",
    "        # The PIC is less than or equal to the MPC for the corresponding\n",
    "        # category. The idea behind the PIC and MPC is similar to how\n",
    "        # sets work in tennis: you're not immediately awarded the full points\n",
    "        # for the set (MPC) if your opponent fails to return the ball,\n",
    "        # instead you're given a smaller # of points (PIC) that allow you to\n",
    "        # incrementally win the set (category).\n",
    "        PIC = [0] * NUM_CATEGORIES\n",
    "        PIC[TRAIT] = MPC[TRAIT]*1.0\n",
    "        PIC[SPECIES] = MPC[SPECIES]/3.0\n",
    "        PIC[EXPERIMENT] = MPC[EXPERIMENT]/1.0\n",
    "        PIC[INTERACTION] = MPC[INTERACTION]/3.0\n",
    "\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            assert PIC[i] <= MPC[i]\n",
    "\n",
    "        # Category Weights (CW)\n",
    "        # It may be helpful to weigh a certain category's fraction of total points\n",
    "        # more or less than another's. Thus, at the end, we'll take a\n",
    "        # weighted average of the category's FTP. The weights must add up to 1.\n",
    "        CW = [0] * NUM_CATEGORIES\n",
    "        CW[TRAIT] = 0.7\n",
    "        CW[SPECIES] = 0.1\n",
    "        CW[EXPERIMENT] = 0.1\n",
    "        CW[INTERACTION] = 0.1\n",
    "\n",
    "        assert np.sum(MPC) == 1\n",
    "\n",
    "        # Points\n",
    "        points = [0] * NUM_CATEGORIES\n",
    "\n",
    "        # Extracted Information\n",
    "        cause_tokens = self.causes.tokens\n",
    "        change_tokens = self.changes.tokens\n",
    "        trait_tokens = self.traits.tokens\n",
    "        species_tokens = [self.sp_doc[span.start] for span in self.species.spans]\n",
    "        experiment_tokens = self.experiment.tokens\n",
    "\n",
    "        print(f\"Cause Tokens: {cause_tokens}\")\n",
    "        print(f\"Change Tokens: {change_tokens}\")\n",
    "        print(f\"Experiment Tokens: {experiment_tokens}\")\n",
    "        print(f\"Trait Tokens: {trait_tokens}\")\n",
    "        print(f\"Species Tokens: {species_tokens}\")\n",
    "         \n",
    "        # This is used to ensure that at least three species\n",
    "        # are mentioned.\n",
    "        seen_species = {}\n",
    "\n",
    "        for sent in self.sp_doc.sents:\n",
    "            # This contains the number of points\n",
    "            # each category has accumulated in the sentence.\n",
    "            curr_points = [0] * NUM_CATEGORIES\n",
    "\n",
    "            # Contains the tokens in the sentence.\n",
    "            sent_tokens = [token for token in sent]\n",
    "\n",
    "            # This is used for the species (must have a nearby cause and/or\n",
    "            # change word).\n",
    "            sent_cause_tokens = set(sent_tokens).intersection(cause_tokens)\n",
    "            sent_change_tokens = set(sent_tokens).intersection(change_tokens)\n",
    "\n",
    "            # We don't want to visit the same species more than one\n",
    "            # in the same sentence as to avoid redundant points.\n",
    "            sent_seen_species = []\n",
    "\n",
    "            print(f\"Sentence Tokens: {sent_tokens}\")\n",
    "            print(f\"Sentence Cause Tokens: {sent_cause_tokens}\")\n",
    "            print(f\"Sentence Change Tokens: {sent_change_tokens}\")\n",
    "            \n",
    "            for token in sent_tokens:\n",
    "                # If each category has reached their maximum number of points,\n",
    "                # we can end the loop early.\n",
    "                all_maxed = True\n",
    "                for i in range(NUM_CATEGORIES):\n",
    "                    if curr_points[i] < MPC[i]:\n",
    "                        all_maxed = False\n",
    "\n",
    "                if all_maxed:\n",
    "                    break\n",
    "\n",
    "                # TRAIT CATEGORY\n",
    "                if curr_points[TRAIT] < MPC[TRAIT] and token in trait_tokens:\n",
    "                    print(\"TRAIT CATEGORY\")\n",
    "                    # To get points in the trait category, there must \n",
    "                    # be (1) a trait; and (2) a change or cause in the token's\n",
    "                    # context.\n",
    "                    token_context = set(self.help.find_unit_context(\n",
    "                        il_unit=token.i, \n",
    "                        ir_unit=token.i, \n",
    "                        il_boundary=token.sent.start, \n",
    "                        ir_boundary=token.sent.end-1, \n",
    "                        verbose=verbose)\n",
    "                    )\n",
    "                    cause_tokens_in_context = set(sent_cause_tokens).intersection(token_context)\n",
    "                    change_tokens_in_context = set(sent_change_tokens).intersection(token_context)\n",
    "\n",
    "                    print(f\"Token ({token}) Context: {token_context}\")\n",
    "                    print(f\"Cause Tokens in Context: {cause_tokens_in_context}\")\n",
    "                    print(f\"Change Tokens in Context: {change_tokens_in_context}\")\n",
    "\n",
    "                    if cause_tokens_in_context or change_tokens_in_context:\n",
    "                        curr_points[TRAIT] += PIC[TRAIT]\n",
    "\n",
    "                        print(f\"Added Points for Trait via Token '{token}'\")\n",
    "\n",
    "                    print()\n",
    "\n",
    "                # EXPERIMENT CATEGORY\n",
    "                if curr_points[EXPERIMENT] < MPC[EXPERIMENT] and token in experiment_tokens:\n",
    "                    curr_points[EXPERIMENT] += PIC[EXPERIMENT]\n",
    "\n",
    "                    print(f\"Added Points for Experiment via Token '{token}'\\n\")\n",
    "\n",
    "                # SPECIES CATEGORY\n",
    "                if token in species_tokens:\n",
    "                    # Find Species Span\n",
    "                    species_span = self.species.span_at_token(token)           \n",
    "\n",
    "                    # Updating Seen Species (in Entire Text)\n",
    "                    past_visits = 0\n",
    "\n",
    "                    # Find Previous Instance of Species (if Any)\n",
    "                    print(\"Seen Species Updated\")\n",
    "                    print(seen_species)\n",
    "                    print()\n",
    "                    \n",
    "                    seen_species_span = self.species.find_same_species(seen_species.keys(), species_span)\n",
    "                    if seen_species_span:\n",
    "                        past_visits = seen_species[seen_species_span]\n",
    "                        seen_species[seen_species_span] += 1\n",
    "                    \n",
    "                    if not past_visits:\n",
    "                        seen_species[species_span] = 1\n",
    "\n",
    "                    print(\"Seen Species Updated\")\n",
    "                    print(seen_species)\n",
    "                    print()\n",
    "                    \n",
    "                    # Checking Seen Species (in Sentence)\n",
    "                    # We only add points if it's a species that has not been seen\n",
    "                    # in the sentence. This is to avoid redundant points. \n",
    "                    # Also, if it species has not been seen at all (is_new_species),\n",
    "                    # then it cannot be a redundant species (we couldn't have seen it in the sentence\n",
    "                    # either).\n",
    "                    redundant_species = False\n",
    "\n",
    "                    if not past_visits:\n",
    "                        if self.species.find_same_species(sent_seen_species, species_span):\n",
    "                            redundant_species = True\n",
    "                    sent_seen_species.append(species_span)\n",
    "\n",
    "                    print(\"Seen Species in Sentence\")\n",
    "                    print(sent_seen_species)\n",
    "                    print()\n",
    "                    \n",
    "                    if redundant_species:\n",
    "                        continue\n",
    "\n",
    "                    # INTERACTION CATEGORY\n",
    "                    # It is helpful to have this category here because (if we've reached here)\n",
    "                    # we're dealing with a new species in the sentence.\n",
    "                    if curr_points[INTERACTION] < MPC[INTERACTION]:\n",
    "                        curr_points[INTERACTION] += PIC[INTERACTION]\n",
    "\n",
    "                        print(f\"Added Points for Interaction via Token '{token}'\\n\")\n",
    "                        \n",
    "                    if curr_points[SPECIES] < MPC[SPECIES]:\n",
    "                        # To get points in the species category, there must be \n",
    "                        # (1) a species; and (2) a change or cause in the phrase\n",
    "                        # (or clause) that the token is a part of.\n",
    "                        token_context = set(self.help.find_unit_context(\n",
    "                            il_unit=token.i, \n",
    "                            ir_unit=token.i, \n",
    "                            il_boundary=token.sent.start, \n",
    "                            ir_boundary=token.sent.end-1, \n",
    "                            verbose=verbose)\n",
    "                        )\n",
    "                        cause_tokens_in_context = set(sent_cause_tokens).intersection(token_context)\n",
    "                        change_tokens_in_context = set(sent_change_tokens).intersection(token_context)\n",
    "\n",
    "                        print(f\"Token ({token}) Context: {token_context}\")\n",
    "                        print(f\"Cause Tokens in Context: {cause_tokens_in_context}\")\n",
    "                        print(f\"Change Tokens in Context: {change_tokens_in_context}\")\n",
    "                        \n",
    "                        if cause_tokens_in_context or change_tokens_in_context:\n",
    "                            curr_points[SPECIES] += PIC[SPECIES]\n",
    "\n",
    "                            print(f\"Added Points for Species via Token '{token}'\")\n",
    "\n",
    "                        print()\n",
    "         \n",
    "            # SENTENCE DONE\n",
    "            # Add Sentence Points to Total Points\n",
    "            for category in [TRAIT, SPECIES, EXPERIMENT, INTERACTION]:\n",
    "                points[category] += min(curr_points[category], MPC[category])\n",
    "\n",
    "        # Calculating Score            \n",
    "        NUM_SENTENCES = len(list(self.sp_doc.sents))\n",
    "\n",
    "        score = 0\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            points[i] = points[i] / (MPC[i] * NUM_SENTENCES)\n",
    "            score += points[i] * CW[i]\n",
    "\n",
    "        # Enforcing 3 or More Species            \n",
    "        if len(seen_species) < 3:\n",
    "            return 0, points\n",
    "            \n",
    "        assert 0.0 <= score <= 1.0\n",
    "        \n",
    "        return score, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cdd412-e94a-4ead-9b99-a0e8edf3448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(name, save_output=False, version=\"\"):\n",
    "    # Redirect Print Statements\n",
    "    # https://stackoverflow.com/questions/7152762/how-to-redirect-print-output-to-a-file\n",
    "    if save_output:\n",
    "        initial_stdout = sys.stdout\n",
    "        f = open(f'./Print{name}{\"\" if not version else f\"-{version}\"}.txt', 'w')\n",
    "        sys.stdout = f\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "\n",
    "    # Load Dataset\n",
    "    data = load_preprocessed_dataset(name)\n",
    "\n",
    "    # We'll be running the points algorithm\n",
    "    # on the abstracts of these papers.\n",
    "    texts = list(data['Abstract'].to_numpy())\n",
    "    \n",
    "    # The scores for each paper will be stored here,\n",
    "    # we'll set this as a column of the dataframe.\n",
    "    scores = []\n",
    "    points = []\n",
    "    trait_points = []\n",
    "    species_points = []\n",
    "    experiment_points = []\n",
    "    interaction_points = []\n",
    "    \n",
    "    # Scan and Evaluate Documents\n",
    "    main = Main()\n",
    "    for i, doc in enumerate(main.sp_nlp.pipe(texts)):\n",
    "        print(f\"{i+1}/{data.shape[0]} - {data.iloc[i]['Title']}\\n\")\n",
    "        main.update_doc(doc, verbose=save_output)\n",
    "\n",
    "        # Empty string literals cause errors, so it's\n",
    "        # being handled here.\n",
    "        if not main.sp_doc or not main.species.tn_doc:\n",
    "            scores.append(0)\n",
    "        else:\n",
    "            score, _points = main.score(verbose=save_output)\n",
    "            scores.append(score)\n",
    "            points.append(_points)\n",
    "            trait_points.append(_points[0])\n",
    "            species_points.append(_points[1])\n",
    "            experiment_points.append(_points[2])\n",
    "            interaction_points.append(_points[3])\n",
    "\n",
    "        if not save_output:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    # Reset Standard Output\n",
    "    if save_output:\n",
    "        sys.stdout = initial_stdout\n",
    "        f.close()\n",
    "\n",
    "    data[\"Score\"] = scores\n",
    "    data[\"Trait Points\"] = trait_points\n",
    "    data[\"Species Points\"] = species_points\n",
    "    data[\"Experiment Points\"] = experiment_points\n",
    "    data[\"Interaction Points\"] = interaction_points\n",
    "    data.sort_values(by='Score', ascending=False, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced84b80-fd17-4774-940f-64037ee22411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score Datasets\n",
    "# dataset_names = [\"Examples\", \"Baseline-1\", \"SubA\", \"SubAFiltered\", \"SubB\", \"SubBFiltered\", \"C\", \"CFiltered\", \"D\", \"DFiltered\"]\n",
    "for name in [\"Baseline-1\"]:\n",
    "    scored_data = score_dataset(name, save_output=False, version='')\n",
    "    store_scored_dataset(scored_data, \"Baseline-2-2\", version='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf75e9b-94b7-433b-a411-72c385d6cbf7",
   "metadata": {},
   "source": [
    "# Inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78034f48-0224-4a95-b0b1-6d90cbedc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_preprocessed_dataset(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd65d6-9de2-4ed2-8cdd-2adec98f76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: In an effort to find traits, I allowed for substring matching. Meaning if a trait we were looking for\n",
    "# was 'rate' and a word had 'rate' in it, it would be recognized as a trait. If there's a species nearby as well,\n",
    "# which is not abnormal, it would be counted as a trait. However, the word 'pirate' was incorrectly counted as\n",
    "# a trait. Now, I'm using regexes (where needed) to match the whole word.\n",
    "text = data.iloc[14].Abstract\n",
    "main = Main()\n",
    "main.update_text(str(text))\n",
    "print(main.score())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0a263c55-0c94-4dd0-8dd2-17c66f9acdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/18/2025 12:24:50 - INFO - \t missing_keys: []\n",
      "06/18/2025 12:24:50 - INFO - \t unexpected_keys: []\n",
      "06/18/2025 12:24:50 - INFO - \t mismatched_keys: []\n",
      "06/18/2025 12:24:50 - INFO - \t error_msgs: []\n",
      "06/18/2025 12:24:50 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replicated experiments in artificial ponds demonstrated that an assemblage of aquatic insects competed with tadpoles of the frogs Hyla andersonii and Bufo woodhousei fowleri. We independently manipulated the presence or absence of aquatic insects, and the abundance of an anuran competitor (O or 150 Bufo w. fowleri per experimental pond), using a completely crossed design for twofactor variance analysis, and observed the responses of initially similar cohorts of Hyla andersonii tadpoles to neither, either, or both insect and anuran competitors. Insects and Bufo significantly depressed the mean individual mass at metamorphosis of Hyla froglets and the cumulative biomass of anurans leaving the ponds at metamorphosis. Neither insects nor Bufo affected the survival or larval period of Hyla. Insects also significantly reduced the mean mass of Bufo, showing that both anurans responded to competition from insects. The intensity of competition between natural densities of insects and Hyla tadpoles was comparable to the intensity of competition between Bufo and Hyla, as a density of 150 Bufo/1000 L.\n"
     ]
    }
   ],
   "source": [
    "# data.loc[data['Title'] == 'Competition Between Aquatic Insects and Vertebrates: Interaction Strength and Higher Order Interactions']\n",
    "text = data.iloc[3].Abstract\n",
    "main = Main()\n",
    "# main.update_text(str(text))\n",
    "print(text)\n",
    "# print(main.score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09abccd9-be79-4cdb-89a7-7e0617880e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: I've expanded the boundaries in which a species can be found (in order for\n",
    "# a trait to count) in both the class and find_unit_context function.\n",
    "# data.loc[data['Title'].str.contains('weevil')]\n",
    "text = data.iloc[20].Abstract\n",
    "main = Main()\n",
    "main.update_text(str(text))\n",
    "print(text)\n",
    "print(main.score())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ff766-2d9b-42cc-8b52-3294da85a697",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1d963-4b82-4f32-870e-5f1a001d3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"The use of chemical pesticides in agriculture is a critical threat to the environment. Implementing the use of biological control practices is an increasing worldwide challenge to cope with this matter. The exploitation of trait-mediated indirect interactions (TMIIs), which is an avoidance behaviour of pests when detecting possible risk, is a new and interesting pathway to follow. Ants, which are predators of many insect pests, are commonly active on plants and release several different chemical traces in the substrate, making them potential candidates for TMII-based management approaches. We tested whether semiochemicals released by two Mediterranean ants, Crematogaster scutellaris and Tapinoma nigerrimum, are able to deter the occurrence of a strongly harmful pest of tree crops, the tephritid Ceratitis capitata, which lays eggs within fruits. Using binary choice tests between a plum previously visited by ants and another used as control, we actually observed an avoidance behaviour by females of C. capitata, which results in a lower amount of progeny production, suggesting that flies can detect the chemical compounds released by ants. This study suggests that scents triggering this deterrence effect are conserved across ant subfamilies and encourages improving this research to achieve a new low-impacting control method against agricultural pests.\"\n",
    "main_1 = Main()\n",
    "main_1.update_text(text_1)\n",
    "\n",
    "text_2 = \"Larvae of the Carolina sawyer Monochamus carolinensis (Olivier) (Cerambycidae) and bark beetle larvae (Scolytidae) often simultaneously feed in phloem of recently killed pine trees. Our investigations reveal that M. carolinensis larvae may act as facultative intraguild predators of bark beetle larvae. Phloem sandwiches were used in four experiments to examine inter- and intraspecific interactions. We discovered that all sizes of M. carolinensis larvae killed bark beetle larvae. Seventy-six percent of the killed bark beetle larvae were consumed by M. carolinensis, including 58% that were entirely ingested. Cannibalism in M. carolinensis occurred in every experimental trial. Based on this evidence, M. carolinensis, and possibly related cerambycid species associated with bark beetles, are facultative intraguild predators of larvae of other phloem inhabiting species. The consequences of this behavior may have important implications for bark beetle population dynamics.\"\n",
    "main_2 = Main()\n",
    "main_2.update_text(text_2)\n",
    "\n",
    "text_3 = \"In simple, linear food chains, top predators can have positive indirect effects on basal resources by causing changes in the traits (e.g. behaviour, feeding rates) of intermediate consumers. Although less is known about trait-mediated indirect interactions (TMIIs) in more complex food webs, it has been suggested that such complexity dampens trophic cascades. We examined TMIIs between a predatory crab (Carcinus maenas) and two ecologically important basal resources, fucoid algae (Ascophyllum nodosum) and barnacles (Semibalanus balanoides), which are consumed by herbivorous (Littorina littorea) and carnivorous (Nucella lapillus) snails, respectively. Because crab predation risk suppresses snail feeding rates, we hypothesized that crabs would also shape direct and indirect interactions among the multiple consumers and resources. We found that the magnitude of TMIIs between the crab and each resource depended on the suite of intermediate consumers present in the food web. Carnivorous snails (Nucella) transmitted TMIIs between crabs and barnacles. However, crabalgae TMIIs were transmitted by both herbivorous (Littorina) and carnivorous (Nucella) snails, and these TMIIs were additive. By causing Nucella to consume fewer barnacles, crab predation risk allowed fucoids that had settled on or between barnacles to remain in the community. Hence, positive interactions between barnacles and algae caused crabalgae TMIIs to be strongest when both consumers were present. Studies of TMIIs in more realistic, reticulate food webs will be necessary for a more complete understanding of how predation risk shapes community dynamics.\"\n",
    "main_3 = Main()\n",
    "main_3.update_text(text_3)\n",
    "\n",
    "text_4 = \"Replicated experiments in artificial ponds demonstrated that an assemblage of aquatic insects competed with tadpoles of the frogs Hyla andersonii and Bufo woodhousei fowleri. We independently manipulated the presence or absence of aquatic insects, and the abundance of an anuran competitor (O or 150 Bufo w. fowleri per experimental pond), using a completely crossed design for twofactor variance analysis, and observed the responses of initially similar cohorts of Hyla andersonii tadpoles to neither, either, or both insect and anuran competitors. Insects and Bufo significantly depressed the mean individual mass at metamorphosis of Hyla froglets and the cumulative biomass of anurans leaving the ponds at metamorphosis. Neither insects nor Bufo affected the survival or larval period of Hyla. Insects also significantly reduced the mean mass of Bufo, showing that both anurans responded to competition from insects. The intensity of competition between natural densities of insects and Hyla tadpoles was comparable to the intensity of competition between Bufo and Hyla, as a density of 150 Bufo/1000 L.\"\n",
    "main_4 = Main()\n",
    "main_4.update_text(text_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b646569-a37d-40cd-8ffd-51701d56d8d0",
   "metadata": {},
   "source": [
    "## 1. Remove Extra Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd954e-dafc-4add-9ae0-a95476207488",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert main_1.help.remove_extra_spaces(\"     \\n\\tHello, world! \") == \"Hello, world!\"\n",
    "assert main_1.help.remove_extra_spaces(\"Hello, world!\") == \"Hello, world!\"\n",
    "assert main_1.help.remove_extra_spaces(\" H  e\\nl l\\to,\\rw\\no\\tr\\rl\\nd! \") == \"H e l l o, w o r l d!\"\n",
    "assert main_1.help.remove_extra_spaces(\"     \\n\\t\\r   \") == \"\"\n",
    "assert main_1.help.remove_extra_spaces(\"HelloWorld\") == \"HelloWorld\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaf722-6ac8-49b3-98dd-50060e2fadcf",
   "metadata": {},
   "source": [
    "## 2. Remove Outer Non-Alphanumeric Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87dc59-c334-4ebb-acaf-616543013020",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert main_1.help.remove_outer_non_alnum(\"HELLO\") == \"HELLO\"\n",
    "assert main_1.help.remove_outer_non_alnum(\"Hello, world!\") == \"Hello, world\"\n",
    "assert main_1.help.remove_outer_non_alnum(\" H\\ne\\nl\\nl\\no,\\nw\\no\\nr\\nl\\nd! \") == \"H\\ne\\nl\\nl\\no,\\nw\\no\\nr\\nl\\nd\"\n",
    "assert main_1.help.remove_outer_non_alnum(\"?!~(!+(@*\") == \"\"\n",
    "assert main_1.help.remove_outer_non_alnum(\"123ABC123\") == \"123ABC123\"\n",
    "assert main_1.help.remove_outer_non_alnum(\"123456789\") == \"123456789\"\n",
    "assert main_1.help.remove_outer_non_alnum(\"123@()(##@456789!@*(#@!\") == \"123@()(##@456789\"\n",
    "assert main_1.help.remove_outer_non_alnum(\"\\\\HeLLo, WorLd\\\\100\") == \"HeLLo, WorLd\\\\100\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54d581-aeaa-4057-bdcd-e4d07154adf3",
   "metadata": {},
   "source": [
    "## 3. Singularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d0f4c-077e-4375-ab07-fae87ed0b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert main_1.help.singularize(\"Dogs\") == [\"dog\"]\n",
    "assert main_1.help.singularize(\"SpEcIeS\") == [\"species\"]\n",
    "assert main_1.help.singularize(\"Black Cats\") == [\"black cat\"]\n",
    "assert main_1.help.singularize(\"red bird\") == [\"red bird\"]\n",
    "assert main_1.help.singularize(\"wolves\") == [\"wolf\", \"wolfe\"]\n",
    "assert main_1.help.singularize(\"bonoboes\") == [\"bonobo\"]\n",
    "assert main_1.help.singularize(\"bonobos\") == [\"bonobo\"]\n",
    "assert main_1.help.singularize(\"canaries\") == [\"canary\"]\n",
    "assert main_1.help.singularize(\"finches\") == [\"finch\"]\n",
    "assert main_1.help.singularize(\"cactuses\") == [\"cactus\"]\n",
    "assert main_1.help.singularize(\"octopi\") == [\"octopus\"]\n",
    "assert main_1.help.singularize(\"goose\") == [\"goose\"]\n",
    "assert main_1.help.singularize(\" \") == [\" \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff29c6d1-abee-4c2b-8b22-81ca7e96c6dd",
   "metadata": {},
   "source": [
    "## 4. Pluralize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf83c6-3440-41d1-8031-c0ce1faa9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert main_1.help.pluralize(\"Blue bird\") == [\"blue birds\"]\n",
    "assert main_1.help.pluralize(\"brown-tailed squirrels\") == [\"brown-tailed squirrels\"]\n",
    "assert main_1.help.pluralize(\"wolf\") == [\"wolfs\", \"wolves\"]\n",
    "assert main_1.help.pluralize(\"staff\") == [\"staffs\"]\n",
    "assert main_1.help.pluralize(\"butterfly\") == [\"butterflies\"]\n",
    "assert main_1.help.pluralize(\"octopus\") == [\"octopuses\", \"octopi\"]\n",
    "assert main_1.help.pluralize(\"monkey\") == [\"monkeys\"]\n",
    "assert main_1.help.pluralize(\"ox\") == [\"oxen\"]\n",
    "assert main_1.help.pluralize(\"mice\") == [\"mice\"]\n",
    "assert main_1.help.pluralize(\"goose\") == [\"geese\"]\n",
    "assert main_1.help.pluralize(\" \") == [\" \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f04248b-f398-4d98-af58-78d7ead9b43f",
   "metadata": {},
   "source": [
    "## 5. Expand Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469f27e-b010-4fc9-bd20-cbcc9be34d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    {\n",
    "        \"unit\": main_1.sp_doc[4],\n",
    "        \"il_unit\": 4,\n",
    "        \"ir_unit\": 4,\n",
    "        \"il_boundary\": 0,\n",
    "        \"ir_boundary\": len(main_1.sp_doc),\n",
    "        \"include\": True,\n",
    "        \"speech\": [\"NOUN\", \"ADP\", \"ADJ\", \"VERB\"],\n",
    "        \"literals\": [],\n",
    "        \"result_text\": \"use of chemical pesticides in agriculture\",\n",
    "        \"main\": main_1\n",
    "    },\n",
    "    {\n",
    "        \"unit\": main_1.sp_doc[19],\n",
    "        \"il_unit\": 19,\n",
    "        \"ir_unit\": 19,\n",
    "        \"il_boundary\": 0,\n",
    "        \"ir_boundary\": 20,\n",
    "        \"include\": True,\n",
    "        \"speech\": [\"NOUN\", \"ADJ\", \"VERB\"],\n",
    "        \"literals\": [],\n",
    "        \"result_text\": \"biological control\",\n",
    "        \"main\": main_1\n",
    "    },\n",
    "    {\n",
    "        \"unit\": main_2.sp_doc[0:5],\n",
    "        \"il_unit\": 0,\n",
    "        \"ir_unit\": 4,\n",
    "        \"il_boundary\": 0,\n",
    "        \"ir_boundary\": 10,\n",
    "        \"include\": False,\n",
    "        \"speech\": [\"PUNCT\", \"SYM\"],\n",
    "        \"literals\": [],\n",
    "        \"result_text\": \"Larvae of the Carolina sawyer Monochamus carolinensis\",\n",
    "        \"main\": main_2\n",
    "    },\n",
    "    {\n",
    "        \"unit\": main_3.sp_doc[0],\n",
    "        \"il_unit\": 0,\n",
    "        \"ir_unit\": 0,\n",
    "        \"il_boundary\": 0,\n",
    "        \"ir_boundary\": 22,\n",
    "        \"include\": False,\n",
    "        \"speech\": [],\n",
    "        \"literals\": [],\n",
    "        \"result_text\": \"In simple, linear food chains, top predators can have positive indirect effects on basal resources by causing changes in the traits\",\n",
    "        \"main\": main_3\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test in enumerate(tests):\n",
    "    unit = test[\"unit\"]\n",
    "    expanded_unit = test[\"main\"].help.expand_unit(\n",
    "        il_unit=test[\"il_unit\"],\n",
    "        ir_unit=test[\"ir_unit\"],\n",
    "        il_boundary=test[\"il_boundary\"],\n",
    "        ir_boundary=test[\"ir_boundary\"],\n",
    "        speech=test[\"speech\"],\n",
    "        literals=test[\"literals\"],\n",
    "        include=test[\"include\"]\n",
    "    )\n",
    "\n",
    "    print(f\"TEST {i+1}:\")\n",
    "    print(f\"Unit: '{unit}'\")\n",
    "    print(f\"Expanded Unit: '{expanded_unit}'\")\n",
    "    print()\n",
    "    \n",
    "    assert expanded_unit.text == test[\"result_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70390e9-02b2-4d1d-bea9-d082251b6410",
   "metadata": {},
   "source": [
    "## 6. Contract Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a25dc-4a30-47d0-bdaa-2842fd9451c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    {\n",
    "        \"unit\": main_4.sp_doc[2:6],\n",
    "        \"il_unit\": 2,\n",
    "        \"ir_unit\": 5,\n",
    "        \"include\": False,\n",
    "        \"speech\": [\"CCONJ\", \"DET\", \"ADJ\", \"ADP\", \"VERB\", \"ADV\", \"SCONJ\"],\n",
    "        \"literals\": [],\n",
    "        \"result_text\": \"ponds\",\n",
    "        \"main\": main_4\n",
    "    },\n",
    "    {\n",
    "        \"unit\": main_4.sp_doc[2:6],\n",
    "        \"il_unit\": 2,\n",
    "        \"ir_unit\": 5,\n",
    "        \"include\": True,\n",
    "        \"speech\": [\"CCONJ\", \"DET\", \"ADJ\", \"ADP\", \"VERB\", \"ADV\", \"SCONJ\"],\n",
    "        \"literals\": [],\n",
    "        \"result_text\": \"in artificial ponds demonstrated\",\n",
    "        \"main\": main_4\n",
    "    },\n",
    "    {\n",
    "        \"unit\": main_3.sp_doc[23:30],\n",
    "        \"il_unit\": 23,\n",
    "        \"ir_unit\": 29,\n",
    "        \"include\": False,\n",
    "        \"speech\": [\"X\", \"PUNCT\", \"SYM\"],\n",
    "        \"literals\": [],\n",
    "        \"result_text\": \"behaviour, feeding rates\",\n",
    "        \"main\": main_3\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test in enumerate(tests):\n",
    "    unit = test[\"unit\"]\n",
    "    contracted_unit = test[\"main\"].help.contract_unit(\n",
    "        il_unit=test[\"il_unit\"],\n",
    "        ir_unit=test[\"ir_unit\"],\n",
    "        speech=test[\"speech\"],\n",
    "        literals=test[\"literals\"],\n",
    "        include=test[\"include\"]\n",
    "    )\n",
    "\n",
    "    print(f\"TEST {i+1}:\")\n",
    "    print(f\"Unit: '{unit}'\")\n",
    "    print(f\"Contracted Unit: '{contracted_unit}'\")\n",
    "    print()\n",
    "    \n",
    "    assert contracted_unit.text == test[\"result_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a858af-f035-4814-b528-7f331461e13b",
   "metadata": {},
   "source": [
    "## 7. Find Unit Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f67b94-d8d3-488c-be79-77932f488bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    {\n",
    "        \"unit\": main_3.sp_doc[23:30],\n",
    "        \"il_unit\": 23,\n",
    "        \"ir_unit\": 29,\n",
    "        \"il_boundary\": 0,\n",
    "        \"ir_boundary\": len(main_3.sp_doc),\n",
    "        \"result_text\": \"(e.g. behaviour, feeding rates)\",\n",
    "        \"main\": main_3\n",
    "    },\n",
    "    {\n",
    "        \"unit\": main_3.sp_doc[23],\n",
    "        \"il_unit\": 23,\n",
    "        \"ir_unit\": 23,\n",
    "        \"il_boundary\": 0,\n",
    "        \"ir_boundary\": len(main_3.sp_doc),\n",
    "        \"result_text\": \"(e.g. behaviour, feeding rates)\",\n",
    "        \"main\": main_3\n",
    "    },\n",
    "    {\n",
    "        \"unit\": main_3.sp_doc[25],\n",
    "        \"il_unit\": 25,\n",
    "        \"ir_unit\": 25,\n",
    "        \"il_boundary\": 0,\n",
    "        \"ir_boundary\": len(main_3.sp_doc),\n",
    "        \"result_text\": \"(e.g. behaviour, feeding rates)\",\n",
    "        \"main\": main_3\n",
    "    },\n",
    "    {\n",
    "        \"unit\": main_1.sp_doc[48],\n",
    "        \"il_unit\": 48,\n",
    "        \"ir_unit\": 48,\n",
    "        \"il_boundary\": 0,\n",
    "        \"ir_boundary\": len(main_1.sp_doc),\n",
    "        \"result_text\": [\"avoidance\", \"behaviour\", \"of\", \"pests\"],\n",
    "        \"main\": main_1\n",
    "    },\n",
    "    {\n",
    "        \"unit\": main_2.sp_doc[15],\n",
    "        \"il_unit\": 15,\n",
    "        \"ir_unit\": 15,\n",
    "        \"il_boundary\": 0,\n",
    "        \"ir_boundary\": len(main_2.sp_doc),\n",
    "        \"result_text\": [\"bark\", \"beetle\", \"larvae\", \"often\", \"simultaneously\", \"feed\", \"in\", \"phloem\", \"of\", \"recently\", \"killed\", \"pine\", \"trees\"],\n",
    "        \"main\": main_2\n",
    "    },\n",
    "    {\n",
    "        \"unit\": main_2.sp_doc[20],\n",
    "        \"il_unit\": 20,\n",
    "        \"ir_unit\": 20,\n",
    "        \"il_boundary\": 0,\n",
    "        \"ir_boundary\": len(main_2.sp_doc),\n",
    "        \"result_text\": [\"bark\", \"beetle\", \"larvae\", \"often\", \"simultaneously\", \"feed\", \"in\", \"phloem\", \"of\", \"recently\", \"killed\", \"pine\", \"trees\"],\n",
    "        \"main\": main_2\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test in enumerate(tests):\n",
    "    unit = test[\"unit\"]\n",
    "    unit_context = test[\"main\"].help.find_unit_context(\n",
    "        il_unit=test[\"il_unit\"],\n",
    "        ir_unit=test[\"ir_unit\"],\n",
    "        il_boundary=test[\"il_boundary\"],\n",
    "        ir_boundary=test[\"ir_boundary\"],\n",
    "    )\n",
    "\n",
    "    print(f\"TEST {i+1}:\")\n",
    "    print(f\"Unit: {unit}\")\n",
    "    print(f\"Unit Context Unit: {unit_context}\")\n",
    "    print()\n",
    "\n",
    "    if isinstance(unit_context, list):\n",
    "        assert [t.text for t in unit_context] == test[\"result_text\"]\n",
    "    else:\n",
    "        assert unit_context.text == test[\"result_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f9c7b-2e5b-4558-9dac-ad5a3480e3f4",
   "metadata": {},
   "source": [
    "## 8. Load Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801875e-c110-4b3e-9ce3-b7b51c205d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token in main_1.sp_doc:\n",
    "#     if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "#         print(token.i, token)\n",
    "\n",
    "expected_main_1_species = [51, 66, 70, 73, 74, 80, 110, 112, 113, 115, 116, 128, 130, 134, 135, 136, 154, 170, 171, 185, 193, 207, 225]\n",
    "expected_main_1_species.sort()\n",
    "\n",
    "actual_main_1_species = [t.i for t in main_1.species.tokens]\n",
    "actual_main_1_species.sort()\n",
    "\n",
    "print(\"TEST 1:\")\n",
    "print(f\"You've Identified: {expected_main_1_species}\")\n",
    "print(f\"Identified Species: {actual_main_1_species}\")\n",
    "print(f\"Difference: {set(expected_main_1_species) - set(actual_main_1_species)}\")\n",
    "print()\n",
    "assert set(actual_main_1_species) >= set(expected_main_1_species)\n",
    "\n",
    "# for token in main_2.sp_doc:\n",
    "#     if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "#         print(token.i, token)\n",
    "\n",
    "expected_main_2_species = [0, 3, 4, 5, 6, 8, 11, 14, 15, 16, 18, 29, 35, 36, 37, 43, 46, 47, 69, 70, 71, 74, 75, 84, 85, 86, 90, 91, 104, 116, 117, 127, 132, 134, 152]\n",
    "expected_main_2_species.sort()\n",
    "\n",
    "actual_main_2_species = [t.i for t in main_2.species.tokens]\n",
    "actual_main_2_species.sort()\n",
    "\n",
    "print(\"TEST 2:\")\n",
    "print(f\"You've Identified: {expected_main_2_species}\")\n",
    "print(f\"Identified Species: {actual_main_2_species}\")\n",
    "print(f\"Difference: {set(expected_main_2_species) - set(actual_main_2_species)}\")\n",
    "print()\n",
    "assert set(actual_main_2_species) >= set(expected_main_2_species)\n",
    "\n",
    "# for token in main_3.sp_doc:\n",
    "#     if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "#         print(token.i, token)\n",
    "\n",
    "expected_main_3_species = [8, 70, 72, 73, 83, 85, 86, 89, 91, 92, 101, 102, 107, 108, 110, 115, 119, 126, 150, 168, 170, 175, 177, 181, 183, 191, 196, 198, 208, 212, 214, 225, 237, 239, 241, 243]\n",
    "expected_main_3_species.sort()\n",
    "\n",
    "actual_main_3_species = [t.i for t in main_3.species.tokens]\n",
    "actual_main_3_species.sort()\n",
    "\n",
    "print(\"TEST 3:\")\n",
    "print(f\"You've Identified: {expected_main_3_species}\")\n",
    "print(f\"Identified Species: {actual_main_3_species}\")\n",
    "print(f\"Difference: {set(expected_main_3_species) - set(actual_main_3_species)}\")\n",
    "print()\n",
    "assert set(actual_main_3_species) >= set(expected_main_3_species)\n",
    "\n",
    "# for token in main_4.sp_doc:\n",
    "#     if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "#         print(token.i, token)\n",
    "\n",
    "expected_main_4_species = [11, 14, 17, 18, 19, 21, 22, 23, 34, 47, 48, 49, 76, 78, 91, 93, 103, 104, 110, 118, 120, 128, 130, 138, 143, 148, 158, 160, 170, 172, 179]\n",
    "expected_main_4_species.sort()\n",
    "\n",
    "actual_main_4_species = [t.i for t in main_4.species.tokens]\n",
    "actual_main_4_species.sort()\n",
    "\n",
    "print(\"TEST 4:\")\n",
    "print(f\"You've Identified: {expected_main_4_species}\")\n",
    "print(f\"Identified Species: {actual_main_4_species}\")\n",
    "print(f\"Difference: {set(expected_main_4_species) - set(actual_main_4_species)}\")\n",
    "print()\n",
    "assert set(actual_main_4_species) >= set(expected_main_4_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe1395-f0b9-46ca-bc1b-5be23f463396",
   "metadata": {},
   "source": [
    "## 9. Find Same Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89132eaa-4dc8-42bb-a019-a5d2a38fc8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1\n",
    "A = main_1.species.spans[0:5]\n",
    "b = main_1.species.spans[6]\n",
    "\n",
    "print(\"TEST 1\")\n",
    "print(f\"A: {A}\")\n",
    "print(f\"b: {b}\")\n",
    "\n",
    "result = main_1.species.find_same_species(A, b)\n",
    "print(f\"Result: {result}\\n\")\n",
    "assert result == A[1]\n",
    "\n",
    "# Test 2\n",
    "A = main_1.species.spans[0:10]\n",
    "b = main_1.species.spans[6]\n",
    "\n",
    "print(\"TEST 2\")\n",
    "print(f\"A: {A}\")\n",
    "print(f\"b: {b}\")\n",
    "\n",
    "result = main_1.species.find_same_species(A, b)\n",
    "print(f\"Result: {result}\\n\")\n",
    "assert result == A[6]\n",
    "\n",
    "# Test 3\n",
    "A = main_1.species.spans[0:3]\n",
    "b = main_1.species.spans[3]\n",
    "\n",
    "print(\"TEST 3\")\n",
    "print(f\"A: {A}\")\n",
    "print(f\"b: {b}\")\n",
    "\n",
    "result = main_1.species.find_same_species(A, b)\n",
    "print(f\"Result: {result}\\n\")\n",
    "assert result == A[0]\n",
    "\n",
    "# Test 4\n",
    "A = main_1.species.spans[14:20]\n",
    "b = main_1.species.spans[13]\n",
    "\n",
    "print(\"TEST 4\")\n",
    "print(f\"A: {A}\")\n",
    "print(f\"b: {b}\")\n",
    "\n",
    "result = main_1.species.find_same_species(A, b)\n",
    "print(f\"Result: {result}\\n\")\n",
    "assert result == A[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6cdce8-a796-49c7-b66c-395624937d72",
   "metadata": {},
   "source": [
    "## 10. Experiment Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0767b3-544a-4201-8c98-da45b385a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for token in main_1.sp_doc:\n",
    "# #     if token.pos_ not in [\"PART\", \"ADP\", \"DET\"]:\n",
    "# #         print(token.i, token)\n",
    "\n",
    "expected_main_1_experiments = [103, 147, 163, 174, 183, 196, 222, ]\n",
    "expected_main_1_experiments.sort()\n",
    "\n",
    "actual_main_1_experiments = [t.i for t in main_1.experiment.tokens]\n",
    "actual_main_1_experiments.sort()\n",
    "\n",
    "print(\"TEST 1:\")\n",
    "print(f\"You've Identified: {expected_main_1_experiments}\")\n",
    "print(f\"Identified Experimental Words: {actual_main_1_experiments}\")\n",
    "print(f\"Identified Experimental Words (Text): {main_1.experiment.tokens}\")\n",
    "print(f\"Difference: {set(expected_main_1_experiments) - set(actual_main_1_experiments)}\")\n",
    "print()\n",
    "assert set(actual_main_1_experiments) >= set(expected_main_1_experiments)\n",
    "\n",
    "# # for token in main_2.sp_doc:\n",
    "# #     if token.pos_ not in [\"PART\", \"ADP\", \"DET\"]:\n",
    "# #         print(token.i, token)\n",
    "\n",
    "expected_main_2_experiments = [32, 55, 57, 64, 109, 114,]\n",
    "expected_main_2_experiments.sort()\n",
    "\n",
    "actual_main_2_experiments = [t.i for t in main_2.experiment.tokens]\n",
    "actual_main_2_experiments.sort()\n",
    "\n",
    "print(\"TEST 2:\")\n",
    "print(f\"You've Identified: {expected_main_2_experiments}\")\n",
    "print(f\"Identified Experimental Words: {actual_main_2_experiments}\")\n",
    "print(f\"Identified Experimental Words (Text): {main_2.experiment.tokens}\")\n",
    "print(f\"Difference: {set(expected_main_2_experiments) - set(actual_main_2_experiments)}\")\n",
    "print()\n",
    "assert set(actual_main_2_experiments) >= set(expected_main_2_experiments)\n",
    "\n",
    "# # for token in main_3.sp_doc:\n",
    "# #     if token.pos_ not in [\"PART\", \"ADP\", \"DET\"]:\n",
    "# #         print(token.i, token)\n",
    "\n",
    "expected_main_3_experiments = [56, 65, 124, 142, 254]\n",
    "expected_main_3_experiments.sort()\n",
    "\n",
    "actual_main_3_experiments = [t.i for t in main_3.experiment.tokens]\n",
    "actual_main_3_experiments.sort()\n",
    "\n",
    "print(\"TEST 3:\")\n",
    "print(f\"You've Identified: {expected_main_3_experiments}\")\n",
    "print(f\"Identified Experimental Words: {actual_main_3_experiments}\")\n",
    "print(f\"Identified Experimental Words (Text): {main_3.experiment.tokens}\")\n",
    "print(f\"Difference: {set(expected_main_3_experiments) - set(actual_main_3_experiments)}\")\n",
    "print()\n",
    "assert set(actual_main_3_experiments) >= set(expected_main_3_experiments)\n",
    "\n",
    "# for token in main_4.sp_doc:\n",
    "#     if token.pos_ not in [\"PART\", \"ADP\", \"DET\"]:\n",
    "#         print(token.i, token)\n",
    "\n",
    "expected_main_4_experiments = [1, 5, 27, 51, 65, 68, 140, 163]\n",
    "expected_main_4_experiments.sort()\n",
    "\n",
    "actual_main_4_experiments = [t.i for t in main_4.experiment.tokens]\n",
    "actual_main_4_experiments.sort()\n",
    "\n",
    "print(\"TEST 4:\")\n",
    "print(f\"You've Identified: {expected_main_4_experiments}\")\n",
    "print(f\"Identified Experimental Words: {actual_main_4_experiments}\")\n",
    "print(f\"Identified Experimental Words (Text): {main_4.experiment.tokens}\")\n",
    "print(f\"Difference: {set(expected_main_4_experiments) - set(actual_main_4_experiments)}\")\n",
    "print()\n",
    "assert set(actual_main_4_experiments) >= set(expected_main_4_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec6d7b-c450-4913-95a6-c85664a0010a",
   "metadata": {},
   "source": [
    "## 11. Cause Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfe7dc-ea92-4f79-8175-adcef829f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    {\n",
    "        \"expected_keywords\": [121, 203, 220],\n",
    "        \"main\": main_1\n",
    "    },\n",
    "    {\n",
    "        \"expected_keywords\": [],\n",
    "        \"main\": main_2\n",
    "    },\n",
    "    {\n",
    "        \"expected_keywords\": [13, 18, 19, 118, 207, 240],\n",
    "        \"main\": main_3\n",
    "    },\n",
    "    {\n",
    "        \"expected_keywords\": [95, 121, 133],\n",
    "        \"main\": main_4\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test in enumerate(tests):\n",
    "    actual_keywords = [t.i for t in test[\"main\"].causes.tokens]\n",
    "    actual_keywords.sort()\n",
    "    \n",
    "    print(f\"TEST {i+1}:\")\n",
    "    print(f\"You've Identified: {test['expected_keywords']}\")\n",
    "    print(f\"Identified Cause Words: {actual_keywords}\")\n",
    "    print(f\"Identified Cause Words (Text): {test['main'].causes.tokens}\")\n",
    "    print(f\"Difference: {set(test['expected_keywords']) - set(actual_keywords)}\")\n",
    "    print()\n",
    "    assert set(actual_keywords) >= set(test['expected_keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94d75b-b875-4e7a-8496-eb92efd67dc9",
   "metadata": {},
   "source": [
    "## 12. Change Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e61ae-958d-4209-83d0-24024061c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    {\n",
    "        \"expected_keywords\": [177],\n",
    "        \"main\": main_1\n",
    "    },\n",
    "    {\n",
    "        \"expected_keywords\": [],\n",
    "        \"main\": main_2\n",
    "    },\n",
    "    {\n",
    "        \"expected_keywords\": [35, 48, 211, 269],\n",
    "        \"main\": main_3\n",
    "    },\n",
    "    {\n",
    "        \"expected_keywords\": [],\n",
    "        \"main\": main_4\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test in enumerate(tests):\n",
    "    actual_keywords = [t.i for t in test[\"main\"].changes.tokens]\n",
    "    actual_keywords.sort()\n",
    "    \n",
    "    print(f\"TEST {i+1}:\")\n",
    "    print(f\"You've Identified: {test['expected_keywords']}\")\n",
    "    print(f\"Identified Change Words: {actual_keywords}\")\n",
    "    print(f\"Identified Change Words (Text): {test['main'].changes.tokens}\")\n",
    "    print(f\"Difference: {set(test['expected_keywords']) - set(actual_keywords)}\")\n",
    "    print()\n",
    "    assert set(actual_keywords) >= set(test['expected_keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4610ba-db67-4b12-9d10-50102b97ca37",
   "metadata": {},
   "source": [
    "## 13. Trait Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb9f96-e7e7-48dc-b124-f28558850713",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    {\n",
    "        \"expected_keywords\": [49, 166],\n",
    "        \"main\": main_1\n",
    "    },\n",
    "    {\n",
    "        \"expected_keywords\": [67],\n",
    "        \"main\": main_2\n",
    "    },\n",
    "    {\n",
    "        \"expected_keywords\": [121],\n",
    "        \"main\": main_3\n",
    "    },\n",
    "    {\n",
    "        \"expected_keywords\": [99, 108, 136],\n",
    "        \"main\": main_4\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test in enumerate(tests):\n",
    "    actual_keywords = [t.i for t in test[\"main\"].traits.tokens]\n",
    "    actual_keywords.sort()\n",
    "    \n",
    "    print(f\"TEST {i+1}:\")\n",
    "    print(f\"You've Identified: {test['expected_keywords']}\")\n",
    "    print(f\"Identified Trait Words: {actual_keywords}\")\n",
    "    print(f\"Identified Trait Words (Text): {test['main'].traits.tokens}\")\n",
    "    print(f\"Difference: {set(test['expected_keywords']) - set(actual_keywords)}\")\n",
    "    print()\n",
    "    assert set(actual_keywords) >= set(test['expected_keywords'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
