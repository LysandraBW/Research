{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb531e7d-d086-4971-ad82-4ebc0061de98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "%run -i \"../utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9161e3-67f0-4d3f-8b41-bbbc13a6a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species:\n",
    "    def __init__(self, scanner):\n",
    "        self.scanner = scanner\n",
    "        self.species_spans = None\n",
    "        self.species_indices = None\n",
    "\n",
    "    def update(self):\n",
    "        if not self.scanner.sp_doc or not self.scanner.tn_doc:\n",
    "            return\n",
    "        # print(\"Updating Species Indices and Spans\")\n",
    "        t0 = time.time()\n",
    "        self.species_spans, self.species_indices = self.load_species_spans()\n",
    "        t1 = time.time()\n",
    "        # print(f\"Load Species Indices and Span: {t1-t0}s\")\n",
    "        \n",
    "    def load_species_spans(self):\n",
    "        spans = []\n",
    "        indices = []\n",
    "        print(\"Species Spans\")\n",
    "        print(self.scanner.tn_doc.ents)\n",
    "        for species_span in self.scanner.tn_doc.ents:\n",
    "            l_species_i = species_span[0].i\n",
    "            r_species_i = species_span[-1].i\n",
    "            # print(f\"Species Span: '{species_span}'\")\n",
    "            # print(f\"Species L Index: {l_species_idx}\")\n",
    "            # print(f\"Species R Index: {r_species_idx}\")\n",
    "            # print(f\"Token Map Keys: {self.scanner.tk_map.keys()}\")\n",
    "            # print(f\"TN Text: {self.scanner.tn_doc.text}\")\n",
    "            # print(f\"SP Text: {self.scanner.sp_doc.text}\")\n",
    "            span = self.scanner.tn_doc[l_species_i:r_species_i+1]\n",
    "            spans.append(span)\n",
    "            indices += [token.i for token in span]\n",
    "            \n",
    "        return (spans, indices)\n",
    "\n",
    "    def is_species(self, token):\n",
    "        return token.i in self.species_indices\n",
    "        \n",
    "    def has_species(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.species_indices:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86af113-a5e5-4add-bcc7-aef2db449a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    def __init__(self, scanner, literals, pos_types, threshold=0.7):\n",
    "        self.scanner = scanner\n",
    "        self.literals = literals\n",
    "        self.threshold = threshold\n",
    "        self.pos_types = pos_types\n",
    "        self.keywords = [self.scanner.sp_nlp(literal) for literal in self.literals]\n",
    "        self.keyword_indices = []\n",
    "\n",
    "    def update(self):\n",
    "        if not self.scanner.sp_doc or not self.scanner.sp_nlp:\n",
    "            return\n",
    "        # print(\"Updating Keyword Indices\")\n",
    "        t0 = time.time()\n",
    "        self.keyword_indices = self.load_keyword_indices()\n",
    "        t1 = time.time()\n",
    "        # print(f\"Keyword Indices: {t1-t0}s\")\n",
    "        \n",
    "    def is_keyword(self, token):\n",
    "        return token.i in self.keyword_indices\n",
    "\n",
    "    def has_keyword(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.keyword_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_keyword_indices(self):\n",
    "        indices = []\n",
    "        for token in self.scanner.sp_doc:\n",
    "            if token.pos_ not in self.pos_types or self.do_not_check(token):\n",
    "                continue\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.literals:\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.scanner.sp_nlp(token.lemma_)\n",
    "            for keyword in self.keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > self.threshold:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def find_keyword_indices(self, tokens, verbose=False):\n",
    "        # if verbose:\n",
    "            # print(\"Looking for Indices:\")\n",
    "            # print(f\"Tokens: {tokens}\")\n",
    "        \n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            # print(f\"Token: {token}, Token Part of Speech (PoS): {token.pos_}, Do Not Check: {self.do_not_check(token)}\")\n",
    "            if token.pos_ not in self.pos_types or self.do_not_check(token):\n",
    "                # print(\"\\tToken Doesn't Count, Continue\")\n",
    "                continue\n",
    "            # Fast Check\n",
    "            # print(f\"Token Lemma: {token.lemma_} v. Literals: {self.literals}\")\n",
    "            if token.lemma_ in self.literals:\n",
    "                # print(\"\\tToken in Literals, Add and Continue\")\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.scanner.sp_nlp(token.lemma_)\n",
    "            for keyword in self.keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                # print(f\"'{lemma}' and '{keyword}' Similarity = {similarity} > {self.threshold}\")\n",
    "                if similarity > self.threshold:\n",
    "                    # print(\"\\tTrue: Add and Continue\")\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def do_not_check(self, token):\n",
    "        return len(token) <= 5 or re.match('^[\\w]+$', token.text) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b9437d-5b6a-4928-8a33-b530f35f4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, scanner):\n",
    "        super().__init__(scanner, {\"increase\", \"decrease\", \"change\", \"shift\", \"cause\", \"produce\"}, [\"NOUN\", \"VERB\"], 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51fbf812-860c-498c-a700-5cd491724bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class References:\n",
    "    def __init__(self, scanner, texts=None):\n",
    "        self.scanner = scanner\n",
    "        self.predictions = None\n",
    "        self.cluster_map = None\n",
    "        self.text_size_in_tokens = 100\n",
    "        if texts:\n",
    "            self.update(texts)\n",
    "\n",
    "    def update(self, text, verbose=False):\n",
    "        if not self.scanner.tn_doc:\n",
    "            return\n",
    "        # print(\"Updating Predictions\")\n",
    "        # t0 = time.time()\n",
    "        \n",
    "        self.predictions = self.scanner.fcoref.predict(texts=[text])\n",
    "        # t1 = time.time()\n",
    "        # print(f\"Predictions: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Updating Cluster Map\")\n",
    "        # t0 = time.time()\n",
    "        self.cluster_map = self.load_cluster_map(self.predictions, verbose=verbose)\n",
    "        # t1 = time.time()\n",
    "        # print(f\"Cluster Map: {t1-t0}s\")\n",
    "\n",
    "    def load_cluster_map(self, predictions, verbose=False):\n",
    "        if verbose:\n",
    "            print(f\"Load Cluster Map:\")\n",
    "        \n",
    "        cluster_map = {}\n",
    "        for prediction in predictions:\n",
    "            clusters = prediction.get_clusters(as_strings=False)\n",
    "            if verbose:\n",
    "                print(f\"Cluster:\\n{prediction.get_clusters(as_strings=True)}\")\n",
    "            \n",
    "            for cluster in clusters:\n",
    "                # It's a cluster of spans,\n",
    "                # but instead it'll be 'converted' into a cluster of tokens.\n",
    "                token_cluster = []\n",
    "            \n",
    "                for span in cluster:\n",
    "                    span_words = self.scanner.tn_doc.text[span[0]:span[1]].split()\n",
    "                    # print(span_words)\n",
    "                    \n",
    "                    word_index = span[0]\n",
    "                    for i in range(len(span_words)):\n",
    "                        word = span_words[i]\n",
    "                        if word_index not in self.scanner.tk_map:\n",
    "                            continue\n",
    "                        token_cluster.append(self.scanner.tk_map[word_index])\n",
    "                        word_index += len(word) + 1\n",
    "\n",
    "                # print(token_cluster)\n",
    "                    \n",
    "                # Mapping\n",
    "                for token in token_cluster:\n",
    "                    cluster_map[token.i] = list(filter(lambda t: t != token, token_cluster))\n",
    "        if verbose:\n",
    "            print()\n",
    "        return cluster_map\n",
    "            \n",
    "    def get_references(self, tokens):\n",
    "        refs = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.cluster_map:\n",
    "                refs += self.cluster_map[index]\n",
    "        return refs\n",
    "\n",
    "    def same_reference(self, token_a, token_b, verbose=False):\n",
    "        if verbose:\n",
    "            print(f\"\\t\\t\\tToken A: {token_a} v. Token B: {token_b}\")\n",
    "        if token_a.i in self.cluster_map and token_b in self.cluster_map[token_a.i]:\n",
    "            if verbose:\n",
    "                print(f\"\\t\\t\\t\\tToken A Cluster: {self.cluster_map[token_a.i]}\")\n",
    "                print(\"\\t\\t\\t\\tToken B in Token A Cluster\")\n",
    "            return True\n",
    "        if token_b.i in self.cluster_map and token_a in self.cluster_map[token_b.i]:\n",
    "            if verbose:\n",
    "                print(f\"\\t\\t\\t\\tToken B Cluster: {self.cluster_map[token_b.i]}\")\n",
    "                print(\"\\t\\t\\t\\tToken A in Token B Cluster\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def same_reference_span(self, span_a, span_b, verbose=False):\n",
    "        if span_a.text.lower() == span_b.text.lower():\n",
    "            if verbose:\n",
    "                print(\"\\t\\t\\tSame String\")\n",
    "            return True\n",
    "        for token_a in span_a:\n",
    "            for token_b in span_b:\n",
    "                if self.same_reference(token_a, token_b):\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0cc244-1a27-4f8f-ae0f-a431da02ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scanner():\n",
    "    def __init__(self):\n",
    "        # Tools\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        self.sp_doc = None\n",
    "        self.tn_doc = None\n",
    "        self.tk_map = None\n",
    "        \n",
    "        # Helpers\n",
    "        self.species = Species(self)\n",
    "        self.references = References(self)\n",
    "        self.changes = ChangeKeywords(self)\n",
    "    \n",
    "    def update(self, doc, verbose=False):\n",
    "        self.sp_doc = doc\n",
    "        self.tn_doc = self.tn_nlp(doc.text)\n",
    "        self.tk_map = self.load_token_map()\n",
    "        self.references.update(doc.text, verbose=verbose)\n",
    "        self.species.update()\n",
    "\n",
    "    def load_token_map(self):\n",
    "        # Maps Characters to Tokens\n",
    "        tk_map = {}\n",
    "        for token in self.tn_doc:\n",
    "            tk_map[token.idx] = token\n",
    "        return tk_map\n",
    "\n",
    "    def get_full_species(self):\n",
    "        full_species = [*self.species.species_spans]\n",
    "        full_indices = [*self.species.species_indices]\n",
    "        \n",
    "        for k, v in self.references.cluster_map.items():\n",
    "            if k in self.species.species_indices:\n",
    "                for token in v:\n",
    "                    if token.i not in full_indices:\n",
    "                        token_span = self.tn_doc[token.i:token.i+1]\n",
    "                        full_species.append(token_span)\n",
    "                        full_indices.append(token.i)\n",
    "            if self.species.has_species(v):\n",
    "                if k not in full_indices:\n",
    "                    token_span = self.tn_doc[k:k+1]\n",
    "                    full_species.append(token_span)\n",
    "                    full_indices.append(k)\n",
    "        \n",
    "        return (full_species, full_indices)\n",
    "\n",
    "    def get_points(self, verbose=False):\n",
    "        points = 0\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Cluster Map:\")\n",
    "            for k, v in self.references.cluster_map.items():\n",
    "                print(f\"{self.tn_doc[k]}: {v}\")\n",
    "            print()\n",
    "\n",
    "        # This dictionary contains the species that have been seen already\n",
    "        # and a number indicating the amount of times they have been visited.\n",
    "        # The key is a tuple wherein the first index contains the start of the span\n",
    "        # and the second (last) index contains the end of the span. The span is the\n",
    "        # species. The value is the number of visitations for that species.\n",
    "        visited_species = {}\n",
    "\n",
    "        # This is used to quickly find nearby species.\n",
    "        species_spans, species_indices = self.get_full_species()\n",
    "        if verbose:\n",
    "            print(f\"All Species (Span Objects):\")\n",
    "            print(\"\\n\".join([f\"'{span.text}' from {span[0].i}-{span[-1].i}\" for span in species_spans]))\n",
    "            print(\"\")\n",
    "        \n",
    "        for species_span in species_spans:\n",
    "            if verbose:\n",
    "                print(\"LOOP\")\n",
    "                print(f\"Current Species: '{species_span.text}' from {species_span[0].i}-{species_span[-1].i}\")\n",
    "                print(f\"Sentence of Current Species:\\n{species_span.sent.text}\")\n",
    "                print()\n",
    "                \n",
    "                # This (visited_species) stores a tuple with the start and end token indices, \n",
    "                # which is why the -1 indexing is not being used here.\n",
    "                print(f\"Visited Species:\")\n",
    "                if visited_species:\n",
    "                    print(\"\\n\".join([f\"'{self.tn_doc[sp[0]:sp[1]+1]}' from {sp[0]}-{sp[1]} visited {n} times.\" for sp, n in visited_species.items()]))\n",
    "                else:\n",
    "                    print(\"None\")\n",
    "                print()\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Expanding Species:\")\n",
    "\n",
    "            # Take the sentence \"blue whale\", for example. TaxoNerd might recognize\n",
    "            # \"whale\", but the adjective \"blue\" is also important in correctly identifying\n",
    "            # the species being referenced. This is why I'm expanding the species.\n",
    "            if species_span[0].text.lower() in [\"species\"]:\n",
    "                i = species_span[0].i\n",
    "                j = i\n",
    "                while j > 0:\n",
    "                    prev_token = self.tn_doc[j-1]\n",
    "                    if prev_token.pos_ not in [\"NOUN\", \"ADJ\", \"PROPN\", \"SYM\"]:\n",
    "                        break\n",
    "                    species_span = self.tn_doc[j-1:i+1]\n",
    "                    if verbose:\n",
    "                        print(f\"\\Expanding Species - Now: {species_span}\")\n",
    "                    j -= 1\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Current Species After Expansion: '{species_span.text}' from {species_span[0].i}-{species_span[-1].i}\")\n",
    "                print(f\"Sentence of Current Species After Expansion:\\n{species_span.sent.text}\") # Should be the same as before.\n",
    "                print()\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Looking for Past Visits:\")\n",
    "            \n",
    "            past_visits = 0\n",
    "            for sp in visited_species.keys():\n",
    "                visited_species_span = self.tn_doc[sp[0]:sp[1]+1]\n",
    "                if verbose:\n",
    "                    print(f\"Comparing '{species_span.text}' and '{visited_species_span.text}'\")\n",
    "                \n",
    "                if self.references.same_reference_span(species_span, visited_species_span, verbose=verbose):\n",
    "                    past_visits = visited_species[sp]\n",
    "                    if verbose:\n",
    "                        print(f\"\\t'{species_span.text}' == '{visited_species_span.text}'\")\n",
    "                        print(f\"\\t\\tNumber of Past Visits: {past_visits}\")\n",
    "                    visited_species[sp] += 1\n",
    "                    break\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Past Visits: {past_visits}\")\n",
    "                print()\n",
    "            \n",
    "            if past_visits == 0:\n",
    "                visited_species[(species_span[0].i, species_span[-1].i)] = 1\n",
    "\n",
    "            # The points will begin to inflate at a certain point, so to reduce\n",
    "            # those effects, I'm limiting the number of references. This might not\n",
    "            # matter anymore with the changes I'm making, though.\n",
    "            if past_visits > 50:\n",
    "                if verbose:\n",
    "                    print(f\"Past Visits Exceed 50, Continue\")\n",
    "                    print()\n",
    "                continue\n",
    "\n",
    "            # These contains the indices of the tokens that start and end\n",
    "            # the sentence.\n",
    "            li = species_span[0].sent.start\n",
    "            ri = species_span[-1].sent.end\n",
    "\n",
    "            # This contains the indices of the tokens that start and end\n",
    "            # the species span.\n",
    "            sli = species_span[0].i\n",
    "            sri = species_span[-1].i\n",
    "\n",
    "            # These are the indices of the tokens to the left of the species span,\n",
    "            # and the indices of the tokens to the right of the species span.\n",
    "            l_token_indices = set([token.i for token in self.tn_doc[li:sli]])\n",
    "            r_token_indices = set([token.i for token in self.tn_doc[sri+1:ri]])\n",
    "\n",
    "            # Patch-Up?\n",
    "            # Handling differences between TaxoNerd's pipeline and SpaCy's\n",
    "            # default pipeline. I realized this issue at like 10:50 AM.\n",
    "            sp_l_token_indices = []\n",
    "            sp_r_token_indices = []\n",
    "            \n",
    "            for char_pos in [token.idx for token in self.tn_doc[li:sli]]:\n",
    "                for spacy_token in self.sp_doc:\n",
    "                    if spacy_token.idx == char_pos:\n",
    "                        sp_l_token_indices.append(spacy_token.i)\n",
    "\n",
    "            for char_pos in [token.idx for token in self.tn_doc[sri+1:ri]]:\n",
    "                for spacy_token in self.sp_doc:\n",
    "                    if spacy_token.idx == char_pos:\n",
    "                        sp_r_token_indices.append(spacy_token.i)\n",
    "\n",
    "            if not sp_l_token_indices or not sp_r_token_indices:\n",
    "                print(\"No Tokens. Possible Error.\")\n",
    "                continue\n",
    "                \n",
    "            sp_li = sp_l_token_indices[0]\n",
    "            sp_ri = sp_r_token_indices[-1]\n",
    "\n",
    "            sp_l_token_indices = set(sp_l_token_indices)\n",
    "            sp_r_token_indices = set(sp_r_token_indices)\n",
    "            \n",
    "            print(\"Santiy Check\")\n",
    "            print(f\"SpaCy Sentence: {self.sp_doc[sp_li:sp_ri].text}\")\n",
    "            print(f\"TaxoNerd Sentence: {self.tn_doc[li:ri].text}\")\n",
    "            # There's slight differences, which I don't know why is happening right now.\n",
    "            # But I can't fix it in time\n",
    "            # assert self.sp_doc[sp_li:sp_ri].text == self.tn_doc[li:ri].text\n",
    "            print()\n",
    "\n",
    "            # Nearby Actions (Modification)\n",
    "            change_indices = set(self.changes.find_keyword_indices(self.sp_doc[sp_li:sp_ri], verbose=verbose))\n",
    "            if verbose:\n",
    "                print(f\"Change (Causative) Verbs in Sentence: {[self.sp_doc[i].text for i in change_indices]}\")\n",
    "\n",
    "            # Now that I'm thinking about it, I'm not sure why I make the distinction between the left and right.\n",
    "            # I'll leave it for now in case it becomes useful later on.\n",
    "            l_changes = sp_l_token_indices.intersection(change_indices)\n",
    "            r_changes = sp_r_token_indices.intersection(change_indices)\n",
    "            if verbose:\n",
    "                print(f\"Change (Causative) Verbs on L-Side of Current Species: {[self.sp_doc[i].text for i in l_changes]}\")\n",
    "                print(f\"Change (Causative) Verbs on R-Side of Current Species: {[self.sp_doc[i].text for i in r_changes]}\")\n",
    "                print()\n",
    "\n",
    "            # There must be a change. If there's none,\n",
    "            # we move onto the next species in the paper/abstract.\n",
    "            if not l_changes and not r_changes:\n",
    "                print(\"No Changes on L-Side or R-Side of Current Species, Continue\")\n",
    "                print()\n",
    "                continue\n",
    "                \n",
    "            # Nearby Species (Interaction)\n",
    "            l_species = l_token_indices.intersection(species_indices)\n",
    "            r_species = r_token_indices.intersection(species_indices)\n",
    "            if verbose:\n",
    "                print(f\"Species on L-Side of Current Species: {[self.tn_doc[i].text for i in l_species]}\")\n",
    "                print(f\"Species in R-Side of Current Species: {[self.tn_doc[i].text for i in r_species]}\")\n",
    "\n",
    "            # There must be a species. If there's none,\n",
    "            # we move onto the next species in the paper/abstract.\n",
    "            if not l_species and not r_species:\n",
    "                print(\"No Species on L-Side or R-Side of Current Species, Continue\")\n",
    "                print()\n",
    "                continue\n",
    "\n",
    "            # Adding Points\n",
    "            points += 10 * (past_visits + 1)\n",
    "            if verbose:\n",
    "                print(f\"Added {10 * (past_visits + 1)} Points for Species, Now {points}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Points: {points}\")\n",
    "        \n",
    "        # Adjustments\n",
    "        number_sentences = len(list(self.tn_doc.sents))\n",
    "        points = 0 if number_sentences == 0 else points / number_sentences\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Adjusted Points ({len(list(self.tn_doc.sents))} Sentences): {points}\")\n",
    "            print()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Final Visited Species:\")\n",
    "            if visited_species:\n",
    "                print(\"\\n\".join([f\"'{self.tn_doc[sp[0]:sp[1]+1]}' from {sp[0]}-{sp[1]} visited {n} times.\" for sp, n in visited_species.items()]))\n",
    "            else:\n",
    "                print(\"None\")\n",
    "            print()\n",
    "        \n",
    "        if len(visited_species) < 3:\n",
    "            if verbose:\n",
    "                print(\"Less Than 3 Species Visited, 0 Points\")\n",
    "            return 0\n",
    "        if verbose:\n",
    "            print(f\"Final Points: {points}\")\n",
    "        return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcddfc05-9263-4112-a554-8c0bccf03eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(name, save_output=False, version=\"\"):\n",
    "    # Redirect Print Statements\n",
    "    # https://stackoverflow.com/questions/7152762/how-to-redirect-print-output-to-a-file\n",
    "    if save_output:\n",
    "        initial_stdout = sys.stdout\n",
    "        f = open(f'./Print{name}{\"\" if not version else f\"-{version}\"}.txt', 'w')\n",
    "        sys.stdout = f\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "\n",
    "    # Load Dataset\n",
    "    data = load_preprocessed_dataset(name)\n",
    "\n",
    "    # We'll be running the points algorithm\n",
    "    # on the abstracts of these papers.\n",
    "    texts = list(data['Abstract'].to_numpy())\n",
    "    \n",
    "    # The scores for each paper will be stored here,\n",
    "    # we'll set this as a column of the dataframe.\n",
    "    scores = []\n",
    "    \n",
    "    # Scan and Evaluate Documents\n",
    "    scanner = Scanner()\n",
    "    for i, doc in enumerate(scanner.sp_nlp.pipe(texts)):\n",
    "        print(f\"{i+1}/{data.shape[0]} - {data.iloc[i]['Title']}\\n\")\n",
    "        scanner.update(doc, verbose=save_output)\n",
    "\n",
    "        # Empty string literals cause errors, so it's\n",
    "        # being handled here.\n",
    "        if not scanner.sp_doc or not scanner.tn_doc:\n",
    "            scores.append(0)\n",
    "        else:\n",
    "            points = scanner.get_points(verbose=save_output)\n",
    "            scores.append(points)\n",
    "\n",
    "        if not save_output:\n",
    "            clear_output(wait=True)\n",
    "        else:\n",
    "            print(\"\\n\\n\\n---END---\\n\\n\\n\")\n",
    "\n",
    "    # Reset Standard Output\n",
    "    if save_output:\n",
    "        sys.stdout = initial_stdout\n",
    "        f.close()\n",
    "\n",
    "    data[\"Score\"] = scores\n",
    "    data.sort_values(by='Score', ascending=False, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c521b55c-7f65-49a8-9127-c2df1f9a18be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/12/2025 17:33:31 - INFO - \t missing_keys: []\n",
      "06/12/2025 17:33:31 - INFO - \t unexpected_keys: []\n",
      "06/12/2025 17:33:31 - INFO - \t mismatched_keys: []\n",
      "06/12/2025 17:33:31 - INFO - \t error_msgs: []\n",
      "06/12/2025 17:33:31 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "06/12/2025 17:33:32 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.09 examples/s]\n",
      "06/12/2025 17:33:33 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/12/2025 17:33:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.83 examples/s]\n",
      "06/12/2025 17:33:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/12/2025 17:34:01 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.20 examples/s]\n",
      "06/12/2025 17:34:02 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "06/12/2025 17:34:06 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.90 examples/s]\n",
      "06/12/2025 17:34:06 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    }
   ],
   "source": [
    "# Score Datasets\n",
    "dataset_names = [\"Examples\", \"Baseline-1\", \"SubA\", \"SubAFiltered\", \"SubB\", \"SubBFiltered\", \"C\", \"CFiltered\", \"D\", \"DFiltered\"]\n",
    "for name in [\"Examples\"]:\n",
    "    scored_data = score_dataset(name, save_output=True, version='')\n",
    "    store_scored_dataset(scored_data, name, version='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e88ae1-37ca-48bc-9cea-e7776c2d713b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
