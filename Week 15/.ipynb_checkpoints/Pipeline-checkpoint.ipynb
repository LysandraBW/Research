{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "292e1a6c-ae95-494f-b933-5bda81576d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import pickle\n",
    "import pymupdf\n",
    "import textacy\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "from pprint import pprint\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import DependencyMatcher, PhraseMatcher\n",
    "from pyalex import Works\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f4b802e-decb-4351-a861-33b2ceef5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = text\n",
    "    cleaned_text = re.sub(r'http\\S+', '', cleaned_text) # Remove URLs\n",
    "    cleaned_text = re.sub(r'-\\n', '', cleaned_text) # Remove Hyphenations\n",
    "    cleaned_text = re.sub(\"\\s+\", \" \", cleaned_text) # Remove Duplicate Spaces\n",
    "    cleaned_text = re.sub(r\"\\s+([?.!,])\", r\"\\1\", cleaned_text) # Remove Spaces Before Punctuation\n",
    "    return cleaned_text\n",
    "\n",
    "def pdf_to_text(url):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        f = pdf_bytes(url)\n",
    "        doc = pymupdf.open(stream=f)\n",
    "        for d in doc:\n",
    "            text += d.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def load_local_documents(group=\"Cleared\"):\n",
    "    documents = []\n",
    "    filenames = glob.glob(f\"../Week 7/Examples/{group}/*.pdf\")\n",
    "    for filename in filenames:\n",
    "        full_text = \"\"\n",
    "        doc = pymupdf.open(filename)\n",
    "        for page in doc:\n",
    "            text = page.get_text()\n",
    "            full_text += \" \" + text\n",
    "        if full_text:\n",
    "            documents.append(clean_text(full_text))\n",
    "    return documents\n",
    "\n",
    "def load_documents():\n",
    "    # Cross Product\n",
    "    keywords = [\n",
    "        # Tier 1\n",
    "        \"trait\",\n",
    "        \"phenotype\",\n",
    "        # Tier 2\n",
    "        \"trait-mediated\",\n",
    "        \"higher-order interaction*\", \n",
    "        \"polymorphism\", \n",
    "        \"interaction modification\",\n",
    "        \"indirect effect*\",\n",
    "        # Tier 3\n",
    "        \"apparent competition\", \n",
    "        \"resource competition\", \n",
    "        \"keystone predation\", \n",
    "        \"intraguild predation*\", \n",
    "        \"intransitive competition\", \n",
    "        \"trophic chain*\", \n",
    "        \"competition chains\", \n",
    "        \"mutual competition\"\n",
    "    ]\n",
    "    # [trait] & [higher-order interaction, polymorphism, ...] & [Tier 3]\n",
    "    number_keywords = len(keywords)\n",
    "    for i in range(4):\n",
    "        for j in range(4, number_keywords, 1):\n",
    "            keywords.append(f\"{keywords[i]} {keywords[j]}\")\n",
    "    \n",
    "\n",
    "    # Loading Texts\n",
    "    texts = []\n",
    "    number_works = 0\n",
    "    number_unfiltered_works = 0\n",
    "    number_keywords = len(keywords)\n",
    "    \n",
    "    for k, keyword in enumerate(keywords):\n",
    "        print(f\"({k + 1}/{number_keywords}) Searching Keyword '{keyword}'\")\n",
    "        pager = Works().search_filter(title=keyword).paginate(per_page=200)\n",
    "        for page in pager:\n",
    "            for work in page:\n",
    "                number_unfiltered_works += 1\n",
    "                \n",
    "                title = work['title']\n",
    "                abstract = work['abstract']\n",
    "                doi = work['doi']\n",
    "                \n",
    "                # Find Full Text\n",
    "                url = None\n",
    "                if work[\"primary_location\"]:\n",
    "                    url = work[\"primary_location\"][\"pdf_url\"]\n",
    "                full_text = \"\" if not url else pdf_to_text(url)\n",
    "                \n",
    "                if not abstract and not full_text:\n",
    "                    continue\n",
    "                texts.append({\n",
    "                    \"Index\": number_works,\n",
    "                    \"Title\": title,\n",
    "                    \"DOI\": doi,\n",
    "                    \"Text\": abstract if abstract and not full_text else full_text,\n",
    "                    \"Points\": 0\n",
    "                })\n",
    "                number_works += 1\n",
    "        k += 1\n",
    "        clear_output(wait=True)        \n",
    "\n",
    "    assert len(texts) == number_works\n",
    "    print(f\"Number Documents: {number_works}, Number Unfiltered Documents: {number_unfiltered_works}\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41873874-541c-4576-bf2e-791b3d31d262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/06/2025 19:36:12 - WARNING - \t From C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def on_topic(text):\n",
    "    # Topic and Threshold\n",
    "    topics = [(\"ecology\", 0.75), (\"interaction\", 0.75)]\n",
    "\n",
    "    for topic, threshold in topics:\n",
    "        # Break Into Parts\n",
    "        # This does not currently break the text;\n",
    "        # it's not exactly needed as it is relatively fast enough.\n",
    "        chunks = []\n",
    "        chunk_length = len(text)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            chunk = text[i:i+chunk_length]\n",
    "    \n",
    "            # Ensure Full Words\n",
    "            j = i + chunk_length\n",
    "            while j < len(text) and text[j] != \" \":\n",
    "                chunk += text[j]\n",
    "                j += 1\n",
    "    \n",
    "            i = j\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "        # Classify\n",
    "        labels = [topic]\n",
    "        scores = {}\n",
    "        for label in labels:\n",
    "            scores[label] = 0\n",
    "            \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            result = classifier(chunk, labels)\n",
    "            for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
    "                scores[label] += score\n",
    "\n",
    "        # This is made slightly more confusing by the whole \"chunks\",\n",
    "        # however, it's just looking at the score of the entire text\n",
    "        # and ensuring that it met or passed the threshold.\n",
    "        mean_score = np.mean(np.array(list(scores.values())) / len(chunks))\n",
    "        if mean_score < threshold:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9090ee67-7e4a-4fc7-b52b-ae6215464bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller:\n",
    "    def __init__(self):\n",
    "        # print(\"Loading SP_NLP\")\n",
    "        t0 = time.time()\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        t1 = time.time()\n",
    "        # print(f\"SP_NLP: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Loading TN_NLP\")\n",
    "        t0 = time.time()\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "        t1 = time.time()\n",
    "        # print(f\"TN_NLP: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Loading FCOREF\")\n",
    "        t0 = time.time()\n",
    "        self.fcoref = FCoref(enable_progress_bar=False, device='cpu')\n",
    "        t1 = time.time()\n",
    "        # print(f\"FCOREF: {t1-t0}s\")\n",
    "        \n",
    "        self.sp_doc = None\n",
    "        self.tn_doc = None\n",
    "        self.tk_map = None\n",
    "    \n",
    "    def update(self, doc):\n",
    "        self.sp_doc = doc\n",
    "        # print(\"Updating TN_DOC\")\n",
    "        t0 = time.time()\n",
    "        self.tn_doc = self.tn_nlp(doc.text)\n",
    "        t1 = time.time()\n",
    "        # print(f\"TN_DOC: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Updating TK_MAP\")\n",
    "        t0 = time.time()\n",
    "        self.tk_map = self.load_token_map()\n",
    "        t1 = time.time()\n",
    "        # print(f\"TK_MAP: {t1-t0}s\")\n",
    "\n",
    "    def load_token_map(self):\n",
    "        tk_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            tk_map[token.idx] = token.i\n",
    "        return tk_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799ea3cf-f021-4de9-bf5c-1baa31fed886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species:\n",
    "    def __init__(self, controller):\n",
    "        self.controller = controller\n",
    "        self.species_spans = None\n",
    "        self.species_indices = None\n",
    "\n",
    "    def update(self):\n",
    "        if not self.controller.sp_doc or not self.controller.tn_doc:\n",
    "            return\n",
    "        # print(\"Updating Species Indices and Spans\")\n",
    "        t0 = time.time()\n",
    "        self.species_spans, self.species_indices = self.load_species_spans()\n",
    "        t1 = time.time()\n",
    "        # print(f\"Load Species Indices and Span: {t1-t0}s\")\n",
    "        \n",
    "    def load_species_spans(self):\n",
    "        spans = []\n",
    "        indices = []\n",
    "        for species_span in self.controller.tn_doc.ents:\n",
    "            l_species_idx = species_span[0].idx\n",
    "            r_species_idx = species_span[-1].idx\n",
    "            \n",
    "            if l_species_idx not in self.controller.tk_map or r_species_idx not in self.controller.tk_map:\n",
    "                raise Exception(\"Invalid Token\")\n",
    "                \n",
    "            l_species_i = self.controller.tk_map[l_species_idx]\n",
    "            r_species_i = self.controller.tk_map[r_species_idx]\n",
    "\n",
    "            span = self.controller.sp_doc[l_species_i:r_species_i+1]\n",
    "            spans.append(span)\n",
    "            indices += [token.i for token in span]\n",
    "        return (spans, indices)\n",
    "\n",
    "    def is_species(self, token):\n",
    "        return token.i in self.species_indices\n",
    "        \n",
    "    def has_species(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.species_indices:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d864d6e-4d78-4ab2-a84c-1e24a66fe989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    def __init__(self, controller, literals, pos_types, threshold=0.7):\n",
    "        self.controller = controller\n",
    "        self.literals = literals\n",
    "        self.threshold = threshold\n",
    "        self.pos_types = pos_types\n",
    "        self.keywords = [self.controller.sp_nlp(literal) for literal in self.literals]\n",
    "        self.keyword_indices = []\n",
    "\n",
    "    def update(self):\n",
    "        if not self.controller.sp_doc or not self.controller.sp_nlp:\n",
    "            return\n",
    "        # print(\"Updating Keyword Indices\")\n",
    "        t0 = time.time()\n",
    "        self.keyword_indices = self.load_keyword_indices()\n",
    "        t1 = time.time()\n",
    "        # print(f\"Keyword Indices: {t1-t0}s\")\n",
    "        \n",
    "    def is_keyword(self, token):\n",
    "        return token.i in self.keyword_indices\n",
    "\n",
    "    def has_keyword(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.i in self.keyword_indices:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load_keyword_indices(self):\n",
    "        indices = []\n",
    "        for token in self.controller.sp_doc:\n",
    "            if token.pos_ not in self.pos_types or self.do_not_check(token):\n",
    "                continue\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.literals:\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.controller.sp_nlp(token.lemma_)\n",
    "            for keyword in self.keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > self.threshold:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def find_keyword_indices(self, tokens):\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token.pos_ not in self.pos_types or self.do_not_check(token):\n",
    "                continue\n",
    "            # Fast Check\n",
    "            if token.lemma_ in self.literals:\n",
    "                indices.append(token.i)\n",
    "                continue\n",
    "            # Comparing Similarity\n",
    "            lemma = self.controller.sp_nlp(token.lemma_)\n",
    "            for keyword in self.keywords:\n",
    "                similarity = keyword.similarity(lemma)\n",
    "                if similarity > self.threshold:\n",
    "                    indices.append(token.i)\n",
    "        return indices\n",
    "\n",
    "    def do_not_check(self, token):\n",
    "        return len(token) <= 5 or re.match('^[\\w]+$', token.text) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4348c355-0b76-4b59-a72a-8ce8766ff2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, controller):\n",
    "        super().__init__(controller, {\"increase\", \"decrease\", \"change\", \"shift\", \"cause\", \"produce\"}, [\"NOUN\", \"VERB\"], 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b389f8c8-8b1a-4fb9-9676-fea3cf8f01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class References:\n",
    "    def __init__(self, controller, texts=None):\n",
    "        self.controller = controller\n",
    "        self.predictions = None\n",
    "        self.cluster_map = None\n",
    "        self.text_size_in_tokens = 100\n",
    "        if texts:\n",
    "            self.update(texts)\n",
    "\n",
    "    def update(self, text):\n",
    "        if not self.controller.sp_doc:\n",
    "            return\n",
    "        # print(\"Updating Predictions\")\n",
    "        t0 = time.time()\n",
    "        texts = []\n",
    "        offsets = []\n",
    "        for i in range(0, len(self.controller.sp_doc), self.text_size_in_tokens):\n",
    "            texts.append(self.controller.sp_doc[i:i+self.text_size_in_tokens].text)\n",
    "            offsets.append(self.controller.sp_doc[i].idx)\n",
    "        self.predictions = self.controller.fcoref.predict(texts=texts)\n",
    "        t1 = time.time()\n",
    "        # print(f\"Predictions: {t1-t0}s\")\n",
    "\n",
    "        # print(\"Updating Cluster Map\")\n",
    "        t0 = time.time()\n",
    "        self.cluster_map = self.load_cluster_map(self.predictions, offsets)\n",
    "        t1 = time.time()\n",
    "        # print(f\"Cluster Map: {t1-t0}s\")\n",
    "\n",
    "    def load_cluster_map(self, predictions, offsets):\n",
    "        cluster_map = {}\n",
    "        for prediction, offset in zip(predictions, offsets):\n",
    "            clusters = prediction.get_clusters(as_strings=False)\n",
    "            for cluster in clusters:\n",
    "                # Converting Spans to Tokens\n",
    "                token_cluster = []\n",
    "                for span in cluster:\n",
    "                    if not span:\n",
    "                        continue\n",
    "                    index = span[0] + offset\n",
    "                    if index not in self.controller.tk_map:\n",
    "                        continue\n",
    "                        # raise Exception(\"Invalid Token\")\n",
    "                    index = self.controller.tk_map[index]\n",
    "                    token_cluster.append(self.controller.sp_doc[index])\n",
    "                # Mapping\n",
    "                for token in token_cluster:\n",
    "                    cluster_map[token.i] = list(filter(lambda t: t != token, token_cluster))\n",
    "        return cluster_map\n",
    "            \n",
    "    def get_references(self, tokens):\n",
    "        refs = []\n",
    "        for token in tokens:\n",
    "            index = token.i\n",
    "            if index in self.cluster_map:\n",
    "                refs += self.cluster_map[index]\n",
    "        return refs\n",
    "\n",
    "    def same_reference(self, token_a, token_b):\n",
    "        if token_a.lemma_.lower() == token_b.lemma_.lower():\n",
    "            return True\n",
    "        if token_a.i in self.cluster_map and token_b in self.cluster_map[token_a.i]:\n",
    "            return True\n",
    "        if token_b.i in self.cluster_map and token_a in self.cluster_map[token_b.i]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def same_reference_span(self, span_a, span_b):\n",
    "        if span_a.text.lower() == span_b.text.lower():\n",
    "            return True\n",
    "        for token_a in span_a:\n",
    "            for token_b in span_b:\n",
    "                if self.same_reference(token_a, token_b):\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9d48c9e-1ffa-4a71-b537-c5f2aed3ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scanner():\n",
    "    def __init__(self, controller):\n",
    "        # Helpers\n",
    "        self.controller = controller\n",
    "        self.species = Species(self.controller)\n",
    "        self.references = References(self.controller)\n",
    "        self.changes = ChangeKeywords(self.controller)\n",
    "\n",
    "        # Used to Evaluate Points\n",
    "        self.level1 = {\"ecological\", \"community\", \"interaction\", \"trait\", \"ecosystem\", \"ecology\"}\n",
    "        self.level2 = {\"model\"}\n",
    "        self.level3 = {\"predator\", \"prey\", \"competitor\", \"resource\", \"predation\", \"specie\", \"result\", \"effect\", \"population\", \"species\", \"invade\", \"presence\"}\n",
    "\n",
    "    def update(self, doc):\n",
    "        self.controller.update(doc)\n",
    "        self.references.update(doc.text)\n",
    "        self.species.update()\n",
    "\n",
    "    def get_full_species(self):\n",
    "        full_species = [*self.species.species_spans]\n",
    "        full_indices = [*self.species.species_indices]\n",
    "        \n",
    "        for k, v in self.references.cluster_map.items():\n",
    "            if k in self.species.species_indices:\n",
    "                for token in v:\n",
    "                    if token.i not in full_indices:\n",
    "                        token_span = self.controller.sp_doc[token.i:token.i+1]\n",
    "                        full_species.append(token_span)\n",
    "                        full_indices.append(token.i)\n",
    "            if self.species.has_species(v):\n",
    "                if k not in full_indices:\n",
    "                    token_span = self.controller.sp_doc[k:k+1]\n",
    "                    full_species.append(token_span)\n",
    "                    full_indices.append(k)\n",
    "        \n",
    "        return (full_species, full_indices)\n",
    "\n",
    "    def get_points(self):\n",
    "        points = 0\n",
    "\n",
    "        # Species Work\n",
    "        visited_species = {}\n",
    "        species_spans, species_indices = self.get_full_species()\n",
    "        \n",
    "        for species_span in species_spans:\n",
    "            # Adjusting Species if Vague\n",
    "            if species_span[0].text.lower() in [\"species\"]:\n",
    "                i = species_span[0].i\n",
    "                j = i\n",
    "                while j > 0:\n",
    "                    prev_token = self.controller.sp_doc[j-1]\n",
    "                    if prev_token.pos_ not in [\"NOUN\", \"ADJ\", \"PROPN\", \"SYM\"]:\n",
    "                        break\n",
    "                    species_span = self.controller.sp_doc[j-1:i+1]\n",
    "                    # print(f\"Adjusted Species: {species_span}\")\n",
    "                    j -= 1\n",
    "                \n",
    "            # Repeating Species\n",
    "            past_visits = 0\n",
    "            for sp in visited_species.keys():\n",
    "                visited_span = self.controller.sp_doc[sp[0]:sp[1]+1]\n",
    "                if self.references.same_reference_span(species_span, visited_span):\n",
    "                    past_visits = visited_species[sp]\n",
    "                    visited_species[sp] += 1\n",
    "                    break\n",
    "            if past_visits == 0:\n",
    "                visited_species[(species_span[0].i, species_span[-1].i)] = 1\n",
    "            if past_visits > 50:\n",
    "                continue\n",
    "                    \n",
    "            li = species_span[0].sent.start\n",
    "            ri = species_span[-1].sent.end\n",
    "\n",
    "            sli = species_span[0].i\n",
    "            sri = species_span[-1].i\n",
    "\n",
    "            l_token_indices = set([token.i for token in self.controller.sp_doc[li:sli]])\n",
    "            r_token_indices = set([token.i for token in self.controller.sp_doc[sri+1:ri]])\n",
    "\n",
    "            # Nearby Actions (Modification)\n",
    "            change_indices = set(self.changes.find_keyword_indices(self.controller.sp_doc[li:ri]))\n",
    "            l_changes = l_token_indices.intersection(change_indices)\n",
    "            r_changes = r_token_indices.intersection(change_indices)\n",
    "\n",
    "            # There must be a change.\n",
    "            if not l_changes and not r_changes:\n",
    "                continue\n",
    "                \n",
    "            # Nearby Species (Interaction)\n",
    "            l_species = l_token_indices.intersection(species_indices)\n",
    "            r_species = r_token_indices.intersection(species_indices)\n",
    "\n",
    "            if l_changes or r_changes:\n",
    "                points += 10 * (past_visits + 1)\n",
    "                change_found = True\n",
    "            if l_species or r_species:\n",
    "                points += 10 * (past_visits + 1)\n",
    "                other_species_found = True\n",
    "            points += min(past_visits * 10, 100)\n",
    "            \n",
    "        print(f\"Species Points After: {points}\")\n",
    "        \n",
    "        # Adjustments\n",
    "        points /= len(list(self.controller.sp_doc.sents))\n",
    "        print(f\"Adjust Points After: {points}\")\n",
    "\n",
    "        print(f\"Visited Species: {visited_species}\")\n",
    "        if len(visited_species) < 3:\n",
    "            return 0\n",
    "        return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e50093-6889-4ddd-bd5e-f58992413644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Documents: 32780, Number Unfiltered Documents: 51896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "# (1) Load and (2) Filter Documents by Topic\n",
    "unfiltered_documents = load_documents()\n",
    "# with open('unfiltered_documents.pickle', 'wb') as file:\n",
    "#     pickle.dump(unfiltered_documents, file)\n",
    "columns = [\"Title\", \"Abstract\", \"DOI\", \"Score\"]\n",
    "\n",
    "# filtered_documents = list(filter(lambda d: on_topic(d[\"Text\"]), unfiltered_documents))\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame([], columns=columns)\n",
    "for work in unfiltered_documents:\n",
    "    work = pd.DataFrame([[work[\"Title\"], work[\"Text\"], work[\"DOI\"], 0]],  columns=columns)\n",
    "    dataset = pd.concat([dataset, work])\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "dataset.to_csv(f'DatasetOriginal.csv')\n",
    "dataset.to_excel(f'DatasetOriginal.xlsx', index=None, header=True)\n",
    "with open('DatasetOriginal.pkl', 'wb') as file:\n",
    "    pickle.dump(dataset, file)\n",
    "\n",
    "dataset.head(5)\n",
    "# print(f\"Number of Documents Before and After Filtering: {len(unfiltered_documents)} => {len(filtered_documents)}\")\n",
    "\n",
    "# # Load Documents\n",
    "# # with open('filtered_documents.pickle', 'rb') as file:\n",
    "# #     documents = pickle.load(file)\n",
    "# #     texts = [d[\"Text\"] for d in documents]\n",
    "# #     number_texts = len(texts)\n",
    "# #     print(f\"Number of Documents: {number_texts}\")\n",
    "# #     assert number_texts == len(documents)\n",
    "\n",
    "# # Scan Documents\n",
    "# scanner = Scanner(Controller())\n",
    "\n",
    "# scores = []\n",
    "# for i, text in enumerate(scanner.controller.sp_nlp.pipe(dataset.Abstract)):\n",
    "#     print(f\"{i+1}/{number_texts}\")\n",
    "#     scanner.update(text)\n",
    "    \n",
    "#     points = scanner.get_points()\n",
    "#     scores.append(points)\n",
    "    \n",
    "#     i += 1\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "# dataset[\"Scores\"] = scores\n",
    "# with open('ScoredDatasetOriginal.pkl', 'wb') as file:\n",
    "#     pickle.dump(dataset, file)\n",
    "# dataset.to_csv(f'ScoredDatasetOriginal.csv')\n",
    "# dataset.to_excel(f'ScoredDatasetOriginal.xlsx', index=None, header=True)\n",
    "# # clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753bb02-664a-4815-be9e-ef77d5d8dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unfiltered_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4e7e5-6437-4ccf-913e-b6fddf31178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_zero_outputs = 0\n",
    "# for doc in output:\n",
    "#     if doc[-1] != 0:\n",
    "#         non_zero_outputs += 1\n",
    "# print(non_zero_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c25172-f317-4a60-83a8-e78c372cdaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents.sort(key=lambda d: d[\"Points\"], reverse=True)\n",
    "\n",
    "# data = [[\"Index\", \"Title\", \"DOI\", \"Text\", \"Points\"], *list(map(lambda d: [d[\"Index\"], d[\"Title\"], d[\"DOI\"], d[\"Text\"], d[\"Points\"]], documents))]\n",
    "# with open('output.csv', 'w', encoding=\"utf-8\") as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerows(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
