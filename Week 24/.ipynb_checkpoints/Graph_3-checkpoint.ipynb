{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3461b7-e257-4943-8101-e65febed4148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import spacy\n",
    "import textacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from taxonerd import TaxoNERD\n",
    "from fastcoref import spacy_component\n",
    "from itertools import permutations, combinations\n",
    "from spacy.matcher import Matcher, DependencyMatcher, PhraseMatcher\n",
    "%run \"./Main.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81fbd7b-d3df-4f29-af76-f0930c703995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_set(pool, current=None, result=None):\n",
    "    if current not in result:\n",
    "        result.append(current)\n",
    "\n",
    "    for item in pool:\n",
    "        if item not in current:\n",
    "            power_set(pool, current={*current, item}, result=result)\n",
    "\n",
    "    return result\n",
    "\n",
    "def interleave(pool, current=None, used=None, skip=None, result=None):    \n",
    "    if current and current not in result:\n",
    "        result.append(current)\n",
    "    \n",
    "    for i in range(len(pool)):\n",
    "        if i == skip:\n",
    "            continue\n",
    "        \n",
    "        for j in range(len(pool[i])):\n",
    "            if (i, j) in used:\n",
    "                continue\n",
    "            \n",
    "            interleave(\n",
    "                pool, \n",
    "                used=[*used, (i, j)],\n",
    "                current=[*current, pool[i][j]],\n",
    "                skip=i, \n",
    "                result=result\n",
    "            )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3290547e-68c6-40d4-b92d-eb478ac2e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, main, is_action=False, tokens=None):\n",
    "        self.main = main\n",
    "        self.tokens = tokens or []\n",
    "        self.is_action = is_action\n",
    "        self.all_tokens = []\n",
    "\n",
    "    \n",
    "\n",
    "    def start(self):\n",
    "        i = math.inf\n",
    "        for token in self.tokens:\n",
    "            i = min(i, token.i)\n",
    "        return i\n",
    "\n",
    "\n",
    "    \n",
    "    def end(self):\n",
    "        i = -math.inf\n",
    "        for token in self.tokens:\n",
    "            i = max(i, token.i)\n",
    "        return i\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_tokens(self):\n",
    "        return self.tokens\n",
    "\n",
    "\n",
    "    \n",
    "    def get_all_tokens(self):\n",
    "        return [*self.tokens, *self.all_tokens]\n",
    "\n",
    "\n",
    "    \n",
    "    def get_traits(self):\n",
    "        tokens = set([*self.tokens, *self.all_tokens])\n",
    "        tokens = tokens & set(self.main.trait.tokens)\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    \n",
    "    def get_species(self):\n",
    "        tokens = set([*self.tokens, *self.all_tokens])\n",
    "        tokens = tokens & set(self.main.species.tokens)\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    \n",
    "    def copy(self):\n",
    "        node = Node(self.main)\n",
    "        node.tokens = [*self.tokens]\n",
    "        node.is_action = self.is_action\n",
    "        node.all_tokens = [*self.all_tokens]\n",
    "        return node\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        tokens = [*self.tokens, *self.all_tokens]\n",
    "        tokens = list(set(tokens))\n",
    "        tokens = sorted(tokens, key=lambda token: token.i)\n",
    "        return f\"{','.join([t.text for t in tokens])}\"\n",
    "\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token_coref(main, all_tokens):\n",
    "        i = 0\n",
    "        size = len(all_tokens)\n",
    "        while i < size:\n",
    "            token = all_tokens[i]\n",
    "            if token.pos_ == \"PRON\":\n",
    "                all_tokens = [*main.coref_map.get(token, [token])]\n",
    "                break\n",
    "            i += 1\n",
    "        return all_tokens\n",
    "    \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token_unit(main, all_tokens):\n",
    "        i = 0\n",
    "        size = len(all_tokens)\n",
    "        unit_map_items = main.units.unit_map.items()\n",
    "        while i < size:\n",
    "            token = all_tokens[i]\n",
    "            for unit_bound, unit in unit_map_items:\n",
    "                token_in_unit = unit_bound[0] <= token.i <= unit_bound[1]\n",
    "                unit_is_valid = unit.label_has([Unit.ITEM])\n",
    "                if token_in_unit and unit_is_valid:\n",
    "                    all_tokens = [*main.sp_doc[unit_bound[0]:unit_bound[1]+1]]\n",
    "                    break\n",
    "            i += 1\n",
    "        return all_tokens\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token_entity(main, all_tokens):\n",
    "        i = 0\n",
    "        size = len(all_tokens)\n",
    "        while i < size:\n",
    "            token = all_tokens[i]\n",
    "            if token in main.entity_map:\n",
    "                all_tokens.extend([*main.entity_map[token]])\n",
    "            i += 1\n",
    "        return all_tokens\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token_noun_chunk(main, all_tokens):\n",
    "        i = 0\n",
    "        size = len(all_tokens)\n",
    "        while i < size:\n",
    "            token = all_tokens[i]\n",
    "            if token in main.noun_chunk_map:\n",
    "                all_tokens.extend([*main.noun_chunk_map[token]])\n",
    "            i += 1\n",
    "        return all_tokens\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token(main, token):\n",
    "        # print(\"Extend Token\")\n",
    "        all_tokens = [token]\n",
    "\n",
    "        # print(f\"Initial: {all_tokens}\")\n",
    "        \n",
    "        all_tokens = Node.extend_token_coref(main, all_tokens)\n",
    "        # print(f\"After Coref: {all_tokens}\")\n",
    "        all_tokens = Node.extend_token_unit(main, all_tokens)\n",
    "        # print(f\"After Unit: {all_tokens}\")\n",
    "        all_tokens = Node.extend_token_entity(main, all_tokens)\n",
    "        # print(f\"After Entity: {all_tokens}\")\n",
    "        all_tokens = Node.extend_token_noun_chunk(main, all_tokens)\n",
    "        # print(f\"After Noun Chunk: {all_tokens}\")\n",
    "        \n",
    "        all_tokens = list(set(all_tokens))\n",
    "        all_tokens = sorted(all_tokens, key=lambda token: token.i)\n",
    "        \n",
    "        return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ab4234-8632-4116-905b-f075f6787380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.order = []\n",
    "\n",
    "    \n",
    "    \n",
    "    def start(self):\n",
    "        i = math.inf\n",
    "        for item in self.order:\n",
    "            i = min(i, item.start())\n",
    "        return i\n",
    "\n",
    "\n",
    "    \n",
    "    def end(self):\n",
    "        i = -math.inf\n",
    "        for item in self.order:\n",
    "            i = max(i, item.end())\n",
    "        return i\n",
    "\n",
    "\n",
    "\n",
    "    def sent_start(self):\n",
    "        start = self.start()\n",
    "        sents = self.main.sp_doc.sents\n",
    "        for i, sent in enumerate(sents):\n",
    "            if sent.start <= start < sent.end:\n",
    "                return i\n",
    "        return -1\n",
    "\n",
    "\n",
    "    \n",
    "    def copy(self):\n",
    "        event = Event(self.main)\n",
    "        event.order = []\n",
    "        for item in self.order:\n",
    "            event.order.append(item.copy())\n",
    "        return event\n",
    "\n",
    "\n",
    "    \n",
    "    def get_tokens(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_tokens())\n",
    "        return ret\n",
    "\n",
    "\n",
    "    \n",
    "    def get_all_tokens(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_all_tokens())\n",
    "        return ret\n",
    "\n",
    "\n",
    "    \n",
    "    def get_species(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_species())\n",
    "        return ret\n",
    "\n",
    "\n",
    "    \n",
    "    def get_traits(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_traits())\n",
    "        return ret\n",
    "    \n",
    "\n",
    "    \n",
    "    def attach_event(self, event, del_end=False):\n",
    "        if del_end:\n",
    "            self.order.pop()\n",
    "        self.order.extend(event.order)\n",
    "        return self\n",
    "    \n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        ret = \"\"\n",
    "\n",
    "        i = 0\n",
    "        size = len(self.order)\n",
    "        while i < size:\n",
    "            ret += f\"({self.order[i]})\"\n",
    "            if i != size - 1:\n",
    "                ret += \"->\"\n",
    "            i += 1\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f935ba34-fa22-45ff-89e1-3eac1ca5cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventManager:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.events = []\n",
    "\n",
    "\n",
    "    \n",
    "    def fix_triple_tokens(self, s_node, v_node, o_node):\n",
    "        # print(\"\\tBefore\")\n",
    "        # print(f\"\\t\\t{s_node.tokens}\")\n",
    "        # print(f\"\\t\\t{v_node.tokens}\")\n",
    "        # print(f\"\\t\\t{o_node.tokens}\")\n",
    "        \n",
    "        # Subject Tokens Transferred to Object\n",
    "        s_transfer_tokens = []\n",
    "        for token in s_node.tokens:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                s_transfer_tokens.append(token)\n",
    "        \n",
    "        s_node.tokens = [t for t in s_node.tokens if t not in s_transfer_tokens]\n",
    "        o_node.tokens.extend(s_transfer_tokens)\n",
    "        \n",
    "        # Object Tokens Transferred to Verb\n",
    "        o_transfer_tokens = []\n",
    "        for token in o_node.tokens:\n",
    "            if token in main.cause.tokens or token in main.change.tokens:\n",
    "                o_transfer_tokens.append(token)\n",
    "        \n",
    "        o_node.tokens = [t for t in o_node.tokens if t not in o_transfer_tokens]\n",
    "        v_node.tokens.extend(o_transfer_tokens)\n",
    "\n",
    "        # print(\"\\tAfter\")\n",
    "        # print(f\"\\t\\t{s_node.tokens}\")\n",
    "        # print(f\"\\t\\t{v_node.tokens}\")\n",
    "        # print(f\"\\t\\t{o_node.tokens}\")\n",
    "\n",
    "        return (s_node, v_node, o_node)\n",
    "\n",
    "\n",
    "    \n",
    "    def fix_triple_direction(self, s_node, v_node, o_node):\n",
    "        v_node_tokens = sorted(v_node.tokens, key=lambda t: t.i)\n",
    "        v_node_tokens_l = v_node_tokens[0]\n",
    "        v_node_tokens_r = v_node_tokens[-1]\n",
    "        \n",
    "        aux = v_node_tokens_l.nbor(-1) and v_node_tokens_l.nbor(-1).pos_ == \"AUX\"\n",
    "        adp = v_node_tokens_r.nbor(1) and v_node_tokens_r.nbor(1).pos_ == \"ADP\"\n",
    "        \n",
    "        if not aux and not adp:\n",
    "            return (s_node, v_node, o_node)\n",
    "        return (o_node, v_node, s_node)\n",
    "\n",
    "\n",
    "    \n",
    "    def fix_triple_all_tokens(self, s_node, v_node, o_node):\n",
    "        s_node.all_tokens = flatten([Node.extend_token(main, t) for t in s_node.tokens])\n",
    "        o_node.all_tokens = flatten([Node.extend_token(main, t) for t in o_node.tokens])\n",
    "        return (s_node, v_node, o_node)\n",
    "\n",
    "\n",
    "    \n",
    "    def push_bound_l(self, bound, min_bound):\n",
    "        print(\"Push L\")\n",
    "        return self.push_bound(\n",
    "            bound, \n",
    "            reverse=True, \n",
    "            unit_condition_fnc=lambda unit: unit.l >= min_bound,\n",
    "            update_bound_fnc=lambda bound, unit: min(bound, unit.l)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def push_bound_r(self, bound, max_bound):\n",
    "        print(\"Push R\")\n",
    "        return self.push_bound(\n",
    "            bound, \n",
    "            reverse=False, \n",
    "            # unit_condition_fnc=lambda unit: unit.r <= max_bound,\n",
    "            unit_condition_fnc=lambda unit: True,\n",
    "            update_bound_fnc=lambda bound, unit: max(bound, unit.r)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def push_bound(self, bound, reverse=False, unit_condition_fnc=None, update_bound_fnc=None):\n",
    "        units = self.main.units.units_at_i(bound)\n",
    "        if not units:\n",
    "            return bound\n",
    "\n",
    "        start = False\n",
    "        unit_map_vals = list(main.units.unit_map.values())\n",
    "        unit_map_vals = sorted(unit_map_vals, key=lambda u: (u.l, u.r), reverse=reverse)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(unit_map_vals):\n",
    "            unit = unit_map_vals[i]\n",
    "            print(f\"Unit: {unit}\")\n",
    "\n",
    "            # Units must be in the same sentence as bound.\n",
    "            if units[0].sent_start() != unit.sent_start():\n",
    "                i += 1\n",
    "                print(\"Not Same Sent\")\n",
    "                continue\n",
    "\n",
    "            if unit not in units:\n",
    "                i += 1\n",
    "                print(\"Not In Unit\")\n",
    "                continue\n",
    "            \n",
    "            accept_labels = [\n",
    "                Unit.LIST,\n",
    "                Unit.ITEM,\n",
    "                Unit.D_CLAUSE,\n",
    "                Unit.P_PHRASE\n",
    "            ]\n",
    "\n",
    "            j = i + 1\n",
    "            while j < len(unit_map_vals) and unit_map_vals[j] not in units:\n",
    "                if unit_map_vals[j].label_has([Unit.FRAGMENT, *accept_labels]):\n",
    "                    print(f\"Push to Include: {unit_map_vals[j]}\")\n",
    "                    bound = update_bound_fnc(bound, unit_map_vals[j])\n",
    "                    break\n",
    "                j += 1\n",
    "                \n",
    "            break\n",
    "\n",
    "        return bound\n",
    "\n",
    "\n",
    "    \n",
    "    def split_tokens(self, tokens):\n",
    "        verbs = [t for t in tokens if t.pos_ == \"VERB\"]\n",
    "        print(verbs)\n",
    "\n",
    "        # L Bound\n",
    "        # It's the first token.\n",
    "        l = tokens[0].i\n",
    "\n",
    "        # M Bound\n",
    "        # The L part of this bound is the first verb.\n",
    "        # The R part of this bound is the last verb.\n",
    "        v_l = verbs[+0].i # Do Not Ask\n",
    "        v_r = verbs[-1].i\n",
    "\n",
    "        # R Bound\n",
    "        # We look for the first noun after the last verb.\n",
    "        i = tokens.index(verbs[-1]) + 1\n",
    "        while i < len(tokens) and tokens[i].pos_ not in [\"PROPN\", \"NOUN\", \"PRON\"]:\n",
    "            i += 1\n",
    "\n",
    "        # No Noun Found in R Side\n",
    "        no_noun_r = i <= 0 or i >= len(tokens)\n",
    "\n",
    "        # No Noun Found in L Side\n",
    "        no_noun_l = not set([t.pos_ for t in self.main.sp_doc[l:v_l+1]]) & set([\"PROPN\", \"NOUN\", \"PRON\"])\n",
    "\n",
    "        if no_noun_r and no_noun_l:\n",
    "            return (-1, -1, -1, -1)\n",
    "        elif no_noun_r:\n",
    "            return (l, v_l, v_r, v_r)\n",
    "        \n",
    "        r = tokens[i].i\n",
    "        if no_noun_l:\n",
    "            return (v_l, v_l, v_r, r)\n",
    "        else:\n",
    "            return (l, v_l, v_r, r)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def convert_partial_tokens(self, *, l, r):\n",
    "        tokens = [self.main.sp_doc[i] for i in range(l, r+1)]\n",
    "        \n",
    "        v_node = Node(self.main, is_action=True)\n",
    "        v_node.tokens = [token for token in tokens if token.pos_ in [\"VERB\", \"ADV\", \"AUX\"]]\n",
    "\n",
    "        o_node = Node(self.main)\n",
    "        o_node.tokens = [token for token in tokens if token not in v_node.tokens]\n",
    "\n",
    "        o_node_valid = o_node.get_species() or o_node.get_traits()\n",
    "        if not o_node_valid:\n",
    "            return None\n",
    "\n",
    "        event = Event(self.main)\n",
    "        event.order = [v_node, o_node]\n",
    "\n",
    "        return event\n",
    "    \n",
    "    \n",
    "    \n",
    "    def convert_tokens(self, tokens, split=None):\n",
    "        tokens = sorted(tokens, key=lambda t: t.i)\n",
    "        speech = [t.pos_ for t in tokens]\n",
    "\n",
    "        # print(f\"Tokens: {tokens}\")\n",
    "        # print(f\"Speech: {speech}\")\n",
    "\n",
    "        # print(f\"'VERB' not in Speech: {'VERB' not in speech}\")\n",
    "        # print(f\"Speech[0] is ADP/SCONJ: {speech[0] in ['ADP', 'SCONJ']}\")\n",
    "        \n",
    "        if \"VERB\" not in speech:\n",
    "            if speech[0] in [\"ADP\", \"SCONJ\"]:\n",
    "                event = Event(self.main)\n",
    "                event.order = [Node(self.main, tokens=tokens)]\n",
    "                # print(f\"Returned Event: {event}\")\n",
    "                return [event]\n",
    "            else:\n",
    "                # print(\"No Verb, No ADP/SCONJ\")\n",
    "                return []\n",
    "\n",
    "        if len(tokens) <= 2:\n",
    "            return []\n",
    "        \n",
    "        l, v_l, v_r, r = split or self.split_tokens(tokens)\n",
    "        \n",
    "        if l == v_l == v_r == r:\n",
    "            return []\n",
    "        elif l == v_l:\n",
    "            event = self.convert_partial_tokens(l=v_l, r=r)\n",
    "            print(\"HELLO\")\n",
    "            print(event)\n",
    "            return [] if not event else [event]\n",
    "        elif r == v_r:\n",
    "            event = self.convert_partial_tokens(l=l, r=v_r)\n",
    "            print(\"HELLO\")\n",
    "            print(event)\n",
    "            return [] if not event else [event]\n",
    "        \n",
    "        print(f\"Split: ({l}, {v_l}, {v_r}, {r})\")\n",
    "        \n",
    "        l = self.push_bound_l(l, tokens[+0].i) # Do Not Ask\n",
    "        print(f\"Pushed L: {l}\")\n",
    "\n",
    "        print(f\"Max Bound: {tokens[-1].i}\")\n",
    "        r = self.push_bound_r(r, tokens[-1].i)\n",
    "        print(f\"Pushed R: {r}\")\n",
    "        \n",
    "        v_node = Node(self.main, is_action=True)\n",
    "        v_node.tokens = [self.main.sp_doc[i] for i in range(v_l, v_r+1)]\n",
    "        print(f\"v_node.tokens: {v_node.tokens}\")\n",
    "        \n",
    "        s_node = Node(self.main)\n",
    "        s_node.tokens = [self.main.sp_doc[i] for i in range(l, v_l)]\n",
    "        print(f\"s_node.tokens: {s_node.tokens}\")\n",
    "            \n",
    "        o_node = Node(self.main)\n",
    "        o_node.tokens = [self.main.sp_doc[i] for i in range(v_r+1, r+1)]\n",
    "        print(f\"o_node.tokens: {o_node.tokens}\")\n",
    "\n",
    "        s_node, v_node, o_node = self.fix_triple_direction(s_node, v_node, o_node)\n",
    "        s_node, v_node, o_node = self.fix_triple_tokens(s_node, v_node, o_node)\n",
    "        s_node, v_node, o_node = self.fix_triple_all_tokens(s_node, v_node, o_node)\n",
    "\n",
    "        print(\"Fixed All Tokens\")\n",
    "        print(f\"s_node.tokens: {s_node.tokens}\")\n",
    "        print(f\"v_node.tokens: {v_node.tokens}\")\n",
    "        print(f\"o_node.tokens: {o_node.tokens}\")\n",
    "\n",
    "        s_node_valid = s_node.get_species() or s_node.get_traits()\n",
    "        o_node_valid = o_node.get_species() or o_node.get_traits()\n",
    "\n",
    "        # print(f\"s_node Valid: {s_node_valid}\")\n",
    "        # print(f\"\\ts_node Traits: {s_node.get_traits()}\")\n",
    "        # print(f\"\\ts_node Species: {s_node.get_species()}\")\n",
    "        \n",
    "        # print(f\"o_node Valid: {o_node_valid}\")\n",
    "        # print(f\"\\to_node Traits: {o_node.get_traits()}\")\n",
    "        # print(f\"\\to_node Species: {o_node.get_species()}\")\n",
    "        \n",
    "        if not s_node_valid and not o_node_valid:\n",
    "            return []\n",
    "        \n",
    "        event = Event(self.main)\n",
    "        event.order = [s_node, v_node, o_node]\n",
    "\n",
    "        events = [event]\n",
    "        events.extend(self.convert_tokens(s_node.tokens))\n",
    "        events.extend(self.convert_tokens(o_node.tokens))\n",
    "        \n",
    "        return [event for event in events if event]\n",
    "\n",
    "\n",
    "    \n",
    "    def convert_unit(self, unit):\n",
    "        tokens = [*self.main.sp_doc[unit.l:unit.r+1]]\n",
    "        return self.convert_tokens(tokens)\n",
    "\n",
    "\n",
    "    \n",
    "    def convert_svo_triple(self, triple):\n",
    "        tokens = [*triple.subject, *triple.object]\n",
    "        tokens = sorted(tokens, key=lambda t: t.i)\n",
    "    \n",
    "        l = tokens[+0].i\n",
    "        v_l = triple.verb[+0].i\n",
    "        v_r = triple.verb[-1].i\n",
    "        r = tokens[-1].i\n",
    "\n",
    "        tokens = [*self.main.sp_doc[l:r+1]]\n",
    "        return self.convert_tokens(tokens, split=(l, v_l, v_r, r))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def distinct_events(self, events):\n",
    "        event_bounds_mapped = {(e.start(), e.end()): e for e in events}\n",
    "        event_bounds = event_bounds_mapped.keys()   \n",
    "        distinct_event_bounds = distinct_bounds(event_bounds, larger=True)    \n",
    "        distinct_events = [event_bounds_mapped[b] for b in distinct_event_bounds]\n",
    "        return distinct_events\n",
    "    \n",
    "\n",
    "    \n",
    "    def find_events(self):\n",
    "        sents = list(self.main.sp_doc.sents)\n",
    "        sents_events = {sent.start: [] for sent in sents}\n",
    "\n",
    "         # Units\n",
    "        for unit_tokens in self.main.units.aggregate_units():\n",
    "            print(f\"Unit Tokens: {unit_tokens}\")\n",
    "            events = self.convert_tokens(unit_tokens)\n",
    "            for event in events:\n",
    "                print(f\"\\tEvent: {event}\")\n",
    "            sent_start = unit_tokens[0].sent.start\n",
    "            sents_events[sent_start].extend(events)\n",
    "        print()\n",
    "        \n",
    "        # Triples\n",
    "        for triple in textacy.extract.subject_verb_object_triples(self.main.sp_doc):\n",
    "            print(f\"Triple: {triple}\")\n",
    "            events = self.convert_svo_triple(triple)\n",
    "            for event in events:\n",
    "                print(f\"\\tEvent: {event}\")\n",
    "            sent_start = triple.verb[0].sent.start\n",
    "            sents_events[sent_start].extend(events)\n",
    "        print()\n",
    "\n",
    "        sents_events = {k: self.distinct_events(v) for k, v in sents_events.items()}\n",
    "        return sents_events\n",
    "    \n",
    "    \n",
    "    \n",
    "    def overlap_in_species(self, x, y):\n",
    "        sp_X = [self.main.species.span_at_token(species) for species in x.get_species()]\n",
    "        sp_Y = [self.main.species.span_at_token(species) for species in y.get_species()]\n",
    "\n",
    "        for sp_x in sp_X:\n",
    "            if self.main.species.find_same_species(sp_Y, sp_x):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "\n",
    "    def overlap_in_traits(self, x, y):\n",
    "        tr_X = set([trait.lemma_.lower() for trait in x.get_traits()])\n",
    "        tr_Y = set([trait.lemma_.lower() for trait in y.get_traits()])\n",
    "\n",
    "        return tr_X & tr_Y\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_non_species_nouns(self, tokens):\n",
    "        nouns = []\n",
    "        for token in tokens:            \n",
    "            if token in self.main.species.tokens:\n",
    "                continue\n",
    "\n",
    "            if token.pos_ not in [\"PROPN\", \"NOUN\"]:\n",
    "                continue\n",
    "            \n",
    "            nouns.append(token.lower_)\n",
    "            nouns.append(token.lemma_.lower())\n",
    "        return nouns\n",
    "\n",
    "\n",
    "    \n",
    "    def overlap_in_tokens(self, x, y):\n",
    "        x_tokens = self.get_non_species_nouns(x.all_tokens)\n",
    "        y_tokens = self.get_non_species_nouns(y.all_tokens)\n",
    "        return set(x_tokens) & set(y_tokens)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def has_causal_phrase(self, x):\n",
    "        speech = [token.pos_ for token in x.tokens]\n",
    "        return speech[0] in [\"ADP\", \"SCONJ\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def has_effectual_phrase(self, y):\n",
    "        lower = [token.lower_ for token in y.tokens]\n",
    "        return \"which\" in lower\n",
    "\n",
    "\n",
    "    \n",
    "    def has_causal_phrase_and_pron(self, n):\n",
    "        speech = set([token.pos_ for token in n.tokens])\n",
    "        causal_speech = speech & set([\"ADP\", \"SCONJ\"])\n",
    "        pronoun_speech = speech & set([\"PRON\"])\n",
    "        return causal_speech and pronoun_speech\n",
    "    \n",
    "    \n",
    "    \n",
    "    def can_merge_intrasent(self, X, Y):\n",
    "        x = X.order[-1]\n",
    "        y = Y.order[+0] # Do Not Ask\n",
    "\n",
    "        can_merge = self.overlap_in_species(x, y)\n",
    "        print(f\"\\tCan Merge by Species: {can_merge}\")\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "\n",
    "        can_merge = self.overlap_in_tokens(x, y)\n",
    "        print(f\"\\tCan Merge by Tokens: {can_merge}\")\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "\n",
    "        can_merge = self.has_causal_phrase(x)\n",
    "        print(f\"\\tCan Merge by Causal Phrase: {can_merge}\")\n",
    "        if can_merge:\n",
    "            return (True, False)\n",
    "\n",
    "        can_merge = self.has_effectual_phrase(y)\n",
    "        print(f\"\\tCan Merge by Effectual Phrase: {can_merge}\")\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "        \n",
    "        return (False, False)\n",
    "    \n",
    "    \n",
    "\n",
    "    def can_merge_intersent(self, X, Y):\n",
    "        x = X.order[-1]\n",
    "        y = Y.order[+0] # Do Not Ask\n",
    "\n",
    "        can_merge = self.overlap_in_species(x, y)\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "\n",
    "        can_merge = self.overlap_in_tokens(x, y)\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "\n",
    "        can_merge = self.has_causal_phrase_and_pron(y)\n",
    "        if can_merge:\n",
    "            return (True, False)\n",
    "        \n",
    "        return (False, False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def intrasent_sequences(self, indices_1D):\n",
    "        power_set_indices = power_set(indices_1D, current=[], result=[])\n",
    "        sequences = []\n",
    "        for subset in power_set_indices:\n",
    "            if subset:\n",
    "                sequences.extend(list(permutations(subset)))\n",
    "        return sequences\n",
    "\n",
    "\n",
    "    \n",
    "    def intersent_sequences(self, indices_2D):\n",
    "        sequences = interleave(indices_2D, current=[], used=[], result=[])\n",
    "        return sequences\n",
    "    \n",
    "    \n",
    "    \n",
    "    def merge_intrasent(self, sent_events):\n",
    "        i = 0\n",
    "        while i + 1 < len(sent_events):\n",
    "            X = sent_events[i]\n",
    "            Y = sent_events[i+1]\n",
    "\n",
    "            print(f\"X: {X}\")\n",
    "            print(f\"Y: {Y}\")\n",
    "            \n",
    "            can_merge, del_end = self.can_merge_intrasent(X, Y)\n",
    "\n",
    "            print(f\"Can Merge: {can_merge}\")\n",
    "            \n",
    "            if not can_merge:\n",
    "                return None\n",
    "\n",
    "            X.attach_event(Y, del_end=del_end)\n",
    "            sent_events.pop()\n",
    "\n",
    "        if len(sent_events[0].order) <= 2:\n",
    "            return None\n",
    "        return sent_events[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def merge_intersent(self, events):\n",
    "        i = 0\n",
    "        while i + 1 < len(events):\n",
    "            X = events[i]\n",
    "            Y = events[i+1]\n",
    "\n",
    "            can_merge, del_end = self.can_merge_intersent(X, Y)\n",
    "            if not can_merge:\n",
    "                return None\n",
    "\n",
    "            X.attach_event(Y, del_end=del_end)\n",
    "            events.pop()\n",
    "\n",
    "        return events[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_events(self):\n",
    "        sents_events = self.find_events()\n",
    "\n",
    "        print(\"Found Events:\")\n",
    "        for events in sents_events.values():\n",
    "            for event in events:\n",
    "                print(event)\n",
    "        print()\n",
    "            \n",
    "        merged_sents_events = []\n",
    "        \n",
    "        for sent_events in sents_events.values():\n",
    "            merged_sents_events.append([])\n",
    "\n",
    "            # Each event can be represented with its index\n",
    "            # for simplicity.\n",
    "            event_indices = list(range(len(sent_events)))\n",
    "            print(f\"Event Indices (1D): {event_indices}\")\n",
    "\n",
    "            # The order in which you merge an event can result\n",
    "            # in a different outcome. For example, E1 + E2 may\n",
    "            # look different to E2 + E1, and so on.\n",
    "\n",
    "            # Idea:\n",
    "            # We try all the possible ways you can merge, which is\n",
    "            # the same as trying all the different orders you can\n",
    "            # merge the events. For example, merging E1, E2, and E3\n",
    "            # can be done in 8 ways: E1, E2, E3, E1 + E2, E2 + E1,\n",
    "            # and so on.\n",
    "            \n",
    "            # These are all the possible orders (or sequences) in \n",
    "            # which you can merge the above events. We will try all\n",
    "            # possible sequences, adding those that work.\n",
    "            sequences = self.intrasent_sequences(event_indices)\n",
    "            print(f\"Possible Sequences (1D): {sequences}\")\n",
    "\n",
    "            for sequence in sequences:\n",
    "                events = [sent_events[i].copy() for i in sequence]\n",
    "                merged_event = self.merge_intrasent(events)\n",
    "                if merged_event:\n",
    "                    print(f\"Merged Event: {merged_event}\")\n",
    "                    merged_sents_events[-1].append(merged_event)\n",
    "            print()\n",
    "        \n",
    "        # Idea:\n",
    "        # Now that we've found all the events that can be made from\n",
    "        # merging the events within a sentence, we can merge across\n",
    "        # sentences. To do this, we also need to try all different\n",
    "        # sequences of events. However, we shouldn't try and merge\n",
    "        # events from the same sentence again, which means that no\n",
    "        # two consecutive events in the sequence can be from the same\n",
    "        # sentence. There's also another issue: we can't use a simple\n",
    "        # 1D-index for each event as we're dealing with a 2D-list of events.\n",
    "        # Therefore, we'll use the index equivalent to its 1D-form, or\n",
    "        # something similar as we don't have a matrix.\n",
    "        # If this doesn't make sense, I wouldn't be surprised, I'm more\n",
    "        # of a pictures person.\n",
    "        i = 0\n",
    "        event_indices = []\n",
    "        index_to_event = {}\n",
    "        for sent_events in merged_sents_events:\n",
    "            event_indices.append([])\n",
    "            for event in sent_events:\n",
    "                index_to_event[i] = event\n",
    "                event_indices[-1].append(i)\n",
    "                i += 1\n",
    "\n",
    "        \n",
    "        print(f\"Event Indices (2D): {event_indices}\")\n",
    "        sequences = self.intersent_sequences(event_indices)\n",
    "        print(f\"Possible Sequences (2D): {sequences}\")\n",
    "        \n",
    "        merged_events = []\n",
    "        \n",
    "        for sequence in sequences:\n",
    "            events = [index_to_event[i].copy() for i in sequence]\n",
    "            merged_event = self.merge_intersent(events)\n",
    "            if merged_event:\n",
    "                print(f\"Merged Event: {merged_event}\")\n",
    "                merged_events.append(merged_event)\n",
    "        \n",
    "        return merged_events\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update(self):\n",
    "        self.events = self.load_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "037079d2-3ece-40a1-923e-84936e2b44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main = Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "495eb192-04e6-4dd0-baae-5bfdf02803e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/07/2025 00:10:09 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 59.29 examples/s]\n",
      "09/07/2025 00:10:16 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:05<00:00,  5.67s/it]\n"
     ]
    }
   ],
   "source": [
    "text = \"We saw that inclusion of cat resulted in the disappearance of dog.\"\n",
    "main.update_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77ea920a-0913-4875-8405-46e6a09cbf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Tokens: [We, saw]\n",
      "Unit Tokens: [that, inclusion, of, cat, resulted, in, the, disappearance, of, dog, .]\n",
      "[resulted]\n",
      "Split: (2, 6, 6, 9)\n",
      "Push L\n",
      "Unit: of dog\n",
      "Not In Unit\n",
      "Unit: in the disappearance\n",
      "Not In Unit\n",
      "Unit: of cat\n",
      "Not In Unit\n",
      "Unit: that inclusion of cat resulted in the disappearance of dog.\n",
      "Push to Include: We saw\n",
      "Pushed L: 0\n",
      "Max Bound: 12\n",
      "Push R\n",
      "Unit: We saw\n",
      "Not In Unit\n",
      "Unit: that inclusion of cat resulted in the disappearance of dog.\n",
      "Push to Include: of cat\n",
      "Pushed R: 9\n",
      "v_node.tokens: [resulted]\n",
      "s_node.tokens: [We, saw, that, inclusion, of, cat]\n",
      "o_node.tokens: [in, the, disappearance]\n",
      "Fixed All Tokens\n",
      "s_node.tokens: [in, the, disappearance]\n",
      "v_node.tokens: [resulted]\n",
      "o_node.tokens: [We, saw, that, inclusion, of, cat]\n",
      "[saw]\n",
      "Split: (0, 1, 1, 3)\n",
      "Push L\n",
      "Unit: of dog\n",
      "Not In Unit\n",
      "Unit: in the disappearance\n",
      "Not In Unit\n",
      "Unit: of cat\n",
      "Not In Unit\n",
      "Unit: that inclusion of cat resulted in the disappearance of dog.\n",
      "Not In Unit\n",
      "Unit: We saw\n",
      "Pushed L: 0\n",
      "Max Bound: 5\n",
      "Push R\n",
      "Unit: We saw\n",
      "Not In Unit\n",
      "Unit: that inclusion of cat resulted in the disappearance of dog.\n",
      "Push to Include: of cat\n",
      "Pushed R: 5\n",
      "v_node.tokens: [saw]\n",
      "s_node.tokens: [We]\n",
      "o_node.tokens: [that, inclusion, of, cat]\n",
      "Fixed All Tokens\n",
      "s_node.tokens: [We]\n",
      "v_node.tokens: [saw]\n",
      "o_node.tokens: [that, inclusion, of, cat]\n",
      "\tEvent: (in,the,disappearance)->(resulted)->(We,saw,that,inclusion,of,cat)\n",
      "\tEvent: (in,the,disappearance)\n",
      "\tEvent: (We)->(saw)->(that,inclusion,of,cat)\n",
      "\tEvent: (that,inclusion,of,cat)\n",
      "\n",
      "\n",
      "Found Events:\n",
      "(in,the,disappearance)->(resulted)->(We,saw,that,inclusion,of,cat)\n",
      "\n",
      "Event Indices (1D): [0]\n",
      "Possible Sequences (1D): [(0,)]\n",
      "Merged Event: (in,the,disappearance)->(resulted)->(We,saw,that,inclusion,of,cat)\n",
      "\n",
      "Event Indices (2D): [[0]]\n",
      "Possible Sequences (2D): [[0]]\n",
      "Merged Event: (in,the,disappearance)->(resulted)->(We,saw,that,inclusion,of,cat)\n",
      "(in,the,disappearance)->(resulted)->(We,saw,that,inclusion,of,cat)\n"
     ]
    }
   ],
   "source": [
    "event_manager = EventManager(main)\n",
    "event_manager.update()\n",
    "\n",
    "for event in event_manager.events:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41fffc71-9497-412b-a025-72467601887f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cat, dog]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.species.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a49c12c-42fd-4ad8-8201-7885c29258bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(cat, dog)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.species.tn_doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8210d2-0775-4e52-985c-e00c5ce294a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
