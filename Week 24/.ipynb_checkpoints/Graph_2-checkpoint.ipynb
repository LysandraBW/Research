{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b0bd8a-5d3a-4417-ad6b-1fc63e7dd50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import spacy\n",
    "import textacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from taxonerd import TaxoNERD\n",
    "from fastcoref import spacy_component\n",
    "from spacy.matcher import Matcher, DependencyMatcher, PhraseMatcher\n",
    "%run \"./Main.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955b168a-290b-4e8a-802c-933b67850019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colors:\n",
    "    HEADER = '\\033[95m'\n",
    "    RED = '\\033[91m'\n",
    "    BLUE = '\\033[94m'\n",
    "    CYAN = '\\033[96m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "baa69741-6e0c-4719-ab00-126243626a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mExample\u001b[0m\n",
      "\u001b[0mAOL \u001b[0m\u001b[0mcaused \u001b[0m\u001b[0mGMAIL\u001b[0m\u001b[0m, \u001b[0m\u001b[0mYAHOO\u001b[0m\u001b[0m, \u001b[0m\u001b[0mand \u001b[0m\u001b[0mOUTLOOK \u001b[0m\u001b[0mto \u001b[0m\u001b[0mshut \u001b[0m\u001b[0mdown\u001b[0m\u001b[0m. \u001b[0m"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(\"../Week 16/Datasets/Baseline-1.csv\")\n",
    "\n",
    "# i = 0\n",
    "# title = df.loc[i, \"Title\"]\n",
    "# abstract = df.loc[i, \"Abstract\"]\n",
    "\n",
    "title = \"Example\"\n",
    "abstract = \"AOL caused GMAIL, YAHOO, and OUTLOOK to shut down.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(abstract)\n",
    "\n",
    "# Title\n",
    "print(f\"{Colors.BOLD}{title}{Colors.ENDC}\")\n",
    "\n",
    "# Abstract\n",
    "verbs = []\n",
    "objects = []\n",
    "subjects = []\n",
    "\n",
    "for triple in textacy.extract.subject_verb_object_triples(doc):\n",
    "    verbs.extend(triple.verb)\n",
    "    objects.extend(triple.object)\n",
    "    subjects.extend(triple.subject)\n",
    "    \n",
    "for token in doc:\n",
    "    color = Colors.ENDC\n",
    "    if token in objects:\n",
    "        color = Colors.YELLOW\n",
    "    if token in subjects:\n",
    "        color = Colors.BLUE\n",
    "    if token in verbs:\n",
    "        color = Colors.RED\n",
    "\n",
    "    if token.sent.end == token.i + 1:\n",
    "        print(f\"{color}{token.text} \", end=f\"{Colors.ENDC}\")\n",
    "    elif token.nbor() and token.nbor().text in [\".\",\"?\",\"!\",\";\", \")\", \",\", \"]\"]:\n",
    "        print(f\"{color}{token.text}\", end=f\"{Colors.ENDC}\")\n",
    "    elif token.sent.start == token.i:\n",
    "        print(f\"{color}{token.text} \", end=f\"{Colors.ENDC}\")\n",
    "    elif token.text not in [\"(\", \"[\"]:\n",
    "        print(f\"{color}{token.text} \", end=f\"{Colors.ENDC}\")\n",
    "    else:\n",
    "        print(f\"{color}{token.text}\", end=f\"{Colors.ENDC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f711463d-b61a-4610-92d7-9274c0f113de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2025 12:22:30 - INFO - \t missing_keys: []\n",
      "09/04/2025 12:22:30 - INFO - \t unexpected_keys: []\n",
      "09/04/2025 12:22:30 - INFO - \t mismatched_keys: []\n",
      "09/04/2025 12:22:30 - INFO - \t error_msgs: []\n",
      "09/04/2025 12:22:30 - INFO - \t Model Parameters: 590.0M, Transformer: 434.6M, Coref head: 155.4M\n",
      "09/04/2025 12:22:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  5.28 examples/s]\n",
      "09/04/2025 12:23:03 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.40s/it]\n"
     ]
    }
   ],
   "source": [
    "main = Main()\n",
    "main.update_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "de3c5a66-4483-4022-8d0c-266983c75abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, main, is_action=False, tokens=None):\n",
    "        self.main = main\n",
    "        self.tokens = tokens or []\n",
    "        self.expanded_tokens = []\n",
    "        self.is_action = is_action\n",
    "\n",
    "    def get_traits(self):\n",
    "        tokens = set([*self.tokens, *self.expanded_tokens])\n",
    "        tokens = tokens & set(self.main.trait.tokens)\n",
    "        return tokens\n",
    "\n",
    "    def get_species(self):\n",
    "        tokens = set([*self.tokens, *self.expanded_tokens])\n",
    "        tokens = tokens & set(self.main.species.tokens)\n",
    "        return tokens\n",
    "\n",
    "    def copy(self):\n",
    "        node = Node(self.main)\n",
    "        node.tokens = [*self.tokens]\n",
    "        node.expanded_tokens = [*self.expanded_tokens]\n",
    "        node.is_action = self.is_action\n",
    "        return node\n",
    "\n",
    "    def start(self):\n",
    "        i = math.inf\n",
    "        for token in self.tokens:\n",
    "            i = min(i, token.i)\n",
    "        return i\n",
    "\n",
    "    def end(self):\n",
    "        i = -math.inf\n",
    "        for token in self.tokens:\n",
    "            i = max(i, token.i)\n",
    "        return i\n",
    "\n",
    "    def get_tokens(self):\n",
    "        return self.tokens\n",
    "\n",
    "    def get_expanded_tokens(self):\n",
    "        return [*self.tokens, *self.expanded_tokens]\n",
    "\n",
    "    def __str__(self):\n",
    "        tokens = list(set([*self.tokens, *self.expanded_tokens]))\n",
    "        tokens = sorted(tokens, key=lambda token: token.i)\n",
    "        return f\"{','.join([t.text for t in tokens])}\"\n",
    "\n",
    "class Order:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.order = []\n",
    "\n",
    "    def start(self):\n",
    "        i = math.inf\n",
    "        for item in self.order:\n",
    "            i = min(i, item.start())\n",
    "        return i\n",
    "\n",
    "    def end(self):\n",
    "        i = -math.inf\n",
    "        for item in self.order:\n",
    "            i = max(i, item.end())\n",
    "        return i\n",
    "\n",
    "    def copy(self):\n",
    "        order = Order(self.main)\n",
    "        order.order = []\n",
    "        for item in self.order:\n",
    "            order.order.append(item.copy())\n",
    "        return order\n",
    "        \n",
    "    def get_tokens(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_tokens())\n",
    "        return ret\n",
    "\n",
    "    def get_expanded_tokens(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_expanded_tokens())\n",
    "        return ret\n",
    "\n",
    "    def get_species(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_species())\n",
    "        return ret\n",
    "\n",
    "    def get_traits(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_traits())\n",
    "        return ret\n",
    "\n",
    "    def __str__(self):\n",
    "        ret = \"\"\n",
    "\n",
    "        i = 0\n",
    "        size = len(self.order)\n",
    "        while i < size:\n",
    "            ret += f\"({self.order[i]})\"\n",
    "            if i != size - 1:\n",
    "                ret += \"->\"\n",
    "            i += 1\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4db42460-80ff-420f-99ac-9027287f15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_bounds(bounds, larger=True):\n",
    "    d_bounds = []\n",
    "\n",
    "    for bound in bounds:\n",
    "        overlap = False\n",
    "        for i, d_bound in enumerate(d_bounds):\n",
    "            surround = bound[0] <= d_bound[0] <= bound[1] and bound[0] <= d_bound[1] <= bound[1]\n",
    "            contains = d_bound[0] <= bound[0] <= d_bound[1] and d_bound[0] <= bound[1] <= d_bound[1]\n",
    "\n",
    "            overlap = surround or contains\n",
    "\n",
    "            bound_length = bound[1] - bound[0]\n",
    "            d_bound_length = d_bound[1] - d_bound[0]\n",
    "            \n",
    "            if (surround and larger) or (contains and not larger):\n",
    "                d_bounds[i] = bound\n",
    "                \n",
    "        if not overlap:\n",
    "            d_bounds.append(bound)\n",
    "\n",
    "    return list(set(d_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1cb34d43-dfd8-4041-9578-13051e3a35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def expand_token(main, token):\n",
    "    expanded_token = [token]\n",
    "\n",
    "    # 1. Expand by Coreference\n",
    "    if token.pos_ == \"PRON\":\n",
    "        expanded_token = [*main.coref_map.get(token, [token])]\n",
    "\n",
    "    # 2. Expand by Unit Map\n",
    "    i = 0\n",
    "    size = len(expanded_token)\n",
    "    while i < size:\n",
    "        exp_token = expanded_token[i]\n",
    "        for unit_bound, unit in main.units.unit_map.items():\n",
    "            if unit_bound[0] <= exp_token.i <= unit_bound[1] and unit.label_has([Unit.ITEM]):\n",
    "                expanded_token = [*main.sp_doc[unit_bound[0]:unit_bound[1]+1]]\n",
    "                break\n",
    "        i += 1\n",
    "    \n",
    "    i = 0\n",
    "    size = len(expanded_token)\n",
    "    while i < size:\n",
    "        exp_token = expanded_token[i]\n",
    "\n",
    "        # 3. Expand by Noun Chunk\n",
    "        if exp_token in main.noun_chunk_map:\n",
    "            expanded_token.extend([*main.noun_chunk_map[exp_token]])\n",
    "\n",
    "        # 4. Expand by Unit\n",
    "        if exp_token in main.entity_map:\n",
    "            expanded_token.extend([*main.entity_map[exp_token]])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Remove Duplicates and Sort\n",
    "    expanded_token = list(set(expanded_token))\n",
    "    expanded_token = sorted(expanded_token, key=lambda token: token.i)\n",
    "    \n",
    "    return expanded_token\n",
    "\n",
    "\n",
    "\n",
    "def create_order(s_node, v_node, o_node):\n",
    "    # 1. Subject Tokens Transferred to Object\n",
    "    sub_transfer_tokens = []\n",
    "    for token in s_node.tokens:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            sub_transfer_tokens.append(token)\n",
    "    \n",
    "    s_node.tokens = [tkn for tkn in s_node.tokens if tkn not in sub_transfer_tokens]\n",
    "    o_node.tokens.extend(sub_transfer_tokens)\n",
    "    \n",
    "    # 2. Object Tokens Transferred to Verb\n",
    "    obj_transfer_tokens = []\n",
    "    for token in o_node.tokens:\n",
    "        if token in main.cause.tokens or token in main.change.tokens:\n",
    "            obj_transfer_tokens.append(token)\n",
    "    \n",
    "    o_node.tokens = [tkn for tkn in o_node.tokens if tkn not in obj_transfer_tokens]\n",
    "    v_node.tokens.extend(obj_transfer_tokens)\n",
    "\n",
    "    # 3. Expand Tokens in Subject\n",
    "    s_expanded_tokens = flatten([expand_token(main, tkn) for tkn in s_node.tokens])\n",
    "    s_node.expanded_tokens = s_expanded_tokens\n",
    "\n",
    "    # 3. Expand Tokens in Object\n",
    "    o_expanded_tokens = flatten([expand_token(main, tkn) for tkn in o_node.tokens])\n",
    "    o_node.expanded_tokens = o_expanded_tokens\n",
    "\n",
    "    # Create Order S -> V -> 0\n",
    "    if (\n",
    "        not s_node.tokens or \n",
    "        not v_node.tokens or \n",
    "        not o_node.tokens\n",
    "    ):\n",
    "        return None\n",
    "    \n",
    "    order = Order(main)\n",
    "    order.order = [s_node, v_node, o_node]\n",
    "    \n",
    "    return order\n",
    "\n",
    "\n",
    "\n",
    "def swap_subject_object(verb):\n",
    "    aux = verb.nbor(-1) and verb.nbor(-1).pos_ == \"AUX\"\n",
    "    adp = verb.nbor(1) and verb.nbor(1).lower_ == \"by\"\n",
    "    return aux or adp\n",
    "\n",
    "\n",
    "\n",
    "def order_tokens(main, tokens):\n",
    "    if len(tokens) <= 1:\n",
    "        return None\n",
    "    \n",
    "    # Sort Tokens by Position in Doc\n",
    "    tokens = sorted(tokens, key=lambda token: token.i)\n",
    "    \n",
    "    verbs = [token for token in tokens if token.pos_ == \"VERB\"]\n",
    "    if not verbs:\n",
    "        # Check for \"(ADP|SCONJ) ...\"\n",
    "        if tokens[0].pos_ in [\"ADP\", \"SCONJ\"]:\n",
    "            order = Order(main)\n",
    "            order.order = [Node(main, tokens=tokens)]\n",
    "            return order\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    if len(tokens) <= 2:\n",
    "        return None\n",
    "    \n",
    "    # Sort Verbs by Token Position in Doc\n",
    "    verbs = sorted(verbs, key=lambda token: token.i)\n",
    "    verb = verbs[0]\n",
    "\n",
    "    if verb == tokens[0]:\n",
    "        return None\n",
    "\n",
    "    # Partition Tokens in Doc (L)\n",
    "    l = tokens[0].i\n",
    "\n",
    "    # Partition Tokens in Doc (R)\n",
    "    # As we don't know the S/O, we stop at the closest noun.\n",
    "    i = tokens.index(verb) + 1\n",
    "    while i < len(tokens) and tokens[i].pos_ not in [\"PROPN\", \"NOUN\", \"PRON\"]:\n",
    "        i += 1\n",
    "    \n",
    "    if i <= 0 or i >= len(tokens):\n",
    "        return None\n",
    "    \n",
    "    r = tokens[i].i\n",
    "\n",
    "    r_units = main.units.units_at_i(r)\n",
    "    if r_units:\n",
    "        start = False\n",
    "\n",
    "        unit_map_values = list(main.units.unit_map.values())\n",
    "        i = 0\n",
    "        while i < len(unit_map_values):\n",
    "            unit = unit_map_values[i]\n",
    "            \n",
    "            if r_units[0].sent_start() != unit.sent_start():\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            if unit in r_units:\n",
    "                start = True\n",
    "            \n",
    "            if not start:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "\n",
    "            print(\"HELLO!!!!\")\n",
    "            print(unit, unit.label_())\n",
    "            if unit.label_has([Unit.FRAGMENT, Unit.LIST, Unit.ITEM, Unit.D_CLAUSE, Unit.P_PHRASE]) and unit.r <= tokens[-1].i:\n",
    "                r = max(r, unit.r)\n",
    "\n",
    "                brk = True\n",
    "                if i + 1 < len(unit_map_values):\n",
    "                    print(\"HEY VENUS\")\n",
    "                    print(unit_map_values[i+1], unit_map_values[i+1].label_())\n",
    "                    print(\"WOO\\n\")\n",
    "                    \n",
    "                if i + 1 < len(unit_map_values) and unit_map_values[i+1].label_has([Unit.LIST, Unit.ITEM, Unit.D_CLAUSE, Unit.P_PHRASE]):\n",
    "                    brk = False\n",
    "\n",
    "                if brk:\n",
    "                    break\n",
    "            else:\n",
    "                print(\"nope\")\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "                \n",
    "    \n",
    "    # Create Nodes\n",
    "    v_node = Node(main, is_action=True)\n",
    "    v_node.tokens = [verb]\n",
    "    # print(\"v_node.tokens\", v_node.tokens)\n",
    "\n",
    "    a_node = Node(main)\n",
    "    a_node.tokens = [main.sp_doc[i] for i in range(l, verb.i)]\n",
    "    # print(\"a_node.tokens\", a_node.tokens)\n",
    "    if not a_node.tokens:\n",
    "        return None\n",
    "\n",
    "    b_node = Node(main)\n",
    "    b_node.tokens = [main.sp_doc[i] for i in range(verb.i+1, r+1)]\n",
    "    # print(\"b_node.tokens\", b_node.tokens)\n",
    "    if not b_node.tokens:\n",
    "        return None\n",
    "\n",
    "    # Swap Subject and Object\n",
    "    if swap_subject_object(verb):\n",
    "        s_node = b_node\n",
    "        o_node = a_node\n",
    "    else:\n",
    "        s_node = a_node\n",
    "        o_node = b_node\n",
    "\n",
    "    order = create_order(s_node, v_node, o_node)\n",
    "    return order\n",
    "\n",
    "\n",
    "\n",
    "def order_triple(main, triple):\n",
    "    # Create Verb Node\n",
    "    v_node = Node(main, is_action=True)\n",
    "    v_node.tokens = [*triple.verb]\n",
    "\n",
    "    # Find Subject and Object Nodes\n",
    "    # We assume that the S and O nodes are on the L and R sides\n",
    "    # of the verb.\n",
    "    svo_tokens = [*triple.subject, *triple.object]\n",
    "    svo_tokens = sorted(svo_tokens, key=lambda token: token.i)\n",
    "\n",
    "    # Positions\n",
    "    l = svo_tokens[0].i\n",
    "    r = svo_tokens[-1].i\n",
    "\n",
    "    # L and R Positions of Verb (Middle, M) Tokens\n",
    "    l_m = triple.verb[0].i\n",
    "    r_m = triple.verb[-1].i\n",
    "\n",
    "    # Create Nodes\n",
    "    a_node = Node(main)\n",
    "    a_node.tokens = [main.sp_doc[i] for i in range(l, l_m)]\n",
    "    a_node.expanded_tokens =  flatten([expand_token(main, tkn) for tkn in a_node.tokens])\n",
    "    \n",
    "    b_node = Node(main)\n",
    "    b_node.tokens = [main.sp_doc[i] for i in range(r_m+1, r+1)]\n",
    "    b_node.expanded_tokens =  flatten([expand_token(main, tkn) for tkn in b_node.tokens])\n",
    "\n",
    "    # Swap Subject and Object\n",
    "    if swap_subject_object(triple.verb[0]):\n",
    "        s_node = b_node\n",
    "        o_node = a_node\n",
    "    else:\n",
    "        s_node = a_node\n",
    "        o_node = b_node\n",
    "\n",
    "    ret = []\n",
    "    \n",
    "    order = create_order(s_node, v_node, o_node)\n",
    "    if order:\n",
    "        ret.append(order)\n",
    "    \n",
    "    sub_order = order_tokens(main, s_node.tokens)\n",
    "    if sub_order:\n",
    "        ret.append(sub_order)\n",
    "    \n",
    "    obj_order = order_tokens(main, o_node.tokens)\n",
    "    if obj_order:\n",
    "        ret.append(obj_order)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "def order_unit(main, unit):\n",
    "    unit_tokens = [*main.sp_doc[unit.l:unit.r+1]]\n",
    "    return order_tokens(main, unit_tokens)\n",
    "\n",
    "\n",
    "\n",
    "def order_text(main):\n",
    "    sents = list(main.sp_doc.sents)\n",
    "    sents_orders = {sent.start: [] for sent in sents}\n",
    "    sents_triples = {sent.start: [] for sent in sents}\n",
    "\n",
    "    # 1. Parse Subject-Verb-Object Triples\n",
    "    for triple in textacy.extract.subject_verb_object_triples(doc):\n",
    "        sents_triples[triple.verb[0].sent.start].append(triple)\n",
    "\n",
    "    for sent in sents:\n",
    "        for triple in sents_triples[sent.start]:\n",
    "            orders = order_triple(main, triple)\n",
    "            sents_orders[sent.start].extend(orders)\n",
    "    \n",
    "    # 2. Parse Units\n",
    "    unit_bounds = list(main.units.unit_map.keys())\n",
    "    distinct_unit_bounds = distinct_bounds(unit_bounds)\n",
    "    units = [unit[1] for unit in main.units.unit_map.items() if unit[0] in distinct_unit_bounds]\n",
    "\n",
    "    i = 0\n",
    "    tokens = []\n",
    "    while i < len(units):\n",
    "        unit = units[i]\n",
    "        tokens.extend([*unit.span()])\n",
    "        \n",
    "        next_unit = None if i + 1 >= len(units) else units[i+1]\n",
    "        if next_unit and next_unit.label_has([Unit.P_PHRASE, Unit.LIST]):\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        unit_order = order_tokens(main, tokens)\n",
    "        unit_sent_start = unit.sent_start()\n",
    "        tokens = []\n",
    "        \n",
    "        if not unit_order or unit_sent_start == -1:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        sents_orders[unit_sent_start].append(unit_order)\n",
    "        i += 1\n",
    "\n",
    "    # 3. Remove Duplicates\n",
    "    sents_orders = {k: discrete_events(v) for k, v in sents_orders.items()}\n",
    "\n",
    "    return sents_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b53b8cb9-c5b2-4d33-93fe-d31bc931a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_events(rels):\n",
    "    rel_bounds_mapped = {(rel.start(), rel.end()): rel for rel in rels}\n",
    "    rel_bounds = rel_bounds_mapped.keys()    \n",
    "    disc_rel_bounds = distinct_bounds(rel_bounds, larger=True)    \n",
    "    disc_rels = [rel_bounds_mapped[bound] for bound in disc_rel_bounds]\n",
    "    return disc_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bdcd5b8b-4f55-4718-9a84-97bbad55e64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO!!!!\n",
      "GMAIL, YAHOO, and OUTLOOK List\n",
      "HEY VENUS\n",
      "GMAIL Item\n",
      "WOO\n",
      "\n",
      "HELLO!!!!\n",
      "GMAIL Item\n",
      "HEY VENUS\n",
      "OUTLOOK Item\n",
      "WOO\n",
      "\n",
      "HELLO!!!!\n",
      "OUTLOOK Item\n",
      "HEY VENUS\n",
      "YAHOO Item\n",
      "WOO\n",
      "\n",
      "HELLO!!!!\n",
      "YAHOO Item\n",
      "HEY VENUS\n",
      "to shut down Prepositional Phrase\n",
      "WOO\n",
      "\n",
      "HELLO!!!!\n",
      "to shut down Prepositional Phrase\n",
      "HEY VENUS\n",
      ". Fragment\n",
      "WOO\n",
      "\n",
      "0 (AOL)->(caused)->(GMAIL,,,YAHOO,,,and,OUTLOOK,to,shut,down)\n"
     ]
    }
   ],
   "source": [
    "sent_orders = order_text(main)\n",
    "orders = []\n",
    "for k, v in sent_orders.items():\n",
    "    for r in v:\n",
    "        orders.append(r)\n",
    "        print(k, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3afaff65-020f-4ce3-b487-7709f9b58bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_link_between_orders(X, Y):\n",
    "    if X.start() < Y.start():\n",
    "        l = X.order[-1].start()\n",
    "        r = Y.order[0].end()\n",
    "    else:\n",
    "        l = Y.order[-1].start()\n",
    "        r = X.order[0].end()\n",
    "        \n",
    "    tokens = X.main.sp_doc[l+1:r]\n",
    "    print(f\"tokens in between X and Y: {tokens}\")\n",
    "    tokens_speech = [token.pos_ for token in tokens]\n",
    "    \n",
    "    cc_tokens = set(X.main.cause.tokens)\n",
    "    \n",
    "    if \"VERB\" in tokens_speech or cc_tokens.intersection(tokens):\n",
    "        print(\"ret True\")\n",
    "        return True\n",
    "    print(\"ret False\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "721cb6f8-9587-471b-876b-71099d0f472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentence_orders(orders):\n",
    "    orders = [order.copy() for order in orders]\n",
    "    orders = sorted(orders, key=lambda order: order.start())\n",
    "\n",
    "    i = 0\n",
    "    while i + 1 < len(orders):\n",
    "        # print(\"HERE\")\n",
    "        a_order = orders[i]\n",
    "        b_order = orders[i+1]\n",
    "\n",
    "        a_order_is_adp = \"ADP\" in [token.pos_ for token in a_order.get_tokens()]\n",
    "\n",
    "        # Merge Current and Next Order\n",
    "        if a_order_is_adp:\n",
    "            print(1)\n",
    "            print(\"a_order_is_adp branch\")\n",
    "            print(a_order)\n",
    "            print(b_order)\n",
    "            print()\n",
    "            print()\n",
    "            a_order.order.extend([Node(b_order.main), *b_order.order])\n",
    "            orders.pop(i+1)\n",
    "            continue\n",
    "        else:\n",
    "            print(2)\n",
    "            print(\"else branch\")\n",
    "            print(a_order)\n",
    "            print(b_order)\n",
    "            merged_order = merge(a_order, b_order)\n",
    "            print(merged_order)\n",
    "            print(bool(merged_order))\n",
    "            print()\n",
    "            print()\n",
    "            if merged_order:\n",
    "                orders[i] = merged_order\n",
    "                orders.pop(i+1)\n",
    "                continue\n",
    "            elif causal_link_between_orders(a_order, b_order):\n",
    "                print(3)\n",
    "                a_order.order.extend([Node(b_order.main), *b_order.order])\n",
    "                orders.pop(i+1)\n",
    "                continue\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32ac875d-2ea1-44d3-a89b-fe56780e12b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(X, Y):\n",
    "    X_str = set([x.text for x in X])\n",
    "    Y_str = set([y.text for y in Y])\n",
    "\n",
    "    if X_str & Y_str:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def merge(X, Y):\n",
    "    X = X.copy()\n",
    "    Y = Y.copy()\n",
    "    \n",
    "    if X.start() < Y.start():\n",
    "        A = X\n",
    "        B = Y\n",
    "    else:\n",
    "        A = Y\n",
    "        B = X\n",
    "    \n",
    "    A_tokens = flatten([item.get_expanded_tokens() for item in A.order[-2:]])\n",
    "    B_tokens = flatten([item.get_expanded_tokens() for item in B.order[:1]])\n",
    "    \n",
    "    if not overlap(A_tokens, B_tokens):\n",
    "        return None\n",
    "\n",
    "    A.order.extend([Node(B.main), *B.order])\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67ccaa30-2893-47ab-84e4-4466120157d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_sent_orders\n",
      "(AOL)->(caused)->(GMAIL,,,YAHOO,,,and,OUTLOOK,to,shut,down)\n",
      "\n",
      "merged_orders\n",
      "(AOL)->(caused)->(GMAIL,,,YAHOO,,,and,OUTLOOK,to,shut,down)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_sent_orders = []\n",
    "for orders in sent_orders.values():\n",
    "    result = merge_sentence_orders(orders)\n",
    "    merged_sent_orders.extend(result)\n",
    "\n",
    "print(\"merged_sent_orders\")\n",
    "for order in merged_sent_orders:\n",
    "    print(order)\n",
    "print()\n",
    "\n",
    "\n",
    "merged_orders = merge_sentence_orders(merged_sent_orders)\n",
    "print(\"merged_orders\")\n",
    "for order in merged_orders:\n",
    "    print(order)\n",
    "print()\n",
    "\n",
    "# ret = merge_sentence_orders(orders)\n",
    "# if ret:\n",
    "#     for order in ret:\n",
    "#         print(order)\n",
    "#         print(len(order.order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "63c3d164-8394-4a6f-b9ed-e6ff7e20e1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1) (Fragment) -> AOL caused\n",
      "(2, 2) (Item) -> GMAIL\n",
      "(2, 7) (List) -> GMAIL, YAHOO, and OUTLOOK\n",
      "(4, 4) (Item) -> YAHOO\n",
      "(7, 7) (Item) -> OUTLOOK\n",
      "(8, 10) (Prepositional Phrase) -> to shut down\n",
      "(11, 11) (Fragment) -> .\n"
     ]
    }
   ],
   "source": [
    "unit_map_bounds = main.units.unit_map.keys()\n",
    "unit_map_bounds = sorted(unit_map_bounds)\n",
    "\n",
    "for bound in unit_map_bounds:\n",
    "    unit = main.units.unit_map[bound]\n",
    "    print(f\"({unit.l}, {unit.r}) ({unit.label_()}) -> {main.sp_doc[unit.l:unit.r+1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7440fb86-c8d3-4876-b3b7-6e988a238363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
