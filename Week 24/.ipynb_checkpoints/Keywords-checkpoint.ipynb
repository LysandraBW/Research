{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb2d5f-a25e-43e7-b5c9-4ca6fa7647ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    REGEX = \"regex\"\n",
    "    VOCAB = \"vocab\"\n",
    "    RULES = \"rules\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, main, *, regexes=[], vocab=[], patterns=[], def_pos=[], def_tag=[], def_threshold=0.7, def_weight=1.0):\n",
    "        self.main = main\n",
    "\n",
    "        # Constraints\n",
    "        self.def_threshold = def_threshold\n",
    "        self.def_tag = def_tag\n",
    "        self.def_pos = def_pos\n",
    "        self.def_weight = def_weight\n",
    "        \n",
    "        # Three Types of Matching\n",
    "        self.vocab, self.vocab_data = self.load_vocab(vocab)\n",
    "        self.regex, self.regex_data = self.load_regex(regexes)\n",
    "        self.rules, self.rules_data = self.load_rules(patterns)\n",
    "\n",
    "        # Quick Lookup\n",
    "        self.match_type_to_data = {\n",
    "            Keywords.REGEX: self.regex_data,\n",
    "            Keywords.VOCAB: self.vocab_data,\n",
    "            Keywords.RULES: self.rules_data\n",
    "        }\n",
    "\n",
    "    \n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        # Matched Tokens in Different Forms\n",
    "        self.token_data, self.mapped_token_data, self.tokens = self.match_tokens(verbose=verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def load_regex(self, regexes):\n",
    "        r = []\n",
    "        r_data = {}\n",
    "\n",
    "        for unit in regexes:\n",
    "            if isinstance(unit, str):\n",
    "                r.append(unit)\n",
    "            else:\n",
    "                regex = unit[\"regex\"]\n",
    "                r.append(regex)\n",
    "                r_data[regex] = {\n",
    "                    \"types\": unit.get(\"types\", []),\n",
    "                    \"weight\": unit.get(\"weight\", self.def_weight)\n",
    "                }\n",
    "\n",
    "        return r, r_data\n",
    "\n",
    "\n",
    "\n",
    "    def load_vocab(self, vocab):\n",
    "        v = []\n",
    "        v_data = {}\n",
    "        \n",
    "        for unit in vocab:\n",
    "            if isinstance(unit, str):\n",
    "                doc = self.main.sp_nlp(unit)\n",
    "                v.append({\n",
    "                    \"doc\": doc,\n",
    "                    \"lemma\": \" \".join([t.lemma_ for t in doc])\n",
    "                })\n",
    "            else:\n",
    "                doc = self.main.sp_nlp(unit[\"word\"])\n",
    "                v.append({\n",
    "                    \"doc\": doc,\n",
    "                    \"tag\": unit.get(\"tag\", self.def_tag),\n",
    "                    \"pos\": unit.get(\"pos\", self.def_pos),\n",
    "                    \"threshold\": unit.get(\"threshold\", self.def_threshold),\n",
    "                    \"lemma\": \" \".join([t.lemma_ for t in doc])\n",
    "                })\n",
    "                v_data[unit[\"word\"]] = {\n",
    "                    \"types\": unit.get(\"types\") or [],\n",
    "                    \"weight\": unit.get(\"weight\", self.def_weight),\n",
    "                }\n",
    "        \n",
    "        return v, v_data\n",
    "\n",
    "\n",
    "\n",
    "    def load_rules(self, patterns):\n",
    "        r = Matcher(self.main.sp_nlp.vocab)\n",
    "        r_data = {}\n",
    "        \n",
    "        for i, unit in enumerate(patterns):\n",
    "            if isinstance(unit, list):\n",
    "                r.add(f\"{i}\", unit)\n",
    "            else:\n",
    "                r.add(unit[\"name\"], unit[\"pattern\"])\n",
    "                r_data[unit[\"name\"]] = {\n",
    "                    \"types\": unit.get(\"types\") or [],\n",
    "                    \"weight\": unit.get(\"weight\", self.def_weight),\n",
    "                }\n",
    "\n",
    "        return r, r_data\n",
    "\n",
    "\n",
    "\n",
    "    def get_match_data(self, token, match_id, match_type):\n",
    "        match_type_data = self.match_type_to_data[match_type]\n",
    "        \n",
    "        if match_id in match_type_data:\n",
    "            return {\n",
    "                \"token\": token,\n",
    "                \"types\": match_type_data[match_id].get(\"types\", []),\n",
    "                \"weight\": match_type_data[match_id].get(\"weight\", self.def_weight)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"token\": token,\n",
    "                \"types\": [],\n",
    "                \"weight\": self.def_weight\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "    def bad_pos(self, pos):\n",
    "        return self.def_pos and pos not in self.def_pos\n",
    "\n",
    "\n",
    "\n",
    "    def bad_tag(self, tag):\n",
    "        return self.def_tag and tag not in self.def_tag\n",
    "\n",
    "\n",
    "\n",
    "    def bad_token(self, token):\n",
    "        return self.bad_pos(token.pos_) or self.bad_tag(token.tag_)\n",
    "\n",
    "\n",
    "\n",
    "    def match_tokens(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        matched_data = []\n",
    "        matched_tokens = []\n",
    "\n",
    "        # Match by Regex\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "\n",
    "        for regex in self.regex:\n",
    "            matches = [(match.start(), match.end()) for match in re.finditer(regex, text, re.IGNORECASE)]\n",
    "            \n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\t'{regex}' Regex Matches: {matches}\")\n",
    "            \n",
    "            for l_char_index, r_char_index in matches:\n",
    "                span = self.main.get_span_at_indices(l_char_index, r_char_index - 1)\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tSpan Matched: {span}\")\n",
    "\n",
    "                for token in span:\n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tPossible Regex Match for Token '{token}' (Position: {token.pos_} and Tag: {token.tag_})\")\n",
    "                        \n",
    "                    if self.bad_token(token):\n",
    "                        continue\n",
    "                    \n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tRegex Matched Token '{token}'\")\n",
    "                        \n",
    "                    matched_tokens.append(token)\n",
    "                    matched_data.append(self.get_match_data(token, regex, Keywords.REGEX))\n",
    "\n",
    "        # Match by Rules        \n",
    "        matches = self.rules(self.main.sp_doc)\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 2:\n",
    "            print(f\"\\tRule Matches: {matches}\")\n",
    "        \n",
    "        for match_id, start, end in matches:\n",
    "            span = self.main.sp_doc[start:end]\n",
    "            name = self.main.sp_nlp.vocab.strings[match_id]\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tPattern '{name}' Matched Span: {span}\")\n",
    "            \n",
    "            for token in span:\n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tPossible Rule Match for Token '{token}' (Position: {token.pos_} and Tag: {token.tag_})\")\n",
    "                    \n",
    "                if self.bad_token(token):\n",
    "                    continue\n",
    "                \n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tRule Matched Token '{token}'\")\n",
    "\n",
    "                matched_tokens.append(token)\n",
    "                matched_data.append(self.get_match_data(token, name, Keywords.RULES))\n",
    "\n",
    "        # Match by Vocab\n",
    "        if self.vocab:\n",
    "            for token in self.main.sp_doc:\n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tPossible Vocab Match for Token '{token}' (Position: {token.pos_} and Tag: {token.tag_})\")\n",
    "                        \n",
    "                if self.bad_token(token) or token in matched_tokens:\n",
    "                    continue\n",
    "    \n",
    "                token_doc = self.main.sp_nlp(token.lower_)\n",
    "                token_lemma = \" \".join([t.lemma_ for t in token_doc])\n",
    "                \n",
    "                for vocab_word in self.vocab:\n",
    "                    # Ensure Correct Tag\n",
    "                    if vocab_word.get(\"tag\"):\n",
    "                        if not [t for t in token_doc if t.tag_ in vocab_word.get(\"tag\")]:\n",
    "                            if verbose and VERBOSE_LEVEL >= 4:\n",
    "                                print(f\"\\t\\t\\tToken '{token_doc}' not in Vocab Word '{vocab_word['doc']}' Tags ({vocab_word.get('tag')})\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Ensure Correct PoS\n",
    "                    if vocab_word.get(\"pos\"):\n",
    "                        if not [t for t in token_doc if t.pos_ in vocab_word.get(\"pos\")]:\n",
    "                            if verbose and VERBOSE_LEVEL >= 4:\n",
    "                                print(f\"\\t\\t\\tToken '{token_doc}' not in Vocab Word '{vocab_word['doc']}' Speech ({vocab_word.get('pos')})\")\n",
    "                            continue\n",
    "    \n",
    "                    # Check Lemma\n",
    "                    if verbose and VERBOSE_LEVEL >= 4:\n",
    "                        print(f\"\\t\\t\\t{token_doc} Lemma ({token_lemma}) and {vocab_word['doc']} Lemma ({vocab_word['lemma']})\")\n",
    "                        \n",
    "                    if token_lemma == vocab_word[\"lemma\"]:\n",
    "                        matched_tokens.append(token)\n",
    "                        matched_data.append(self.get_match_data(token, vocab_word[\"doc\"].text, Keywords.VOCAB))\n",
    "    \n",
    "                        if verbose and VERBOSE_LEVEL >= 3:\n",
    "                            print(f\"\\t\\tVocab (Lemma) Matched Token '{token}'\")\n",
    "                        \n",
    "                        break\n",
    "    \n",
    "                    # Check Similarity\n",
    "                    similarity = vocab_word[\"doc\"].similarity(token_doc)\n",
    "    \n",
    "                    if verbose and VERBOSE_LEVEL >= 4:\n",
    "                        print(f\"\\t\\t\\t{token_doc} and {vocab_word['doc']} Similarity: {similarity}\")\n",
    "                        \n",
    "                    if similarity >= vocab_word.get(\"threshold\", self.def_threshold):\n",
    "                        matched_tokens.append(token)\n",
    "                        matched_data.append(self.get_match_data(token, vocab_word[\"doc\"].text, Keywords.VOCAB))\n",
    "    \n",
    "                        if verbose and VERBOSE_LEVEL >= 3:\n",
    "                            print(f\"\\t\\tVocab Matched Token '{token}'\")\n",
    "                            \n",
    "                        break\n",
    "\n",
    "        # Mapping Match(ed Token) Data\n",
    "        mapped_matched_data = {}\n",
    "        for matched_token_data in matched_data:\n",
    "            mapped_matched_data[matched_token_data[\"token\"]] = matched_token_data\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"Output of match_tokens\")\n",
    "            print(f\"Token Data: {matched_data}\")\n",
    "            print(f\"Mapped Token Data: {mapped_matched_data}\")\n",
    "            print(f\"Token: {matched_tokens}\")\n",
    "        \n",
    "        return matched_data, mapped_matched_data, matched_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef65aa93-cee2-4a78-8dd6-d93274aa19e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            regexes=[\n",
    "                \"study\", \n",
    "                \"hypothesis\", \n",
    "                \"experiment\", \n",
    "                \"found\", \n",
    "                \"discover\", \n",
    "                \"compare\", \n",
    "                \"finding\", \n",
    "                \"result\", \n",
    "                \"test\", \n",
    "                \"examine\", \n",
    "                \"model\",\n",
    "                \"measure\",\n",
    "                \"manipulate\",\n",
    "                \"assess\",\n",
    "                \"conduct\",\n",
    "                \"data\",\n",
    "                \"analyze\",\n",
    "                \"sample\",\n",
    "                \"observe\",\n",
    "                \"observation\",\n",
    "                \"predict\",\n",
    "                \"suggest\",\n",
    "                \"method\",\n",
    "                \"investigation\",\n",
    "                \"trial\",\n",
    "                \"experimental\",\n",
    "                \"evidence\",\n",
    "                \"demonstrate\",\n",
    "                \"analysis\",\n",
    "                \"show\",\n",
    "                \"compare\",\n",
    "                \"comparable\",\n",
    "                \"control group\", \n",
    "                \"independent\",\n",
    "                \"dependent\",\n",
    "                \"applied\",\n",
    "                \"treatment\",\n",
    "                \"survery\",\n",
    "                \"evaluate\",\n",
    "                \"ran\"\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256660a-8c21-43b7-ad30-1d667cfac2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main,\n",
    "            patterns=[\n",
    "                [[{\"LOWER\": {\"IN\": [\"theory\", \"theorized\", \"theories\", \"review\", \"reviews\", \"meta-analysis\"]}}]]\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c35b2-c732-4729-b447-c67dae086e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeTopicKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            regexes=[\n",
    "                r\"co-?evolution\",\n",
    "                r\"evolution\",\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d86ac-4fd2-4457-a768-fd3fe4c6668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CauseKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            regexes=[\n",
    "                \"increas(e|es|ed|ing)\", \n",
    "                \"decreas(e|es|ed|ing)\",\n",
    "                \"chang(e|es|ed|ing)\", \n",
    "                \"shift\", \n",
    "                \"caus(e|es|ed|ing)\", \n",
    "                \"produc(e|es|ed|ing)\", \n",
    "                \"trigger\", \n",
    "                \"suppress\", \n",
    "                \"inhibit\",\n",
    "                \"encourag(e|es|ed|ing)\",\n",
    "                \"allow\",\n",
    "                \"influenc(e|es|ed|ing)\",\n",
    "                \"affect\",\n",
    "                \"alter\",\n",
    "                \"induc(e|es|ed|ing)\",\n",
    "                \"result in\",\n",
    "                \"contribute\",\n",
    "                \"impact\",\n",
    "                \"deter\",\n",
    "                \"depressed\",\n",
    "                \"when\",\n",
    "                \"because\",\n",
    "                \"reduc(e|es|ed|ing)\",\n",
    "                \"kill(e|es|ed|ing)\",\n",
    "                \"result\",\n",
    "                \"made\",\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"SCONJ\", \"NOUN\"],\n",
    "            def_threshold=0.8\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "\n",
    "    \n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d0d2f-e9dd-4135-8bd3-78a2b053efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            regexes=[\n",
    "                \"few\", \n",
    "                \"more\", \n",
    "                \"increase\", \n",
    "                \"decrease\", \n",
    "                \"less\", \n",
    "                \"short\", \n",
    "                \"long\", \n",
    "                \"greater\"\n",
    "                \"shift\",\n",
    "                \"fluctuate\",\n",
    "                \"adapt\",\n",
    "                \"grow\",\n",
    "                \"rise\"\n",
    "                \"surge\",\n",
    "                \"intensify\",\n",
    "                \"amplify\",\n",
    "                \"multiply\",\n",
    "                \"decline\",\n",
    "                \"reduce\",\n",
    "                \"drop\",\n",
    "                \"diminish\",\n",
    "                \"fall\",\n",
    "                \"lessen\",\n",
    "                \"doubled\",\n",
    "                \"tripled\",\n",
    "                \"lower\",\n",
    "                \"adjust\",\n",
    "                \"reject\",\n",
    "                # Match Examples:\n",
    "                # 1. \"one... as...\"\n",
    "                # 2. \"2x than...\"\n",
    "                r\"(one|two|three|four|five|six|seven|eight|nine|ten|twice|thrice|([0-9]+|[0-9]+.[0-9]+)(x|%))[\\s-]+[^\\s]*[\\s-]+(as|more|than|likely)([\\s-]+|$)\"\n",
    "            ],\n",
    "            def_pos=[\"NOUN\", \"ADJ\", \"ADV\", \"VERB\", \"NUM\"],\n",
    "            def_threshold=0.75\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose=verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "\n",
    "    \n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in self.main.sp_doc:\n",
    "            # Already Matched\n",
    "            if token in tokens:\n",
    "                filtered.append(token)\n",
    "            \n",
    "            # Comparative Adjective\n",
    "            # Looking for words like \"bigger\" and \"better\".\n",
    "            elif token.pos_ == \"ADJ\" and token.tag_ == \"JJR\":\n",
    "                filtered.append(token)\n",
    "                continue\n",
    "            \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75784a5b-a39c-4148-8cba-157c85c412c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraitKeywords(Keywords):\n",
    "    FOOD = \"Food\"\n",
    "    PRESENT = \"Present\"\n",
    "    NOT_APPLICABLE = \"N/A\"\n",
    "    \n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            regexes=[\n",
    "                r\"behaviou?r\", \n",
    "                r\"[^A-Za-z]+rate\", \n",
    "                \"colou?r\",\n",
    "                \"biomass\",\n",
    "                r\"[^A-Za-z]+mass\", \n",
    "                r\"[^A-Za-z]+size\",\n",
    "                \"number\",\n",
    "                \"length\", \n",
    "                \"pattern\", \n",
    "                \"weight\",\n",
    "                \"shape\", \n",
    "                \"efficiency\", \n",
    "                \"trait\",\n",
    "                \"phenotype\",\n",
    "                \"demography\",\n",
    "                \"scent\",\n",
    "                \"population (structure|mechanic)s?\",\n",
    "                \"ability\", \n",
    "                \"capacity\", \n",
    "                \"height\", \n",
    "                \"width\", \n",
    "                \"[A-Za-z]+span\",\n",
    "                {\"regex\": \"diet\", \"types\": [TraitKeywords.FOOD]},\n",
    "                {\"regex\": \"food\", \"types\": [TraitKeywords.FOOD, TraitKeywords.NOT_APPLICABLE]},\n",
    "                {\"regex\": \"feeding\", \"types\": [TraitKeywords.FOOD]},\n",
    "                \"nest\",\n",
    "                \"substrate\",\n",
    "                \"breeding\",\n",
    "                r\"[^A-Za-z]+age[^A-Za-z]+\",\n",
    "                \"lifespan\",\n",
    "                \"development\",\n",
    "                \"output\",\n",
    "                \"time\",\n",
    "                \"period\",\n",
    "                \"level\",\n",
    "                \"configuration\",\n",
    "                \"dimorphism\",\n",
    "                \"capability\",\n",
    "                \"regulation\",\n",
    "                \"excretion\",\n",
    "                \"luminescence\",\n",
    "                r\"[^A-Za-z]+role\",\n",
    "                \"sensitivity\",\n",
    "                \"resistance\",\n",
    "                r\"(un|(^|\\s)[A-Za-z]*-)infected\",\n",
    "                \"temperature\",\n",
    "                \"density\",\n",
    "                {\"regex\": \"presen(t|ce)\", \"types\": [TraitKeywords.PRESENT]},\n",
    "                {\"regex\": \"absen(t|ce)\", \"types\": [TraitKeywords.PRESENT]},\n",
    "                \"oviposition\",\n",
    "                \"semiochemicals\",\n",
    "                \"chemicals\",\n",
    "                \"content\",\n",
    "                \"level\"\n",
    "            ],\n",
    "            # Ideally, I would only include nouns, but sometimes\n",
    "            # they're recognized as adjectives.\n",
    "            def_pos=[\"NOUN\", \"ADJ\"]\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "\n",
    "    \n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Unfiltered Trait Tokens: {tokens}\")\n",
    "        \n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            expanded_token = self.main.expand_unit(\n",
    "                il_unit=token.i, \n",
    "                ir_unit=token.i, \n",
    "                il_boundary=token.sent.start, \n",
    "                ir_boundary=token.sent.end-1, \n",
    "                speech=[\"PUNCT\"],\n",
    "                include=False,\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "            valid_token = True\n",
    "\n",
    "            # Check for Species\n",
    "            if not self.main.species.has_species(expanded_token, verbose=verbose):\n",
    "                valid_token = False\n",
    "\n",
    "            # Check Noun Requirement\n",
    "            for chunk in self.main.sp_doc.noun_chunks:\n",
    "                if token in chunk and chunk[-1] != token:\n",
    "                    valid_token = False\n",
    "\n",
    "            if valid_token:\n",
    "                filtered.append(token)\n",
    "            \n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Filtered Trait Tokens: {filtered}\")\n",
    "        \n",
    "        return filtered\n",
    "\n",
    "\n",
    "    def merge_traits(self, traits):\n",
    "        merged = {}\n",
    "        \n",
    "        for trait in traits:\n",
    "            found_trait = False\n",
    "            for m_trait in merged:\n",
    "                if self.main.has_same_base_nouns(trait, m_trait) or self.main.is_same_text(trait, m_trait):\n",
    "                    merged[m_trait].append(trait)\n",
    "                    found_trait = True\n",
    "                    break\n",
    "                \n",
    "                trait_types = []\n",
    "                for thing in trait:\n",
    "                    things = self.main.trait.mapped_token_data.get(thing)\n",
    "                    if things:\n",
    "                        trait_types.extend(things[\"types\"])\n",
    "                    \n",
    "                m_trait_types = []\n",
    "                for thing in m_trait:\n",
    "                    things = self.main.trait.mapped_token_data.get(thing)\n",
    "                    if things:\n",
    "                        m_trait_types.extend(things[\"types\"])\n",
    "    \n",
    "                # print(trait_types, m_trait_types)\n",
    "    \n",
    "                if not trait_types or not m_trait_types:\n",
    "                    continue\n",
    "    \n",
    "                type_intersection = set(trait_types).intersection(m_trait_types)\n",
    "                if type_intersection.intersection(['Food']):\n",
    "                    merged[m_trait].append(trait)\n",
    "                    found_trait = True\n",
    "                    break\n",
    "                \n",
    "            if not found_trait:\n",
    "                merged[trait] = [trait]\n",
    "    \n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b3740-dcb4-4310-83dd-8157e8139216",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main,\n",
    "            regexes=[\n",
    "                \"compare\",\n",
    "                \"examine\",\n",
    "                \"evaluate\",\n",
    "                \"assess\",\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f9eaa-0b3b-4a89-b1a8-745a3725b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariablityKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main,\n",
    "            vocab=[\n",
    "                {\"word\": \"different\", \"pos\": [\"ADJ\", \"NOUN\"]},\n",
    "                {\"word\": \"vary\", \"pos\": [\"VERB\", \"NOUN\"]},\n",
    "                {\"word\": \"varied\", \"pos\": [\"VERB\", \"NOUN\"]},\n",
    "                {\"word\": \"compare\", \"pos\": [\"VERB\"]}\n",
    "            ],\n",
    "            regexes=[\n",
    "                r\"between\",\n",
    "                r\"against\",\n",
    "                r\"independen(t|ts|tly|cy)\",\n",
    "                r\"dependen(t|ts|tly|cy)\",\n",
    "                r\"treatments?\",\n",
    "                r\"effect\",\n",
    "                r\"control\",\n",
    "                r\"(with|without)[A-Za-z]*(with|without)\",\n",
    "                r\"(^| )(un|not)[-| ]?([A-Za-z]+) [^!;?.\\n]* \\3\",\n",
    "                r\"([A-Za-z]+) [^!;?.\\n]* (un|not)[-| ]?\\1( |$)\",\n",
    "                r\"when\",\n",
    "                r\"where\",\n",
    "                \n",
    "            ],\n",
    "            patterns=[\n",
    "                [[{\"LOWER\": {\"IN\": [\"neither\", \"either\", \"both\"]}}, {\"OP\": \"*\", \"TAG\": {\"NOT_IN\": [\".\"]}}, {\"LOWER\": {\"IN\": [\"or\", \"and\"]}}]],\n",
    "                [[{\"LOWER\": {\"IN\": [\"with\", \"without\"]}}, {\"OP\": \"*\", \"TAG\": {\"NOT_IN\": [\".\"]}}, {\"LOWER\": {\"IN\": [\"with\", \"without\"]}}]],\n",
    "                [[{\"LOWER\": {\"IN\": [\"at\"]}}, {\"POS\": \"NUM\"}]],\n",
    "                [[{\"LOWER\": {\"IN\": [\"at\"]}}, {\"LOWER\": {\"IN\": [\"several\", \"unique\", \"multiple\", \"different\"]}}]],\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a959ab-6f99-458a-9914-46f9025fa1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main,\n",
    "            patterns=[\n",
    "                [[\n",
    "                    {\n",
    "                        \"POS\": \"PRON\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"POS\": \"AUX\",\n",
    "                        \"OP\": \"?\",\n",
    "                    }, \n",
    "                ]],\n",
    "            ],\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
