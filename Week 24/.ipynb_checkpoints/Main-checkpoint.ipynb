{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3382ea6c-1d9f-4ca2-aa2b-aae201096481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import spacy\n",
    "import textacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from taxonerd import TaxoNERD\n",
    "from spacy.matcher import Matcher, DependencyMatcher, PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba2de5b-846b-4b33-8bb9-4c30eb13fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE_LEVEL = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0345a5f8-3a0a-4318-890a-3b45f1c92236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def flatten(arr):\n",
    "    if not isinstance(arr, list):\n",
    "        return [arr]\n",
    "\n",
    "    flat = []\n",
    "    for val in arr:\n",
    "        flat.extend(flatten(val))\n",
    "\n",
    "    return flat\n",
    "\n",
    "def find(arr, foo):\n",
    "    for val in arr:\n",
    "        if foo(val):\n",
    "            return val\n",
    "    return None\n",
    "\n",
    "def find_all(arr, foo):\n",
    "    bar = []\n",
    "    for val in arr:\n",
    "        if foo(val):\n",
    "            bar.append(val)\n",
    "    return bar\n",
    "\n",
    "def find_index(arr, foo):\n",
    "    for i in range(len(arr)):\n",
    "        if foo(arr[i]):\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1b2ae10a-34ec-4cff-a894-af0ecb5b2cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity:\n",
    "    # Labels\n",
    "    LIST = 1\n",
    "    ITEM = 2\n",
    "    QUOTE = 3\n",
    "    BREAK = 4\n",
    "    END = 5\n",
    "    AND_OR_END = 6\n",
    "    COLON = 7\n",
    "    COLON_BREAK = 8\n",
    "    I_CLAUSE = 9\n",
    "    D_CLAUSE = 10\n",
    "    P_PHRASE = 11\n",
    "    BRACKETS = 12\n",
    "    FRAGMENT = 13\n",
    "\n",
    "    def __init__(self, doc, label=None, l=None, r=None, children=None):\n",
    "        self.doc = doc\n",
    "        self.label = label\n",
    "        self.l = l\n",
    "        self.r = r\n",
    "        self.children = children or []\n",
    "\n",
    "    def label_(self):\n",
    "        if self.label == Entity.LIST:\n",
    "            return \"List\"\n",
    "        if self.label == Entity.ITEM:\n",
    "            return \"Item\"\n",
    "        if self.label == Entity.QUOTE:\n",
    "            return \"Quote\"\n",
    "        if self.label == Entity.BREAK:\n",
    "            return \"Break\"\n",
    "        if self.label == Entity.END:\n",
    "            return \"End\"\n",
    "        if self.label == Entity.AND_OR_END:\n",
    "            return \"And or End\"\n",
    "        if self.label == Entity.COLON:\n",
    "            return \"Colon\"\n",
    "        if self.label == Entity.COLON_BREAK:\n",
    "            return \"Colon Break\"\n",
    "        if self.label == Entity.I_CLAUSE:\n",
    "            return \"Independent Clause\"\n",
    "        if self.label == Entity.D_CLAUSE:\n",
    "            return \"Dependent Clause\"\n",
    "        if self.label == Entity.P_PHRASE:\n",
    "            return \"Prepositional Phrase\"\n",
    "        if self.label == Entity.BRACKETS:\n",
    "            return \"Brackets\"\n",
    "        if self.label == Entity.FRAGMENT:\n",
    "            return \"Fragment\"\n",
    "        return \"None\"\n",
    "        \n",
    "    def size(self):\n",
    "        return self.r - self.l + 1\n",
    "\n",
    "    def span(self):\n",
    "        return self.doc[self.l:self.r+1]\n",
    "\n",
    "    def lower(self):\n",
    "        return self.doc[self.l:self.r+1].text.lower()\n",
    "\n",
    "    def start(self):\n",
    "        return self.doc[self.l]\n",
    "\n",
    "    def end(self):\n",
    "        return self.doc[self.r]\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens(*, ent=None, ents=None):\n",
    "        if ents:\n",
    "            tokens = flatten([list(ent.span()) for ent in ents])\n",
    "            tokens = sorted(tokens, key=lambda token: token.i)\n",
    "            return tokens\n",
    "        if ent:\n",
    "            tokens = list(ent.span())\n",
    "            return tokens\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def is_conjunction(token):\n",
    "        return token.lower_ in [\"and\", \"or\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def same_speech(speech_1, speech_2):\n",
    "        nouns = [\"NOUN\", \"PRON\", \"PROPN\"]\n",
    "        if speech_1 in nouns and speech_2 in nouns:\n",
    "            return True\n",
    "        return speech_1 == speech_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63134aac-a92d-4427-b9ec-44844b0e7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quotes:\n",
    "    def __init__(self, main, entities):\n",
    "        self.main = main\n",
    "        self.entities = entities\n",
    "\n",
    "    def is_quote(self, i):\n",
    "        return i < len(self.entities) and self.entities[i].lower() == \"\\\"\"\n",
    "    \n",
    "    def identify(self):\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(self.entities):\n",
    "            if not self.is_quote(i):\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            self.entities[i].label = Entity.QUOTE\n",
    "            \n",
    "            while not self.is_quote(i+1):\n",
    "                self.entities[i].r = self.entities[i+1].r\n",
    "                self.entities.pop(i+1)\n",
    "\n",
    "            if self.is_quote(i+1):\n",
    "                self.entities[i].r = self.entities[i+1].r\n",
    "                self.entities.pop(i+1)\n",
    "\n",
    "        return self.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc96a95c-c33e-4bc8-8613-b25b0c838dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brackets:\n",
    "    MATCHES = {\n",
    "        \"[\": \"]\", \n",
    "        \"(\": \")\",\n",
    "        \"—\": \"—\",\n",
    "    }\n",
    "\n",
    "    OPENING = MATCHES.keys()\n",
    "    CLOSING = MATCHES.values()\n",
    "\n",
    "    def __init__(self, main, entities):\n",
    "        self.main = main\n",
    "        self.stack = []\n",
    "        self.entities = [*entities]\n",
    "\n",
    "    def is_opening(self, i):\n",
    "        return i < len(self.entities) and self.entities[i].lower()[0] in Brackets.OPENING\n",
    "\n",
    "    def is_closing(self, i):\n",
    "        return i < len(self.entities) and self.entities[i].lower()[0] in Brackets.CLOSING\n",
    "\n",
    "    def closes(self, i):\n",
    "        opener = self.entities[self.stack[-1]].lower()[0]\n",
    "        closer = self.entities[i].lower()[0]\n",
    "        return Brackets.MATCHES[opener] == closer\n",
    "    \n",
    "    def identify(self):\n",
    "        self.stack = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(self.entities):\n",
    "            # print(i, self.entities[i].span())\n",
    "            \n",
    "            # Closing\n",
    "            if self.is_closing(i) and self.stack:\n",
    "                j = None if not self.closes(i) else self.stack.pop()\n",
    "                \n",
    "                if not self.stack and j != None:\n",
    "                    self.entities[j].r = self.entities[i].r\n",
    "                    self.entities.pop(i)\n",
    "                    continue\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "            # Opening\n",
    "            elif self.is_opening(i):\n",
    "                if not self.stack:\n",
    "                    self.entities[i].label = Entity.BRACKETS\n",
    "                self.stack.append(i)\n",
    "                i += 1\n",
    "\n",
    "            # Consuming\n",
    "            elif self.stack:\n",
    "                # If you're at the end of the possible entities,\n",
    "                # and the list is unclosed, we must stop.\n",
    "                if i + 1 >= len(self.entities):\n",
    "                    break\n",
    "                self.entities[self.stack[0]].r = self.entities[i+1].r\n",
    "                self.entities.pop(i)\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        # for ent in self.entities:\n",
    "        #     if ent.label == Entity.BRACKETS:\n",
    "        #         print(f\"Bracket: {ent.span()}\")\n",
    "        \n",
    "        return self.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2847950f-3df8-49da-94f0-1f992076d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Separators:\n",
    "    def __init__(self, main, entities):\n",
    "        self.main = main\n",
    "        self.entities = [*entities]\n",
    "\n",
    "    def is_break(self, i):\n",
    "        if i >= len(self.entities):\n",
    "            return False\n",
    "        \n",
    "        if self.entities[i].lower() not in [\";\", \",\"]:\n",
    "            return False\n",
    "\n",
    "        # Breaks cannot have a following conjunction.\n",
    "        # Else, it would be an end and not a break.\n",
    "        return not bool(\n",
    "            i + 1 < len(self.entities) and \n",
    "            self.entities[i+1].size() == 1 and \n",
    "            self.entities[i+1].span()[0].pos_ in [\"CCONJ\"]\n",
    "        )\n",
    "\n",
    "    def is_end(self, i):\n",
    "        if i >= len(self.entities):\n",
    "            return False\n",
    "        \n",
    "        if self.entities[i].lower() not in [\";\", \",\"]:\n",
    "            return False\n",
    "        \n",
    "        return not self.is_break(i)\n",
    "\n",
    "    def identify(self):\n",
    "        i = 0\n",
    "\n",
    "        while i < len(self.entities):\n",
    "            # Break\n",
    "            if self.is_break(i):\n",
    "                self.entities[i].label = Entity.BREAK\n",
    "                i += 1\n",
    "\n",
    "            # End\n",
    "            elif self.is_end(i):\n",
    "                conj = self.entities[i+1].start().lower_\n",
    "                self.entities[i].label = Entity.AND_OR_END if conj in [\"and\", \"or\"] else Entity.END\n",
    "                self.entities[i].r += 1\n",
    "                self.entities.pop(i+1)\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "                \n",
    "        return self.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c4b3ac-65cf-4db0-a32b-d263ffb22703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colons:\n",
    "    def __init__(self, main, entities):\n",
    "        self.main = main\n",
    "        self.entities = [*entities]\n",
    "\n",
    "    def identify(self):\n",
    "        i = 0\n",
    "\n",
    "        while i < len(self.entities):\n",
    "            if self.entities[i].lower()[-1] != \":\":\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if not self.entities[i].label:\n",
    "                self.entities[i].label = Entity.COLON_BREAK\n",
    "\n",
    "            if i + 1 < len(self.entities):\n",
    "                self.entities[i+1].label = Entity.COLON\n",
    "                self.entities[i+1].r = self.entities[-1].r\n",
    "                self.entities = self.entities[:i+2]\n",
    "            \n",
    "            break\n",
    "\n",
    "        return self.entities        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b2424fe-9d13-43c8-861a-9ce910dd8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Independent_Clauses:\n",
    "    def __init__(self, main, entities):\n",
    "        self.main = main\n",
    "        self.entities = [*entities]\n",
    "        self.allowed = []\n",
    "\n",
    "    def end(self, i):    \n",
    "        if i >= len(self.entities):\n",
    "            return True\n",
    "\n",
    "        if self.entities[i].label in self.allowed:\n",
    "            return True\n",
    "        \n",
    "        # Here, we check if the entity after\n",
    "        # the supposed end is a clause. If it\n",
    "        # is, then we can end at the current entity.\n",
    "        return bool(\n",
    "            i + 1 < len(self.entities) and \n",
    "            self.entities[i+1].label in [\n",
    "                Entity.COLON,\n",
    "                Entity.COLON_BREAK,\n",
    "                Entity.I_CLAUSE,\n",
    "                Entity.D_CLAUSE,\n",
    "                Entity.P_PHRASE\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def identify(self, allowed):\n",
    "        self.allowed = allowed\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        while i < len(self.entities):\n",
    "            if self.entities[i].label not in self.allowed:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Skip Clause\n",
    "            if self.entities[i].label in [\n",
    "                Entity.I_CLAUSE, \n",
    "                Entity.D_CLAUSE, \n",
    "                Entity.P_PHRASE\n",
    "            ]:\n",
    "                i = entities[i].r + 1\n",
    "                continue\n",
    "\n",
    "            # Create Clause\n",
    "            self.entities[i].label = Entity.I_CLAUSE\n",
    "            while not self.end(i+1):\n",
    "                self.entities[i].r = self.entities[i+1].r\n",
    "\n",
    "                # Add Child\n",
    "                if self.entities[i+1].label in [Entity.BRACKETS, Entity.QUOTE]:\n",
    "                    self.entities[i].children.append(self.entities[i+1])\n",
    "                    \n",
    "                self.entities.pop(i+1)\n",
    "\n",
    "            i += 1\n",
    "            \n",
    "        return self.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b12c64-fae5-4390-96ad-ff7d9eac1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dependent_Clauses:\n",
    "    RELATIVE_NOUNS = [\n",
    "        \"who\",\n",
    "        \"whom\",\n",
    "        \"which\",\n",
    "        \"what\",\n",
    "        \"that\",\n",
    "        \"whose\",\n",
    "        \"whomever\",\n",
    "        \"whoever\",\n",
    "        \"whichever\",\n",
    "        \"whatever\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, main, entities):\n",
    "        self.main = main\n",
    "        self.entities = entities\n",
    "        self.separator = None\n",
    "\n",
    "    def end(self, i):\n",
    "        if i >= len(self.entities):\n",
    "            return True\n",
    "\n",
    "        # Here, we check if the entity after\n",
    "        # is a clause. As we don't combine two\n",
    "        # clauses, we must end here if that is\n",
    "        # the case.\n",
    "        if bool(\n",
    "            i + 1 < len(self.entities) and \n",
    "            self.entities[i+1].label in [\n",
    "                Entity.COLON, \n",
    "                Entity.COLON_BREAK,\n",
    "                Entity.I_CLAUSE,\n",
    "                Entity.D_CLAUSE,\n",
    "                Entity.P_PHRASE\n",
    "            ]\n",
    "        ):\n",
    "            return True\n",
    "\n",
    "        return bool(\n",
    "            self.entities[i].lower()[0] == self.separator or\n",
    "            self.entities[i].lower() in Dependent_Clauses.RELATIVE_NOUNS or\n",
    "            self.entities[i].start().pos_ in [\"SCONJ\"]\n",
    "        )\n",
    "\n",
    "    def identify(self, separator):\n",
    "        self.separator = separator\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        while i < len(self.entities):\n",
    "            # Skip\n",
    "            if self.entities[i].label in [\n",
    "                Entity.COLON,\n",
    "                Entity.COLON_BREAK,\n",
    "                Entity.I_CLAUSE, \n",
    "                Entity.D_CLAUSE, \n",
    "                Entity.P_PHRASE\n",
    "            ]:\n",
    "                i = self.entities[i].r + 1\n",
    "                continue\n",
    "\n",
    "            # Indicators of Dependent Clause\n",
    "            rel = self.entities[i].lower() in Dependent_Clauses.RELATIVE_NOUNS\n",
    "            sub = self.entities[i].start().pos_ == \"SCONJ\"\n",
    "            \n",
    "            if not sub and not rel:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Create Clause\n",
    "            self.entities[i].label = Entity.D_CLAUSE\n",
    "            while not self.end(i+1):\n",
    "                self.entities[i].r = self.entities[i+1].r\n",
    "\n",
    "                # Add Child\n",
    "                if self.entities[i+1].label in [Entity.BRACKETS, Entity.QUOTE]:\n",
    "                    self.entities[i].children.append(self.entities[i+1])\n",
    "                \n",
    "                self.entities.pop(i+1)\n",
    "\n",
    "            i += 1\n",
    "        \n",
    "        return self.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825f1b95-0a34-4062-8cc3-cbeb3e8da381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prepositional_Phrases:\n",
    "    \n",
    "    def __init__(self, main, entities):\n",
    "        self.main = main\n",
    "        self.entities = [*entities]\n",
    "\n",
    "    # A prepositional phrase is typically ended by a noun.\n",
    "    # Therefore, when we run into a noun, we end the phrase.\n",
    "    # We must also check that it is the last of the first noun(s)\n",
    "    # we encounter.\n",
    "    def last_noun(self, i):\n",
    "        if bool(\n",
    "            # 1. End\n",
    "            i >= len(self.entities) or \n",
    "            \n",
    "            # 2. Noun\n",
    "            self.entities[i].start().pos_ not in [\n",
    "                \"NOUN\", \n",
    "                \"PROPN\", \n",
    "                \"PRON\"\n",
    "            ]\n",
    "        ):\n",
    "            return False\n",
    "        \n",
    "        return bool(\n",
    "            i + 1 > len(self.entities) - 1 or \n",
    "            (\n",
    "                self.entities[i+1].size() == 1 and \n",
    "                self.entities[i+1].start().pos_ not in [\n",
    "                    \"NOUN\", \n",
    "                    \"PROPN\", \n",
    "                    \"PRON\", \n",
    "                    \"PART\"\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def end(self, i):\n",
    "        return bool(\n",
    "            # 1. End of List\n",
    "            i + 1 >= len(self.entities) or\n",
    "            \n",
    "            # 2. Clause\n",
    "            self.entities[i+1].label in [\n",
    "                Entity.COLON,\n",
    "                Entity.COLON_BREAK,\n",
    "                Entity.I_CLAUSE,\n",
    "                Entity.D_CLAUSE,\n",
    "                Entity.P_PHRASE\n",
    "            ] or\n",
    "            \n",
    "            # 3. Noun\n",
    "            self.last_noun(i)\n",
    "        )\n",
    "\n",
    "    def identify(self):    \n",
    "        i = 0\n",
    "        \n",
    "        while i < len(self.entities):\n",
    "            # Skip\n",
    "            if bool(\n",
    "                self.entities[i].size() != 1 or\n",
    "                self.entities[i].start().pos_ != \"ADP\"\n",
    "            ):\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Create Clause\n",
    "            self.entities[i].label = Entity.P_PHRASE\n",
    "            while not self.end(i+1):\n",
    "                self.entities[i].r = self.entities[i+1].r\n",
    "\n",
    "                # Add Child\n",
    "                if self.entities[i+1].label in [Entity.BRACKETS, Entity.QUOTE]:\n",
    "                    self.entities[i].children.append(self.entities[i+1])\n",
    "                \n",
    "                self.entities.pop(i+1)\n",
    "\n",
    "            if self.last_noun(i+1):\n",
    "                self.entities[i].r = self.entities[i+1].r\n",
    "                self.entities.pop(i+1)\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        return self.entities   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e63a979-1694-495d-982e-99953cea64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lists:\n",
    "    NOUNS = [\"NOUN\", \"PRON\", \"PROPN\"]\n",
    "    \n",
    "    def __init__(self, main, entities):\n",
    "        self.main = main\n",
    "        self.entities = [*entities]\n",
    "        self.separator = None\n",
    "\n",
    "    def is_stop(self, entity):\n",
    "        is_break = entity.label == Entity.BREAK and entity.lower()[0] == self.separator\n",
    "        is_clause = entity.label in [\n",
    "            Entity.I_CLAUSE, \n",
    "            Entity.D_CLAUSE, \n",
    "            Entity.P_PHRASE,\n",
    "            Entity.COLON,\n",
    "            Entity.COLON_BREAK\n",
    "        ]\n",
    "        return is_break or is_clause\n",
    "\n",
    "    def find_lists(self, sep):\n",
    "        self.separator = sep\n",
    "        \n",
    "        lists = [\n",
    "            [\n",
    "                [None, None]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        i = 0\n",
    "        while i < len(self.entities):\n",
    "            entity = self.entities[i]\n",
    "\n",
    "            opened = lists[-1][0] != [None, None]\n",
    "            remove_list = entity.label in [Entity.COLON, Entity.COLON_BREAK]\n",
    "            close_list = entity.label in [Entity.AND_OR_END] and entity.lower()[0] == sep\n",
    "            close_item = entity.label in [Entity.BREAK] and entity.lower() == sep\n",
    "        \n",
    "            # Close List\n",
    "            if opened and close_list:\n",
    "                # Invalid List, Remove\n",
    "                if len(lists[-1]) < 2:\n",
    "                    lists[-1] = [[None, None]]\n",
    "                    i += 1\n",
    "                    continue\n",
    "                    \n",
    "                # Find the L Index of Last Item\n",
    "                last_item_l = i + 1\n",
    "\n",
    "                # Find the R Index of Last Item\n",
    "                last_item_r = last_item_l\n",
    "                \n",
    "                length = find_index(self.entities[last_item_l:], lambda e: self.is_stop(e))\n",
    "                if length > 0:\n",
    "                    last_item_r += length - 1\n",
    "                elif length == -1:\n",
    "                    last_item_r = len(self.entities) - 1\n",
    "\n",
    "                # Add Last Item\n",
    "                lists[-1].append([last_item_l, last_item_r])\n",
    "                lists.append([[None, None]])\n",
    "                i += 1\n",
    "\n",
    "            # Close Item\n",
    "            elif opened and close_item:\n",
    "                lists[-1].append([i + 1, i])\n",
    "                i += 1\n",
    "                \n",
    "            # Remove List\n",
    "            elif opened and remove_list:\n",
    "                lists[-1] = [[None, None]]\n",
    "                i += 1\n",
    "            \n",
    "            # Continue Item\n",
    "            else:\n",
    "                if not opened:\n",
    "                    lists[-1][0] = [i, i]\n",
    "                else:\n",
    "                    lists[-1][-1][1] += 1\n",
    "                i += 1\n",
    "        \n",
    "        # If we reach the end of the list and the last\n",
    "        # list is invalid (< 3 items), we remove it.\n",
    "        if bool(\n",
    "            lists and len(lists[-1]) < 3 or \n",
    "            (\n",
    "                lists and\n",
    "                not find(self.entities[lists[-1][0][0]:], lambda e: e.label == Entity.AND_OR_END and e.lower()[0] == sep)\n",
    "            )\n",
    "        ):\n",
    "            lists.pop()\n",
    "        \n",
    "        # In each item, we look for pairs (e.g. X and Y).\n",
    "        # We only handle one conjunction.\n",
    "        num_lists = len(lists)\n",
    "        for i, lst in enumerate(lists):\n",
    "            if i >= num_lists:\n",
    "                break\n",
    "            \n",
    "            for l, r in lst:\n",
    "                tokens = Entity.tokens(ents=self.entities[l:r+1])\n",
    "                conj = find_all(tokens, lambda t: Entity.is_conjunction(t))\n",
    "                if len(conj) == 1:\n",
    "                    lists.append([[l, r]])\n",
    "\n",
    "        # If there's no lists at all, we check if there's a pairing.\n",
    "        # We should divvy the entities up by any separators, but\n",
    "        # pairs aren't of too much importance.\n",
    "        # TODO: ADD ABOVE FUNCTIONALITY\n",
    "        tokens = Entity.tokens(ents=self.entities)\n",
    "        num_conj = len(find_all(tokens, lambda t: Entity.is_conjunction(t)))\n",
    "        if not lists and num_conj == 1:\n",
    "            lists.append([[0, len(self.entities) - 1]])\n",
    "\n",
    "        # Here we remove duplicates, I'm not sure if duplicates still\n",
    "        # occur, I observed them once, but this is here in case.\n",
    "        i = 0\n",
    "        while i < len(lists):\n",
    "            if lists[i] in lists[i+1:]:\n",
    "                lists.pop(i)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        # Remove Invalid Lists\n",
    "        i = 0\n",
    "        while i < len(lists):\n",
    "            # The list contains one item and that item only contains one\n",
    "            # token, or the list has two items.\n",
    "            if bool(\n",
    "                (\n",
    "                    len(lists[i]) == 1 and \n",
    "                    lists[i][0][0] == lists[i][0][1]\n",
    "                ) or\n",
    "                len(lists[i]) == 2\n",
    "            ):\n",
    "                lists.pop(i)\n",
    "            else:\n",
    "                i += 1\n",
    "         \n",
    "        return lists\n",
    "\n",
    "    def clean_lists(self, lists):\n",
    "        overlaps = []\n",
    "\n",
    "        i = 0\n",
    "        while i + 1 < len(lists):\n",
    "            a = lists[i]\n",
    "            b = lists[i+1]\n",
    "                  \n",
    "            if a[-1] != b[0]:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if len(a) <= 1 or len(b) <= 1:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # No Way to Split\n",
    "            if a[-1][1] - a[-1][0] <= 1:\n",
    "                overlaps.extend([i, i + 1])\n",
    "                i += 2\n",
    "            else:\n",
    "                a[-1][1] = a[-1][0]\n",
    "                b[0][0] = b[0][1]\n",
    "                i += 2\n",
    "        \n",
    "        lists = [l for i, l in enumerate(lists) if i not in overlaps]\n",
    "        return lists\n",
    "\n",
    "    def expand_noun(self, tokens, start, direction):\n",
    "        for group in [*self.main.sp_doc.noun_chunks, *self.main.sp_doc.ents]:\n",
    "            tokens_i = [t.i for t in group]\n",
    "            if tokens[start].i in tokens_i:\n",
    "                while start >= 0 and start < len(tokens) and tokens[start].i in tokens_i:\n",
    "                    start += 1 * direction\n",
    "                start += 1 * direction * -1\n",
    "            break\n",
    "        \n",
    "        return start\n",
    "        \n",
    "    def char_bound_list(self, lst):\n",
    "        # print(\"Character Bound List\")\n",
    "        # We bound each item according to characters or a speech.\n",
    "        # We find these bounds from the \"base item\", the second to last item.\n",
    "        base_tokens = Entity.tokens(ents=self.entities[lst[-2][0]:lst[-2][1]+1])\n",
    "        \n",
    "        # As we're bounding by characters, primarily, the left bound is just\n",
    "        # the characters of the first token\n",
    "        l_bound = base_tokens[0].lower_\n",
    "\n",
    "        # The right bound is the first tag, of the below set of tags, that we\n",
    "        # encounter in the base tokens. If there's not such a token, we cannot\n",
    "        # bound the items.\n",
    "        speech = [\"NOUN\", \"PROPN\", \"PRON\", \"VERB\", \"NUM\"]\n",
    "        r_bound = None\n",
    "        for i in range(len(base_tokens) - 1, -1, -1):\n",
    "            if base_tokens[i].pos_ in speech:\n",
    "                r_bound = base_tokens[i]\n",
    "                break\n",
    "\n",
    "        if not r_bound:\n",
    "            return None\n",
    "\n",
    "        # The inner items are already bounded on the left and right sides.\n",
    "        # All we need to check is whether the start matches with the left bound.\n",
    "        inner_items = lst[1:-2]\n",
    "\n",
    "        for i, item in enumerate(inner_items):\n",
    "            l = item[0]\n",
    "            r = item[1]\n",
    "            \n",
    "            tokens = Entity.tokens(ents=self.entities[l:r+1])\n",
    "\n",
    "            # If it doesn't match, we check if the next set of items can be\n",
    "            # bounded. If not, we cannot bound the list.\n",
    "            if tokens[0].lower_ != l_bound:\n",
    "                if len(inner_items) - i - 1 >= 2:\n",
    "                    return self.bound_list(lst[i+2:])\n",
    "                return None\n",
    "            \n",
    "        # Check for L Bound in Starting Item\n",
    "        start_tokens = Entity.tokens(ents=self.entities[lst[0][0]:lst[0][1]+1])\n",
    "        start_l = len(start_tokens) - 1\n",
    "        while start_l >= 0 and start_tokens[start_l].lower_ != l_bound:\n",
    "            start_l -= 1\n",
    "\n",
    "        # L Bound Not Found\n",
    "        if start_l < 0:\n",
    "            # If the list is greater than 4 items, we can\n",
    "            # cut off the starting item, and try again.\n",
    "            if len(inner_items) >= 2:\n",
    "                return self.bound_list(lst[1:])\n",
    "            return None\n",
    "\n",
    "        # If the first of the start tokens is a noun, there may be more\n",
    "        # to include.\n",
    "        if start_tokens[start_l].pos_ in Lists.NOUNS:\n",
    "            start_l = self.expand_noun(start_tokens, start_l, -1)\n",
    "                    \n",
    "        # Check for R Bound in Ending Item\n",
    "        end_tokens = Entity.tokens(ents=self.entities[lst[-1][0]:lst[-1][1]+1])\n",
    "        end_r = 0\n",
    "        num_end_tokens = len(end_tokens)\n",
    "        while end_r < num_end_tokens and end_tokens[end_r].pos_ not in speech:\n",
    "            end_r += 1\n",
    "\n",
    "        if end_r >= num_end_tokens:\n",
    "            return None\n",
    "\n",
    "        # If the last of the end tokens is a noun, there may be more\n",
    "        # to include.\n",
    "        if end_tokens[end_r].pos_ in Lists.NOUNS:\n",
    "            end_r = self.expand_noun(end_tokens, end_r, 1)\n",
    "        \n",
    "        # Create List\n",
    "        entity_start_item = Entity(self.main.sp_doc, label=Entity.ITEM, l=start_tokens[start_l].i, r=start_tokens[-1].i)\n",
    "        entity_end_item = Entity(self.main.sp_doc, label=Entity.ITEM, l=end_tokens[0].i, r=end_tokens[end_r].i)\n",
    "        \n",
    "        entity_list = Entity(self.main.sp_doc, label=Entity.LIST, l=start_tokens[start_l].i, r=end_tokens[end_r].i)\n",
    "        entity_list.children.extend([entity_start_item, entity_end_item])\n",
    "        \n",
    "        for item in lst[1:-1]:\n",
    "            tokens = Entity.tokens(ents=self.entities[item[0]:item[1]+1])\n",
    "            entity_item = Entity(self.main.sp_doc, label=Entity.ITEM, l=tokens[0].i, r=tokens[-1].i)\n",
    "            entity_list.children.append(entity_item)\n",
    "\n",
    "        return entity_list\n",
    "            \n",
    "    def bound_list(self, lst):\n",
    "        # print(\"Bound List\")\n",
    "        # Base Item (2nd to Last Item) Tokens\n",
    "        base_tokens = Entity.tokens(ents=self.entities[lst[-2][0]:lst[-2][1]+1])\n",
    "        num_base_tokens = len(base_tokens)\n",
    "\n",
    "        # print(f\"Base Tokens: {base_tokens}\")\n",
    "        # print(f\"Number Base Tokens: {num_base_tokens}\")\n",
    "        \n",
    "        # Bound\n",
    "        speech = [\"NOUN\", \"PROPN\", \"PRON\", \"VERB\", \"NUM\"]\n",
    "\n",
    "        # Find L Bound\n",
    "        l_bound = None\n",
    "        for i in range(0, num_base_tokens):\n",
    "            if base_tokens[i].pos_ in speech:\n",
    "                l_bound = base_tokens[i]\n",
    "                break\n",
    "\n",
    "        if not l_bound:\n",
    "            return None\n",
    "        \n",
    "        # Find R Bound\n",
    "        r_bound = None\n",
    "        for i in range(num_base_tokens - 1, -1, -1):\n",
    "            if base_tokens[i].pos_ in speech:\n",
    "                r_bound = base_tokens[i]\n",
    "                break\n",
    "\n",
    "        if not r_bound:\n",
    "            return None\n",
    "\n",
    "        # print(f\"L Bound Speech: {l_bound.pos_}\")\n",
    "        # print(f\"R Bound Speech: {r_bound.pos_}\")\n",
    "        \n",
    "        # Check Inner Items\n",
    "        # The inner items must have the left bound,\n",
    "        # the right bound isn't as important.\n",
    "        inner_items = lst[1:-1]\n",
    "\n",
    "        verb_seen = False\n",
    "        for i, item in enumerate(inner_items):\n",
    "            l = item[0]\n",
    "            r = item[1]\n",
    "            \n",
    "            item_tokens = Entity.tokens(ents=self.entities[l:r+1])\n",
    "            item_speech = [token.pos_ for token in item_tokens]\n",
    "\n",
    "            # Must be Homogeneous\n",
    "            if \"VERB\" not in item_speech and verb_seen:\n",
    "                if len(inner_items) >= 2:\n",
    "                    return self.bound_list(lst[1:])  \n",
    "                else:\n",
    "                    return None\n",
    "            elif \"VERB\" in item_speech:\n",
    "                verb_seen = True\n",
    "\n",
    "            # Not Found\n",
    "            if l_bound.pos_ not in item_speech:\n",
    "                # We check if the list starting at the next\n",
    "                # item has a chance. If it does, that becomes\n",
    "                # the list.\n",
    "                if len(inner_items) - i + 1 >= 2:\n",
    "                    return self.bound_list(lst[i+2:])\n",
    "                return None\n",
    "        \n",
    "        # Check Starting Item\n",
    "        start_tokens = Entity.tokens(ents=self.entities[lst[0][0]:lst[0][1]+1])\n",
    "        start_l = len(start_tokens) - 1\n",
    "        \n",
    "        while start_l >= 0 and not Entity.same_speech(start_tokens[start_l].pos_, l_bound.pos_):\n",
    "            start_l -= 1\n",
    "\n",
    "        if start_l < 0:\n",
    "            if len(inner_items) >= 2:\n",
    "                return self.bound_list(lst[1:])\n",
    "            return None\n",
    "\n",
    "        # Adjust Starting Item\n",
    "        if l_bound.pos_ in Lists.NOUNS:\n",
    "            start_l = self.expand_noun(start_tokens, start_l, -1)\n",
    "        \n",
    "        # Check Ending Item\n",
    "        end_tokens = Entity.tokens(ents=self.entities[lst[-1][0]:lst[-1][1]+1])\n",
    "        end_r = 0\n",
    "        num_end_tokens = len(end_tokens)\n",
    "\n",
    "        while end_r < num_end_tokens and not Entity.same_speech(end_tokens[end_r].pos_, r_bound.pos_):\n",
    "            end_r += 1\n",
    "\n",
    "        if end_r >= num_end_tokens:\n",
    "            return None\n",
    "\n",
    "        # Adjust Ending Item\n",
    "        if r_bound.pos_ in Lists.NOUNS:\n",
    "            end_r = self.expand_noun(end_tokens, end_r, 1)\n",
    "\n",
    "        # Create List\n",
    "        entity_start_item = Entity(self.main.sp_doc, label=Entity.ITEM, l=start_tokens[start_l].i, r=start_tokens[-1].i)\n",
    "        entity_end_item = Entity(self.main.sp_doc, label=Entity.ITEM, l=end_tokens[0].i, r=end_tokens[end_r].i)\n",
    "        \n",
    "        entity_list = Entity(self.main.sp_doc, label=Entity.LIST, l=start_tokens[start_l].i, r=end_tokens[end_r].i)\n",
    "        entity_list.children.extend([entity_start_item, entity_end_item])\n",
    "\n",
    "        for item in lst[1:-1]:\n",
    "            tokens = Entity.tokens(ents=self.entities[item[0]:item[1]+1])\n",
    "            entity_item = Entity(self.main.sp_doc, label=Entity.ITEM, l=tokens[0].i, r=tokens[-1].i)\n",
    "            entity_list.children.append(entity_item)\n",
    "\n",
    "        return entity_list\n",
    "\n",
    "    def char_bound_pair(self, pair):\n",
    "        # print(\"Character Bound Pair\")\n",
    "        tokens = Entity.tokens(ents=self.entities[pair[0][0]:pair[0][1]+1])\n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        m = find_index(tokens, lambda t: Entity.is_conjunction(t))\n",
    "        l = m - 1\n",
    "        r = m + 1\n",
    "\n",
    "        # Bound L by R Token Characters\n",
    "        i = m - 1\n",
    "        while i >= 0 and tokens[i].lower_ != tokens[m + 1].lower_:\n",
    "            i -= 1\n",
    "\n",
    "        if i < 0:\n",
    "            return None\n",
    "\n",
    "        # Bound R by L Token Speech\n",
    "        j =  m + 1\n",
    "        while j < num_tokens and not Entity.same_speech(tokens[m-1].pos_, tokens[j].pos_):\n",
    "            j += 1\n",
    "\n",
    "        if j >= num_tokens:\n",
    "            return None\n",
    "        \n",
    "        e_item_l = Entity(self.main.sp_doc, label=Entity.ITEM, l=tokens[i].i, r=tokens[m-1].i)\n",
    "        e_item_r = Entity(self.main.sp_doc, label=Entity.ITEM, l=tokens[m+1].i, r=tokens[j].i)\n",
    "        e_list = Entity(self.main.sp_doc, label=Entity.LIST, l=tokens[i].i, r=tokens[j].i, children=[e_item_l, e_item_r])\n",
    "        return e_list\n",
    "    \n",
    "    def bound_pair(self, pair):\n",
    "        # print(\"Bound Pair\")\n",
    "        tokens = Entity.tokens(ents=self.entities[pair[0][0]:pair[0][1]+1])\n",
    "        tokens = sorted(tokens, key=lambda t: t.i)\n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        m = find_index(tokens, lambda t: Entity.is_conjunction(t))\n",
    "        l = m - 1\n",
    "        r = m + 1\n",
    "\n",
    "        speech = [\"NOUN\", \"PROPN\", \"PRON\", \"VERB\", \"NUM\"]\n",
    "\n",
    "        # Find L Bound\n",
    "        l_bound = None\n",
    "        l_bound_i = None\n",
    "        for i in range(m + 1, num_tokens):\n",
    "            if tokens[i].pos_ in speech:\n",
    "                l_bound = tokens[i].pos_\n",
    "                l_bound_i = tokens[i].i\n",
    "                break\n",
    "\n",
    "        if not l_bound:\n",
    "            return None\n",
    "\n",
    "        # Find R Bound\n",
    "        r_bound = None\n",
    "        r_bound_i = None\n",
    "        for i in range(m - 1, -1, -1):\n",
    "            if tokens[i].pos_ in speech:\n",
    "                r_bound = tokens[i].pos_\n",
    "                r_bound_i = tokens[i].i\n",
    "                break\n",
    "\n",
    "        if not r_bound:\n",
    "            return None\n",
    "\n",
    "        # Bound L\n",
    "        while l >= 0 and not Entity.same_speech(tokens[l].pos_, l_bound):\n",
    "            l -= 1\n",
    "\n",
    "        if l < 0:\n",
    "            return None\n",
    "\n",
    "        # Adjust L if Noun\n",
    "        if l_bound in Lists.NOUNS:\n",
    "            l = self.expand_noun(tokens, l, -1)\n",
    "            \n",
    "        # Bound R\n",
    "        while r < num_tokens and not Entity.same_speech(tokens[r].pos_, r_bound):\n",
    "            r += 1\n",
    "        \n",
    "        if r >= num_tokens:\n",
    "            return None\n",
    "\n",
    "        # Adjust R if Noun\n",
    "        if r_bound in Lists.NOUNS:\n",
    "            r = self.expand_noun(tokens, r, 1)\n",
    "        \n",
    "        e_item_l = Entity(self.main.sp_doc, label=Entity.ITEM, l=tokens[l].i, r=r_bound_i)\n",
    "        e_item_r = Entity(self.main.sp_doc, label=Entity.ITEM, l=l_bound_i, r=tokens[r].i)\n",
    "\n",
    "        e_list = Entity(self.main.sp_doc, label=Entity.LIST, l=tokens[l].i, r=tokens[r].i)\n",
    "        e_list.children.extend([e_item_l, e_item_r])\n",
    "        \n",
    "        return e_list\n",
    "\n",
    "    def bound_lists(self, lists):\n",
    "        # print(f\"Lists: {lists}\")\n",
    "        # for items in lists:\n",
    "        #     print(f\"\\tList: {items}\")\n",
    "        #     for item in items:\n",
    "        #         print(f\"\\t\\tItem: {Entity.tokens(ents=self.entities[item[0]:item[1]+1])}\")\n",
    "        \n",
    "        bound_lists = []\n",
    "        \n",
    "        for lst in lists:\n",
    "            bound = None\n",
    "        \n",
    "            if len(lst) == 1:\n",
    "                bound = self.char_bound_pair(lst)\n",
    "                if not bound:\n",
    "                    bound = self.bound_pair(lst)\n",
    "            else:\n",
    "                bound = self.char_bound_list(lst)\n",
    "                if not bound:\n",
    "                    bound = self.bound_list(lst)\n",
    "            \n",
    "            if bound:\n",
    "                bound_lists.append(bound)\n",
    "\n",
    "        # print(f\"Bounded Lists: {bound_lists}\")\n",
    "        # for bound_list in bound_lists:\n",
    "        #     print(f\"\\tBound List ({bound_list.l}, {bound_list.r}): {bound_list.span()}\")\n",
    "        \n",
    "        # Map (L, R) to Entity List\n",
    "        mapped_bounds = {}\n",
    "        for lst in bound_lists:\n",
    "            mapped_bounds[(lst.l, lst.r)] = lst\n",
    "        bounds = list(mapped_bounds.keys())\n",
    "\n",
    "        # Find Largest Coverage of Bounds\n",
    "        max_coverage = []\n",
    "        \n",
    "        for bound in bounds:\n",
    "            overlap = False\n",
    "            for i, max_bound in enumerate(max_coverage):\n",
    "                contains = max_bound[0] <= bound[0] <= max_bound[1] or max_bound[0] <= bound[1] <= max_bound[1]\n",
    "                surround = bound[0] <= max_bound[0] <= bound[1] or bound[0] <= max_bound[1] <= bound[1]\n",
    "                \n",
    "                if contains or surround:\n",
    "                    overlap = True\n",
    "                \n",
    "                    if bound[1] - bound[0] > max_bound[1] - max_bound[0]:\n",
    "                        max_coverage[i] = bound\n",
    "            \n",
    "            if not overlap:\n",
    "                max_coverage.append(bound)\n",
    "\n",
    "        # print(f\"(Max Coverage) Bounds: {max_coverage}\")\n",
    "        # print(f\"Entities: {[(e.l, e.r) for e in self.entities]}\")\n",
    "        \n",
    "        # Integrate Lists\n",
    "        for bound in max_coverage:\n",
    "            l_overlap = None\n",
    "            l_overlap_i = None\n",
    "            \n",
    "            r_overlap = None\n",
    "            r_overlap_i = None\n",
    "            \n",
    "            i = 0\n",
    "            while i < len(self.entities):\n",
    "                entity = self.entities[i]\n",
    "                \n",
    "                # Overlap w/ Left\n",
    "                if not l_overlap and entity.l <= bound[0] <= entity.r:\n",
    "                    l_overlap = entity\n",
    "                    l_overlap_i = i\n",
    "    \n",
    "                # Overlap w/ Right\n",
    "                if entity.l <= bound[1] <= entity.r:\n",
    "                    r_overlap = entity\n",
    "                    r_overlap_i = i\n",
    "\n",
    "                if l_overlap and r_overlap:\n",
    "                    break\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            # if l_overlap:\n",
    "            #     print(f\"L Overlap: {l_overlap.span()}, {l_overlap.label}\")\n",
    "\n",
    "            # if r_overlap:\n",
    "            #     print(f\"R Overlap: {r_overlap.span()}, {r_overlap.label}\")\n",
    "            \n",
    "            if l_overlap.label in [Entity.I_CLAUSE, Entity.D_CLAUSE, Entity.P_PHRASE]:\n",
    "                if l_overlap.l == mapped_bounds[bound].l:\n",
    "                    # Replace (Not in Use)\n",
    "                    # self.entities = self.entities[:l_overlap_i] + self.entities[r_overlap_i+1:]\n",
    "                    # self.entities.insert(l_overlap_i, mapped_bounds[bound])\n",
    "\n",
    "                    # Add Children\n",
    "                    l_overlap.r = max(l_overlap.r, mapped_bounds[bound].r)\n",
    "                    l_overlap.children.append(mapped_bounds[bound])\n",
    "                    self.entities = self.entities[:l_overlap_i+1] + self.entities[r_overlap_i+1:]\n",
    "                else:\n",
    "                    # Split (Not in Use)\n",
    "                    # l_overlap.r = mapped_bounds[bound].l - 1\n",
    "                    # self.entities = self.entities[:l_overlap_i+1] + self.entities[r_overlap_i+1:]\n",
    "                    # self.entities.insert(l_overlap_i + 1, mapped_bounds[bound])\n",
    "                    \n",
    "                    # Add Children\n",
    "                    l_overlap.r = max(l_overlap.r, mapped_bounds[bound].r)\n",
    "                    l_overlap.children.append(mapped_bounds[bound])\n",
    "                    self.entities = self.entities[:l_overlap_i+1] + self.entities[r_overlap_i+1:]\n",
    "                    \n",
    "            else:\n",
    "                self.entities = self.entities[:l_overlap_i] + self.entities[r_overlap_i+1:]\n",
    "                self.entities.insert(l_overlap_i, mapped_bounds[bound])\n",
    "        \n",
    "        return self.entities\n",
    "        \n",
    "    def identify(self, sep):\n",
    "        lists = self.find_lists(sep)\n",
    "        lists = self.clean_lists(lists)\n",
    "        lists = self.bound_lists(lists)   \n",
    "        return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "527f4fe4-daf7-49d0-9061-e7355d80515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parts:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.root = Entity(self.main.sp_doc)\n",
    "        self.reg = []\n",
    "\n",
    "    def matches(self, token_i, sent_i, label):\n",
    "        sent_reg = self.reg[sent_i]\n",
    "        for k, v in sent_reg:\n",
    "            if k[0] <= token_i <= k[1]:\n",
    "                return v.label == label\n",
    "        return False\n",
    "\n",
    "    def load_registry(self, ent):\n",
    "        reg = {(ent.l, ent.r): ent}\n",
    "        for child in ent.children:\n",
    "            if not child.label:\n",
    "                continue\n",
    "            reg.update(self.load_registry(child))\n",
    "        return reg\n",
    "    \n",
    "    def update(self):\n",
    "        reg = []\n",
    "        for sent in self.main.sp_doc.sents:\n",
    "            tokens = list(sent)\n",
    "            # print(f\"Parsing Sentence: {tokens}\")\n",
    "            # print(f\"\\tSize: {len(tokens)}\")\n",
    "            reg.append(self.load_entities(tokens))\n",
    "        self.reg = reg\n",
    "    \n",
    "    def load_entities(self, tokens):\n",
    "        entities = []\n",
    "        for token in tokens:\n",
    "            entity = Entity(\n",
    "                self.main.sp_doc, \n",
    "                l=token.i, \n",
    "                r=token.i\n",
    "            )\n",
    "            entities.append(entity)\n",
    "\n",
    "        entities = Quotes(self.main, entities).identify()\n",
    "        entities = Brackets(self.main, entities).identify()\n",
    "        entities = Separators(self.main, entities).identify()\n",
    "        \n",
    "        sep = \",\"\n",
    "        for entity in entities:\n",
    "            if \";\" == entity.lower()[0]:\n",
    "                sep = \";\"\n",
    "                break\n",
    "        \n",
    "        entities = Colons(self.main, entities).identify()\n",
    "        entities = Dependent_Clauses(self.main, entities).identify(sep)\n",
    "        entities = Independent_Clauses(self.main, entities).identify([Entity.END])\n",
    "        entities = Prepositional_Phrases(self.main, entities).identify()\n",
    "        entities = Lists(self.main, entities).identify(sep)\n",
    "        entities = Independent_Clauses(self.main, entities).identify([Entity.AND_OR_END])\n",
    "\n",
    "        # Merge Individual Entities\n",
    "        for ent in entities:\n",
    "            print(ent.label, ent.lower())\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(entities):\n",
    "            if not entities[i].label:\n",
    "                while i + 1 < len(entities) and not entities[i+1].label:\n",
    "                    entities.pop(i+1)\n",
    "                    entities[i].r += 1\n",
    "                entities[i].label = Entity.FRAGMENT\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        for ent in entities:\n",
    "            print(ent.label, ent.lower())\n",
    "        \n",
    "        # Create Registry\n",
    "        parent = Entity(self.main.sp_doc, l=-1, r=-1, children=entities)\n",
    "        \n",
    "        registry = self.load_registry(parent)\n",
    " \n",
    "        return registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "420eadf6-b6ad-4118-a005-d172cbc982f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    # There is not a defined conversion method for these words.\n",
    "    # This is the default list of irregular nouns. It maps the\n",
    "    # the singular version to the plural version (SP).\n",
    "    IRREGULAR_NOUNS_SP = {\n",
    "        \"ox\": \"oxen\",\n",
    "        \"goose\": \"geese\",\n",
    "        \"mouse\": \"mice\",\n",
    "        \"bacterium\": \"bacteria\"\n",
    "    }\n",
    "\n",
    "    # This is the reversed version of the dictionary above, meaning \n",
    "    # that the plural version is mapped to the singular version \n",
    "    # (PS).\n",
    "    IRREGULAR_NOUNS_PS = {v: k for k, v in IRREGULAR_NOUNS_SP.items()}\n",
    "    \n",
    "    # The singular and plural versions of these words are the same. \n",
    "    # This is the default list of zero plural nouns.\n",
    "    ZERO_PLURAL_NOUNS = [\n",
    "        \"species\", \n",
    "        \"deer\", \n",
    "        \"fish\", \n",
    "        \"moose\", \n",
    "        \"sheep\", \n",
    "        \"swine\", \n",
    "        \"buffalo\", \n",
    "        \"trout\", \n",
    "        \"cattle\"\n",
    "    ]\n",
    "\n",
    "    # These pairs of characters define symbols that enclose other\n",
    "    # information in a text.\n",
    "    ENCLOSURES = {\n",
    "        \"(\": \")\",\n",
    "        \"[\": \"]\",\n",
    "        \"{\": \"}\"\n",
    "    }\n",
    "\n",
    "    LAX_ENCLOSURES = {\n",
    "        \"(\": \")\",\n",
    "        \"[\": \"]\",\n",
    "        \"{\": \"}\",\n",
    "        \"—\": \"—\"\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self, main, irregular_nouns_sp=IRREGULAR_NOUNS_SP, irregular_nouns_ps=IRREGULAR_NOUNS_PS, zero_plural_nouns=ZERO_PLURAL_NOUNS):\n",
    "        self.main = main\n",
    "        self.zero_plural_nouns = zero_plural_nouns\n",
    "        self.irregular_nouns_sp = irregular_nouns_sp\n",
    "        self.irregular_nouns_ps = irregular_nouns_ps\n",
    "        self.irregular_plural_nouns = list(self.irregular_nouns_sp.values())\n",
    "        self.irregular_singular_nouns = list(self.irregular_nouns_sp.keys())\n",
    "\n",
    "\n",
    "\n",
    "    def delete_extra_whitespace(self, string):\n",
    "        # Duplicate spaces, spaces before punctuation marks,\n",
    "        # and outside spaces are removed.\n",
    "        string = re.sub(r\"\\s+\", \" \", string)\n",
    "        string = re.sub(r\"\\s+([?.!,])\", r\"\\1\", string)\n",
    "        string = string.strip()\n",
    "        return string\n",
    "\n",
    "\n",
    "\n",
    "    def delete_outer_non_alnum(self, string):\n",
    "        while string:\n",
    "            start_len = len(string)\n",
    "            # Remove Leading Non-Alphanumeric Character\n",
    "            if string and not string[0].isalnum():\n",
    "                string = string[1:]\n",
    "            # Remove Trailing Non-Alphanumeric Character\n",
    "            if string and not string[-1].isalnum():\n",
    "                string = string[:-1]\n",
    "            # No Changes Made\n",
    "            if start_len == len(string):\n",
    "                break\n",
    "        return string\n",
    "\n",
    "\n",
    "\n",
    "    def get_parentheticals(self, text, enclosures=ENCLOSURES, flatten=False):\n",
    "        # The parenthetical would be the content inside of a pair\n",
    "        # of matching parentheses, brackets, or braces.\n",
    "        parentheticals = []\n",
    "        \n",
    "        # This contains the text that's not inside of any\n",
    "        # enclosure.\n",
    "        base_text = []\n",
    "        \n",
    "        # This is used for building groups, which often has a \n",
    "        # nested structure.\n",
    "        stacks = []\n",
    "        \n",
    "        # These are the pairs of characters that we recognize\n",
    "        # as defining the parenthetical.\n",
    "        openers = list(enclosures.keys())\n",
    "        closers = list(enclosures.values())\n",
    "        \n",
    "        # This contains the opening characters of the groups \n",
    "        # that are currently open (e.g. '(', '['). We use it \n",
    "        # so that we know whether to open or close a group.\n",
    "        opened = []\n",
    "        \n",
    "        for i, char in enumerate(text):\n",
    "            # Open Group\n",
    "            if char in openers:\n",
    "                stacks.append([])\n",
    "                opened.append(char)\n",
    "            # Close Group\n",
    "            elif opened and char == enclosures.get(opened[-1], \"\"):\n",
    "                parentheticals.append(stacks.pop())\n",
    "                opened.pop()\n",
    "            # Add to Group\n",
    "            elif opened:\n",
    "                stacks[-1].append(i)\n",
    "            # Add to Base Text\n",
    "            else:\n",
    "                base_text.append(i)\n",
    "        \n",
    "        # We close the remaining groups that have not\n",
    "        # been closed.\n",
    "        while stacks:\n",
    "            parentheticals.append(stacks.pop())\n",
    "            \n",
    "        # Cluster Groups' Indices\n",
    "        # A list in the lists of indices (where each list represents a group of text) could have \n",
    "        # an interruption (e.g. [0, 1, 2, 10 15]) because of a parenthetical. So, we cluster the\n",
    "        # indices in each list to make the output more useful (e.g. [(0, 3), (10, 16)]).\n",
    "        lists_of_indices = [*parentheticals, base_text]        \n",
    "        lists_of_clustered_indices = []\n",
    "\n",
    "        for list_of_indices in lists_of_indices:\n",
    "            if not list_of_indices:\n",
    "                continue\n",
    "\n",
    "            # We start off with a single cluster that is made up of the\n",
    "            # first index. If the next index follows the first index, \n",
    "            # we continue the cluster. If it doesn't, we create a new cluster.\n",
    "            clustered_indices = [[list_of_indices[0], list_of_indices[0] + 1]]\n",
    "            \n",
    "            for index in list_of_indices[1:]:\n",
    "                if clustered_indices[-1][1] == index:\n",
    "                    clustered_indices[-1][1] = index + 1\n",
    "                else:\n",
    "                    clustered_indices.append([index, index + 1])\n",
    "\n",
    "            # Add Clustered Indices\n",
    "            lists_of_clustered_indices.append(clustered_indices)\n",
    "            \n",
    "        if flatten:\n",
    "            flattened_clusters = []\n",
    "            # We are placing each cluster of indices into one list.\n",
    "            # This removes the context of the larger parenthetical,\n",
    "            # but the context may be cumbersome instead of useful.\n",
    "            for list_of_clustered_indices in lists_of_clustered_indices:\n",
    "                for clustered_indices in list_of_clustered_indices:\n",
    "                    flattened_clusters.append(clustered_indices)\n",
    "            lists_of_clustered_indices = flattened_clusters\n",
    "        \n",
    "        return lists_of_clustered_indices\n",
    "\n",
    "\n",
    "\n",
    "    def separate_span_by_parenthetical(self, span):\n",
    "        span_parentheticals = []\n",
    "        \n",
    "        # The clusters of the span represented with tuples of char indices\n",
    "        # (e.g. [(0, 1), (1, 5), (5, 10)]. This is a list of clustered\n",
    "        # indices (like above).\n",
    "        text_clusters = self.get_parentheticals(span.text, flatten=True)\n",
    "        \n",
    "        for cluster in text_clusters:\n",
    "            if span.text[cluster[0]:cluster[1]].isspace():\n",
    "                continue\n",
    "\n",
    "            l_char_index = span[0].idx + cluster[0]\n",
    "            r_char_index = span[0].idx + cluster[1] - 1\n",
    "\n",
    "            # Instead of having a tuple dictating the start and end of a cluster,\n",
    "            # we can use a span -- it's much simpler.\n",
    "            cluster_as_span = self.get_span_at_indices(l_char_index, r_char_index)\n",
    "            if not cluster_as_span:\n",
    "                continue\n",
    "            \n",
    "            span_parentheticals.append(cluster_as_span)\n",
    "\n",
    "        return span_parentheticals\n",
    "\n",
    "\n",
    "\n",
    "    def separate_spans_by_parenthetical(self, spans):\n",
    "        all_span_parentheticals = []\n",
    "        for span in spans:\n",
    "            all_span_parentheticals.extend(self.separate_span_by_parenthetical(span))\n",
    "        return all_span_parentheticals\n",
    "\n",
    "    \n",
    " \n",
    "    def singularize(self, string):\n",
    "        string = string.lower()\n",
    "        \n",
    "        # The string to singularize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        if not words:\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is a zero plural\n",
    "        # or a singular irregular noun, there's no changes\n",
    "        # to make. For example, \"red sheep\" and \"ox\" are \n",
    "        # already singular.\n",
    "        if (\n",
    "            words[-1] in self.zero_plural_nouns or \n",
    "            words[-1] in self.irregular_singular_nouns\n",
    "        ):\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is an irregular\n",
    "        # plural noun, we rely on a dictionary with the\n",
    "        # corresponding mapping.\n",
    "        if words[-1] in self.irregular_plural_nouns:\n",
    "            words[-1] = self.irregular_nouns_ps[words[-1]]\n",
    "            singulars = [self.delete_extra_whitespace(\" \".join(words))]\n",
    "            return singulars\n",
    "        \n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to uncertainty), we need to\n",
    "        # include all possible versions.\n",
    "        singulars = []\n",
    "        singular_endings = self.get_singular(words[-1])\n",
    "\n",
    "        if not singular_endings:\n",
    "            return [string]\n",
    "        \n",
    "        for singular_ending in singular_endings:\n",
    "            singular = self.delete_extra_whitespace(\" \".join([*words[:-1], singular_ending]))\n",
    "            singulars.append(singular)\n",
    "            \n",
    "        return singulars\n",
    "\n",
    "\n",
    "\n",
    "    def get_singular(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Replace -ies with -y\n",
    "        if re.fullmatch(r\".*ies$\", string):\n",
    "            versions.append(f'{string[:-3]}y')\n",
    "            return versions\n",
    "\n",
    "        # Replace -ves with -f and -fe\n",
    "        if re.fullmatch(r\".*ves$\", string):\n",
    "            versions.append(f'{string[:-3]}f')\n",
    "            versions.append(f'{string[:-3]}fe')\n",
    "            return versions\n",
    "\n",
    "        # Delete -es \n",
    "        if re.fullmatch(r\".*es$\", string):\n",
    "            versions.append(f'{string[:-2]}')\n",
    "            return versions\n",
    "\n",
    "        # Replace -i with -us\n",
    "        if re.fullmatch(r\".*i$\", string):\n",
    "            versions.append(f'{string[:-1]}us')\n",
    "            return versions\n",
    "\n",
    "        # Delete -s\n",
    "        if re.fullmatch(r\".*s$\", string):\n",
    "            versions.append(f'{string[:-1]}')\n",
    "            return versions\n",
    "\n",
    "        return versions\n",
    "\n",
    "\n",
    "    \n",
    "    def pluralize(self, string):\n",
    "        string = string.lower()\n",
    "        \n",
    "        # The string to pluralize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        if not words:\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is a zero plural\n",
    "        # or a plural irregular noun, there's no changes\n",
    "        # to make. For example, \"red sheep\" and \"oxen\" are \n",
    "        # already singular.\n",
    "        if (\n",
    "            words[-1] in self.zero_plural_nouns or \n",
    "            words[-1] in self.irregular_plural_nouns\n",
    "        ):\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is an irregular\n",
    "        # singular noun, we rely on a dictionary with the\n",
    "        # corresponding mapping.\n",
    "        if words[-1] in self.irregular_singular_nouns:\n",
    "            words[-1] = self.irregular_nouns_sp[words[-1]]\n",
    "            return [self.delete_extra_whitespace(\" \".join(words))]\n",
    "        \n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to error), we need to\n",
    "        # handle them all.\n",
    "        plurals = []\n",
    "        plural_endings = self.get_plural(words[-1])\n",
    "\n",
    "        if not plural_endings:\n",
    "            return [string]\n",
    "            \n",
    "        for plural_ending in plural_endings:\n",
    "            plural = self.delete_extra_whitespace(\" \".join([*words[:-1], plural_ending]))\n",
    "            plurals.append(plural)\n",
    "            \n",
    "        return plurals\n",
    "\n",
    "    \n",
    "  \n",
    "    def get_plural(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Words that end with -us often have\n",
    "        # two different plural versions: -es and -i.\n",
    "        # For example, the plural version of cactus \n",
    "        # can be cactuses or cacti.\n",
    "        if re.fullmatch(r\".*us$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            versions.append(f'{string[:-2]}i')\n",
    "            return versions\n",
    "\n",
    "        # The -es ending is added to the words below.\n",
    "        if re.fullmatch(r\".*([^l]s|sh|ch|x|z)$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # Words that end with a consonant followed by 'y'\n",
    "        # are made plural by replacing the 'y' with -ies.\n",
    "        # For example, the plural version of canary is\n",
    "        # canaries.\n",
    "        if re.fullmatch(r\".*([^aeiou])(y)$\", string):\n",
    "            versions.append(f'{string[:-1]}ies')\n",
    "            return versions\n",
    "            \n",
    "        # The plural version of words ending with -f\n",
    "        # and -fe aren't clear. To be safe, I will add\n",
    "        # both versions.\n",
    "        if (re.fullmatch(r\".*(f)(e?)$\", string) and not re.fullmatch(r\".*ff$\", string)):\n",
    "            last_clean = re.sub(r\"(f)(e?)$\", \"\", string)\n",
    "            versions.append(f'{last_clean}fs')\n",
    "            versions.append(f'{last_clean}ves')\n",
    "            return versions\n",
    "\n",
    "        # People add -s or -es to words that end with 'o'.\n",
    "        # To be safe, both versions are added.\n",
    "        if re.fullmatch(r\".*([^aeiou])o$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # If there's no -s at the end of the string and\n",
    "        # the other cases didn't run, we add an -s.\n",
    "        if re.fullmatch(r\".*[^s]$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "        \n",
    "        return versions\n",
    "\n",
    "\n",
    " \n",
    "    def expand_unit(self, *, il_unit, ir_unit, il_boundary, ir_boundary, speech=[], literals=[], include=True, direction='BOTH', verbose=False):\n",
    "        UNIT = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if il_unit > ir_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} greater than ir_unit of {ir_unit}\")\n",
    "            return None\n",
    "        \n",
    "        if direction in ['BOTH', 'LEFT'] and il_boundary > il_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} less than il_boundary of {il_boundary}\")\n",
    "            return None\n",
    "        \n",
    "        if direction in ['BOTH', 'RIGHT'] and ir_boundary < ir_unit:\n",
    "            print(f\"Error: ir_unit of {ir_unit} greater than ir_boundary of {ir_boundary}\")\n",
    "            return None\n",
    "        \n",
    "        # Move Left\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            # The indices are inclusive, therefore, when \n",
    "            # the condition fails, il_unit will be equal\n",
    "            # to il_boundary.\n",
    "            while il_unit > il_boundary:\n",
    "                # We assume that the current token is allowed,\n",
    "                # and look to the token to the left.\n",
    "                l_token = self.main.sp_doc[il_unit-1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = l_token.pos_ in speech or l_token.lower_ in literals\n",
    "\n",
    "                # Case 1: include=False, in_set=True\n",
    "                # If we're not meant to include the defined tokens, and the\n",
    "                # current token is in that set, we stop expanding.\n",
    "                # Case 2: include=True, in_set=False\n",
    "                # If we're meant to include the defined tokens, and the current\n",
    "                # token is not in that set, we stop expanding.\n",
    "                # Case 3: include=in_set\n",
    "                # If we're meant to include the defined tokens, and the current\n",
    "                # token is in that set, we continue expanding. If we're not meant\n",
    "                # to include the defined tokens, and the current token is not\n",
    "                # in that set, we continue expanding.\n",
    "                if include ^ in_set:\n",
    "                    break\n",
    "                \n",
    "                # Else, the left token is valid, and\n",
    "                # we continue to expand.\n",
    "                il_unit -= 1\n",
    "\n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            # Likewise, when the condition fails,\n",
    "            # ir_unit will be equal to the ir_boundary.\n",
    "            # The ir_boundary is also inclusive.\n",
    "            while ir_unit < ir_boundary:\n",
    "                # Assuming that the current token is valid,\n",
    "                # we look to the right to see if we can\n",
    "                # expand.\n",
    "                r_token = self.main.sp_doc[ir_unit+1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = r_token.pos_ in speech or r_token.lower_ in literals\n",
    "                if include ^ in_set:\n",
    "                    break\n",
    "\n",
    "                # Else, the token is valid and\n",
    "                # we continue.\n",
    "                ir_unit += 1\n",
    "\n",
    "        assert il_unit >= il_boundary and ir_unit <= ir_boundary\n",
    "        \n",
    "        expanded_unit = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Expanded Unit of '{UNIT}': {expanded_unit}\")\n",
    "        \n",
    "        return expanded_unit\n",
    "\n",
    "\n",
    "    \n",
    "    def contract_unit(self, *, il_unit, ir_unit, speech=[], literals=[], include=True, direction='BOTH', verbose=False):\n",
    "        UNIT = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if il_unit > ir_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} greater than ir_unit of {ir_unit}\")\n",
    "            return None\n",
    "        \n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit < ir_unit:\n",
    "                # We must check if the current token is not allowed. If it's\n",
    "                # not allowed, we contract (remove).\n",
    "                token = self.main.sp_doc[il_unit]\n",
    "\n",
    "                # include = True means that we want the tokens that match\n",
    "                # the speech and/or literals in the contracted unit.\n",
    "                \n",
    "                # include = False means that we don't want the tokens that\n",
    "                # match the speech and/or literals in the contracted unit.\n",
    "                \n",
    "                # Case 1: include = True, in_set = True\n",
    "                # We have a token that's meant to be included in the set.\n",
    "                # However, we're contracting, which means we would end up\n",
    "                # removing the token if we continue. Therefore, we break.\n",
    "                \n",
    "                # Case 2: include = False, in_set = False\n",
    "                # We have a token that's not in the set which defines the\n",
    "                # tokens that aren't meant to be included. Therefore, we \n",
    "                # have a token that is meant to be included. If we continue,\n",
    "                # we would end up removing this token. Therefore, we break.\n",
    "                \n",
    "                # Default:\n",
    "                # If we have a token that's in the set (in_set=True) of\n",
    "                # tokens we're not supposed to include in the contracted \n",
    "                # unit (include=False), we need to remove it. Likewise, if\n",
    "                # we have a token that's not in the set (in_set=False) of\n",
    "                # tokens to include in the contracted unit (include=True),\n",
    "                # we need to remove it.\n",
    "                \n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include == in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid, thus we continue.\n",
    "                il_unit += 1\n",
    "\n",
    "        # Move Left      \n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit > il_unit:\n",
    "                token = self.main.sp_doc[ir_unit]\n",
    "\n",
    "                # The token is invalid and we\n",
    "                # stop contracting.\n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include == in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid and we continue.\n",
    "                ir_unit -= 1\n",
    "\n",
    "        assert il_unit <= ir_unit\n",
    "        \n",
    "        contracted_unit = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Contracted Unit of '{UNIT}': {contracted_unit}\")\n",
    "        \n",
    "        return contracted_unit\n",
    "\n",
    "\n",
    "    \n",
    "    def find_unit_context(self, *, il_unit, ir_unit, il_boundary, ir_boundary, speech=[\"ADJ\", \"NOUN\", \"ADP\", \"ADV\", \"PART\", \"PROPN\", \"VERB\", \"PRON\", \"DET\", \"AUX\", \"PART\", \"SCONJ\"], literals=[], include=True, enclosures=LAX_ENCLOSURES, comma_encloses=False, verbose=False):\n",
    "        UNIT = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if il_unit > ir_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} greater than ir_unit of {ir_unit}\")\n",
    "            return None\n",
    "        \n",
    "        if il_boundary > il_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} less than il_boundary of {il_boundary}\")\n",
    "            return None\n",
    "        \n",
    "        if ir_boundary < ir_unit:\n",
    "            print(f\"Error: ir_unit of {ir_unit} greater than ir_boundary of {ir_boundary}\")\n",
    "            return None\n",
    "        \n",
    "        # Caveat: Parentheticals\n",
    "        # The context of a unit inside a set of enclosures should\n",
    "        # not go farther than the boundaries of those enclosures.\n",
    "        # However, we need to manually determine whether the unit\n",
    "        # is in parentheses (or any set of the matching symbols\n",
    "        # below).\n",
    "        openers = list(enclosures.keys())\n",
    "        closers = list(enclosures.values())\n",
    "        enclosing_chars = [*closers, *openers]\n",
    "\n",
    "        # Look for Group Punctuation on the Left\n",
    "        i = il_unit\n",
    "        opener = None\n",
    "        while i > il_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.lower_ in enclosing_chars and token.lower_ != \",\":\n",
    "                opener = token\n",
    "                break\n",
    "            i -= 1\n",
    "\n",
    "        # Look for Group Punctuation on the Right\n",
    "        i = ir_unit\n",
    "        closer = None\n",
    "        while i < ir_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.lower_ in enclosing_chars and token.lower_ != \",\":\n",
    "                closer = token\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        # If there's a group punctuation on the left\n",
    "        # and right, and they match each other (e.g. '(' and ')'),\n",
    "        # we return the text between the punctuations.\n",
    "        parenthetical = opener and closer and enclosures.get(opener.lower_) == closer.text\n",
    "        if parenthetical:\n",
    "            context = [t for t in self.main.sp_doc[opener.i:closer.i+1]]\n",
    "            \n",
    "            if verbose and VERBOSE_LEVEL >= 1:\n",
    "                print(f\"Parenthetical - Unit Context of '{UNIT}': {context}\")\n",
    "            \n",
    "            return context\n",
    "\n",
    "        # We can also check whether the unit it enclosed\n",
    "        # in a comma or two, only if a comma can enclose.\n",
    "        if comma_encloses:\n",
    "            i = il_unit\n",
    "            i_token = self.main.sp_doc[i]\n",
    "            while i > il_boundary:\n",
    "                i_token = self.main.sp_doc[i]\n",
    "                if i_token.lower_ in [\",\", \";\", \"—\"]:\n",
    "                    break\n",
    "                i -= 1\n",
    "\n",
    "            j = ir_unit\n",
    "            j_token = self.main.sp_doc[j]\n",
    "            while j < ir_boundary:\n",
    "                j_token = self.main.sp_doc[j]\n",
    "                if j_token.lower_ in [\",\", \";\", \"—\"]:\n",
    "                    break\n",
    "                j += 1\n",
    "\n",
    "            if (i_token and i_token.lower_ == \",\") or (j_token and j_token.lower_ == \",\"):\n",
    "                context = [t for t in self.main.sp_doc[i:j+1]]\n",
    "            \n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Comma - Unit Context of '{UNIT}': {context}\")\n",
    "                    \n",
    "                return context\n",
    "            \n",
    "        # As the unit is not a parenthetical, we will expand\n",
    "        # outwards until we run into a stopping token. The exclude\n",
    "        # list contains tokens that should be excluded from the\n",
    "        # context. Currently, it will contain any parentheticals\n",
    "        # that we run into.\n",
    "        exclude = []\n",
    "\n",
    "        # We can modify the enclosures after handling the parenthetical\n",
    "        # situation to make the code easier.\n",
    "        if comma_encloses:\n",
    "            enclosures[\",\"] : \",\"\n",
    "        \n",
    "        # Expand Left\n",
    "        while il_unit > il_boundary:\n",
    "            # Assuming that the current token is fine,\n",
    "            # we look to the left.\n",
    "            l_token = self.main.sp_doc[il_unit-1]\n",
    "\n",
    "            if l_token.lower_ not in closers:\n",
    "                in_set = l_token.pos_ in speech or l_token.lower_ in literals\n",
    "                if in_set ^ include:\n",
    "                    break\n",
    "                il_unit -= 1\n",
    "            # If it's a closing enclosure (e.g. ')', ']'),\n",
    "            # we need to skip over whatever is contained in\n",
    "            # that punctuation.\n",
    "            else:\n",
    "                i = il_unit - 1\n",
    "                \n",
    "                token = self.main.sp_doc[i]\n",
    "                exclude.append(token)\n",
    "\n",
    "                # We continue until we reach the boundary or\n",
    "                # we find the matching opening character.\n",
    "                closed = []\n",
    "                \n",
    "                while i > il_boundary:\n",
    "                    token = self.main.sp_doc[i]\n",
    "                    # Found Closer\n",
    "                    if token.lower_ in closers:\n",
    "                        exclude.append(token)\n",
    "                        closed.append(token.lower_)\n",
    "                    # Currently Closed\n",
    "                    elif closed:\n",
    "                        exclude.append(token)\n",
    "                        # Found Opener\n",
    "                        if enclosures.get(token.lower_) == closed[-1]:\n",
    "                            closed.pop()\n",
    "                    else:\n",
    "                        break\n",
    "                    i -= 1\n",
    "                \n",
    "                il_unit = i\n",
    "\n",
    "        # Expand Right\n",
    "        while ir_unit < ir_boundary:\n",
    "            # We're checking the token to the right\n",
    "            # to see if we can expand or not.\n",
    "            r_token = self.main.sp_doc[ir_unit+1]\n",
    "\n",
    "            if r_token.lower_ not in openers:\n",
    "                in_set = r_token.pos_ in speech or r_token.lower_ in literals\n",
    "                if in_set ^ include:\n",
    "                    break\n",
    "                ir_unit += 1\n",
    "            # If the token to the right is an opener (e.g. '(', '['), we must skip\n",
    "            # it, the parenthetical inside, and the closer.\n",
    "            else:\n",
    "                i = ir_unit + 1\n",
    "                \n",
    "                token = self.main.sp_doc[i]\n",
    "                exclude.append(token)\n",
    "\n",
    "                # We continue until we reach the boundary or\n",
    "                # we find all the closers for the openers.\n",
    "                opened = []\n",
    "                \n",
    "                while i < ir_boundary:\n",
    "                    token = self.main.sp_doc[i]\n",
    "                    # Found Opener\n",
    "                    if token.lower_ in openers:\n",
    "                        exclude.append(token)\n",
    "                        opened.append(token.lower_)\n",
    "                    # Currently Opened\n",
    "                    elif opened:\n",
    "                        exclude.append(token)\n",
    "                        # Found Closer\n",
    "                        if token.lower_ == enclosures.get(opened[-1]):\n",
    "                            opened.pop()\n",
    "                    else:\n",
    "                        break\n",
    "                    i += 1\n",
    "                \n",
    "                ir_unit = i\n",
    "        \n",
    "        # We remove the excluded tokens and return the context.\n",
    "        context = [t for t in self.main.sp_doc[il_unit:ir_unit+1] if t not in exclude]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Unit Context of '{UNIT}': {context}\")\n",
    "        \n",
    "        return context\n",
    "\n",
    "\n",
    "    \n",
    "    def get_span_at_indices(self, l_index, r_index):\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "\n",
    "        while text[l_index].isspace():\n",
    "            l_index += 1\n",
    "\n",
    "        while text[r_index].isspace():\n",
    "            r_index -= 1\n",
    "\n",
    "        if l_index > r_index:\n",
    "            print(f\"Error: l_index of {l_index} greater than r_index of {r_index}\")\n",
    "            return None\n",
    "            \n",
    "        l_token_i = self.main.token_at_char(l_index).i\n",
    "        r_token_i = self.main.token_at_char(r_index).i\n",
    "        \n",
    "        return self.main.sp_doc[l_token_i:r_token_i+1]\n",
    "\n",
    "\n",
    "    \n",
    "    def get_base_nouns(self, span, return_tokens=False, immediate_stop=False):\n",
    "        ending_nouns = []\n",
    "        \n",
    "        reversed_span = [t for t in span]\n",
    "        reversed_span.reverse()\n",
    "        \n",
    "        for token in reversed_span:\n",
    "            if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                ending_nouns.append(token if return_tokens else self.main.sp_doc[token.i:token.i+1])\n",
    "                if immediate_stop:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return ending_nouns\n",
    "\n",
    "\n",
    "\n",
    "    def flatten(self, arr):\n",
    "        flat_arr = []\n",
    "\n",
    "        if not isinstance(arr, list):\n",
    "            return [arr]\n",
    "\n",
    "        for element in arr:\n",
    "            flat_arr.extend(self.flatten(element))\n",
    "\n",
    "        return flat_arr\n",
    "\n",
    "\n",
    "    def is_same_text(self, sp_a, sp_b):\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        sp_a_text = sp_a.text.lower()\n",
    "\n",
    "        if sp_a_text == sp_b_text:\n",
    "            return True\n",
    "            \n",
    "        sp_a_singular_texts = [sp_a_text] if sp_a[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(sp_a_text)\n",
    "        sp_b_singular_texts = [sp_b_text] if sp_b[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(sp_b_text)\n",
    "\n",
    "        if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def has_same_base_nouns(self, sp_a, sp_b):\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        sp_b_0_text = sp_b[0].lower_\n",
    "        sp_b_0_is_noun = sp_b[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "        \n",
    "        sp_b_nouns = []\n",
    "        sp_b_num_adjectives = 0\n",
    "        \n",
    "        for token in sp_b:\n",
    "            if not sp_b_nouns and token.pos_ == \"ADJ\":\n",
    "                sp_b_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_b_nouns.append(token)\n",
    "\n",
    "        if not sp_b_nouns:\n",
    "            return False\n",
    "\n",
    "        sp_b_nouns_text = [noun.lower_ for noun in sp_b_nouns]\n",
    "        sp_b_singular_texts = [\" \".join(sp_b_nouns_text)] if sp_b_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(\" \".join(sp_b_nouns_text))\n",
    "\n",
    "        sp_a_text = sp_a.text.lower()\n",
    "        sp_a_0_text = sp_a[0].lower_\n",
    "        sp_a_0_is_noun = sp_a[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "        # Case Example: 'Hyla' v. 'Hyla tadpoles'\n",
    "        if sp_a_0_text == sp_b_0_text and (sp_a_0_is_noun or sp_b_0_is_noun):\n",
    "            if sp_a_text in sp_b_text or sp_b_text in sp_a_text:\n",
    "                return True\n",
    "        \n",
    "        # Case Example: 'dogs' v. 'red dogs'\n",
    "        sp_a_nouns = []\n",
    "        sp_a_num_adjectives = 0\n",
    "        for token in sp_a:\n",
    "            if not sp_a_nouns and token.pos_ == \"ADJ\":\n",
    "                sp_a_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_a_nouns.append(token)\n",
    "        \n",
    "        if not sp_a_nouns:\n",
    "            return False\n",
    "        \n",
    "        sp_a_nouns_text = [noun.lower_ for noun in sp_a_nouns]\n",
    "        \n",
    "        if sp_a_nouns and sp_b_nouns and (\n",
    "            (sp_a_num_adjectives == 1 and sp_b_num_adjectives == 0) or \n",
    "            (sp_b_num_adjectives == 1 and sp_a_num_adjectives == 0)\n",
    "        ):\n",
    "            sp_a_singular_texts = [\" \".join(sp_a_nouns_text)] if sp_a_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(\" \".join(sp_a_nouns_text))\n",
    "            if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "                return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ae18e1ee-aac4-4aae-aee0-a9f7c9bb3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species:\n",
    "    def __init__(self, main):\n",
    "        # Tools\n",
    "        self.main = main\n",
    "        self.tn_nlp = TaxoNERD(prefer_gpu=False).load(model=\"en_ner_eco_biobert\", exclude=[\"tagger\", \"parser\", \"attribute_ruler\"])\n",
    "        self.tn_doc = None\n",
    "        \n",
    "        # Contains any spans that have been identified\n",
    "        # as a species.\n",
    "        self.spans = None\n",
    "        self.span_starts = None\n",
    "        \n",
    "        # Contains any tokens that have been identified\n",
    "        # as a species or being a part of a species.\n",
    "        self.tokens = None\n",
    "        \n",
    "        # Used to quickly access the span that a token\n",
    "        # belongs to.\n",
    "        self.token_to_span = None\n",
    "        \n",
    "        # Maps a string to an array of strings wherein\n",
    "        # the strings involved in the key-value pair \n",
    "        # have been identified as an alternate name of each other.\n",
    "        self.alternate_names = None\n",
    "        \n",
    "        # Includes words that (1) are to be identified as species; and\n",
    "        # (2) are sometimes not identified as species, more or less.\n",
    "        self.dictionary = [\"juvenile\", \"juveniles\", \"adult\", \"adults\", \"prey\", \"predator\", \"predators\", \"species\", \"tree\", \"cat\", \"dog\", \"fly\", \"flies\", \"plant\", \"plants\"]\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, text, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        self.tn_doc = self.tn_nlp(text)\n",
    "        self.spans, self.tokens, self.token_to_span, self.span_starts = self.load_species(verbose=verbose)\n",
    "        self.alternate_names = self.load_alternate_names(self.spans)\n",
    "\n",
    "\n",
    "\n",
    "    def convert_tn_spans_to_sp_spans(self, tn_spans):\n",
    "        sp_spans = []\n",
    "\n",
    "        for tn_span in tn_spans:\n",
    "            l_char_index = self.tn_doc[tn_span.start].idx\n",
    "            r_char_index = l_char_index + len(tn_span.text) - 1\n",
    "\n",
    "            try:\n",
    "                l_sp_token_i = self.main.token_at_char(l_char_index).i\n",
    "                r_sp_token_i = self.main.token_at_char(r_char_index).i\n",
    "            except Exception as e:\n",
    "                print(f\"Error: Couldn't find token at character index of {l_char_index} and token index of {l_sp_token_i}.\")\n",
    "                print(f\"Error: Couldn't find token at character index of {r_char_index} and token index of {r_sp_token_i}.\")\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            sp_span = self.main.sp_doc[l_sp_token_i:r_sp_token_i+1]\n",
    "            if sp_span.text != tn_span:\n",
    "                print(f\"Error: SpaCy span does not match TaxoNerd span.\")\n",
    "                continue\n",
    "            \n",
    "            sp_spans.append(sp_span)\n",
    "\n",
    "        return sp_spans\n",
    "\n",
    "\n",
    "\n",
    "    def load_search_strings(self, verbose=False):\n",
    "        search_strings = [*self.dictionary]\n",
    "        \n",
    "        # Creating a Broad Set of Species\n",
    "        spans = self.convert_tn_spans_to_sp_spans(self.tn_doc.ents)\n",
    "        spans = self.main.separate_spans_by_parenthetical(spans)\n",
    "\n",
    "        # Add Ending Nouns to Set\n",
    "        all_nouns = []\n",
    "        for span in spans:\n",
    "            nouns = self.main.get_base_nouns(span, immediate_stop=True)\n",
    "            # print(f\"Base Nouns for '{span}': {nouns}\")\n",
    "            if nouns:\n",
    "                all_nouns.extend(nouns)\n",
    "        # print(f\"'All Nouns': {all_nouns}\")\n",
    "        spans.extend(all_nouns)\n",
    "\n",
    "        # Adding Plural and Singular Versions of Spans\n",
    "        for span in spans:\n",
    "            text = span.text.lower()\n",
    "            text = self.main.delete_extra_whitespace(self.main.delete_outer_non_alnum(text))\n",
    "\n",
    "            # Blank Text or No Letters\n",
    "            if not text or not [c for c in text if c.isalpha()]:\n",
    "                continue\n",
    "\n",
    "            search_strings.append(text)\n",
    "\n",
    "            # Add Plural Version\n",
    "            singular = span[-1].pos_ == \"NOUN\" and span[-1].tag_ == \"NN\"\n",
    "            if singular:\n",
    "                plural_version = self.main.pluralize(text)\n",
    "                search_strings.extend(plural_version)\n",
    "\n",
    "            # Add Singular Version\n",
    "            plural = span[-1].pos_ == \"NOUN\" and span[-1].tag_ == \"NNS\"\n",
    "            if plural:\n",
    "                singular_version = self.main.singularize(text)\n",
    "                search_strings.extend(singular_version)\n",
    "\n",
    "        # Remove Duplicates\n",
    "        search_strings = list(set(search_strings))\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Search Strings: {search_strings}\")\n",
    "        \n",
    "        return search_strings\n",
    "\n",
    "\n",
    "\n",
    "    def load_alternate_names(self, spans, verbose=False):\n",
    "        spans.sort(key=lambda span: span.start)\n",
    "\n",
    "        # It's useful to know if a different name refers to a\n",
    "        # species we have already seen. For example, in\n",
    "        # \"predatory crab (Carcinus maenas)\", \"predatory crab\"\n",
    "        # is an alternative name for \"Carcinus maenas\" and\n",
    "        # vice versa. This is used so that the species can be\n",
    "        # properly tracked and redundant points are less\n",
    "        # likely to be given.\n",
    "        alternate_names = {}\n",
    "        \n",
    "        # Finding and Storing Alternative Names\n",
    "        for i, species_span in enumerate(spans):\n",
    "            # There's not a next species to\n",
    "            # evaluate.\n",
    "            if i + 1 >= len(spans):\n",
    "                break\n",
    "            \n",
    "            next_species_span = spans[i+1]\n",
    "            \n",
    "            # If there's one token between the species and the next species,\n",
    "            # we check if the next species is surrounded by punctuation.\n",
    "            if next_species_span.start - species_span.end == 1:\n",
    "                # Token Before and After the Next Species\n",
    "                before_next = self.main.sp_doc[next_species_span.start-1]\n",
    "\n",
    "                # Out of Bounds Error\n",
    "                if next_species_span.end >= len(self.main.sp_doc):\n",
    "                    continue\n",
    "                    \n",
    "                after_next = self.main.sp_doc[next_species_span.end]\n",
    "\n",
    "                if before_next.lower_ in [\"(\"] and after_next.lower_ in [\")\"]:\n",
    "                    sp_1_text = species_span.text.lower()\n",
    "                    sp_2_text = next_species_span.text.lower()\n",
    "                    \n",
    "                    if sp_1_text not in alternate_names:\n",
    "                        alternate_names[sp_1_text] = []\n",
    "                    \n",
    "                    if sp_2_text not in alternate_names:\n",
    "                        alternate_names[sp_2_text] = []\n",
    "                    \n",
    "                    alternate_names[sp_1_text].append(sp_2_text)\n",
    "                    alternate_names[sp_2_text].append(sp_1_text)\n",
    "            \n",
    "            # If there's no token between the species and the next,\n",
    "            # species we assume that they refer to the same species.\n",
    "            elif next_species_span.start - species_span.end == 0:\n",
    "                sp_1_text = species_span.text.lower()\n",
    "                sp_2_text = next_species_span.text.lower()\n",
    "                \n",
    "                if sp_1_text not in alternate_names:\n",
    "                    alternate_names[sp_1_text] = []\n",
    "                \n",
    "                if sp_2_text not in alternate_names:\n",
    "                    alternate_names[sp_2_text] = []\n",
    "\n",
    "                alternate_names[sp_1_text].append(sp_2_text)\n",
    "                alternate_names[sp_2_text].append(sp_1_text)\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Alternate Names: {alternate_names}\")\n",
    "\n",
    "        return alternate_names\n",
    "\n",
    "\n",
    "\n",
    "    def bar(self, foo):\n",
    "        if not foo or len(foo) == 1:\n",
    "            return foo\n",
    "\n",
    "        foo.sort()\n",
    "        b = [foo[0]]\n",
    "        \n",
    "        l = 0\n",
    "        \n",
    "        for i in range(1, len(foo)):\n",
    "            if foo[i] - foo[l] <= 1:\n",
    "                l = i\n",
    "            else:\n",
    "                b.append(foo[i])\n",
    "                l = i\n",
    "            \n",
    "        return b\n",
    "\n",
    "\n",
    "    \n",
    "    def load_species(self, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Load Search Strings from Species Spans\n",
    "        search_strings = self.load_search_strings(verbose=verbose)\n",
    "\n",
    "        # Search for Species\n",
    "        # The results are stored in different \n",
    "        # forms below.\n",
    "        spans = []\n",
    "        tokens = []\n",
    "        token_to_span = {}\n",
    "\n",
    "        # Where we're searching for species.\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "\n",
    "        for string in search_strings:\n",
    "            matches = re.finditer(re.escape(string), text, re.IGNORECASE)\n",
    "\n",
    "            for l_char_index, r_char_index, matched_text in [(match.start(), match.end(), match.group()) for match in matches]:    \n",
    "                # The full word must match, not just a substring inside of it.\n",
    "                # So, if the species we're looking for is \"ant\", only \"ant\"\n",
    "                # will match -- not \"pants\" or \"antebellum\". Therefore, the\n",
    "                # characters to the left and right of the matched string cannot\n",
    "                # be letters.\n",
    "                l_char_is_letter = l_char_index > 0 and text[l_char_index-1].isalpha()\n",
    "                r_char_is_letter = r_char_index < len(text) and text[r_char_index].isalpha()\n",
    "                \n",
    "                if l_char_is_letter or r_char_is_letter or not matched_text:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    l_token_i = self.main.token_at_char(l_char_index).i\n",
    "                    r_token_i = self.main.token_at_char(r_char_index-1).i\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: Unable to find token at index of {l_char_index}.\")\n",
    "                    print(f\"Error: Unable to find token at index of {r_char_index}.\")\n",
    "                    print(f\"\\tMatched: '{matched_text}'\")\n",
    "                    print(e)\n",
    "                    continue\n",
    "\n",
    "                # This is the matched substring (which would be\n",
    "                # a species) as a span in the parent document.\n",
    "                span = self.main.sp_doc[l_token_i:r_token_i+1]\n",
    "                \n",
    "                # Expand Species\n",
    "                # Let's say there's a word like \"squirrel\". That's a bit ambiguous. \n",
    "                # Is it a brown squirrel, a bonobo? If the species is possibly missing\n",
    "                # information (like an adjective to the left of it), we should expand\n",
    "                # in order to get a full picture of the species.\n",
    "                unclear_1 = len(span) == 1 and span[0].pos_ == \"NOUN\"\n",
    "                unclear_2 = span.start > 0 and self.main.sp_doc[span.start-1].pos_ in [\"ADJ\"]\n",
    "                \n",
    "                if unclear_1 or unclear_2:\n",
    "                    span = self.main.expand_unit(\n",
    "                        il_unit=span.start, \n",
    "                        ir_unit=span.end-1,\n",
    "                        il_boundary=0,\n",
    "                        ir_boundary=len(self.main.sp_doc),\n",
    "                        speech=[\"ADJ\", \"PROPN\"],\n",
    "                        literals=[\"-\"],\n",
    "                        include=True,\n",
    "                        direction=\"LEFT\",\n",
    "                        verbose=verbose\n",
    "                    )\n",
    "                \n",
    "                # Remove Outer Symbols\n",
    "                # There are times where a species is identified with a parenthesis\n",
    "                # nearby. Here, we remove that parenthesis (and any other symbols).\n",
    "                span = self.main.contract_unit(\n",
    "                    il_unit=span.start, \n",
    "                    ir_unit=span.end-1, \n",
    "                    speech=[\"PUNCT\", \"SYM\", \"DET\", \"PART\"],\n",
    "                    include=False,\n",
    "                    verbose=verbose\n",
    "                )\n",
    "\n",
    "                if not span:\n",
    "                    print(f\"Error: Span does not exist; left character index {l_char_index}.\")\n",
    "                    print(f\"\\tMatched: '{matched_text}'\")\n",
    "                    continue\n",
    "            \n",
    "                # A species must have a noun or a\n",
    "                # proper noun. This may help discard\n",
    "                # bad results.\n",
    "                letter_found = False\n",
    "                for token in span:\n",
    "                    if token.pos_ in [\"NOUN\", \"PROPN\"] or token.lower_ in self.dictionary:\n",
    "                        letter_found = True\n",
    "                        break\n",
    "\n",
    "                if not letter_found:\n",
    "                    continue\n",
    "\n",
    "                # Adding Species\n",
    "                spans.append(span)\n",
    "                for token in span:\n",
    "                    if token in tokens or token.pos_ in [\"PUNCT\", \"SYM\", \"DET\", \"PART\"]:\n",
    "                        continue\n",
    "                    tokens.append(token)\n",
    "                    token_to_span[token] = span\n",
    "        \n",
    "        spans = list({span.start: span for span in spans}.values())\n",
    "        spans.sort(key=lambda span: span.start)\n",
    "\n",
    "        # for span in spans:\n",
    "        #     print(span, span.start, span.end)\n",
    "        \n",
    "        # Remove Overlapping Spans\n",
    "        i = 0\n",
    "        while i < len(spans):\n",
    "            j = i + 1\n",
    "            while j < len(spans):\n",
    "                if spans[i].start <= spans[j].start <= spans[i].end and spans[i].start <= spans[j].end <= spans[i].end:\n",
    "                    # print(f\"Span at i: ({spans[i].start}, {spans[i].end})\")\n",
    "                    # print(f\"Span at j: ({spans[j].start}, {spans[j].end})\")\n",
    "                    # print(f'Overlap between \"{spans[i]}\" and \"{spans[j]}\"')\n",
    "                    spans.pop(j)\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        span_starts = [span[0] for span in spans]\n",
    "        span_indices = self.bar([span[0].i for span in spans])\n",
    "        span_starts = [self.main.sp_doc[i] for i in span_indices]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"Output of load_species:\")\n",
    "            print(f\"Spans: {spans}\")\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "            print(f\"Mapped Tokens: {token_to_span}\")\n",
    "            print(f\"Span Starts: {span_starts}\")\n",
    "        \n",
    "        return (spans, tokens, token_to_span, span_starts)\n",
    "\n",
    "\n",
    "\n",
    "    def is_alternate(self, sp_a, sp_b):\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        sp_a_text = sp_a.text.lower()\n",
    "            \n",
    "        # Species B is an alternate name for Species A\n",
    "        if sp_b_text in self.alternate_names.get(sp_a_text, []):\n",
    "            return True\n",
    "        \n",
    "        # Species A is an alternate name for Species B\n",
    "        if sp_a_text in self.alternate_names.get(sp_b_text, []):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def is_same_text(self, sp_a, sp_b):\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        sp_a_text = sp_a.text.lower()\n",
    "\n",
    "        if sp_a_text == sp_b_text:\n",
    "            return True\n",
    "            \n",
    "        sp_a_singular_texts = [sp_a_text] if sp_a[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(sp_a_text)\n",
    "        sp_b_singular_texts = [sp_b_text] if sp_b[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(sp_b_text)\n",
    "\n",
    "        if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def has_same_base_nouns(self, sp_a, sp_b):\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        sp_b_0_text = sp_b[0].lower_\n",
    "        sp_b_0_is_noun = sp_b[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "        \n",
    "        sp_b_nouns = []\n",
    "        sp_b_num_adjectives = 0\n",
    "        \n",
    "        for token in sp_b:\n",
    "            if not sp_b_nouns and token.pos_ == \"ADJ\":\n",
    "                sp_b_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_b_nouns.append(token)\n",
    "\n",
    "        if not sp_b_nouns:\n",
    "            return False\n",
    "\n",
    "        sp_b_nouns_text = [noun.lower_ for noun in sp_b_nouns]\n",
    "        sp_b_singular_texts = [\" \".join(sp_b_nouns_text)] if sp_b_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(\" \".join(sp_b_nouns_text))\n",
    "\n",
    "        sp_a_text = sp_a.text.lower()\n",
    "        sp_a_0_text = sp_a[0].lower_\n",
    "        sp_a_0_is_noun = sp_a[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "        # Case Example: 'Hyla' v. 'Hyla tadpoles'\n",
    "        if sp_a_0_text == sp_b_0_text and (sp_a_0_is_noun or sp_b_0_is_noun):\n",
    "            if sp_a_text in sp_b_text or sp_b_text in sp_a_text:\n",
    "                return True\n",
    "        \n",
    "        # Case Example: 'dogs' v. 'red dogs'\n",
    "        sp_a_nouns = []\n",
    "        sp_a_num_adjectives = 0\n",
    "        for token in sp_a:\n",
    "            if not sp_a_nouns and token.pos_ == \"ADJ\":\n",
    "                sp_a_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_a_nouns.append(token)\n",
    "        \n",
    "        if not sp_a_nouns:\n",
    "            return False\n",
    "        \n",
    "        sp_a_nouns_text = [noun.lower_ for noun in sp_a_nouns]\n",
    "        \n",
    "        if sp_a_nouns and sp_b_nouns and (\n",
    "            (sp_a_num_adjectives == 1 and sp_b_num_adjectives == 0) or \n",
    "            (sp_b_num_adjectives == 1 and sp_a_num_adjectives == 0)\n",
    "        ):\n",
    "            sp_a_singular_texts = [\" \".join(sp_a_nouns_text)] if sp_a_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(\" \".join(sp_a_nouns_text))\n",
    "            if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def find_same_species(self, sp_A, sp_b, verbose=False):\n",
    "        verbose=False\n",
    "        # METHOD 1: Check for Literal Matches\n",
    "        for sp_a in sp_A:\n",
    "            if self.is_same_text(sp_a, sp_b):\n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Method 1: Match Between '{sp_a}' and '{sp_b}'\")\n",
    "                return sp_a\n",
    "\n",
    "        # METHOD 2: Check Alternate Names\n",
    "        for sp_a in sp_A:\n",
    "            if self.is_alternate(sp_a, sp_b):\n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Method 2: Match Between '{sp_a}' and '{sp_b}'\")\n",
    "                return sp_a\n",
    "        \n",
    "        # METHOD 3: Check Nouns\n",
    "        # This is used if one or none of the species being compared\n",
    "        # has 1 adjective.\n",
    "        for sp_a in sp_A:\n",
    "            if self.has_same_base_nouns(sp_a, sp_b):\n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Method 3: Match Between '{sp_a}' and '{sp_b}'\")\n",
    "                return sp_a\n",
    "\n",
    "        # METHOD 4: Last Ditch Effort\n",
    "        # If there's been no matches, we just look for one string inside of\n",
    "        # another.\n",
    "        for sp_a in sp_A:\n",
    "            sp_a_text = sp_a.text.lower()\n",
    "            sp_b_text = sp_b.text.lower()\n",
    "            \n",
    "            r_sp_a_text = re.compile(f\"(\\s|^){re.escape(sp_a_text)}(\\s|$)\", re.IGNORECASE)\n",
    "            r_sp_b_text = re.compile(f\"(\\s|^){re.escape(sp_b_text)}(\\s|$)\", re.IGNORECASE)\n",
    "            \n",
    "            if re.match(r_sp_a_text, sp_b_text) or re.match(r_sp_b_text, sp_a_text):\n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Method 4: Match Between '{sp_a}' and '{sp_b}'\")\n",
    "                return sp_a\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"No Matches Between {sp_A} and {sp_b}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    def span_at_token(self, token):\n",
    "        if token in self.token_to_span:\n",
    "            return self.token_to_span[token]\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    def is_species(self, token):\n",
    "        return token in self.tokens\n",
    "\n",
    "\n",
    "\n",
    "    def has_species(self, tokens, verbose=False):\n",
    "        for token in tokens:\n",
    "            if token in self.tokens:\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tToken '{token}' is Species\")\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4e72857e-a892-4b1e-b9bf-9abcf33d2232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords:\n",
    "    REGEX = \"regex\"\n",
    "    VOCAB = \"vocab\"\n",
    "    RULES = \"rules\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, main, *, regexes=[], vocab=[], patterns=[], def_pos=[], def_tag=[], def_threshold=0.7, def_weight=1.0):\n",
    "        self.main = main\n",
    "\n",
    "        # Constraints\n",
    "        self.def_threshold = def_threshold\n",
    "        self.def_tag = def_tag\n",
    "        self.def_pos = def_pos\n",
    "        self.def_weight = def_weight\n",
    "        \n",
    "        # Three Types of Matching\n",
    "        self.vocab, self.vocab_data = self.load_vocab(vocab)\n",
    "        self.regex, self.regex_data = self.load_regex(regexes)\n",
    "        self.rules, self.rules_data = self.load_rules(patterns)\n",
    "\n",
    "        # Quick Lookup\n",
    "        self.match_type_to_data = {\n",
    "            Keywords.REGEX: self.regex_data,\n",
    "            Keywords.VOCAB: self.vocab_data,\n",
    "            Keywords.RULES: self.rules_data\n",
    "        }\n",
    "\n",
    "    \n",
    "\n",
    "    def update(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        # Matched Tokens in Different Forms\n",
    "        self.token_data, self.mapped_token_data, self.tokens = self.match_tokens(verbose=verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def load_regex(self, regexes):\n",
    "        r = []\n",
    "        r_data = {}\n",
    "\n",
    "        for unit in regexes:\n",
    "            if isinstance(unit, str):\n",
    "                r.append(unit)\n",
    "            else:\n",
    "                regex = unit[\"regex\"]\n",
    "                r.append(regex)\n",
    "                r_data[regex] = {\n",
    "                    \"types\": unit.get(\"types\", []),\n",
    "                    \"weight\": unit.get(\"weight\", self.def_weight)\n",
    "                }\n",
    "\n",
    "        return r, r_data\n",
    "\n",
    "\n",
    "\n",
    "    def load_vocab(self, vocab):\n",
    "        v = []\n",
    "        v_data = {}\n",
    "        \n",
    "        for unit in vocab:\n",
    "            if isinstance(unit, str):\n",
    "                doc = self.main.sp_nlp(unit)\n",
    "                v.append({\n",
    "                    \"doc\": doc,\n",
    "                    \"lemma\": \" \".join([t.lemma_ for t in doc])\n",
    "                })\n",
    "            else:\n",
    "                doc = self.main.sp_nlp(unit[\"word\"])\n",
    "                v.append({\n",
    "                    \"doc\": doc,\n",
    "                    \"tag\": unit.get(\"tag\", self.def_tag),\n",
    "                    \"pos\": unit.get(\"pos\", self.def_pos),\n",
    "                    \"threshold\": unit.get(\"threshold\", self.def_threshold),\n",
    "                    \"lemma\": \" \".join([t.lemma_ for t in doc])\n",
    "                })\n",
    "                v_data[unit[\"word\"]] = {\n",
    "                    \"types\": unit.get(\"types\") or [],\n",
    "                    \"weight\": unit.get(\"weight\", self.def_weight),\n",
    "                }\n",
    "        \n",
    "        return v, v_data\n",
    "\n",
    "\n",
    "\n",
    "    def load_rules(self, patterns):\n",
    "        r = Matcher(self.main.sp_nlp.vocab)\n",
    "        r_data = {}\n",
    "        \n",
    "        for i, unit in enumerate(patterns):\n",
    "            if isinstance(unit, list):\n",
    "                r.add(f\"{i}\", unit)\n",
    "            else:\n",
    "                r.add(unit[\"name\"], unit[\"pattern\"])\n",
    "                r_data[unit[\"name\"]] = {\n",
    "                    \"types\": unit.get(\"types\") or [],\n",
    "                    \"weight\": unit.get(\"weight\", self.def_weight),\n",
    "                }\n",
    "\n",
    "        return r, r_data\n",
    "\n",
    "\n",
    "\n",
    "    def get_match_data(self, token, match_id, match_type):\n",
    "        match_type_data = self.match_type_to_data[match_type]\n",
    "        \n",
    "        if match_id in match_type_data:\n",
    "            return {\n",
    "                \"token\": token,\n",
    "                \"types\": match_type_data[match_id].get(\"types\", []),\n",
    "                \"weight\": match_type_data[match_id].get(\"weight\", self.def_weight)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"token\": token,\n",
    "                \"types\": [],\n",
    "                \"weight\": self.def_weight\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "    def bad_pos(self, pos):\n",
    "        return self.def_pos and pos not in self.def_pos\n",
    "\n",
    "\n",
    "\n",
    "    def bad_tag(self, tag):\n",
    "        return self.def_tag and tag not in self.def_tag\n",
    "\n",
    "\n",
    "\n",
    "    def bad_token(self, token):\n",
    "        return self.bad_pos(token.pos_) or self.bad_tag(token.tag_)\n",
    "\n",
    "\n",
    "\n",
    "    def match_tokens(self, verbose=False):\n",
    "        # SpaCy Doc DNE or Indexing Map DNE\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        matched_data = []\n",
    "        matched_tokens = []\n",
    "\n",
    "        # Match by Regex\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "        \n",
    "        for regex in self.regex:\n",
    "            matches = [(match.start(), match.end()) for match in re.finditer(regex, text, re.IGNORECASE)]\n",
    "            \n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\t'{regex}' Regex Matches: {matches}\")\n",
    "            \n",
    "            for l_char_index, r_char_index in matches:\n",
    "                span = self.main.get_span_at_indices(l_char_index, r_char_index - 1)\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tSpan Matched: {span}\")\n",
    "\n",
    "                for token in span:\n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tPossible Regex Match for Token '{token}' (Position: {token.pos_} and Tag: {token.tag_})\")\n",
    "                        \n",
    "                    if self.bad_token(token):\n",
    "                        continue\n",
    "                    \n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tRegex Matched Token '{token}'\")\n",
    "                        \n",
    "                    matched_tokens.append(token)\n",
    "                    matched_data.append(self.get_match_data(token, regex, Keywords.REGEX))\n",
    "\n",
    "        # Match by Rules\n",
    "        matches = self.rules(self.main.sp_doc)\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 2:\n",
    "            print(f\"\\tRule Matches: {matches}\")\n",
    "        \n",
    "        for match_id, start, end in matches:\n",
    "            span = self.main.sp_doc[start:end]\n",
    "            name = self.main.sp_nlp.vocab.strings[match_id]\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tPattern '{name}' Matched Span: {span}\")\n",
    "            \n",
    "            for token in span:\n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tPossible Rule Match for Token '{token}' (Position: {token.pos_} and Tag: {token.tag_})\")\n",
    "                    \n",
    "                if self.bad_token(token):\n",
    "                    continue\n",
    "                \n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tRule Matched Token '{token}'\")\n",
    "\n",
    "                matched_tokens.append(token)\n",
    "                matched_data.append(self.get_match_data(token, name, Keywords.RULES))\n",
    "\n",
    "        # Match by Vocab\n",
    "        for token in self.main.sp_doc:\n",
    "            if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tPossible Vocab Match for Token '{token}' (Position: {token.pos_} and Tag: {token.tag_})\")\n",
    "                    \n",
    "            if self.bad_token(token) or token in matched_tokens:\n",
    "                continue\n",
    "\n",
    "            token_doc = self.main.sp_nlp(token.lower_)\n",
    "            token_lemma = \" \".join([t.lemma_ for t in token_doc])\n",
    "            \n",
    "            for vocab_word in self.vocab:\n",
    "                # Ensure Correct Tag\n",
    "                if vocab_word.get(\"tag\"):\n",
    "                    if not [t for t in token_doc if t.tag_ in vocab_word.get(\"tag\")]:\n",
    "                        if verbose and VERBOSE_LEVEL >= 4:\n",
    "                            print(f\"\\t\\t\\tToken '{token_doc}' not in Vocab Word '{vocab_word['doc']}' Tags ({vocab_word.get('tag')})\")\n",
    "                        continue\n",
    "                \n",
    "                # Ensure Correct PoS\n",
    "                if vocab_word.get(\"pos\"):\n",
    "                    if not [t for t in token_doc if t.pos_ in vocab_word.get(\"pos\")]:\n",
    "                        if verbose and VERBOSE_LEVEL >= 4:\n",
    "                            print(f\"\\t\\t\\tToken '{token_doc}' not in Vocab Word '{vocab_word['doc']}' Speech ({vocab_word.get('pos')})\")\n",
    "                        continue\n",
    "\n",
    "                # Check Lemma\n",
    "                if verbose and VERBOSE_LEVEL >= 4:\n",
    "                    print(f\"\\t\\t\\t{token_doc} Lemma ({token_lemma}) and {vocab_word['doc']} Lemma ({vocab_word['lemma']})\")\n",
    "                    \n",
    "                if token_lemma == vocab_word[\"lemma\"]:\n",
    "                    matched_tokens.append(token)\n",
    "                    matched_data.append(self.get_match_data(token, vocab_word[\"doc\"].text, Keywords.VOCAB))\n",
    "\n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tVocab (Lemma) Matched Token '{token}'\")\n",
    "                    \n",
    "                    break\n",
    "\n",
    "                # Check Similarity\n",
    "                similarity = vocab_word[\"doc\"].similarity(token_doc)\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 4:\n",
    "                    print(f\"\\t\\t\\t{token_doc} and {vocab_word['doc']} Similarity: {similarity}\")\n",
    "                    \n",
    "                if similarity >= vocab_word.get(\"threshold\", self.def_threshold):\n",
    "                    matched_tokens.append(token)\n",
    "                    matched_data.append(self.get_match_data(token, vocab_word[\"doc\"].text, Keywords.VOCAB))\n",
    "\n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tVocab Matched Token '{token}'\")\n",
    "                        \n",
    "                    break\n",
    "\n",
    "        # Mapping Match(ed Token) Data\n",
    "        mapped_matched_data = {}\n",
    "        for matched_token_data in matched_data:\n",
    "            mapped_matched_data[matched_token_data[\"token\"]] = matched_token_data\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"Output of match_tokens\")\n",
    "            print(f\"Token Data: {matched_data}\")\n",
    "            print(f\"Mapped Token Data: {mapped_matched_data}\")\n",
    "            print(f\"Token: {matched_tokens}\")\n",
    "        \n",
    "        return matched_data, mapped_matched_data, matched_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "28bb80f7-7826-4bd8-8ba8-c1447cd8e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"study\", \n",
    "                \"hypothesis\", \n",
    "                \"experiment\", \n",
    "                \"found\", \n",
    "                \"discover\", \n",
    "                \"compare\", \n",
    "                \"finding\", \n",
    "                \"result\", \n",
    "                \"test\", \n",
    "                \"examine\", \n",
    "                \"model\",\n",
    "                \"measure\",\n",
    "                \"manipulate\",\n",
    "                \"assess\",\n",
    "                \"conduct\",\n",
    "                \"data\",\n",
    "                \"analyze\",\n",
    "                \"sample\",\n",
    "                \"observe\",\n",
    "                \"observation\",\n",
    "                \"predict\",\n",
    "                \"suggest\",\n",
    "                \"method\",\n",
    "                \"investigation\",\n",
    "                \"trial\",\n",
    "                \"experimental\",\n",
    "                \"evidence\",\n",
    "                \"demonstrate\",\n",
    "                \"analysis\",\n",
    "                \"show\",\n",
    "                \"compare\",\n",
    "                \"comparable\",\n",
    "                \"control group\", \n",
    "                \"independent\",\n",
    "                \"dependent\",\n",
    "                \"applied\",\n",
    "                \"treatment\",\n",
    "                \"survery\",\n",
    "                \"evaluate\",\n",
    "                \"ran\"\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "83d72ee8-a678-44d4-b78a-8bfd0f60341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeExperimentKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main,\n",
    "            patterns=[\n",
    "                [[{\"LOWER\": {\"IN\": [\"theory\", \"theorized\", \"theories\", \"review\", \"reviews\", \"meta-analysis\"]}}]]\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "403b3825-9c79-4c79-84ac-f6e6230cc8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeTopicKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            regexes=[\n",
    "                r\"co-?evolution\",\n",
    "                r\"evolution\",\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\", \"ADJ\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e886c5d3-f551-4557-adcf-1513063cf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CauseKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"increase\", \n",
    "                \"decrease\", \n",
    "                \"change\", \n",
    "                \"shift\", \n",
    "                \"cause\", \n",
    "                \"produce\", \n",
    "                \"trigger\", \n",
    "                \"suppress\", \n",
    "                \"inhibit\",\n",
    "                \"encourage\",\n",
    "                \"allow\",\n",
    "                \"influence\",\n",
    "                \"affect\",\n",
    "                \"alter\",\n",
    "                \"induce\",\n",
    "                \"produce\",\n",
    "                \"result in\",\n",
    "                # \"associated with\",\n",
    "                # \"correlated with\",\n",
    "                \"contribute\",\n",
    "                \"impact\",\n",
    "                \"deter\",\n",
    "                \"depressed\",\n",
    "                \"when\",\n",
    "                \"because\",\n",
    "                \"reduce\",\n",
    "                \"killed\",\n",
    "                # \"supported\"\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"SCONJ\", \"NOUN\"],\n",
    "            # def_tag=[\"VB\", \"VBD\", \"WRB\", \"IN\", \"VBG\"],\n",
    "            # def_threshold=0.75\n",
    "            def_threshold=0.8\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "\n",
    "    \n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b4b15d83-0d2e-400a-be2a-ab705e836a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            vocab=[\n",
    "                \"few\", \n",
    "                \"more\", \n",
    "                \"increase\", \n",
    "                \"decrease\", \n",
    "                \"less\", \n",
    "                \"short\", \n",
    "                \"long\", \n",
    "                \"greater\"\n",
    "                \"shift\",\n",
    "                \"fluctuate\",\n",
    "                \"adapt\",\n",
    "                \"grow\",\n",
    "                \"rise\"\n",
    "                \"surge\",\n",
    "                \"intensify\",\n",
    "                \"amplify\",\n",
    "                \"multiply\",\n",
    "                \"decline\",\n",
    "                \"reduce\",\n",
    "                \"drop\",\n",
    "                \"diminish\",\n",
    "                \"fall\",\n",
    "                \"lessen\",\n",
    "                \"doubled\",\n",
    "                \"tripled\",\n",
    "                \"lower\",\n",
    "                \"adjust\",\n",
    "                \"reject\",\n",
    "            ],\n",
    "            regexes=[\n",
    "                # Match Examples:\n",
    "                # 1. \"one... as...\"\n",
    "                # 2. \"2x than...\"\n",
    "                r\"(one|two|three|four|five|six|seven|eight|nine|ten|twice|thrice|([0-9]+|[0-9]+.[0-9]+)(x|%))[\\s-]+[^\\s]*[\\s-]+(as|more|than|likely)([\\s-]+|$)\"\n",
    "            ],\n",
    "            def_pos=[\"NOUN\", \"ADJ\", \"ADV\", \"VERB\", \"NUM\"],\n",
    "            def_threshold=0.75\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose=verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "\n",
    "    \n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        filtered = []\n",
    "        for token in self.main.sp_doc:\n",
    "            # Already Matched\n",
    "            if token in tokens:\n",
    "                filtered.append(token)\n",
    "            \n",
    "            # Comparative Adjective\n",
    "            # Looking for words like \"bigger\" and \"better\".\n",
    "            elif token.pos_ == \"ADJ\" and token.tag_ == \"JJR\":\n",
    "                filtered.append(token)\n",
    "                continue\n",
    "            \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "25122d9a-1c5a-4fcc-a75d-1b94591fbb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraitKeywords(Keywords):\n",
    "    FOOD = \"Food\"\n",
    "    PRESENT = \"Present\"\n",
    "    NOT_APPLICABLE = \"N/A\"\n",
    "    \n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main, \n",
    "            regexes=[\n",
    "                r\"behaviou?r\", \n",
    "                r\"[^A-Za-z]+rate\", \n",
    "                \"colou?r\",\n",
    "                \"biomass\",\n",
    "                r\"[^A-Za-z]+mass\", \n",
    "                r\"[^A-Za-z]+size\",\n",
    "                \"number\",\n",
    "                \"length\", \n",
    "                \"pattern\", \n",
    "                \"weight\",\n",
    "                \"shape\", \n",
    "                \"efficiency\", \n",
    "                \"trait\",\n",
    "                \"phenotype\",\n",
    "                \"demography\",\n",
    "                \"scent\",\n",
    "                \"population (structure|mechanic)s?\",\n",
    "                \"ability\", \n",
    "                \"capacity\", \n",
    "                \"height\", \n",
    "                \"width\", \n",
    "                \"[A-Za-z]+span\",\n",
    "                {\"regex\": \"diet\", \"types\": [TraitKeywords.FOOD]},\n",
    "                {\"regex\": \"food\", \"types\": [TraitKeywords.FOOD, TraitKeywords.NOT_APPLICABLE]},\n",
    "                {\"regex\": \"feeding\", \"types\": [TraitKeywords.FOOD]},\n",
    "                \"nest\",\n",
    "                \"substrate\",\n",
    "                \"breeding\",\n",
    "                r\"[^A-Za-z]+age[^A-Za-z]+\",\n",
    "                \"lifespan\",\n",
    "                \"development\",\n",
    "                \"output\",\n",
    "                \"time\",\n",
    "                \"period\",\n",
    "                \"level\",\n",
    "                \"configuration\",\n",
    "                \"dimorphism\",\n",
    "                \"capability\",\n",
    "                \"regulation\",\n",
    "                \"excretion\",\n",
    "                \"luminescence\",\n",
    "                r\"[^A-Za-z]+role\",\n",
    "                \"sensitivity\",\n",
    "                \"resistance\",\n",
    "                r\"(un|(^|\\s)[A-Za-z]*-)infected\",\n",
    "                \"temperature\",\n",
    "                \"density\",\n",
    "                {\"regex\": \"presen(t|ce)\", \"types\": [TraitKeywords.PRESENT]},\n",
    "                {\"regex\": \"absen(t|ce)\", \"types\": [TraitKeywords.PRESENT]},\n",
    "                \"oviposition\",\n",
    "                \"semiochemicals\",\n",
    "                \"chemicals\",\n",
    "                \"content\",\n",
    "                \"level\"\n",
    "            ],\n",
    "            # Ideally, I would only include nouns, but sometimes\n",
    "            # they're recognized as adjectives.\n",
    "            def_pos=[\"NOUN\", \"ADJ\"]\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def update(self, verbose=False):\n",
    "        Keywords.update(self, verbose)\n",
    "        self.tokens = self.filter_tokens(self.tokens, verbose)\n",
    "\n",
    "\n",
    "    \n",
    "    def filter_tokens(self, tokens, verbose=False):\n",
    "        verbose=True\n",
    "        if not self.main.sp_doc or not self.main.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Unfiltered Trait Tokens: {tokens}\")\n",
    "        \n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            expanded_token = self.main.expand_unit(\n",
    "                il_unit=token.i, \n",
    "                ir_unit=token.i, \n",
    "                il_boundary=token.sent.start, \n",
    "                ir_boundary=token.sent.end-1, \n",
    "                speech=[\"PUNCT\"],\n",
    "                include=False,\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "            # print(\"Filtering Token\")\n",
    "            # print(token)\n",
    "            # print(expanded_token)\n",
    "            \n",
    "            valid_token = True\n",
    "            \n",
    "            if not self.main.species.has_species(expanded_token, verbose=verbose):\n",
    "                valid_token = False\n",
    "\n",
    "            for chunk in self.main.sp_doc.noun_chunks:\n",
    "                # print(chunk)\n",
    "                if token in chunk and chunk[-1] != token:\n",
    "                    valid_token = False\n",
    "\n",
    "            if valid_token:\n",
    "                filtered.append(token)\n",
    "            \n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Filtered Trait Tokens: {filtered}\")\n",
    "        \n",
    "        return filtered\n",
    "\n",
    "\n",
    "    def merge_traits(self, traits):\n",
    "        merged = {}\n",
    "        \n",
    "        for trait in traits:\n",
    "            found_trait = False\n",
    "            for m_trait in merged:\n",
    "                if self.main.has_same_base_nouns(trait, m_trait) or self.main.is_same_text(trait, m_trait):\n",
    "                    merged[m_trait].append(trait)\n",
    "                    found_trait = True\n",
    "                    break\n",
    "                \n",
    "                trait_types = []\n",
    "                for thing in trait:\n",
    "                    things = self.main.trait.mapped_token_data.get(thing)\n",
    "                    if things:\n",
    "                        trait_types.extend(things[\"types\"])\n",
    "                    \n",
    "                m_trait_types = []\n",
    "                for thing in m_trait:\n",
    "                    things = self.main.trait.mapped_token_data.get(thing)\n",
    "                    if things:\n",
    "                        m_trait_types.extend(things[\"types\"])\n",
    "    \n",
    "                # print(trait_types, m_trait_types)\n",
    "    \n",
    "                if not trait_types or not m_trait_types:\n",
    "                    continue\n",
    "    \n",
    "                type_intersection = set(trait_types).intersection(m_trait_types)\n",
    "                if type_intersection.intersection(['Food']):\n",
    "                    merged[m_trait].append(trait)\n",
    "                    found_trait = True\n",
    "                    break\n",
    "                \n",
    "            if not found_trait:\n",
    "                merged[trait] = [trait]\n",
    "    \n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6e107006-f2b4-43fa-9240-d5d70906721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main,\n",
    "            vocab=[\n",
    "                \"compare\",\n",
    "                \"examine\",\n",
    "                \"evaluate\",\n",
    "                \"assess\",\n",
    "            ],\n",
    "            def_pos=[\"VERB\", \"NOUN\"], \n",
    "            def_threshold=0.8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "712540a5-fab5-492a-9321-4ce2af0e9502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariabilityKeywords(Keywords):\n",
    "    def __init__(self, main):\n",
    "        super().__init__(\n",
    "            main,\n",
    "            vocab=[\n",
    "                {\"word\": \"different\", \"pos\": [\"ADJ\", \"NOUN\"]},\n",
    "                {\"word\": \"vary\", \"pos\": [\"VERB\", \"NOUN\"]},\n",
    "                {\"word\": \"varied\", \"pos\": [\"VERB\", \"NOUN\"]},\n",
    "                {\"word\": \"compare\", \"pos\": [\"VERB\"]}\n",
    "            ],\n",
    "            regexes=[\n",
    "                r\"between\",\n",
    "                r\"against\",\n",
    "                r\"independen(t|ts|tly|cy)\",\n",
    "                r\"dependen(t|ts|tly|cy)\",\n",
    "                r\"treatments?\",\n",
    "                r\"effect\",\n",
    "                r\"control\",\n",
    "                r\"(with|without)[A-Za-z]*(with|without)\",\n",
    "                r\"(^| )(un|not)[-| ]?([A-Za-z]+) [^!;?.\\n]* \\3\",\n",
    "                r\"([A-Za-z]+) [^!;?.\\n]* (un|not)[-| ]?\\1( |$)\",\n",
    "                # I've added these two words because they can sometimes\n",
    "                # hint at a variable.\n",
    "                r\"when\",\n",
    "                r\"where\",\n",
    "                \n",
    "            ],\n",
    "            patterns=[\n",
    "                [[{\"LOWER\": {\"IN\": [\"neither\", \"either\", \"both\"]}}, {\"OP\": \"*\", \"TAG\": {\"NOT_IN\": [\".\"]}}, {\"LOWER\": {\"IN\": [\"or\", \"and\"]}}]],\n",
    "                [[{\"LOWER\": {\"IN\": [\"with\", \"without\"]}}, {\"OP\": \"*\", \"TAG\": {\"NOT_IN\": [\".\"]}}, {\"LOWER\": {\"IN\": [\"with\", \"without\"]}}]],\n",
    "                [[{\"LOWER\": {\"IN\": [\"at\"]}}, {\"POS\": \"NUM\"}]],\n",
    "                [[{\"LOWER\": {\"IN\": [\"at\"]}}, {\"LOWER\": {\"IN\": [\"several\", \"unique\", \"multiple\", \"different\"]}}]],\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "90c99722-7d97-4889-aa87-59ad1d81b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main(Base):\n",
    "    def __init__(self):\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.sp_doc = None\n",
    "        super().__init__(self)\n",
    "\n",
    "        # Maps Character Position to Token in Document\n",
    "        # Used to handle differences between different\n",
    "        # pipelines and tools.\n",
    "        self.index_map = None\n",
    "\n",
    "        # Parsers\n",
    "        # This is currently not being used, I may\n",
    "        # add it back later.\n",
    "        self.parts = Parts(self)\n",
    "        self.species = Species(self)\n",
    "        self.trait = TraitKeywords(self)\n",
    "        self.cause = CauseKeywords(self)\n",
    "        self.change = ChangeKeywords(self)\n",
    "        # self.experiment = ExperimentKeywords(self)\n",
    "        # self.variability = VariabilityKeywords(self)\n",
    "        # self.test = TestKeywords(self)\n",
    "        # self.not_experiment = NegativeExperimentKplf)\n",
    "\n",
    "\n",
    "    \n",
    "    def update_doc(self, doc, verbose=False):\n",
    "        self.sp_doc = doc\n",
    "        self.index_map = self.load_index_map()\n",
    "        self.parts.update()\n",
    "        self.species.update(doc.text, verbose=False)\n",
    "        self.trait.update(verbose=False)\n",
    "        self.cause.update(verbose=False)\n",
    "        self.change.update(verbose=False)\n",
    "        # self.experiment.update(verbose=False)\n",
    "        # self.not_experiment.update(verbose=False)\n",
    "        # self.not_topic.update(verbose=False)\n",
    "        # self.variability.update(verbose=False)\n",
    "        # self.test.update(verbose=False)\n",
    "\n",
    "\n",
    "    \n",
    "    def update_text(self, text, verbose=False):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.update_doc(self.sp_doc, verbose=verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def load_index_map(self):\n",
    "        # SpaCy Doc Not Found\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Map Character Index to Token\n",
    "        index_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            l_char_index = token.idx\n",
    "            r_char_index = token.idx + len(token)\n",
    "\n",
    "            for i in range(l_char_index, r_char_index):\n",
    "                index_map[i] = token\n",
    "\n",
    "        return index_map\n",
    "\n",
    "\n",
    "\n",
    "    def token_at_char(self, char_index):\n",
    "        # SpaCy Doc or Indexing Map Not Found\n",
    "        if not self.sp_doc or not self.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        if char_index in self.index_map:\n",
    "            return self.index_map[char_index]\n",
    "\n",
    "        raise Exception(f\"Token at Index {char_index} Not Found\")\n",
    "    \n",
    "\n",
    "    \n",
    "    def valid_trait_token(self, data, verbose=False):\n",
    "        verbose = True\n",
    "        token = data[\"token\"]\n",
    "        \n",
    "        # print(f\"Validate Trait Token '{token}'\")\n",
    "        if token not in self.trait.tokens:\n",
    "            return 0\n",
    "\n",
    "        # print(f\"Token '{token}' in Trait Tokens\")\n",
    "        \n",
    "        # Check if Applicable\n",
    "        token_data = self.trait.mapped_token_data[token]\n",
    "        \n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Token '{token}'\")\n",
    "            print(f\"Types: {token_data['types']}\")\n",
    "            \n",
    "        if TraitKeywords.NOT_APPLICABLE in token_data[\"types\"]:\n",
    "            # print(f\"\\tToken Not Applicable\")\n",
    "            return 0\n",
    "\n",
    "        token_context = set(self.find_unit_context(\n",
    "            il_unit=token.i, \n",
    "            ir_unit=token.i, \n",
    "            il_boundary=token.sent.start, \n",
    "            ir_boundary=token.sent.end-1, \n",
    "            verbose=verbose)\n",
    "        )\n",
    "        \n",
    "        causes = set(data[\"sent_cause_tokens\"]).intersection(token_context)\n",
    "        changes = set(data[\"sent_change_tokens\"]).intersection(token_context)\n",
    "        \n",
    "        return 1.0 if causes or changes else 0.25\n",
    "\n",
    "\n",
    "\n",
    "    def valid_species_token(self, data, verbose=False):\n",
    "        token = data[\"token\"]\n",
    "        if token not in self.species.tokens:\n",
    "            return 0\n",
    "        \n",
    "        token_context = set(self.find_unit_context(\n",
    "            il_unit=token.i, \n",
    "            ir_unit=token.i, \n",
    "            il_boundary=token.sent.start, \n",
    "            ir_boundary=token.sent.end-1, \n",
    "            verbose=verbose)\n",
    "        )\n",
    "        \n",
    "        causes = set(data[\"sent_cause_tokens\"]).intersection(token_context)\n",
    "        changes = set(data[\"sent_change_tokens\"]).intersection(token_context)\n",
    "\n",
    "        return 1 if causes or changes else 0\n",
    "    \n",
    "    \n",
    "\n",
    "    def valid_trait(self, verbose=False):\n",
    "        traits = self.trait.merge_traits([self.sp_doc[token.i:token.i+1] for token in self.trait.tokens])\n",
    "\n",
    "        # For full points, you should have a trait that's mentioned\n",
    "        # at least two times in different areas.\n",
    "        two_mentions = False\n",
    "\n",
    "        for trait, instances in traits.items():\n",
    "            if len(instances) < 2:\n",
    "                continue\n",
    "\n",
    "            locations = set([instance.sent.start for instance in instances])\n",
    "            if len(locations) >= 2:\n",
    "                return 1\n",
    "\n",
    "        if traits:\n",
    "            return 0.33\n",
    "        return 0\n",
    "\n",
    "\n",
    "    \n",
    "    def valid_trait_variation(self, verbose=False):\n",
    "        verbose=True\n",
    "        max_trait_variation_points = 0\n",
    "        \n",
    "        sentences = list(self.sp_doc.sents)\n",
    "        num_sentences = len(sentences)\n",
    "\n",
    "        for i in range(num_sentences):\n",
    "            sent_i = sentences[i]\n",
    "            sent_i_tokens = set([token for token in sent_i])\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tSentence I: {sent_i}\")\n",
    "            \n",
    "            sent_i_test_tokens = sent_i_tokens.intersection(self.test.tokens)\n",
    "            sent_i_experiment_tokens = sent_i_tokens.intersection(self.experiment.tokens)\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tSentence I Test Tokens: {sent_i_test_tokens}\")\n",
    "                print(f\"\\tSentence I Experiment Tokens: {sent_i_experiment_tokens}\")\n",
    "\n",
    "            if not sent_i_test_tokens and not sent_i_experiment_tokens:\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tNo Experiment or Test Tokens in Sentence I\")\n",
    "                continue\n",
    "\n",
    "            trait_variation_points_i = 0\n",
    "\n",
    "            if sent_i_experiment_tokens:\n",
    "                trait_variation_points_i = 0.10\n",
    "            \n",
    "            if sent_i_test_tokens:\n",
    "                trait_variation_points_i = 0.25\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tTrait Variation Points for I: {trait_variation_points_i}\")\n",
    "\n",
    "            sent_i_trait_tokens = sent_i_tokens.intersection(self.trait.tokens)\n",
    "            sent_i_species_tokens = sent_i_tokens.intersection(self.species.tokens)\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tSentence I Trait Tokens: {sent_i_trait_tokens}\")\n",
    "\n",
    "            if not sent_i_trait_tokens and not sent_i_species_tokens:\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tNo Trait or Specie Tokens in Sentence I\")\n",
    "                continue\n",
    "\n",
    "            t_variables = []\n",
    "            s_variables = []\n",
    "            sent_i_variability_tokens = sent_i_tokens.intersection(self.variability.tokens)\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tSentence I Variability Tokens: {sent_i_variability_tokens}\")\n",
    "\n",
    "            deduct_points = not sent_i_variability_tokens\n",
    "            \n",
    "            if sent_i_variability_tokens:\n",
    "                for token in sent_i_variability_tokens:\n",
    "                    trait_in_context = set(self.find_unit_context(\n",
    "                        il_unit=token.i, \n",
    "                        ir_unit=token.i, \n",
    "                        il_boundary=token.sent.start,\n",
    "                        ir_boundary=token.sent.end-1, \n",
    "                        speech=[\"PUNCT\"],\n",
    "                        include=False,\n",
    "                        comma_encloses=True,\n",
    "                        verbose=verbose\n",
    "                    )).intersection(self.trait.tokens)\n",
    "\n",
    "                    specie_in_context = set(self.find_unit_context(\n",
    "                        il_unit=token.i, \n",
    "                        ir_unit=token.i, \n",
    "                        il_boundary=token.sent.start,\n",
    "                        ir_boundary=token.sent.end-1, \n",
    "                        speech=[\"PUNCT\"],\n",
    "                        include=False,\n",
    "                        comma_encloses=True,\n",
    "                        verbose=verbose\n",
    "                    )).intersection(self.species.tokens)\n",
    "                    \n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tVariability Token '{token}' Traits in Context: {trait_in_context}\") \n",
    "                        print(f\"\\t\\tVariability Token '{token}' Specie in Context: {specie_in_context}\") \n",
    "\n",
    "                    if not specie_in_context and not trait_in_context:\n",
    "                        if verbose and VERBOSE_LEVEL >= 3:\n",
    "                            print(f\"\\t\\tNo Traits in Variability Token '{token}' Context\")\n",
    "                        continue\n",
    "\n",
    "                    deduct_points = False\n",
    "                    t_variables.extend(trait_in_context)\n",
    "                    s_variables.extend(specie_in_context)\n",
    "\n",
    "            t_variables = list(set(t_variables))\n",
    "            s_variables = list(set(s_variables))\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tt_variables: {t_variables}\")\n",
    "                print(f\"\\ts_variables: {s_variables}\")\n",
    "            \n",
    "            if t_variables or s_variables:\n",
    "                trait_variation_points_i += 0.25\n",
    "            else:\n",
    "                trait_variation_points_i += 0.15\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tTrait Variation Points for I: {trait_variation_points_i}\")\n",
    "\n",
    "            assert trait_variation_points_i <= 0.5\n",
    "\n",
    "            for j in range(i, num_sentences):\n",
    "                sent_j = sentences[j]\n",
    "                sent_j_tokens = set([token for token in sent_j])\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tSentence J: {sent_j}\")\n",
    "\n",
    "                sent_j_cause_tokens = sent_j_tokens.intersection(self.cause.tokens)\n",
    "                sent_j_change_tokens = sent_j_tokens.intersection(self.change.tokens)\n",
    "                sent_j_species_tokens = sent_j_tokens.intersection(self.species.span_starts)\n",
    "                sent_j_trait_tokens = sent_j_tokens.intersection(self.trait.tokens)\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tSentence J Cause Tokens: {sent_j_cause_tokens}\")\n",
    "                    print(f\"\\tSentence J Change Tokens: {sent_j_change_tokens}\")\n",
    "                    print(f\"\\tSentence J Species Tokens: {sent_j_species_tokens}\")\n",
    "                    print(f\"\\tSentence J Trait Tokens: {sent_j_trait_tokens}\")\n",
    "                \n",
    "                if not sent_j_species_tokens or (not sent_j_cause_tokens and not sent_j_change_tokens):\n",
    "                    if verbose and VERBOSE_LEVEL >= 2:\n",
    "                        print(f\"\\tUnsatisfied Conditions for Sentence J\")\n",
    "                    continue\n",
    "\n",
    "                trait_variation_points_j = 0\n",
    "                \n",
    "                if (not sent_j_species_tokens and not sent_j_trait_tokens) or (not t_variables and not s_variables):\n",
    "                    trait_variation_points_j += 0.25\n",
    "                elif i == j:\n",
    "                    trait_variation_points_j += 0.25\n",
    "                elif i != j:\n",
    "                    # Check if Variable Referenced Again via Types\n",
    "                    variable_types = set(self.flatten([self.trait.mapped_token_data[token][\"types\"] for token in t_variables]))\n",
    "                    sent_j_trait_types = set(self.flatten([self.trait.mapped_token_data[token][\"types\"] for token in sent_j_trait_tokens]))\n",
    "\n",
    "                    if verbose and VERBOSE_LEVEL >= 2:\n",
    "                        print(f\"\\tVariable Types: {variable_types}\")\n",
    "                        print(f\"\\tTrait Types in Sentence J: {sent_j_trait_types}\")\n",
    "                        \n",
    "                    # Check if Variable Referenced Again via Literals\n",
    "                    variable_strings = set([token.lower_ for token in t_variables])\n",
    "                    sent_j_trait_strings = set([token.lower_ for token in sent_j_trait_tokens])\n",
    "\n",
    "                    if verbose and VERBOSE_LEVEL >= 2:\n",
    "                        print(f\"\\tVariable Trait (as Strings): {variable_strings}\")\n",
    "                        print(f\"\\tTrait (as Strings) in Sentence J: {sent_j_trait_strings}\")\n",
    "\n",
    "                    # Check if Trait Referenced\n",
    "                    t_variable_referenced = bool(variable_types & sent_j_trait_types) or bool(variable_strings & sent_j_trait_strings)\n",
    "\n",
    "                    # Check if Species Rerferenced\n",
    "                    sent_j_species_spans = [self.sp_doc[token.i:token.i+1] for token in sent_j_species_tokens]\n",
    "                    s_variables_spans = [self.sp_doc[token.i:token.i+1] for token in s_variables]\n",
    "                    overlap = [self.species.find_same_species(sent_j_species_spans, species) for species in s_variables_spans]\n",
    "                    s_variable_referenced = any(overlap)\n",
    "\n",
    "                    if verbose and VERBOSE_LEVEL >= 2:\n",
    "                        print(f\"\\tT Variable Referenced? {t_variable_referenced}\")\n",
    "                        print(f\"\\tS Variable Referenced? {s_variable_referenced}\")\n",
    "                    \n",
    "                    if t_variable_referenced or s_variable_referenced:\n",
    "                        trait_variation_points_j += 0.50\n",
    "                    else:\n",
    "                        trait_variation_points_j += 0.25\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tTrait Variation Points for J: {trait_variation_points_j}\")\n",
    "\n",
    "                assert trait_variation_points_j <= 0.5\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\ti: {i}\")\n",
    "                    print(f\"\\tj: {j}\")\n",
    "\n",
    "                trait_variation_points = trait_variation_points_i + trait_variation_points_j\n",
    "\n",
    "                if deduct_points:\n",
    "                    trait_variation_points *= 0.6375\n",
    "                \n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tTrait Variation Points: {trait_variation_points}\")\n",
    "                \n",
    "                max_trait_variation_points = max(max_trait_variation_points, trait_variation_points)\n",
    "                \n",
    "                if max_trait_variation_points >= 1:\n",
    "                    return 1\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Max Trait Variation Points: {max_trait_variation_points}\")\n",
    "            \n",
    "        return max_trait_variation_points\n",
    "\n",
    "\n",
    "    \n",
    "    def update_seen_species(self, data, verbose=False):\n",
    "        # Unpacking Data\n",
    "        token = data[\"token\"]\n",
    "        seen_species = data[\"seen_species\"]\n",
    "        sent_seen_species = data[\"sent_seen_species\"]\n",
    "        sent_num_unique_species = data[\"sent_num_unique_species\"]\n",
    "        \n",
    "        # Update Seen Species in Entire Text\n",
    "        span = self.species.span_at_token(token)\n",
    "        prev_ref = self.species.find_same_species(seen_species.keys(), span, verbose=verbose)\n",
    "        \n",
    "        if prev_ref:\n",
    "            seen_species[prev_ref] += 1\n",
    "        else:\n",
    "            seen_species[span] = 1\n",
    "\n",
    "        # Check Seen Species in Sentence\n",
    "        # We only add points if it's a species that has not been seen in the sentence. \n",
    "        # This is to avoid redundant points. The points are used so that a species that\n",
    "        # has been previously seen can still be used as long as it hasn't been awarded\n",
    "        # points already.\n",
    "        prev_sent_ref = self.species.find_same_species(sent_seen_species.keys(), span, verbose=verbose)\n",
    "        if prev_sent_ref:\n",
    "            sent_seen_species[prev_sent_ref][\"visits\"] += 1\n",
    "            ref = prev_sent_ref\n",
    "        else:\n",
    "            sent_seen_species[span] = {\n",
    "                \"visits\": 1,\n",
    "                \"points\": 0\n",
    "            }\n",
    "            ref = span\n",
    "        \n",
    "        # Update Number of Unique Species in Sentence\n",
    "        if not prev_sent_ref:\n",
    "            sent_num_unique_species += 1\n",
    "\n",
    "        return {\n",
    "            \"seen_species\": seen_species, \n",
    "            \"sent_seen_species\": sent_seen_species, \n",
    "            # Reference to the species, either its previous\n",
    "            # reference, or the given species.\n",
    "            \"ref\": ref,\n",
    "            # Reference to the species that was last seen\n",
    "            # in the sentence.\n",
    "            \"seen_in_sent\": prev_sent_ref, \n",
    "            \"sent_num_unique_species\": sent_num_unique_species, \n",
    "        }\n",
    "\n",
    "    \n",
    "    \n",
    "    def score(self, verbose=False):\n",
    "        NUM_CATEGORIES = 6\n",
    "\n",
    "        TRAIT = 0\n",
    "        SPECIES = 1\n",
    "        EXPERIMENT = 2\n",
    "        INTERACTION = 3\n",
    "        NOT_TOPIC = 4\n",
    "        TRAIT_VARIATION = 5\n",
    "\n",
    "        # Max # of Points of Category per Sentence (MPC)\n",
    "        # A category can collect points from each sentence. However,\n",
    "        # there's a maximum number of points it can collect. This is\n",
    "        # determined by the MPC.\n",
    "        MPC = [1] * NUM_CATEGORIES\n",
    "    \n",
    "        # Points per Instance of Category (PIC)\n",
    "        # Each token is evaluated to check whether a category\n",
    "        # can be given points. The number of points given, if\n",
    "        # the token is determined to be satisfactory, is the PIC.\n",
    "        # The PIC is less than or equal to the MPC for the corresponding\n",
    "        # category. The idea behind the PIC and MPC is similar to how\n",
    "        # sets work in tennis: you're not immediately awarded the full points\n",
    "        # for the set (MPC) if your opponent fails to return the ball,\n",
    "        # instead you're given a smaller # of points (PIC) that allow you to\n",
    "        # incrementally win the set (category).\n",
    "        PIC = [0] * NUM_CATEGORIES\n",
    "        PIC[TRAIT] = MPC[TRAIT]\n",
    "        PIC[SPECIES] = MPC[SPECIES]/2.0\n",
    "        PIC[EXPERIMENT] = MPC[EXPERIMENT]*1\n",
    "        PIC[INTERACTION] = MPC[INTERACTION]/2.0\n",
    "        PIC[NOT_TOPIC] = MPC[NOT_TOPIC]\n",
    "\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            assert 0 <= PIC[i] <= MPC[i]\n",
    "\n",
    "        # Category Weights (CW)\n",
    "        # It may be helpful to weigh a certain category's fraction of total points\n",
    "        # more or less than another's. Thus, at the end, we'll take a\n",
    "        # weighted average of the category's FTP. The weights must add up to 1.\n",
    "        CW = [0] * NUM_CATEGORIES\n",
    "        CW[TRAIT] = 0.3\n",
    "        CW[SPECIES] = 0.1\n",
    "        CW[EXPERIMENT] = 0.1\n",
    "        CW[INTERACTION] = 0.1\n",
    "        CW[NOT_TOPIC] = 0.1\n",
    "        CW[TRAIT_VARIATION] = 0.3\n",
    "\n",
    "        assert round(np.sum(CW)) == 1\n",
    "\n",
    "        # Leniency\n",
    "        # There are certain categories that aren't going to be as frequent as others.\n",
    "        # For example, the trait category. You could try and decrease the influence\n",
    "        # of said category by lowering its MPC and/or increasing the PIC (so that it's\n",
    "        # easier to achieve the FTP). However, this could make it harder to meaningfully\n",
    "        # represent the category. The idea of leniency is to remove (some) sentences that had 0\n",
    "        # points from the scoring. This increases the FTP as, for example, instead of comparing\n",
    "        # 0.5 points to a total of 2.5 points, you can compare 0.5 to 2.0 points, and so on.\n",
    "        # A leniency of 1 means that all sentences that received 0 points will be removed from\n",
    "        # the scoring. A leniency of 0 means that all the sentences are included in the scoring.\n",
    "        LEN = [0] * NUM_CATEGORIES\n",
    "        LEN[TRAIT] = 0\n",
    "        LEN[SPECIES] = 0.5\n",
    "        LEN[EXPERIMENT] = 0.5\n",
    "        LEN[INTERACTION] = 0.2\n",
    "        LEN[NOT_TOPIC] = 0\n",
    "\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            assert 0 <= LEN[i] <= 1\n",
    "\n",
    "        # Banned Sentences\n",
    "        # Not allowed to benefit from leniency.\n",
    "        banned = [[0] * len(list(self.sp_doc.sents)) for _ in range(NUM_CATEGORIES)]\n",
    "\n",
    "        # Points\n",
    "        points = [0] * NUM_CATEGORIES\n",
    "        binned_points = [0] * NUM_CATEGORIES\n",
    "        \n",
    "        num_zero_pt_sents = [0] * NUM_CATEGORIES\n",
    "        seen_species = {}\n",
    "\n",
    "        # For Testing\n",
    "        species_instances = []\n",
    "        interaction_instances = []\n",
    "        \n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"Extracted Information\")\n",
    "            print(f\"Cause Tokens: {self.cause.tokens}\")\n",
    "            print(f\"Change Tokens: {self.change.tokens}\")\n",
    "            print(f\"Trait Tokens: {self.trait.tokens}\")\n",
    "            print(f\"Species Tokens: {self.species.tokens}\")\n",
    "            print(f\"Experiment Tokens: {self.experiment.tokens}\")\n",
    "            print(f\"Not-Experiment Tokens: {self.not_experiment.tokens}\")\n",
    "            print(f\"Not-Topic Tokens: {self.not_topic.tokens}\")\n",
    "            print(f\"Variability Tokens: {self.variability.tokens}\")\n",
    "            print(f\"Test Tokens: {self.test.tokens}\")\n",
    "        \n",
    "        for sent_i, sent in enumerate(self.sp_doc.sents):\n",
    "            # Current Points in Sentence\n",
    "            curr_points = [0] * NUM_CATEGORIES\n",
    "            sent_tokens = [token for token in sent]\n",
    "\n",
    "            data = {\n",
    "                \"sent_cause_tokens\": set(sent_tokens).intersection(self.cause.tokens),\n",
    "                \"sent_change_tokens\": set(sent_tokens).intersection(self.change.tokens),\n",
    "                \"sent_seen_species\": {},\n",
    "                \"sent_num_unique_species\": 0    \n",
    "            }\n",
    "            \n",
    "            species_instances.append([])\n",
    "            interaction_instances.append([])\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tSentence: {sent}\")\n",
    "                print(f\"\\tSentence Cause Tokens: {data['sent_cause_tokens']}\")\n",
    "                print(f\"\\tSentence Change Tokens: {data['sent_change_tokens']}\")\n",
    "            \n",
    "            for token in sent_tokens:\n",
    "                # If each category has reached their maximum number of points,\n",
    "                # we can end the loop early.\n",
    "                all_maxed = True\n",
    "                for i in range(NUM_CATEGORIES):\n",
    "                    if i == TRAIT_VARIATION:\n",
    "                        continue\n",
    "                    if curr_points[i] < MPC[i]:\n",
    "                        all_maxed = False\n",
    "\n",
    "                if all_maxed:\n",
    "                    break\n",
    "\n",
    "                # Update Token in Data\n",
    "                data[\"token\"] = token\n",
    "                \n",
    "                # Not Topic Points\n",
    "                if curr_points[NOT_TOPIC] < MPC[NOT_TOPIC]:\n",
    "                    if token in self.not_topic.tokens:\n",
    "                        curr_points[NOT_TOPIC] += PIC[NOT_TOPIC]\n",
    "\n",
    "                        if verbose and VERBOSE_LEVEL >= 3:\n",
    "                            print(f\"\\t\\t+ Points for Not-Topic\")\n",
    "\n",
    "                        \n",
    "                # Trait Points\n",
    "                if curr_points[TRAIT] < MPC[TRAIT]:\n",
    "                    if token in self.trait.tokens:\n",
    "                        scale = self.valid_trait_token(data, verbose=verbose)\n",
    "                        curr_points[TRAIT] += scale * PIC[TRAIT]\n",
    "\n",
    "                        if verbose and VERBOSE_LEVEL >= 3 and scale:\n",
    "                            print(f\"\\t\\t+ Points for Trait\")\n",
    "                \n",
    "                \n",
    "                # Not Experiment Points\n",
    "                if token in self.not_experiment.tokens:\n",
    "                    curr_points[EXPERIMENT] -= 2 * PIC[EXPERIMENT]\n",
    "                    banned[EXPERIMENT][sent_i] = 1\n",
    "                    \n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\t- Points for Experiment\")\n",
    "                \n",
    "                \n",
    "                # Experiment Points\n",
    "                elif curr_points[EXPERIMENT] < MPC[EXPERIMENT]:\n",
    "                    if token in self.experiment.tokens:\n",
    "                        scale = 1 if token.pos_ == \"VERB\" else 0.5\n",
    "                        curr_points[EXPERIMENT] += scale * PIC[EXPERIMENT]\n",
    "\n",
    "                        if verbose and VERBOSE_LEVEL >= 3:\n",
    "                            print(f\"\\t\\t+ Points for Experiment\")\n",
    "                \n",
    "                        \n",
    "                # Species and/or Interaction Points\n",
    "                if token in self.species.span_starts:\n",
    "                    # Update Species\n",
    "                    update_data = data\n",
    "                    update_data[\"seen_species\"] = seen_species\n",
    "                    updated_data = self.update_seen_species(update_data, verbose=verbose)\n",
    "\n",
    "                    # Unpacking Updated Data\n",
    "                    seen_species = updated_data[\"seen_species\"]\n",
    "                    seen_in_sent = updated_data[\"seen_in_sent\"]\n",
    "                    sent_seen_species = updated_data[\"sent_seen_species\"]\n",
    "                    sent_num_unique_species = updated_data[\"sent_num_unique_species\"]\n",
    "                    ref = updated_data[\"ref\"]\n",
    "                    \n",
    "                    data[\"sent_seen_species\"] = sent_seen_species\n",
    "                    data[\"sent_num_unique_species\"] = sent_num_unique_species\n",
    "\n",
    "                    if not seen_in_sent and sent_num_unique_species == 1 and sent_seen_species[ref][\"visits\"] == 1:\n",
    "                        interaction_instances[-1].append(ref.text)\n",
    "                    \n",
    "                    if not seen_in_sent:\n",
    "                        # Interaction Points\n",
    "                        # It would make sense to add this fix here as well.\n",
    "                        # if sent_seen_species[ref][\"points\"] <= 0:\n",
    "                        # No it wouldn't\n",
    "                        if curr_points[INTERACTION] < MPC[INTERACTION]:\n",
    "                            if sent_num_unique_species == 2:\n",
    "                                curr_points[INTERACTION] = 2.0 * PIC[INTERACTION]\n",
    "                                interaction_instances[-1].append(ref.text)\n",
    "                                \n",
    "                                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                                    print(f\"\\t\\t+ Points for Interaction\")\n",
    "                            \n",
    "                            elif sent_num_unique_species > 2:\n",
    "                                curr_points[INTERACTION] += PIC[INTERACTION]\n",
    "                                interaction_instances[-1].append(ref.text)\n",
    "    \n",
    "                                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                                    print(f\"\\t\\t+ Points for Interaction\")\n",
    "\n",
    "                    # Species Points\n",
    "                    # I added this to fix something, but I can't remember\n",
    "                    # what it was supposed to fix. Shouldn't seen_in_sent handle this?\n",
    "                    # Did I forget to indent this chunk of code, and reinvent seen_in_sent?\n",
    "                    # Can't remember.\n",
    "                    # I remember now, the species may have been seen in the sentence, but if\n",
    "                    # it hasn't been awarded any points, it can still be used.\n",
    "                    if sent_seen_species[ref][\"points\"] <= 0:\n",
    "                        if curr_points[SPECIES] < MPC[SPECIES]:\n",
    "                            scale = self.valid_species_token(data)\n",
    "                            \n",
    "                            curr_points[SPECIES] += scale * PIC[SPECIES]\n",
    "                            sent_seen_species[ref][\"points\"] += scale * PIC[SPECIES]\n",
    "\n",
    "                            if scale:\n",
    "                                species_instances[-1].append(ref.text)\n",
    "                            \n",
    "                            if verbose and scale and VERBOSE_LEVEL >= 3:\n",
    "                                print(f\"\\t\\t+ Points for Species\")\n",
    "\n",
    "            \n",
    "            # Add Sentence Points to Total Points\n",
    "            for i in range(NUM_CATEGORIES):\n",
    "                is_banned = banned[i][sent_i]\n",
    "\n",
    "                if curr_points[i] <= 0 and not is_banned:\n",
    "                    num_zero_pt_sents[i] += 1\n",
    "                \n",
    "                if not is_banned:\n",
    "                    points[i] += max(0, min(curr_points[i], MPC[i]))\n",
    "\n",
    "        \n",
    "        # Trait & Trait Variation Points\n",
    "        points[TRAIT_VARIATION] = self.valid_trait_variation(verbose=False)\n",
    "        points[TRAIT] = self.valid_trait(verbose=False)\n",
    "        \n",
    "        # Bins\n",
    "        bins = []\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            bins.append([-math.inf, 0.33, 0.66, math.inf])\n",
    "        bins[TRAIT_VARIATION] = [-math.inf, 0.5, 1, math.inf]\n",
    "        \n",
    "        # Calculating Score            \n",
    "        NUM_SENTENCES = len(list(self.sp_doc.sents))\n",
    "        score = 0\n",
    "        binned_score = 0\n",
    "\n",
    "        for i in range(NUM_CATEGORIES):\n",
    "            if i not in [TRAIT, TRAIT_VARIATION]:\n",
    "                num_non_zero_pt_sents = NUM_SENTENCES - num_zero_pt_sents[i]\n",
    "                \n",
    "                banned_tax = 0\n",
    "                for b in banned[i]:\n",
    "                    if b:\n",
    "                        banned_tax += 1\n",
    "                \n",
    "                # print(f\"Banned Tax: {banned_tax}\")\n",
    "                lenient_num_sentences = max(num_non_zero_pt_sents, (1 - LEN[i]) * (NUM_SENTENCES) + banned_tax)\n",
    "    \n",
    "                # Calculating FTP\n",
    "                points[i] = points[i] / (MPC[i] * lenient_num_sentences)\n",
    "    \n",
    "                # Take the Inverse for Not-Topic\n",
    "                if i == NOT_TOPIC:\n",
    "                    points[i] = 1 - points[i]\n",
    "\n",
    "            # Bin Points\n",
    "            for b in range(len(bins[i]) - 2, -1, -1):\n",
    "                # print(bins[i][b], \"<=\", points[i], \"<=\", bins[i][b+1])\n",
    "\n",
    "                if bins[i][b] <= points[i] <= bins[i][b+1]:\n",
    "                    binned_points[i] = b + 1\n",
    "                    # binned_points[i] = bins[i][b]\n",
    "                    break\n",
    "\n",
    "            points[i] = max(0, min(points[i], 1))\n",
    "            binned_points[i] = max(0, min(binned_points[i], math.inf))\n",
    "            \n",
    "            score += max(0, min(points[i], 1)) * CW[i]\n",
    "            # Redundant, but we move\n",
    "            binned_score += max(0, min(binned_points[i], math.inf)) * CW[i]\n",
    "\n",
    "        # Enforcing 3 or More Species            \n",
    "        # if len(seen_species) < 3:\n",
    "        #     return 0, 0\n",
    "\n",
    "        # Removing, it's now on a scale from 0.0 to 3.0.\n",
    "        # I could just change the values, but eeeh.\n",
    "        # assert 0.0 <= score <= 1.0\n",
    "        # assert 0.0 <= binned_score <= 1.0\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Score, Points: {score}, {points}\")\n",
    "            print(f\"Binned Score, Binned Points: {binned_score}, {binned_points}\")\n",
    "    \n",
    "        return {\n",
    "            \"s_score\": 0 if len(seen_species) < 3 else score,\n",
    "            \"s_bin_score\": 0 if len(seen_species) < 3 else binned_score,\n",
    "            \"score\": score,\n",
    "            \"bin_score\": binned_score,\n",
    "            \"points\": {\n",
    "                \"trait\": points[TRAIT],\n",
    "                \"species\": points[SPECIES],\n",
    "                \"experiment\": points[EXPERIMENT],\n",
    "                \"interaction\": points[INTERACTION],\n",
    "                \"not_topic\": points[NOT_TOPIC],\n",
    "                \"trait_variation\": points[TRAIT_VARIATION],\n",
    "            },\n",
    "            \"binned_points\": {\n",
    "                \"trait\": binned_points[TRAIT],\n",
    "                \"species\": binned_points[SPECIES],\n",
    "                \"experiment\": binned_points[EXPERIMENT],\n",
    "                \"interaction\": binned_points[INTERACTION],\n",
    "                \"not_topic\": binned_points[NOT_TOPIC],\n",
    "                \"trait_variation\": binned_points[TRAIT_VARIATION],\n",
    "            },\n",
    "            \"interaction_instances\": interaction_instances,\n",
    "            \"species_instances\": species_instances,\n",
    "            \"causes\": self.cause.tokens,\n",
    "            \"changes\": self.change.tokens,\n",
    "            \"traits\": self.trait.tokens,\n",
    "            \"species\": self.species.tokens,\n",
    "            \"experiments\": self.experiment.tokens,\n",
    "            \"not_experiments\": self.not_experiment.tokens,\n",
    "            \"not_topics\": self.not_topic.tokens,\n",
    "            \"variability\": self.variability.tokens,\n",
    "            \"tests\": self.test.tokens,\n",
    "            \"seen_species\": seen_species\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
