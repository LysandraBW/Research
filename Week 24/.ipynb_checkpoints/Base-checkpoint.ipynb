{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0394b-0c5a-421a-aff4-5f0cd96568b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    # There is not a defined conversion method for these words.\n",
    "    # This is the default list of irregular nouns. It maps the\n",
    "    # the singular version to the plural version (SP).\n",
    "    IRREGULAR_NOUNS_SP = {\n",
    "        \"ox\": \"oxen\",\n",
    "        \"goose\": \"geese\",\n",
    "        \"mouse\": \"mice\",\n",
    "        \"bacterium\": \"bacteria\"\n",
    "    }\n",
    "\n",
    "    # This is the reversed version of the dictionary above, meaning \n",
    "    # that the plural version is mapped to the singular version \n",
    "    # (PS).\n",
    "    IRREGULAR_NOUNS_PS = {v: k for k, v in IRREGULAR_NOUNS_SP.items()}\n",
    "    \n",
    "    # The singular and plural versions of these words are the same. \n",
    "    # This is the default list of zero plural nouns.\n",
    "    ZERO_PLURAL_NOUNS = [\n",
    "        \"species\", \n",
    "        \"deer\", \n",
    "        \"fish\", \n",
    "        \"moose\", \n",
    "        \"sheep\", \n",
    "        \"swine\", \n",
    "        \"buffalo\", \n",
    "        \"trout\", \n",
    "        \"cattle\"\n",
    "    ]\n",
    "\n",
    "    # These pairs of characters define symbols that enclose other\n",
    "    # information in a text.\n",
    "    ENCLOSURES = {\n",
    "        \"(\": \")\",\n",
    "        \"[\": \"]\",\n",
    "        \"{\": \"}\"\n",
    "    }\n",
    "\n",
    "    LAX_ENCLOSURES = {\n",
    "        \"(\": \")\",\n",
    "        \"[\": \"]\",\n",
    "        \"{\": \"}\",\n",
    "        \"窶能": \"窶能"\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self, main, irregular_nouns_sp=IRREGULAR_NOUNS_SP, irregular_nouns_ps=IRREGULAR_NOUNS_PS, zero_plural_nouns=ZERO_PLURAL_NOUNS):\n",
    "        self.main = main\n",
    "        self.zero_plural_nouns = zero_plural_nouns\n",
    "        self.irregular_nouns_sp = irregular_nouns_sp\n",
    "        self.irregular_nouns_ps = irregular_nouns_ps\n",
    "        self.irregular_plural_nouns = list(self.irregular_nouns_sp.values())\n",
    "        self.irregular_singular_nouns = list(self.irregular_nouns_sp.keys())\n",
    "\n",
    "\n",
    "\n",
    "    def delete_extra_whitespace(self, string):\n",
    "        # Duplicate spaces, spaces before punctuation marks,\n",
    "        # and outside spaces are removed.\n",
    "        string = re.sub(r\"\\s+\", \" \", string)\n",
    "        string = re.sub(r\"\\s+([?.!,])\", r\"\\1\", string)\n",
    "        string = string.strip()\n",
    "        return string\n",
    "\n",
    "\n",
    "\n",
    "    def delete_outer_non_alnum(self, string):\n",
    "        while string:\n",
    "            start_len = len(string)\n",
    "            # Remove Leading Non-Alphanumeric Character\n",
    "            if string and not string[0].isalnum():\n",
    "                string = string[1:]\n",
    "            # Remove Trailing Non-Alphanumeric Character\n",
    "            if string and not string[-1].isalnum():\n",
    "                string = string[:-1]\n",
    "            # No Changes Made\n",
    "            if start_len == len(string):\n",
    "                break\n",
    "        return string\n",
    "\n",
    "\n",
    "\n",
    "    def get_parentheticals(self, text, enclosures=ENCLOSURES, flatten=False):\n",
    "        # The parenthetical would be the content inside of a pair\n",
    "        # of matching parentheses, brackets, or braces.\n",
    "        parentheticals = []\n",
    "        \n",
    "        # This contains the text that's not inside of any\n",
    "        # enclosure.\n",
    "        base_text = []\n",
    "        \n",
    "        # This is used for building groups, which often has a \n",
    "        # nested structure.\n",
    "        stacks = []\n",
    "        \n",
    "        # These are the pairs of characters that we recognize\n",
    "        # as defining the parenthetical.\n",
    "        openers = list(enclosures.keys())\n",
    "        closers = list(enclosures.values())\n",
    "        \n",
    "        # This contains the opening characters of the groups \n",
    "        # that are currently open (e.g. '(', '['). We use it \n",
    "        # so that we know whether to open or close a group.\n",
    "        opened = []\n",
    "        \n",
    "        for i, char in enumerate(text):\n",
    "            # Open Group\n",
    "            if char in openers:\n",
    "                stacks.append([])\n",
    "                opened.append(char)\n",
    "            # Close Group\n",
    "            elif opened and char == enclosures.get(opened[-1], \"\"):\n",
    "                parentheticals.append(stacks.pop())\n",
    "                opened.pop()\n",
    "            # Add to Group\n",
    "            elif opened:\n",
    "                stacks[-1].append(i)\n",
    "            # Add to Base Text\n",
    "            else:\n",
    "                base_text.append(i)\n",
    "        \n",
    "        # We close the remaining groups that have not\n",
    "        # been closed.\n",
    "        while stacks:\n",
    "            parentheticals.append(stacks.pop())\n",
    "            \n",
    "        # Cluster Groups' Indices\n",
    "        # A list in the lists of indices (where each list represents a group of text) could have \n",
    "        # an interruption (e.g. [0, 1, 2, 10 15]) because of a parenthetical. So, we cluster the\n",
    "        # indices in each list to make the output more useful (e.g. [(0, 3), (10, 16)]).\n",
    "        lists_of_indices = [*parentheticals, base_text]        \n",
    "        lists_of_clustered_indices = []\n",
    "\n",
    "        for list_of_indices in lists_of_indices:\n",
    "            if not list_of_indices:\n",
    "                continue\n",
    "\n",
    "            # We start off with a single cluster that is made up of the\n",
    "            # first index. If the next index follows the first index, \n",
    "            # we continue the cluster. If it doesn't, we create a new cluster.\n",
    "            clustered_indices = [[list_of_indices[0], list_of_indices[0] + 1]]\n",
    "            \n",
    "            for index in list_of_indices[1:]:\n",
    "                if clustered_indices[-1][1] == index:\n",
    "                    clustered_indices[-1][1] = index + 1\n",
    "                else:\n",
    "                    clustered_indices.append([index, index + 1])\n",
    "\n",
    "            # Add Clustered Indices\n",
    "            lists_of_clustered_indices.append(clustered_indices)\n",
    "            \n",
    "        if flatten:\n",
    "            flattened_clusters = []\n",
    "            # We are placing each cluster of indices into one list.\n",
    "            # This removes the context of the larger parenthetical,\n",
    "            # but the context may be cumbersome instead of useful.\n",
    "            for list_of_clustered_indices in lists_of_clustered_indices:\n",
    "                for clustered_indices in list_of_clustered_indices:\n",
    "                    flattened_clusters.append(clustered_indices)\n",
    "            lists_of_clustered_indices = flattened_clusters\n",
    "        \n",
    "        return lists_of_clustered_indices\n",
    "\n",
    "\n",
    "\n",
    "    def separate_span_by_parenthetical(self, span):\n",
    "        span_parentheticals = []\n",
    "        \n",
    "        # The clusters of the span represented with tuples of char indices\n",
    "        # (e.g. [(0, 1), (1, 5), (5, 10)]. This is a list of clustered\n",
    "        # indices (like above).\n",
    "        text_clusters = self.get_parentheticals(span.text, flatten=True)\n",
    "        \n",
    "        for cluster in text_clusters:\n",
    "            if span.text[cluster[0]:cluster[1]].isspace():\n",
    "                continue\n",
    "\n",
    "            l_char_index = span[0].idx + cluster[0]\n",
    "            r_char_index = span[0].idx + cluster[1] - 1\n",
    "\n",
    "            # Instead of having a tuple dictating the start and end of a cluster,\n",
    "            # we can use a span -- it's much simpler.\n",
    "            cluster_as_span = self.get_span_at_indices(l_char_index, r_char_index)\n",
    "            if not cluster_as_span:\n",
    "                continue\n",
    "            \n",
    "            span_parentheticals.append(cluster_as_span)\n",
    "\n",
    "        return span_parentheticals\n",
    "\n",
    "\n",
    "\n",
    "    def separate_spans_by_parenthetical(self, spans):\n",
    "        all_span_parentheticals = []\n",
    "        for span in spans:\n",
    "            all_span_parentheticals.extend(self.separate_span_by_parenthetical(span))\n",
    "        return all_span_parentheticals\n",
    "\n",
    "    \n",
    " \n",
    "    def singularize(self, string):\n",
    "        string = string.lower()\n",
    "        \n",
    "        # The string to singularize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        if not words:\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is a zero plural\n",
    "        # or a singular irregular noun, there's no changes\n",
    "        # to make. For example, \"red sheep\" and \"ox\" are \n",
    "        # already singular.\n",
    "        if (\n",
    "            words[-1] in self.zero_plural_nouns or \n",
    "            words[-1] in self.irregular_singular_nouns\n",
    "        ):\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is an irregular\n",
    "        # plural noun, we rely on a dictionary with the\n",
    "        # corresponding mapping.\n",
    "        if words[-1] in self.irregular_plural_nouns:\n",
    "            words[-1] = self.irregular_nouns_ps[words[-1]]\n",
    "            singulars = [self.delete_extra_whitespace(\" \".join(words))]\n",
    "            return singulars\n",
    "        \n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to uncertainty), we need to\n",
    "        # include all possible versions.\n",
    "        singulars = []\n",
    "        singular_endings = self.get_singular(words[-1])\n",
    "\n",
    "        if not singular_endings:\n",
    "            return [string]\n",
    "        \n",
    "        for singular_ending in singular_endings:\n",
    "            singular = self.delete_extra_whitespace(\" \".join([*words[:-1], singular_ending]))\n",
    "            singulars.append(singular)\n",
    "            \n",
    "        return singulars\n",
    "\n",
    "\n",
    "\n",
    "    def get_singular(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Replace -ies with -y\n",
    "        if re.fullmatch(r\".*ies$\", string):\n",
    "            versions.append(f'{string[:-3]}y')\n",
    "            return versions\n",
    "\n",
    "        # Replace -ves with -f and -fe\n",
    "        if re.fullmatch(r\".*ves$\", string):\n",
    "            versions.append(f'{string[:-3]}f')\n",
    "            versions.append(f'{string[:-3]}fe')\n",
    "            return versions\n",
    "\n",
    "        # Delete -es \n",
    "        if re.fullmatch(r\".*es$\", string):\n",
    "            versions.append(f'{string[:-2]}')\n",
    "            return versions\n",
    "\n",
    "        # Replace -i with -us\n",
    "        if re.fullmatch(r\".*i$\", string):\n",
    "            versions.append(f'{string[:-1]}us')\n",
    "            return versions\n",
    "\n",
    "        # Delete -s\n",
    "        if re.fullmatch(r\".*s$\", string):\n",
    "            versions.append(f'{string[:-1]}')\n",
    "            return versions\n",
    "\n",
    "        return versions\n",
    "\n",
    "\n",
    "    \n",
    "    def pluralize(self, string):\n",
    "        string = string.lower()\n",
    "        \n",
    "        # The string to pluralize should not have any\n",
    "        # non-alphanumeric characters at the end, or else\n",
    "        # the algorithm will not work.\n",
    "        words = re.split(r\" \", string)\n",
    "\n",
    "        if not words:\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is a zero plural\n",
    "        # or a plural irregular noun, there's no changes\n",
    "        # to make. For example, \"red sheep\" and \"oxen\" are \n",
    "        # already singular.\n",
    "        if (\n",
    "            words[-1] in self.zero_plural_nouns or \n",
    "            words[-1] in self.irregular_plural_nouns\n",
    "        ):\n",
    "            return [string]\n",
    "\n",
    "        # If the last word in the string is an irregular\n",
    "        # singular noun, we rely on a dictionary with the\n",
    "        # corresponding mapping.\n",
    "        if words[-1] in self.irregular_singular_nouns:\n",
    "            words[-1] = self.irregular_nouns_sp[words[-1]]\n",
    "            return [self.delete_extra_whitespace(\" \".join(words))]\n",
    "        \n",
    "        # We take the singular form of the last word and\n",
    "        # add it back in to the other words. As there could\n",
    "        # be multiple forms (due to error), we need to\n",
    "        # handle them all.\n",
    "        plurals = []\n",
    "        plural_endings = self.get_plural(words[-1])\n",
    "\n",
    "        if not plural_endings:\n",
    "            return [string]\n",
    "            \n",
    "        for plural_ending in plural_endings:\n",
    "            plural = self.delete_extra_whitespace(\" \".join([*words[:-1], plural_ending]))\n",
    "            plurals.append(plural)\n",
    "            \n",
    "        return plurals\n",
    "\n",
    "    \n",
    "  \n",
    "    def get_plural(self, string):\n",
    "        versions = []\n",
    "\n",
    "        # Words that end with -us often have\n",
    "        # two different plural versions: -es and -i.\n",
    "        # For example, the plural version of cactus \n",
    "        # can be cactuses or cacti.\n",
    "        if re.fullmatch(r\".*us$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            versions.append(f'{string[:-2]}i')\n",
    "            return versions\n",
    "\n",
    "        # The -es ending is added to the words below.\n",
    "        if re.fullmatch(r\".*([^l]s|sh|ch|x|z)$\", string):\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # Words that end with a consonant followed by 'y'\n",
    "        # are made plural by replacing the 'y' with -ies.\n",
    "        # For example, the plural version of canary is\n",
    "        # canaries.\n",
    "        if re.fullmatch(r\".*([^aeiou])(y)$\", string):\n",
    "            versions.append(f'{string[:-1]}ies')\n",
    "            return versions\n",
    "            \n",
    "        # The plural version of words ending with -f\n",
    "        # and -fe aren't clear. To be safe, I will add\n",
    "        # both versions.\n",
    "        if (re.fullmatch(r\".*(f)(e?)$\", string) and not re.fullmatch(r\".*ff$\", string)):\n",
    "            last_clean = re.sub(r\"(f)(e?)$\", \"\", string)\n",
    "            versions.append(f'{last_clean}fs')\n",
    "            versions.append(f'{last_clean}ves')\n",
    "            return versions\n",
    "\n",
    "        # People add -s or -es to words that end with 'o'.\n",
    "        # To be safe, both versions are added.\n",
    "        if re.fullmatch(r\".*([^aeiou])o$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "            versions.append(f'{string}es')\n",
    "            return versions\n",
    "\n",
    "        # If there's no -s at the end of the string and\n",
    "        # the other cases didn't run, we add an -s.\n",
    "        if re.fullmatch(r\".*[^s]$\", string):\n",
    "            versions.append(f'{string}s')\n",
    "        \n",
    "        return versions\n",
    "\n",
    "\n",
    " \n",
    "    def expand_unit(self, *, il_unit, ir_unit, il_boundary, ir_boundary, speech=[], literals=[], include=True, direction='BOTH', verbose=False):\n",
    "        UNIT = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if il_unit > ir_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} greater than ir_unit of {ir_unit}\")\n",
    "            return None\n",
    "        \n",
    "        if direction in ['BOTH', 'LEFT'] and il_boundary > il_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} less than il_boundary of {il_boundary}\")\n",
    "            return None\n",
    "        \n",
    "        if direction in ['BOTH', 'RIGHT'] and ir_boundary < ir_unit:\n",
    "            print(f\"Error: ir_unit of {ir_unit} greater than ir_boundary of {ir_boundary}\")\n",
    "            return None\n",
    "        \n",
    "        # Move Left\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            # The indices are inclusive, therefore, when \n",
    "            # the condition fails, il_unit will be equal\n",
    "            # to il_boundary.\n",
    "            while il_unit > il_boundary:\n",
    "                # We assume that the current token is allowed,\n",
    "                # and look to the token to the left.\n",
    "                l_token = self.main.sp_doc[il_unit-1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = l_token.pos_ in speech or l_token.lower_ in literals\n",
    "\n",
    "                # Case 1: include=False, in_set=True\n",
    "                # If we're not meant to include the defined tokens, and the\n",
    "                # current token is in that set, we stop expanding.\n",
    "                # Case 2: include=True, in_set=False\n",
    "                # If we're meant to include the defined tokens, and the current\n",
    "                # token is not in that set, we stop expanding.\n",
    "                # Case 3: include=in_set\n",
    "                # If we're meant to include the defined tokens, and the current\n",
    "                # token is in that set, we continue expanding. If we're not meant\n",
    "                # to include the defined tokens, and the current token is not\n",
    "                # in that set, we continue expanding.\n",
    "                if include ^ in_set:\n",
    "                    break\n",
    "                \n",
    "                # Else, the left token is valid, and\n",
    "                # we continue to expand.\n",
    "                il_unit -= 1\n",
    "\n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            # Likewise, when the condition fails,\n",
    "            # ir_unit will be equal to the ir_boundary.\n",
    "            # The ir_boundary is also inclusive.\n",
    "            while ir_unit < ir_boundary:\n",
    "                # Assuming that the current token is valid,\n",
    "                # we look to the right to see if we can\n",
    "                # expand.\n",
    "                r_token = self.main.sp_doc[ir_unit+1]\n",
    "\n",
    "                # If the token is invalid, we stop expanding.\n",
    "                in_set = r_token.pos_ in speech or r_token.lower_ in literals\n",
    "                if include ^ in_set:\n",
    "                    break\n",
    "\n",
    "                # Else, the token is valid and\n",
    "                # we continue.\n",
    "                ir_unit += 1\n",
    "\n",
    "        assert il_unit >= il_boundary and ir_unit <= ir_boundary\n",
    "        \n",
    "        expanded_unit = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Expanded Unit of '{UNIT}': {expanded_unit}\")\n",
    "        \n",
    "        return expanded_unit\n",
    "\n",
    "\n",
    "    \n",
    "    def contract_unit(self, *, il_unit, ir_unit, speech=[], literals=[], include=True, direction='BOTH', verbose=False):\n",
    "        UNIT = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if il_unit > ir_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} greater than ir_unit of {ir_unit}\")\n",
    "            return None\n",
    "        \n",
    "        # Move Right\n",
    "        if direction in ['BOTH', 'LEFT']:\n",
    "            while il_unit < ir_unit:\n",
    "                # We must check if the current token is not allowed. If it's\n",
    "                # not allowed, we contract (remove).\n",
    "                token = self.main.sp_doc[il_unit]\n",
    "\n",
    "                # include = True means that we want the tokens that match\n",
    "                # the speech and/or literals in the contracted unit.\n",
    "                \n",
    "                # include = False means that we don't want the tokens that\n",
    "                # match the speech and/or literals in the contracted unit.\n",
    "                \n",
    "                # Case 1: include = True, in_set = True\n",
    "                # We have a token that's meant to be included in the set.\n",
    "                # However, we're contracting, which means we would end up\n",
    "                # removing the token if we continue. Therefore, we break.\n",
    "                \n",
    "                # Case 2: include = False, in_set = False\n",
    "                # We have a token that's not in the set which defines the\n",
    "                # tokens that aren't meant to be included. Therefore, we \n",
    "                # have a token that is meant to be included. If we continue,\n",
    "                # we would end up removing this token. Therefore, we break.\n",
    "                \n",
    "                # Default:\n",
    "                # If we have a token that's in the set (in_set=True) of\n",
    "                # tokens we're not supposed to include in the contracted \n",
    "                # unit (include=False), we need to remove it. Likewise, if\n",
    "                # we have a token that's not in the set (in_set=False) of\n",
    "                # tokens to include in the contracted unit (include=True),\n",
    "                # we need to remove it.\n",
    "                \n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include == in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid, thus we continue.\n",
    "                il_unit += 1\n",
    "\n",
    "        # Move Left      \n",
    "        if direction in ['BOTH', 'RIGHT']:\n",
    "            while ir_unit > il_unit:\n",
    "                token = self.main.sp_doc[ir_unit]\n",
    "\n",
    "                # The token is invalid and we\n",
    "                # stop contracting.\n",
    "                in_set = token.pos_ in speech or token.lower_ in literals\n",
    "                if include == in_set:\n",
    "                    break\n",
    "\n",
    "                # The token is valid and we continue.\n",
    "                ir_unit -= 1\n",
    "\n",
    "        assert il_unit <= ir_unit\n",
    "        \n",
    "        contracted_unit = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Contracted Unit of '{UNIT}': {contracted_unit}\")\n",
    "        \n",
    "        return contracted_unit\n",
    "\n",
    "\n",
    "    \n",
    "    def find_unit_context(self, *, il_unit, ir_unit, il_boundary, ir_boundary, speech=[\"ADJ\", \"NOUN\", \"ADP\", \"ADV\", \"PART\", \"PROPN\", \"VERB\", \"PRON\", \"DET\", \"AUX\", \"PART\", \"SCONJ\"], literals=[], include=True, enclosures=LAX_ENCLOSURES, comma_encloses=False, verbose=False):\n",
    "        UNIT = self.main.sp_doc[il_unit:ir_unit+1]\n",
    "        \n",
    "        if il_unit > ir_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} greater than ir_unit of {ir_unit}\")\n",
    "            return None\n",
    "        \n",
    "        if il_boundary > il_unit:\n",
    "            print(f\"Error: il_unit of {il_unit} less than il_boundary of {il_boundary}\")\n",
    "            return None\n",
    "        \n",
    "        if ir_boundary < ir_unit:\n",
    "            print(f\"Error: ir_unit of {ir_unit} greater than ir_boundary of {ir_boundary}\")\n",
    "            return None\n",
    "        \n",
    "        # Caveat: Parentheticals\n",
    "        # The context of a unit inside a set of enclosures should\n",
    "        # not go farther than the boundaries of those enclosures.\n",
    "        # However, we need to manually determine whether the unit\n",
    "        # is in parentheses (or any set of the matching symbols\n",
    "        # below).\n",
    "        openers = list(enclosures.keys())\n",
    "        closers = list(enclosures.values())\n",
    "        enclosing_chars = [*closers, *openers]\n",
    "\n",
    "        # Look for Group Punctuation on the Left\n",
    "        i = il_unit\n",
    "        opener = None\n",
    "        while i > il_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.lower_ in enclosing_chars and token.lower_ != \",\":\n",
    "                opener = token\n",
    "                break\n",
    "            i -= 1\n",
    "\n",
    "        # Look for Group Punctuation on the Right\n",
    "        i = ir_unit\n",
    "        closer = None\n",
    "        while i < ir_boundary:\n",
    "            token = self.main.sp_doc[i]\n",
    "            if token.lower_ in enclosing_chars and token.lower_ != \",\":\n",
    "                closer = token\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        # If there's a group punctuation on the left\n",
    "        # and right, and they match each other (e.g. '(' and ')'),\n",
    "        # we return the text between the punctuations.\n",
    "        parenthetical = opener and closer and enclosures.get(opener.lower_) == closer.text\n",
    "        if parenthetical:\n",
    "            context = [t for t in self.main.sp_doc[opener.i:closer.i+1]]\n",
    "            \n",
    "            if verbose and VERBOSE_LEVEL >= 1:\n",
    "                print(f\"Parenthetical - Unit Context of '{UNIT}': {context}\")\n",
    "            \n",
    "            return context\n",
    "\n",
    "        # We can also check whether the unit it enclosed\n",
    "        # in a comma or two, only if a comma can enclose.\n",
    "        if comma_encloses:\n",
    "            i = il_unit\n",
    "            i_token = self.main.sp_doc[i]\n",
    "            while i > il_boundary:\n",
    "                i_token = self.main.sp_doc[i]\n",
    "                if i_token.lower_ in [\",\", \";\", \"窶能"]:\n",
    "                    break\n",
    "                i -= 1\n",
    "\n",
    "            j = ir_unit\n",
    "            j_token = self.main.sp_doc[j]\n",
    "            while j < ir_boundary:\n",
    "                j_token = self.main.sp_doc[j]\n",
    "                if j_token.lower_ in [\",\", \";\", \"窶能"]:\n",
    "                    break\n",
    "                j += 1\n",
    "\n",
    "            if (i_token and i_token.lower_ == \",\") or (j_token and j_token.lower_ == \",\"):\n",
    "                context = [t for t in self.main.sp_doc[i:j+1]]\n",
    "            \n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Comma - Unit Context of '{UNIT}': {context}\")\n",
    "                    \n",
    "                return context\n",
    "            \n",
    "        # As the unit is not a parenthetical, we will expand\n",
    "        # outwards until we run into a stopping token. The exclude\n",
    "        # list contains tokens that should be excluded from the\n",
    "        # context. Currently, it will contain any parentheticals\n",
    "        # that we run into.\n",
    "        exclude = []\n",
    "\n",
    "        # We can modify the enclosures after handling the parenthetical\n",
    "        # situation to make the code easier.\n",
    "        if comma_encloses:\n",
    "            enclosures[\",\"] : \",\"\n",
    "        \n",
    "        # Expand Left\n",
    "        while il_unit > il_boundary:\n",
    "            # Assuming that the current token is fine,\n",
    "            # we look to the left.\n",
    "            l_token = self.main.sp_doc[il_unit-1]\n",
    "\n",
    "            if l_token.lower_ not in closers:\n",
    "                in_set = l_token.pos_ in speech or l_token.lower_ in literals\n",
    "                if in_set ^ include:\n",
    "                    break\n",
    "                il_unit -= 1\n",
    "            # If it's a closing enclosure (e.g. ')', ']'),\n",
    "            # we need to skip over whatever is contained in\n",
    "            # that punctuation.\n",
    "            else:\n",
    "                i = il_unit - 1\n",
    "                \n",
    "                token = self.main.sp_doc[i]\n",
    "                exclude.append(token)\n",
    "\n",
    "                # We continue until we reach the boundary or\n",
    "                # we find the matching opening character.\n",
    "                closed = []\n",
    "                \n",
    "                while i > il_boundary:\n",
    "                    token = self.main.sp_doc[i]\n",
    "                    # Found Closer\n",
    "                    if token.lower_ in closers:\n",
    "                        exclude.append(token)\n",
    "                        closed.append(token.lower_)\n",
    "                    # Currently Closed\n",
    "                    elif closed:\n",
    "                        exclude.append(token)\n",
    "                        # Found Opener\n",
    "                        if enclosures.get(token.lower_) == closed[-1]:\n",
    "                            closed.pop()\n",
    "                    else:\n",
    "                        break\n",
    "                    i -= 1\n",
    "                \n",
    "                il_unit = i\n",
    "\n",
    "        # Expand Right\n",
    "        while ir_unit < ir_boundary:\n",
    "            # We're checking the token to the right\n",
    "            # to see if we can expand or not.\n",
    "            r_token = self.main.sp_doc[ir_unit+1]\n",
    "\n",
    "            if r_token.lower_ not in openers:\n",
    "                in_set = r_token.pos_ in speech or r_token.lower_ in literals\n",
    "                if in_set ^ include:\n",
    "                    break\n",
    "                ir_unit += 1\n",
    "            # If the token to the right is an opener (e.g. '(', '['), we must skip\n",
    "            # it, the parenthetical inside, and the closer.\n",
    "            else:\n",
    "                i = ir_unit + 1\n",
    "                \n",
    "                token = self.main.sp_doc[i]\n",
    "                exclude.append(token)\n",
    "\n",
    "                # We continue until we reach the boundary or\n",
    "                # we find all the closers for the openers.\n",
    "                opened = []\n",
    "                \n",
    "                while i < ir_boundary:\n",
    "                    token = self.main.sp_doc[i]\n",
    "                    # Found Opener\n",
    "                    if token.lower_ in openers:\n",
    "                        exclude.append(token)\n",
    "                        opened.append(token.lower_)\n",
    "                    # Currently Opened\n",
    "                    elif opened:\n",
    "                        exclude.append(token)\n",
    "                        # Found Closer\n",
    "                        if token.lower_ == enclosures.get(opened[-1]):\n",
    "                            opened.pop()\n",
    "                    else:\n",
    "                        break\n",
    "                    i += 1\n",
    "                \n",
    "                ir_unit = i\n",
    "        \n",
    "        # We remove the excluded tokens and return the context.\n",
    "        context = [t for t in self.main.sp_doc[il_unit:ir_unit+1] if t not in exclude]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Unit Context of '{UNIT}': {context}\")\n",
    "        \n",
    "        return context\n",
    "\n",
    "\n",
    "    \n",
    "    def get_span_at_indices(self, l_index, r_index):\n",
    "        text = self.main.sp_doc.text.lower()\n",
    "\n",
    "        while text[l_index].isspace():\n",
    "            l_index += 1\n",
    "\n",
    "        while text[r_index].isspace():\n",
    "            r_index -= 1\n",
    "\n",
    "        if l_index > r_index:\n",
    "            print(f\"Error: l_index of {l_index} greater than r_index of {r_index}\")\n",
    "            return None\n",
    "            \n",
    "        l_token_i = self.main.token_at_char(l_index).i\n",
    "        r_token_i = self.main.token_at_char(r_index).i\n",
    "        \n",
    "        return self.main.sp_doc[l_token_i:r_token_i+1]\n",
    "\n",
    "\n",
    "    \n",
    "    def get_base_nouns(self, span, return_tokens=False, immediate_stop=False):\n",
    "        ending_nouns = []\n",
    "        \n",
    "        reversed_span = [t for t in span]\n",
    "        reversed_span.reverse()\n",
    "        \n",
    "        for token in reversed_span:\n",
    "            if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                ending_nouns.append(token if return_tokens else self.main.sp_doc[token.i:token.i+1])\n",
    "                if immediate_stop:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return ending_nouns\n",
    "\n",
    "\n",
    "\n",
    "    def flatten(self, arr):\n",
    "        flat_arr = []\n",
    "\n",
    "        if not isinstance(arr, list):\n",
    "            return [arr]\n",
    "\n",
    "        for element in arr:\n",
    "            flat_arr.extend(self.flatten(element))\n",
    "\n",
    "        return flat_arr\n",
    "\n",
    "\n",
    "    def is_same_text(self, sp_a, sp_b):\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        sp_a_text = sp_a.text.lower()\n",
    "\n",
    "        if sp_a_text == sp_b_text:\n",
    "            return True\n",
    "            \n",
    "        sp_a_singular_texts = [sp_a_text] if sp_a[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(sp_a_text)\n",
    "        sp_b_singular_texts = [sp_b_text] if sp_b[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(sp_b_text)\n",
    "\n",
    "        if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def has_same_base_nouns(self, sp_a, sp_b):\n",
    "        sp_b_text = sp_b.text.lower()\n",
    "        sp_b_0_text = sp_b[0].lower_\n",
    "        sp_b_0_is_noun = sp_b[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "        \n",
    "        sp_b_nouns = []\n",
    "        sp_b_num_adjectives = 0\n",
    "        \n",
    "        for token in sp_b:\n",
    "            if not sp_b_nouns and token.pos_ == \"ADJ\":\n",
    "                sp_b_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_b_nouns.append(token)\n",
    "\n",
    "        if not sp_b_nouns:\n",
    "            return False\n",
    "\n",
    "        sp_b_nouns_text = [noun.lower_ for noun in sp_b_nouns]\n",
    "        sp_b_singular_texts = [\" \".join(sp_b_nouns_text)] if sp_b_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(\" \".join(sp_b_nouns_text))\n",
    "\n",
    "        sp_a_text = sp_a.text.lower()\n",
    "        sp_a_0_text = sp_a[0].lower_\n",
    "        sp_a_0_is_noun = sp_a[0].pos_ in [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "        # Case Example: 'Hyla' v. 'Hyla tadpoles'\n",
    "        if sp_a_0_text == sp_b_0_text and (sp_a_0_is_noun or sp_b_0_is_noun):\n",
    "            if sp_a_text in sp_b_text or sp_b_text in sp_a_text:\n",
    "                return True\n",
    "        \n",
    "        # Case Example: 'dogs' v. 'red dogs'\n",
    "        sp_a_nouns = []\n",
    "        sp_a_num_adjectives = 0\n",
    "        for token in sp_a:\n",
    "            if not sp_a_nouns and token.pos_ == \"ADJ\":\n",
    "                sp_a_num_adjectives += 1\n",
    "            elif token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "                sp_a_nouns.append(token)\n",
    "        \n",
    "        if not sp_a_nouns:\n",
    "            return False\n",
    "        \n",
    "        sp_a_nouns_text = [noun.lower_ for noun in sp_a_nouns]\n",
    "        \n",
    "        if sp_a_nouns and sp_b_nouns and (\n",
    "            (sp_a_num_adjectives == 1 and sp_b_num_adjectives == 0) or \n",
    "            (sp_b_num_adjectives == 1 and sp_a_num_adjectives == 0)\n",
    "        ):\n",
    "            sp_a_singular_texts = [\" \".join(sp_a_nouns_text)] if sp_a_nouns[-1].tag_ in [\"NN\", \"NNP\"] else self.main.singularize(\" \".join(sp_a_nouns_text))\n",
    "            if set(sp_a_singular_texts).intersection(sp_b_singular_texts):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def distinct_bounds(self, bounds, larger=True):\n",
    "        dounds = []\n",
    "    \n",
    "        for bound in bounds:\n",
    "            overlap = False\n",
    "            \n",
    "            for i, dound in enumerate(dounds):\n",
    "                surround = bound[0] <= dound[0] <= bound[1] and bound[0] <= dound[1] <= bound[1]\n",
    "                contains = dound[0] <= bound[0] <= dound[1] and dound[0] <= bound[1] <= dound[1]\n",
    "                overlaps = surround or contains\n",
    "                \n",
    "                if (surround and larger) or (contains and not larger):\n",
    "                    dounds[i] = bound\n",
    "                    \n",
    "            if not overlaps:\n",
    "                dounds.append(bound)\n",
    "    \n",
    "        return list(set(dounds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
