{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd0eda-b0cf-4305-aba8-6a48fc703cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import spacy\n",
    "import textacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from taxonerd import TaxoNERD\n",
    "from fastcoref import spacy_component\n",
    "from spacy.matcher import Matcher, DependencyMatcher, PhraseMatcher\n",
    "%run \"./Main.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1d90de-9571-4c05-944a-1dfe34162dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Week 16/Datasets/Baseline-1.csv\")\n",
    "\n",
    "i = 0\n",
    "title = df.loc[i, \"Title\"]\n",
    "abstract = df.loc[i, \"Abstract\"]\n",
    "\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Abstract: {abstract}\")\n",
    "\n",
    "main = Main()\n",
    "main.update_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2000e-b79d-4168-a60f-fcb3ab06c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_token(doc, token, sent_i):\n",
    "    ext = []\n",
    "    \n",
    "    if token.pos_ == \"PRON\" and token in main.coref_map:\n",
    "        ext = [*main.coref_map[token]]\n",
    "    else:\n",
    "        ext = [token]\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    size = len(ext)\n",
    "    while i < size:\n",
    "        ext_token = ext[i]\n",
    "        for ent_pos, ent in main.parts.reg[sent_i].items():\n",
    "            if ent_pos[0] <= ext_token.i <= ent_pos[1] and ent.label in [Entity.LIST]:\n",
    "                ext = [*doc[ent_pos[0]:ent_pos[1]+1]]\n",
    "                break\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "    i = 0\n",
    "    size = len(ext)\n",
    "    while i < size:\n",
    "        ext_token = ext[i]\n",
    "        if ext_token in main.noun_chunk_map:\n",
    "            ext.extend([*main.noun_chunk_map[ext_token]])\n",
    "\n",
    "        if ext_token in main.ent_map:\n",
    "            ext.extend([*main.ent_map[ext_token]])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    ext = list(set(ext))\n",
    "    \n",
    "    return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e89ce-93c9-488f-ae68-d9b2fe3f00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "        self.neighbors = []\n",
    "\n",
    "    def __str__(self):\n",
    "        ret = \"\"\n",
    "        for neighbor in self.neighbors:\n",
    "            ret += f\"{self.tokens}-->{neighbor[0].tokens}-->{neighbor[1].tokens}\"\n",
    "            if neighbor != self.neighbors[-1]:\n",
    "                ret += \"\\n\"\n",
    "            \n",
    "        return ret\n",
    "\n",
    "# def graph_part(doc, l_i, r_i):\n",
    "#     rel_tokens = doc[l_i:r_i+1]\n",
    "#     action_tokens = [*main.cause.tokens, *main.change.tokens]\n",
    "\n",
    "#     rel_action_tokens = set(rel_tokens).intersection(action_tokens)\n",
    "#     rel_action_tokens = list(rel_action_tokens)\n",
    "    \n",
    "#     if not rel_action_tokens:\n",
    "#         return None\n",
    "\n",
    "#     rel_action_tokens = sorted(rel_action_tokens, key=lambda t: t.i)\n",
    "    \n",
    "#     partition_i = rel_action_tokens[-1].i\n",
    "\n",
    "#     sub_node = Node()\n",
    "#     sub_node.tokens = [*list(doc[l_i:partition_i+1])]\n",
    "\n",
    "#     obj_node = Node()\n",
    "#     obj_node.tokens = [*list(doc[partition_i+1:r_i+1])]\n",
    "\n",
    "#     sub_node.neighbors.append((Node(), obj_node))\n",
    "    \n",
    "#     return sub_node\n",
    "\n",
    "def graph_ent(doc, entities):\n",
    "    i = 0\n",
    "    while i < len(entities):\n",
    "        entity = entities[i]\n",
    "        entity_tokens = [doc[_] for _ in range(entity.l, entity.r+1)]\n",
    "\n",
    "        graph_bar(doc, entity_tokens)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "def graph_bar(doc, tokens):\n",
    "    # print()\n",
    "    # print()\n",
    "    # print(tokens)\n",
    "    verbs = [token for token in tokens if token.pos_ == \"VERB\"]\n",
    "    if not verbs:\n",
    "        # print(\"No Verbs\")\n",
    "        return\n",
    "\n",
    "    verb = verbs[0]\n",
    "    \n",
    "    l = tokens[0].i\n",
    "    r = tokens.index(verb) + 1\n",
    "    \n",
    "    while r < len(tokens) and tokens[r].pos_ not in [\"PROPN\", \"NOUN\", \"PRON\"]:\n",
    "        r += 1\n",
    "\n",
    "    if r <= 0 or r >= len(tokens):\n",
    "        return\n",
    "    \n",
    "    r = tokens[r].i\n",
    "\n",
    "    swap = (verb.nbor(-1) and verb.nbor(-1).pos_ == \"AUX\") or (verb.nbor(1) and verb.nbor(1).lower_ == \"by\")\n",
    "\n",
    "    verb_node = Node()\n",
    "    verb_node.tokens = [verb]\n",
    "    \n",
    "    a_node = Node()\n",
    "    a_node.tokens = [doc[_] for _ in range(verb.i + 1, r + 1)]\n",
    "    if not a_node.tokens:\n",
    "        return\n",
    "\n",
    "    b_node = Node()\n",
    "    b_node.tokens = [doc[_] for _ in range(l, verb.i)]\n",
    "    if not b_node.tokens:\n",
    "        return\n",
    "\n",
    "    if not swap:\n",
    "        sub_node = b_node \n",
    "        obj_node = a_node\n",
    "    else:\n",
    "        sub_node = a_node \n",
    "        obj_node = b_node\n",
    "\n",
    "    # TRANSFER\n",
    "    sub_transfer_tokens = []\n",
    "    for token in sub_node.tokens:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            sub_transfer_tokens.append(token)\n",
    "    sub_node.tokens = [token for token in sub_node.tokens if token not in sub_transfer_tokens]\n",
    "    obj_node.tokens.extend(sub_transfer_tokens)\n",
    "\n",
    "    # TRANSFER\n",
    "    obj_transfer_tokens = []\n",
    "    for token in obj_node.tokens:\n",
    "        if token in main.cause.tokens or token in main.change.tokens:\n",
    "            obj_transfer_tokens.append(token)\n",
    "    obj_node.tokens = [token for token in obj_node.tokens if token not in obj_transfer_tokens]\n",
    "    verb_node.tokens.extend(obj_transfer_tokens)\n",
    "    \n",
    "    sub_node.neighbors.append((verb_node, obj_node))\n",
    "    print(sub_node)\n",
    "    \n",
    "def graph(doc):\n",
    "    triples = list(textacy.extract.subject_verb_object_triples(doc))\n",
    "\n",
    "    sents = list(doc.sents)\n",
    "    sents_triples = {sent.start: [] for sent in sents}\n",
    "    \n",
    "    for triple in triples:\n",
    "        sents_triples[triple.verb[0].sent.start].append(triple)\n",
    "\n",
    "    for sent_i, sent in enumerate(sents):\n",
    "        tokens = [token for token in sent]\n",
    "\n",
    "        # Subject-Verb-Object Triples\n",
    "        for triple in sents_triples[sent.start]:\n",
    "            if triple.subject[0] not in tokens:\n",
    "                continue\n",
    "\n",
    "            if triple.object[-1] not in tokens:\n",
    "                continue\n",
    "\n",
    "            cont_loop = False\n",
    "            for verb in triple.verb:\n",
    "                if verb.lemma_.lower() in [\"show\", \"showed\"]:\n",
    "                    cont_loop = True\n",
    "                    break\n",
    "            if cont_loop:\n",
    "                continue\n",
    "            \n",
    "            obj_node = Node()\n",
    "            obj_node.tokens = [doc[i] for i in range(triple.verb[-1].i+1, triple.object[-1].i+1)]\n",
    "            obj_node_i = tokens.index(obj_node.tokens[-1])\n",
    "            \n",
    "            obj_node_tokens_ext = flatten([extend_token(doc, token, sent_i) for token in obj_node.tokens])\n",
    "            obj_node_tokens_ext = list(set(obj_node_tokens_ext))\n",
    "            obj_node_tokens_ext = sorted(obj_node_tokens_ext, key=lambda t: t.i)\n",
    "            obj_node.tokens = obj_node_tokens_ext\n",
    "            # print(f\"{obj_node.tokens} Extended: {obj_node_tokens_ext}\")\n",
    "\n",
    "            graph_bar(doc, obj_node.tokens)\n",
    "            \n",
    "            verb_node = Node()\n",
    "            verb_node.tokens = [*triple.verb]\n",
    "\n",
    "            sub_node = Node()\n",
    "            sub_node.tokens = [doc[i] for i in range(triple.subject[0].i, triple.verb[0].i)]\n",
    "            sub_node_i = tokens.index(sub_node.tokens[0])\n",
    "            \n",
    "            sub_node_tokens_ext = flatten([extend_token(doc, token, sent_i) for token in sub_node.tokens])\n",
    "            sub_node_tokens_ext = list(set(sub_node_tokens_ext))\n",
    "            sub_node_tokens_ext = sorted(sub_node_tokens_ext, key=lambda t: t.i)\n",
    "            sub_node.tokens = sub_node_tokens_ext\n",
    "            # print(f\"{sub_node.tokens} Extended: {sub_node_tokens_ext}\")\n",
    "            sub_node.neighbors.append((verb_node, obj_node))\n",
    "\n",
    "            graph_bar(doc, sub_node.tokens)\n",
    "            \n",
    "            # Update Available Tokens\n",
    "            for token in sub_node.tokens:\n",
    "                if token in tokens:\n",
    "                    sub_node_i = tokens.index(token)\n",
    "\n",
    "            for token in reversed(obj_node.tokens):\n",
    "                if token in tokens:\n",
    "                    obj_node_i = tokens.index(token)\n",
    "\n",
    "            used_tokens_i = [tokens.index(token) for token in [*sub_node.tokens, *obj_node.tokens] if token in tokens]\n",
    "            used_tokens_i = sorted(used_tokens_i)\n",
    "            tokens = tokens[:used_tokens_i[0]] + tokens[used_tokens_i[-1]+1:]\n",
    "\n",
    "            print(sub_node)\n",
    "            # print(tokens)\n",
    "\n",
    "        # Parts\n",
    "        entities = list(main.parts.reg[sent_i].values())\n",
    "        graph_ent(doc, entities)\n",
    "\n",
    "graph(main.sp_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2264625a-32dd-468a-8ff9-ae332a8bdbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main.sp_doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee08a1-04b8-4cea-856b-a0a7c10fa2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "title = df.loc[i, \"Title\"]\n",
    "txt = df.loc[i, \"Abstract\"]\n",
    "doc = main.sp_nlp(txt)\n",
    "\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Text: {txt}\\n\")\n",
    "\n",
    "subs = []\n",
    "objs = []\n",
    "verbs = []\n",
    "\n",
    "triples = textacy.extract.subject_verb_object_triples(doc)\n",
    "for triple in triples:\n",
    "    print(triple)\n",
    "    subs.extend(triple.subject)\n",
    "    objs.extend(triple.object)\n",
    "    verbs.extend(triple.verb)\n",
    "\n",
    "class Colors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "for token in doc:\n",
    "    color = Colors.ENDC\n",
    "    if token in subs:\n",
    "        color = Colors.OKBLUE\n",
    "    if token in objs:\n",
    "        color = Colors.WARNING\n",
    "    if token in verbs:\n",
    "        color = Colors.FAIL\n",
    "    \n",
    "    # Start of Sentence\n",
    "    if token.sent.start == token.i:\n",
    "        print(f\"{color}{token.text}\", end=f\"{Colors.ENDC}\")\n",
    "    # End of Sentence\n",
    "    elif token.sent.end == token.i + 1:\n",
    "        print(f\"{color}{token.text} \", end=f\"{Colors.ENDC}\")\n",
    "    elif token.pos_ in [\"PUNCT\", \"SYM\"]:\n",
    "        print(f\"{color}{token.text}\", end=f\"{Colors.ENDC}\")\n",
    "    # In Sentence\n",
    "    else:\n",
    "        print(f\"{color} {token.text}\", end=f\"{Colors.ENDC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b3443d-bd4a-4bab-90cb-57b10cc053d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar(i):\n",
    "    ret = []\n",
    "    for num in range(0, i):\n",
    "        ret.append(num)\n",
    "    return ret\n",
    "\n",
    "bar(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65348b88-8869-4765-b606-9fa364f274b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[bar(i) for i in range(1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d968984-1c6a-4294-9861-c045320c10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten([bar(i) for i in range(1, 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b5f6a2-c985-4116-b836-b43625dd1f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
