{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0bd8a-5d3a-4417-ad6b-1fc63e7dd50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import spacy\n",
    "import textacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from taxonerd import TaxoNERD\n",
    "from fastcoref import spacy_component\n",
    "from spacy.matcher import Matcher, DependencyMatcher, PhraseMatcher\n",
    "%run \"./Main.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "955b168a-290b-4e8a-802c-933b67850019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colors:\n",
    "    HEADER = '\\033[95m'\n",
    "    RED = '\\033[91m'\n",
    "    BLUE = '\\033[94m'\n",
    "    CYAN = '\\033[96m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa69741-6e0c-4719-ab00-126243626a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Week 16/Datasets/Baseline-1.csv\")\n",
    "\n",
    "i = 0\n",
    "title = df.loc[i, \"Title\"]\n",
    "abstract = df.loc[i, \"Abstract\"]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(abstract)\n",
    "\n",
    "# Title\n",
    "print(f\"{Colors.BOLD}{title}{Colors.ENDC}\")\n",
    "\n",
    "# Abstract\n",
    "verbs = []\n",
    "objects = []\n",
    "subjects = []\n",
    "\n",
    "for triple in textacy.extract.subject_verb_object_triples(doc):\n",
    "    verbs.extend(triple.verb)\n",
    "    objects.extend(triple.object)\n",
    "    subjects.extend(triple.subject)\n",
    "    \n",
    "for token in doc:\n",
    "    color = Colors.ENDC\n",
    "    if token in objects:\n",
    "        color = Colors.YELLOW\n",
    "    if token in subjects:\n",
    "        color = Colors.BLUE\n",
    "    if token in verbs:\n",
    "        color = Colors.RED\n",
    "\n",
    "    if token.sent.end == token.i + 1:\n",
    "        print(f\"{color}{token.text} \", end=f\"{Colors.ENDC}\")\n",
    "    elif token.nbor() and token.nbor().text in [\".\",\"?\",\"!\",\";\", \")\", \",\", \"]\"]:\n",
    "        print(f\"{color}{token.text}\", end=f\"{Colors.ENDC}\")\n",
    "    elif token.sent.start == token.i:\n",
    "        print(f\"{color}{token.text} \", end=f\"{Colors.ENDC}\")\n",
    "    elif token.text not in [\"(\", \"[\"]:\n",
    "        print(f\"{color}{token.text} \", end=f\"{Colors.ENDC}\")\n",
    "    else:\n",
    "        print(f\"{color}{token.text}\", end=f\"{Colors.ENDC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f711463d-b61a-4610-92d7-9274c0f113de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\thinc\\shims\\pytorch.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n",
      "08/27/2025 16:18:11 - INFO - \t missing_keys: []\n",
      "08/27/2025 16:18:11 - INFO - \t unexpected_keys: []\n",
      "08/27/2025 16:18:11 - INFO - \t mismatched_keys: []\n",
      "08/27/2025 16:18:11 - INFO - \t error_msgs: []\n",
      "08/27/2025 16:18:11 - INFO - \t Model Parameters: 590.0M, Transformer: 434.6M, Coref head: 155.4M\n",
      "08/27/2025 16:18:53 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  4.33 examples/s]\n",
      "08/27/2025 16:19:02 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:16<00:00, 16.95s/it]\n"
     ]
    }
   ],
   "source": [
    "main = Main()\n",
    "main.update_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de3c5a66-4483-4022-8d0c-266983c75abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, main, is_action=False):\n",
    "        self.main = main\n",
    "        self.tokens = []\n",
    "        self.expanded_tokens = []\n",
    "        self.is_action = is_action\n",
    "\n",
    "    def traits(self):\n",
    "        tokens = set([*self.tokens, *self.expanded_tokens])\n",
    "        tokens = tokens & set(self.main.trait.tokens)\n",
    "        return tokens\n",
    "\n",
    "    def species(self):\n",
    "        tokens = set([*self.tokens, *self.expanded_tokens])\n",
    "        tokens = tokens & set(self.main.species.tokens)\n",
    "        return tokens\n",
    "\n",
    "    def start(self):\n",
    "        i = math.inf\n",
    "        for token in self.tokens:\n",
    "            i = min(i, token.i)\n",
    "        return i\n",
    "\n",
    "    def end(self):\n",
    "        i = -math.inf\n",
    "        for token in self.tokens:\n",
    "            i = max(i, token.i)\n",
    "        return i\n",
    "\n",
    "    def __str__(self):\n",
    "        tokens = list(set([*self.tokens, *self.expanded_tokens]))\n",
    "        tokens = sorted(tokens, key=lambda token: token.i)\n",
    "        return f\"{','.join([t.text for t in tokens])}\"\n",
    "\n",
    "class Order:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.order = []\n",
    "\n",
    "    def start(self):\n",
    "        i = math.inf\n",
    "        for item in self.order:\n",
    "            i = min(i, item.start())\n",
    "        return i\n",
    "\n",
    "    def end(self):\n",
    "        i = -math.inf\n",
    "        for item in self.order:\n",
    "            i = max(i, item.start())\n",
    "        return i\n",
    "\n",
    "    def __str__(self):\n",
    "        ret = \"\"\n",
    "\n",
    "        i = 0\n",
    "        size = len(self.order)\n",
    "        while i < size:\n",
    "            ret += f\"({self.order[i]})\"\n",
    "            if i != size - 1:\n",
    "                ret += \"->\"\n",
    "            i += 1\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1cb34d43-dfd8-4041-9578-13051e3a35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_token(main, sent_i, token):\n",
    "    expanded_token = [token]\n",
    "\n",
    "    if token.pos_ == \"PRON\":\n",
    "        expanded_token = [*main.coref_map.get(token, [token])]\n",
    "\n",
    "    i = 0\n",
    "    size = len(expanded_token)\n",
    "    while i < size:\n",
    "        exp_token = expanded_token[i]\n",
    "        for ent_pos, ent in main.parts.reg[sent_i].items():\n",
    "            if ent_pos[0] <= exp_token.i <= ent_pos[1] and ent.label in [Entity.LIST]:\n",
    "                expanded_token = [*main.sp_doc[ent_pos[0]:ent_pos[1]+1]]\n",
    "                break\n",
    "        i += 1\n",
    "\n",
    "    i = 0\n",
    "    size = len(expanded_token)\n",
    "    while i < size:\n",
    "        exp_token = expanded_token[i]\n",
    "        if exp_token in main.noun_chunk_map:\n",
    "            expanded_token.extend([*main.noun_chunk_map[exp_token]])\n",
    "\n",
    "        if exp_token in main.ent_map:\n",
    "            expanded_token.extend([*main.ent_map[exp_token]])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    expanded_token = list(set(expanded_token))\n",
    "    return expanded_token\n",
    "\n",
    "def order_tokens(main, sent_i, tokens):\n",
    "    if len(tokens) <= 2:\n",
    "        return None\n",
    "    \n",
    "    # Sort Tokens by Position in Doc\n",
    "    tokens = sorted(tokens, key=lambda token: token.i)\n",
    "    print(tokens)\n",
    "    \n",
    "    verbs = [token for token in tokens if token.pos_ == \"VERB\"]\n",
    "    if not verbs:\n",
    "        return\n",
    "\n",
    "    # Sort Verbs by Token Position in Doc\n",
    "    verbs = sorted(verbs, key=lambda token: token.i)\n",
    "    verb = verbs[0]\n",
    "\n",
    "    if verb == tokens[0]:\n",
    "        return None\n",
    "\n",
    "    # Partition Tokens in Doc (L)\n",
    "    l = tokens[0].i\n",
    "\n",
    "    # Partition Tokens in Doc (R)\n",
    "    i = tokens.index(verb) + 1\n",
    "    while i < len(tokens) and tokens[i].pos_ not in [\"PROPN\", \"NOUN\", \"PRON\"]:\n",
    "        i += 1\n",
    "    \n",
    "    if i <= 0 or i >= len(tokens):\n",
    "        return None\n",
    "    \n",
    "    r = tokens[i].i\n",
    "    print(r)\n",
    "\n",
    "    # Swap Subj. and Obj.\n",
    "    aux = verb.nbor(-1) and verb.nbor(-1).pos_ == \"AUX\"\n",
    "    adp = verb.nbor(1) and verb.nbor(1).lower_ == \"by\"\n",
    "    swap = aux or adp\n",
    "\n",
    "    # Creating Nodes\n",
    "    v_node = Node(main, is_action=True)\n",
    "    v_node.tokens = [verb]\n",
    "\n",
    "    a_node = Node(main)\n",
    "    a_node.tokens = [main.sp_doc[i] for i in range(l, verb.i)]\n",
    "    if not a_node.tokens:\n",
    "        return None\n",
    "    print(a_node.tokens)\n",
    "\n",
    "    b_node = Node(main)\n",
    "    b_node.tokens = [main.sp_doc[i] for i in range(verb.i+1, r+1)]\n",
    "    if not b_node.tokens:\n",
    "        return None\n",
    "    print(b_node.tokens)\n",
    "\n",
    "    if swap:\n",
    "        s_node = b_node\n",
    "        o_node = a_node\n",
    "    else:\n",
    "        s_node = a_node\n",
    "        o_node = b_node\n",
    "\n",
    "    # Subject Tokens Transferred to Object\n",
    "    sub_transfer_tokens = []\n",
    "    for token in s_node.tokens:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            sub_transfer_tokens.append(token)\n",
    "    s_node.tokens = [tkn for tkn in s_node.tokens if tkn not in sub_transfer_tokens]\n",
    "    o_node.tokens.extend(sub_transfer_tokens)\n",
    "\n",
    "    # Object Tokens Transferred to Verb\n",
    "    obj_transfer_tokens = []\n",
    "    for token in o_node.tokens:\n",
    "        if token in main.cause.tokens or token in main.change.tokens:\n",
    "            obj_transfer_tokens.append(token)\n",
    "    o_node.tokens = [tkn for tkn in o_node.tokens if tkn not in obj_transfer_tokens]\n",
    "    v_node.tokens.extend(obj_transfer_tokens)\n",
    "\n",
    "    # Expand Tokens in Subject and Object\n",
    "    s_expanded_tokens = flatten([expand_token(main, sent_i, tkn) for tkn in s_node.tokens])\n",
    "    s_node.expanded_tokens = s_expanded_tokens\n",
    "    \n",
    "    o_expanded_tokens = flatten([expand_token(main, sent_i, tkn) for tkn in o_node.tokens])\n",
    "    o_node.expanded_tokens = o_expanded_tokens\n",
    "\n",
    "    # Create Order S -> V -> 0\n",
    "    if (\n",
    "        not s_node.tokens or \n",
    "        not v_node.tokens or \n",
    "        not o_node.tokens\n",
    "    ):\n",
    "        return None\n",
    "    \n",
    "    order = Order(main)\n",
    "    order.order = [s_node, v_node, o_node]\n",
    "    return order\n",
    "\n",
    "def order_entity(main, sent_i, ent):\n",
    "    assert ent.doc == main.sp_doc\n",
    "    ent_tokens = [main.sp_doc[i] for i in range(ent.l, ent.r+1)]\n",
    "    print(ent_tokens)\n",
    "    return order_tokens(main, sent_i, ent_tokens)\n",
    "\n",
    "def order_document(main):\n",
    "    sents = list(main.sp_doc.sents)\n",
    "    sents_orders = {sent.start: [] for sent in sents}\n",
    "    sents_triples = {sent.start: [] for sent in sents}\n",
    "\n",
    "    # Subject-Verb-Object Triples\n",
    "    for triple in textacy.extract.subject_verb_object_triples(doc):\n",
    "        sents_triples[triple.verb[0].sent.start].append(triple)\n",
    "\n",
    "    for sent_i, sent in enumerate(sents):\n",
    "        # Sentence SVOs\n",
    "        for triple in sents_triples[sent.start]:\n",
    "            # print(f\"{Colors.BOLD}{Colors.BLUE}{triple}{Colors.ENDC}\")\n",
    "            l_m = triple.verb[0].i\n",
    "            r_m = triple.verb[-1].i\n",
    "            \n",
    "            v_node = Node(main, is_action=True)\n",
    "            v_node.tokens = [*triple.verb]\n",
    "            \n",
    "            # We assume that the verb is between\n",
    "            # the subject and object.\n",
    "            svo_tokens = [*triple.subject, *triple.object]\n",
    "            svo_tokens = sorted(svo_tokens, key=lambda token: token.i)\n",
    "            \n",
    "            l = svo_tokens[0].i\n",
    "            r = svo_tokens[-1].i\n",
    "\n",
    "            a_node = Node(main)\n",
    "            a_node.tokens = [main.sp_doc[i] for i in range(l, l_m)]\n",
    "            a_node.expanded_tokens =  flatten([expand_token(main, sent_i, tkn) for tkn in a_node.tokens])\n",
    "            \n",
    "            b_node = Node(main)\n",
    "            b_node.tokens = [main.sp_doc[i] for i in range(r_m+1, r+1)]\n",
    "            b_node.expanded_tokens =  flatten([expand_token(main, sent_i, tkn) for tkn in b_node.tokens])\n",
    "\n",
    "            # TODO\n",
    "            swap = False\n",
    "            if swap:\n",
    "                pass\n",
    "            else:\n",
    "                s_node = a_node\n",
    "                o_node = b_node\n",
    "\n",
    "            order = Order(main)\n",
    "            order.order = [s_node, v_node, o_node]\n",
    "            # if order:\n",
    "            #     print(order)\n",
    "            sents_orders[sent.start].append(order)\n",
    "\n",
    "            # Evaluate Subject and Object\n",
    "            # s_order = order_tokens(main, sent_i, s_node.tokens)\n",
    "            # if s_order:\n",
    "            #     print(s_order)\n",
    "            \n",
    "            # o_order = order_tokens(main, sent_i, o_node.tokens)\n",
    "            # if o_order:\n",
    "            #     print(o_order)\n",
    "            # sents_orders[sent.start].extend([s_order, o_order])\n",
    "\n",
    "        # for ent_pos, ent in main.parts.reg[sent_i].items():\n",
    "        #     print(ent_pos, ent.label_(), ent.lower())\n",
    "        \n",
    "        entities = list(main.parts.reg[sent_i].items())\n",
    "        for entity_range, entity in entities:\n",
    "            # print(entity_range)\n",
    "            if entity_range[0] == 219:\n",
    "                print(entity.lower())\n",
    "                e_order = order_entity(main, sent_i, entity)\n",
    "                if e_order:\n",
    "                    print(e_order)\n",
    "            # sents_orders[sent.start].append(e_order)\n",
    "\n",
    "    return sents_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bdcd5b8b-4f55-4718-9a84-97bbad55e64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that had settled on or between barnacles to remain in the community.\n",
      "[that, had, settled, on, or, between, barnacles, to, remain, in, the, community, .]\n",
      "[that, had, settled, on, or, between, barnacles, to, remain, in, the, community, .]\n",
      "225\n",
      "[that, had]\n",
      "[on, or, between, barnacles]\n",
      "(that,had,settled,on,or,between,barnacles,to,remain)->(settled)->(that,had,settled,on,or,between,barnacles,to,remain)\n",
      "that had settled on or between barnacles to remain\n",
      "[that, had, settled, on, or, between, barnacles, to, remain]\n",
      "[that, had, settled, on, or, between, barnacles, to, remain]\n",
      "225\n",
      "[that, had]\n",
      "[on, or, between, barnacles]\n",
      "(that,had,settled,on,or,between,barnacles,to,remain)->(settled)->(that,had,settled,on,or,between,barnacles,to,remain)\n",
      "that had settled\n",
      "[that, had, settled]\n",
      "[that, had, settled]\n"
     ]
    }
   ],
   "source": [
    "ret = order_document(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ab7fa-1209-4616-904a-54c69371dea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55634fa4-ecfb-4931-a7de-ab36616202bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bfcb41-6981-415e-868d-421d0849c2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
