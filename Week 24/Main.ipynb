{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382ea6c-1d9f-4ca2-aa2b-aae201096481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import csv\n",
    "# import sys\n",
    "# import json\n",
    "# import math\n",
    "# import spacy\n",
    "# import textacy\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pylab as plt\n",
    "# from taxonerd import TaxoNERD\n",
    "# from fastcoref import spacy_component\n",
    "# from spacy.matcher import Matcher, DependencyMatcher, PhraseMatcher\n",
    "%run \"Helper.ipynb\"\n",
    "%run \"Base.ipynb\"\n",
    "%run \"Entity.ipynb\"\n",
    "%run \"Species.ipynb\"\n",
    "%run \"Keywords.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba2de5b-846b-4b33-8bb9-4c30eb13fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE_LEVEL = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c99722-7d97-4889-aa87-59ad1d81b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main(Base):\n",
    "    def __init__(self):\n",
    "        self.sp_nlp = spacy.load(\"en_core_web_trf\")\n",
    "        self.sp_nlp.add_pipe(\n",
    "            \"fastcoref\",\n",
    "            config={'model_architecture': 'LingMessCoref', 'model_path': 'biu-nlp/lingmess-coref', 'device': 'cpu'}\n",
    "        )\n",
    "        self.sp_doc = None\n",
    "        super().__init__(self)\n",
    "\n",
    "        # Index Map\n",
    "        # This maps the a character's index \n",
    "        # to the token it belongs to in the \n",
    "        # text. It helps with handling the \n",
    "        # differences between different pipelines\n",
    "        # and tools.\n",
    "        self.index_map = None\n",
    "\n",
    "        # Entity Map\n",
    "        # This maps a token to the entity\n",
    "        # it belongs to, if any.\n",
    "        self.entity_map = None\n",
    "\n",
    "        # Noun Chunk Map\n",
    "        # This maps a token to the noun chunk\n",
    "        # it belongs to, if any.\n",
    "        self.noun_chunk_map = None\n",
    "\n",
    "        # This maps a token to its resolved\n",
    "        # \"noun\", for lack of a better word.\n",
    "        # For example, \"he\" could be mapped\n",
    "        # to \"Bob\", and \"which\" to \"books\".\n",
    "        self.coref_map = None\n",
    "        \n",
    "        # Species\n",
    "        # This also identifies keywords\n",
    "        # in the text, specifically species.\n",
    "        self.species = Species(self)\n",
    "\n",
    "        # Keywords\n",
    "        # These objects help identify\n",
    "        # important keywords in the text.\n",
    "        self.trait = TraitKeywords(self)\n",
    "        self.cause = CauseKeywords(self)\n",
    "        self.change = ChangeKeywords(self)\n",
    "        self.causal = CausalKeywords(self)\n",
    "\n",
    "        # Grammatical Units\n",
    "        # This is used to identify units\n",
    "        # in a sentence, like lists, brackets,\n",
    "        # quotes, clauses, and so on.\n",
    "        self.units = Units(self)\n",
    "\n",
    "    \n",
    "    def update_doc(self, doc, verbose=False):\n",
    "        self.sp_doc = doc\n",
    "        self.index_map = self.load_index_map()\n",
    "        self.entity_map = self.load_entity_map()\n",
    "        self.noun_chunk_map = self.load_noun_chunk_map()\n",
    "        self.coref_map = self.load_coref_map()\n",
    "\n",
    "        # Update the Helpers\n",
    "        self.units.update()\n",
    "        self.species.update(doc.text, verbose=False)\n",
    "        self.trait.update(verbose=False)\n",
    "        self.cause.update(verbose=False)\n",
    "        self.change.update(verbose=False)\n",
    "        self.causal.update(verbose=False)\n",
    "\n",
    "    \n",
    "    def update_text(self, text, verbose=False):\n",
    "        self.sp_doc = self.sp_nlp(text)\n",
    "        self.update_doc(self.sp_doc, verbose=verbose)\n",
    "\n",
    "        \n",
    "    def load_index_map(self):\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        # Map Character Index to Token\n",
    "        index_map = {}\n",
    "        for token in self.sp_doc:\n",
    "            l_char_index = token.idx\n",
    "            r_char_index = token.idx + len(token)\n",
    "\n",
    "            for i in range(l_char_index, r_char_index):\n",
    "                index_map[i] = token\n",
    "\n",
    "        return index_map\n",
    "    \n",
    "\n",
    "    def load_noun_chunk_map(self):\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "        noun_chunk_map = {}\n",
    "        for noun_chunk in self.sp_doc.noun_chunks:\n",
    "            for token in noun_chunk:\n",
    "                noun_chunk_map[token] = noun_chunk\n",
    "        return noun_chunk_map\n",
    "\n",
    "\n",
    "    def load_entity_map(self):\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "        entity_map = {}\n",
    "        for ent in self.sp_doc.ents:\n",
    "            for token in ent:\n",
    "                entity_map[token] = ent\n",
    "        return entity_map\n",
    "\n",
    "    \n",
    "    def load_coref_map(self):\n",
    "        if self.sp_doc is None:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        coref_map = {}\n",
    "        for cluster in self.sp_doc._.coref_clusters:\n",
    "            ref_l_token = self.index_map[cluster[0][0]].i\n",
    "            ref_r_token = self.index_map[cluster[0][1]-1].i\n",
    "            \n",
    "            ref_span = self.sp_doc[ref_l_token:ref_r_token+1]\n",
    "\n",
    "            for start, end in cluster[1:]:\n",
    "                l_token = self.index_map[start].i\n",
    "                r_token = self.index_map[end-1].i\n",
    "\n",
    "                span = self.sp_doc[l_token:r_token+1]\n",
    "                for token in span:\n",
    "                    coref_map[token] = ref_span\n",
    "\n",
    "        last_noun = None\n",
    "        for token in self.sp_doc:\n",
    "            if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                last_noun = self.sp_doc[token.i:token.i+1]\n",
    "        \n",
    "            if token.lower_ in [\"which\"] and last_noun:\n",
    "                coref_map[token] = last_noun\n",
    "                last_noun = None\n",
    "                \n",
    "        return coref_map\n",
    "        \n",
    "        \n",
    "    def token_at_char(self, char_index):\n",
    "        if not self.sp_doc or not self.index_map:\n",
    "            raise Exception(\"DNE\")\n",
    "\n",
    "        if char_index in self.index_map:\n",
    "            return self.index_map[char_index]\n",
    "\n",
    "        raise Exception(f\"Token at Index {char_index} Not Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45dfdd-436e-4aa9-af9b-b2f311bc420e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
