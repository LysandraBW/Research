{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3461b7-e257-4943-8101-e65febed4148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import spacy\n",
    "import textacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from taxonerd import TaxoNERD\n",
    "from fastcoref import spacy_component\n",
    "from itertools import permutations, combinations\n",
    "from spacy.matcher import Matcher, DependencyMatcher, PhraseMatcher\n",
    "%run \"./Main.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037079d2-3ece-40a1-923e-84936e2b44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main = Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495eb192-04e6-4dd0-baae-5bfdf02803e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Due to the presence of fish, the cat population increased. This caused the population of fish to decrease.\"\n",
    "main.update_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81fbd7b-d3df-4f29-af76-f0930c703995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_bounds(self, bounds, larger=True):\n",
    "    dounds = []\n",
    "\n",
    "    for bound in bounds:\n",
    "        overlap = False\n",
    "        \n",
    "        for i, dound in enumerate(dounds):\n",
    "            surround = bound[0] <= dound[0] <= bound[1] and bound[0] <= dound[1] <= bound[1]\n",
    "            contains = dound[0] <= bound[0] <= dound[1] and dound[0] <= bound[1] <= dound[1]\n",
    "            overlaps = surround or contains\n",
    "            \n",
    "            if (surround and larger) or (contains and not larger):\n",
    "                dounds[i] = bound\n",
    "                \n",
    "        if not overlaps:\n",
    "            dounds.append(bound)\n",
    "\n",
    "    return list(set(dounds))\n",
    "\n",
    "def power_set(pool, current=None, result=None):\n",
    "    if current not in result:\n",
    "        result.append(current)\n",
    "\n",
    "    for item in pool:\n",
    "        if item not in current:\n",
    "            power_set(pool, current={*current, item}, result=result)\n",
    "\n",
    "    return result\n",
    "\n",
    "def interleave(pool, current=None, used=None, skip=None, result=None):    \n",
    "    if current and current not in result:\n",
    "        result.append(current)\n",
    "    \n",
    "    for i in range(len(pool)):\n",
    "        if i == skip:\n",
    "            continue\n",
    "        \n",
    "        for j in range(len(pool[i])):\n",
    "            if (i, j) in used:\n",
    "                continue\n",
    "            \n",
    "            interleave(\n",
    "                pool, \n",
    "                used=[*used, (i, j)],\n",
    "                current=[*current, pool[i][j]],\n",
    "                skip=i, \n",
    "                result=result\n",
    "            )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290547e-68c6-40d4-b92d-eb478ac2e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, main, is_action=False, tokens=None):\n",
    "        self.main = main\n",
    "        self.tokens = tokens or []\n",
    "        self.is_action = is_action\n",
    "        self.all_tokens = []\n",
    "\n",
    "    \n",
    "\n",
    "    def start(self):\n",
    "        i = math.inf\n",
    "        for token in self.tokens:\n",
    "            i = min(i, token.i)\n",
    "        return i\n",
    "\n",
    "\n",
    "    \n",
    "    def end(self):\n",
    "        i = -math.inf\n",
    "        for token in self.tokens:\n",
    "            i = max(i, token.i)\n",
    "        return i\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_tokens(self):\n",
    "        return self.tokens\n",
    "\n",
    "\n",
    "    \n",
    "    def get_all_tokens(self):\n",
    "        return [*self.tokens, *self.all_tokens]\n",
    "\n",
    "\n",
    "    \n",
    "    def get_traits(self):\n",
    "        tokens = set([*self.tokens, *self.all_tokens])\n",
    "        tokens = tokens & set(self.main.trait.tokens)\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    \n",
    "    def get_species(self):\n",
    "        tokens = set([*self.tokens, *self.all_tokens])\n",
    "        tokens = tokens & set(self.main.species.tokens)\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    \n",
    "    def copy(self):\n",
    "        node = Node(self.main)\n",
    "        node.tokens = [*self.tokens]\n",
    "        node.is_action = self.is_action\n",
    "        node.all_tokens = [*self.all_tokens]\n",
    "        return node\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        tokens = [*self.tokens, *self.all_tokens]\n",
    "        tokens = list(set(tokens))\n",
    "        tokens = sorted(tokens, key=lambda token: token.i)\n",
    "        return f\"{','.join([t.text for t in tokens])}\"\n",
    "\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token_coref(main, all_tokens):\n",
    "        i = 0\n",
    "        size = len(all_tokens)\n",
    "        while i < size:\n",
    "            token = all_tokens[i]\n",
    "            if token.pos_ != \"PRON\":\n",
    "                all_tokens = [*main.coref_map.get(token, [token])]\n",
    "                break\n",
    "            i += 1\n",
    "        return all_tokens\n",
    "    \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token_unit(main, all_tokens):\n",
    "        i = 0\n",
    "        size = len(all_tokens)\n",
    "        unit_map_items = main.units.unit_map.items()\n",
    "        while i < size:\n",
    "            token = all_tokens[i]\n",
    "            for unit_bound, unit in unit_map_items:\n",
    "                token_in_unit = unit_bound[0] <= token.i <= unit_bound[1]\n",
    "                unit_is_valid = unit.label_has([Unit.ITEM])\n",
    "                if token_in_unit and unit_is_valid:\n",
    "                    all_tokens = [*main.sp_doc[unit_bound[0]:unit_bound[1]+1]]\n",
    "                    break\n",
    "            i += 1\n",
    "        return all_tokens\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token_entity(main, all_tokens):\n",
    "        i = 0\n",
    "        size = len(all_tokens)\n",
    "        while i < size:\n",
    "            token = all_tokens[i]\n",
    "            if token in main.entity_map:\n",
    "                all_tokens.extend([*main.entity_map[token]])\n",
    "            i += 1\n",
    "        return all_tokens\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token_noun_chunk(main, all_tokens):\n",
    "        i = 0\n",
    "        size = len(all_tokens)\n",
    "        while i < size:\n",
    "            token = all_tokens[i]\n",
    "            if token in main.noun_chunk_map:\n",
    "                all_tokens.extend([*main.noun_chunk_map[token]])\n",
    "            i += 1\n",
    "        return all_tokens\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token(main, token):\n",
    "        all_tokens = [token]\n",
    "\n",
    "        all_tokens = Node.extend_token_coref(main, all_tokens)\n",
    "        all_tokens = Node.extend_token_unit(main, all_tokens)\n",
    "        all_tokens = Node.extend_token_entity(main, all_tokens)\n",
    "        all_tokens = Node.extend_token_noun_chunk(main, all_tokens)\n",
    "        \n",
    "        all_tokens = list(set(all_tokens))\n",
    "        all_tokens = sorted(all_tokens, key=lambda token: token.i)\n",
    "        \n",
    "        return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab4234-8632-4116-905b-f075f6787380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.order = []\n",
    "\n",
    "    \n",
    "    \n",
    "    def start(self):\n",
    "        i = math.inf\n",
    "        for item in self.order:\n",
    "            i = min(i, item.start())\n",
    "        return i\n",
    "\n",
    "\n",
    "    \n",
    "    def end(self):\n",
    "        i = -math.inf\n",
    "        for item in self.order:\n",
    "            i = max(i, item.end())\n",
    "        return i\n",
    "\n",
    "\n",
    "\n",
    "    def sent_start(self):\n",
    "        start = self.start()\n",
    "        sents = self.main.sp_doc.sents\n",
    "        for i, sent in enumerate(sents):\n",
    "            if sent.start <= start < sent.end:\n",
    "                return i\n",
    "        return -1\n",
    "\n",
    "\n",
    "    \n",
    "    def copy(self):\n",
    "        order = Order(self.main)\n",
    "        order.order = []\n",
    "        for item in self.order:\n",
    "            order.order.append(item.copy())\n",
    "        return order\n",
    "\n",
    "\n",
    "    \n",
    "    def get_tokens(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_tokens())\n",
    "        return ret\n",
    "\n",
    "\n",
    "    \n",
    "    def get_all_tokens(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_all_tokens())\n",
    "        return ret\n",
    "\n",
    "\n",
    "    \n",
    "    def get_species(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_species())\n",
    "        return ret\n",
    "\n",
    "\n",
    "    \n",
    "    def get_traits(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_traits())\n",
    "        return ret\n",
    "    \n",
    "\n",
    "    \n",
    "    def attach_event(self, event, del_end=False):\n",
    "        if del_end:\n",
    "            self.order.pop()\n",
    "        self.order.extend(event.order)\n",
    "        return self\n",
    "    \n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        ret = \"\"\n",
    "\n",
    "        i = 0\n",
    "        size = len(self.order)\n",
    "        while i < size:\n",
    "            ret += f\"({self.order[i]})\"\n",
    "            if i != size - 1:\n",
    "                ret += \"->\"\n",
    "            i += 1\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935ba34-fa22-45ff-89e1-3eac1ca5cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventManager:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.events = []\n",
    "\n",
    "\n",
    "    \n",
    "    def fix_triple_tokens(self, s_node, v_node, o_node):\n",
    "        # Subject Tokens Transferred to Object\n",
    "        s_transfer_tokens = []\n",
    "        for token in s_node.tokens:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                s_transfer_tokens.append(token)\n",
    "        \n",
    "        s_node.tokens = [t for t in s_node.tokens if t not in s_transfer_tokens]\n",
    "        o_node.tokens.extend(s_transfer_tokens)\n",
    "        \n",
    "        # Object Tokens Transferred to Verb\n",
    "        o_transfer_tokens = []\n",
    "        for token in o_node.tokens:\n",
    "            if token in main.cause.tokens or token in main.change.tokens:\n",
    "                o_transfer_tokens.append(token)\n",
    "        \n",
    "        o_node.tokens = [t for t in o_node.tokens if t not in o_transfer_tokens]\n",
    "        v_node.tokens.extend(o_transfer_tokens)\n",
    "\n",
    "        return (s_node, v_node, o_node)\n",
    "\n",
    "\n",
    "    \n",
    "    def fix_triple_direction(self, s_node, v_node, o_node):\n",
    "        v_node_tokens = sorted(v_node.tokens, key=lambda t: t.i)\n",
    "        v_node_tokens_l = v_node_tokens[0]\n",
    "        v_node_tokens_r = v_node_tokens[-1]\n",
    "        \n",
    "        aux = v_node_tokens_l.nbor(-1) and v_node_tokens_l.nbor(-1).pos_ == \"AUX\"\n",
    "        adp = v_node_tokens_r.nbor(1) and v_node_tokens_r.nbor(1).lower_ == \"by\"\n",
    "        if not aux and not adp:\n",
    "            return (s_node, v_node, o_node)\n",
    "        return (o_node, v_node, s_node)\n",
    "\n",
    "\n",
    "    \n",
    "    def fix_triple_all_tokens(self, s_node, v_node, o_node):\n",
    "        s_node.all_tokens = flatten([Node.extend_token(main, t) for t in s_node.tokens])\n",
    "        o_node.all_tokens = flatten([Node.extend_token(main, t) for t in o_node.tokens])\n",
    "        return (s_node, v_node, o_node)\n",
    "\n",
    "\n",
    "    \n",
    "    def push_bound_l(self, bound, min_bound):\n",
    "        return self.push_bound(\n",
    "            bound, \n",
    "            reverse=True, \n",
    "            unit_condition_fnc=lambda unit: unit.l >= min_bound,\n",
    "            update_bound_fnc=lambda bound, unit: min(bound, unit.l)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def push_bound_r(self, bound, max_bound):\n",
    "        return self.push_bound(\n",
    "            bound, \n",
    "            reverse=False, \n",
    "            unit_condition_fnc=lambda unit: unit.r <= max_bound,\n",
    "            update_bound_fnc=lambda bound, unit: max(bound, unit.r)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def push_bound(self, bound, reverse=False, unit_condition_fnc=None, update_bound_fnc=None):\n",
    "        units = self.main.units.units_at_i(bound)\n",
    "        if not units:\n",
    "            return bound\n",
    "\n",
    "        start = False\n",
    "        unit_map_vals = list(main.units.unit_map.values())\n",
    "        unit_map_vals = sorted(unit_map_vals, key=lambda u: (u.l, u.r), reverse=reverse)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(unit_map_vals):\n",
    "            unit = unit_map_vals[i]\n",
    "\n",
    "            # Units must be in the same sentence as bound.\n",
    "            if units[0].sent_start() != unit.sent_start():\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if unit in units:\n",
    "                start = True\n",
    "            else:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            accept_labels = [\n",
    "                Unit.LIST,\n",
    "                Unit.ITEM,\n",
    "                Unit.D_CLAUSE,\n",
    "                Unit.P_PHRASE\n",
    "            ]\n",
    "\n",
    "            if unit.label_has([Unit.FRAGMENT, *accept_labels]) and unit_condition_fnc(unit):\n",
    "                bound = update_bound(bound, unit)\n",
    "\n",
    "                # If we can continue pushing, we continue. If not,\n",
    "                # we break from the loop.\n",
    "                if i + 1 >= len(unit_map_vals) or not unit_map_vals[i+1].label_has(accept_labels):\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return bound\n",
    "\n",
    "\n",
    "    \n",
    "    def split_tokens(self, tokens):\n",
    "        verbs = [t for t in tokens if t.pos_ == \"VERB\"]\n",
    "\n",
    "        # L Bound\n",
    "        # It's the first token.\n",
    "        l = tokens[0].i\n",
    "\n",
    "        # M Bound\n",
    "        # The L part of this bound is the first verb.\n",
    "        # The R part of this bound is the last verb.\n",
    "        v_l = verbs[+0].i # Do Not Ask\n",
    "        v_r = verbs[-1].i\n",
    "\n",
    "        # R Bound\n",
    "        # We look for the first noun after the last verb.\n",
    "        i = tokens.index(verbs[-1]) + 1\n",
    "        while i < len(tokens) and tokens[i].pos_ not in [\"PROPN\", \"NOUN\", \"PRON\"]:\n",
    "            i += 1\n",
    "\n",
    "        if i <= 0 or i >= len(tokens):\n",
    "            return None\n",
    "\n",
    "        r = tokens[i].i\n",
    "\n",
    "        # Return Split\n",
    "        return (l, v_l, v_r, r)\n",
    "\n",
    "\n",
    "    \n",
    "    def convert_tokens(self, tokens, split=None):\n",
    "        tokens = sorted(tokens, key=lambda t: t.i)\n",
    "        speech = [t.pos_ for t in tokens]\n",
    "        \n",
    "        if \"VERB\" not in speech and speech[0] in [\"ADP\", \"SCONJ\"]:\n",
    "            event = Event(self.main)\n",
    "            event.order = [Node(self.main, tokens=tokens)]\n",
    "            return event\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        if len(tokens) <= 2:\n",
    "            return None\n",
    "        \n",
    "        l, v_l, v_r, r = split or self.split_tokens(tokens)\n",
    "        \n",
    "        l = self.push_bound_l(l, tokens[+0].i) # Do Not Ask\n",
    "        r = self.push_bound_r(r, tokens[-1].i)\n",
    "\n",
    "        v_node = Node(self.main, is_action=True)\n",
    "        v_node.tokens = [self.main.sp_doc[i] for i in range(v_l, v_r+1)]\n",
    "        \n",
    "        s_node = Node(self.main)\n",
    "        s_node.tokens = [self.main.sp_doc[i] for i in range(l, v_l)]\n",
    "            \n",
    "        o_node = Node(self.main)\n",
    "        o_node.tokens = [self.main.sp_doc[i] for i in range(v_l+1, r+1)]\n",
    "\n",
    "        s_node, v_node, o_node = self.fix_triple_direction(s_node, v_node, o_node)\n",
    "        s_node, v_node, o_node = self.fix_triple_tokens(s_node, v_node, o_node)\n",
    "        s_node, v_node, o_node = self.fix_triple_all_tokens(s_node, v_node, o_node)\n",
    "\n",
    "        s_node_valid = s_node.get_species() or s_node.get_traits()\n",
    "        o_node_valid = o_node.get_species() or o_node.get_traits()\n",
    "\n",
    "        if not s_node_valid and not o_node_valid:\n",
    "            return None\n",
    "        \n",
    "        event = Event()\n",
    "        event.order = [s_node, v_node, o_node]\n",
    "\n",
    "        events = [\n",
    "            event,\n",
    "            self.convert_tokens(s_node.tokens),\n",
    "            self.convert_tokens(o_node.tokens)\n",
    "        ]\n",
    "        \n",
    "        return [event for event in events if event]\n",
    "\n",
    "\n",
    "    \n",
    "    def convert_unit(self, unit):\n",
    "        tokens = [*main.sp_doc[unit.l:unit.r+1]]\n",
    "        return self.convert_tokens(tokens)\n",
    "\n",
    "\n",
    "    \n",
    "    def convert_svo_triple(self, triple):\n",
    "        tokens = [*triple.subject, *triple.object]\n",
    "        tokens = sorted(tokens, key=lambda t: t.i)\n",
    "    \n",
    "        l = tokens[+0].i\n",
    "        v_l = triple.verb[+0].i\n",
    "        v_r = triple.verb[-1].i\n",
    "        r = tokens[-1].i\n",
    "        \n",
    "        return self.convert_tokens(tokens, split=(l, v_l, v_r, r))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def distinct_events(self, events):\n",
    "        event_bounds_mapped = {(e.start(), e.end()): rel for e in events}\n",
    "        event_bounds = event_bounds_mapped.keys()   \n",
    "        distinct_event_bounds = distinct_bounds(event_bounds, larger=True)    \n",
    "        distinct_events = [event_bounds_mapped[b] for b in distinct_event_bounds]\n",
    "        return distinct_events\n",
    "    \n",
    "\n",
    "    \n",
    "    def find_events(self):\n",
    "        sents = list(self.main.sp_doc.sents)\n",
    "        sents_events = {sent.start: [] for sent in sents}\n",
    "\n",
    "         # Units\n",
    "        for unit_tokens in self.main.units.aggregate_units():\n",
    "            events = self.convert_tokens(unit_tokens)\n",
    "            sent_start = unit_tokens[0].sent.start\n",
    "            sents_events[sent_start].extend(events)\n",
    "        \n",
    "        # Triples\n",
    "        for triple in textacy.extract.subject_verb_object_triples(self.main.sp_doc):\n",
    "            events = self.convert_svo_triple(triple)\n",
    "            sent_start = triple.verb[0].sent.start\n",
    "            sents_events[sent_start].extend(events)\n",
    "\n",
    "        sents_events = {k: self.distinct_events(v) for k, v in sents_events.items()}\n",
    "        return sents_events\n",
    "    \n",
    "    \n",
    "    \n",
    "    def overlap_in_species(self, x, y):\n",
    "        sp_X = [self.main.species.span_at_token(species) for species in x.get_species()]\n",
    "        sp_Y = [self.main.species.span_at_token(species) for species in y.get_species()]\n",
    "\n",
    "        for sp_x in sp_X:\n",
    "            if self.main.species.find_same_species(sp_Y, sp_x):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "\n",
    "    def overlap_in_traits(self, x, y):\n",
    "        tr_X = set([trait.lemma_.lower() for trait in x.get_traits()])\n",
    "        tr_Y = set([trait.lemma_.lower() for trait in y.get_traits()])\n",
    "\n",
    "        return tr_X & tr_Y\n",
    "    \n",
    "    \n",
    "    \n",
    "    def overlap_in_tokens(self, x, y):\n",
    "        # This takes the nouns (proper nouns included) \n",
    "        # from a list of tokens. Two string versions\n",
    "        # of the token are included. I know it is long.\n",
    "        get_nouns = lambda tokens: flatten([[token.lower_, token.lemma_.lower()] for token in tokens if token.pos_ in [\"PROPN\", \"NOUN\"]])\n",
    "        \n",
    "        x_tokens = get_nouns(x.all_tokens)\n",
    "        y_tokens = get_nouns(y.all_tokens)\n",
    "\n",
    "        return set(x_tokens) & set(y_tokens)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def has_causal_phrase(self, n):\n",
    "        speech = [token.pos_ for token in n.tokens]\n",
    "        return set([\"ADP\", \"SCONJ\"]) & set(speech)\n",
    "    \n",
    "    \n",
    "\n",
    "    def has_causal_phrase_and_pron(self, n):\n",
    "        speech = set([token.pos_ for token in n.tokens])\n",
    "        causal_speech = speech & set([\"ADP\", \"SCONJ\"])\n",
    "        pronoun_speech = speech & set([\"PRON\"])\n",
    "        return causal_speech and pronoun_speech\n",
    "    \n",
    "    \n",
    "    \n",
    "    def can_merge_intrasent(self, X, Y):\n",
    "        x = X.order[-1]\n",
    "        y = Y.order[+0] # Do Not Ask\n",
    "\n",
    "        can_merge = self.overlap_in_species(x, y)\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "\n",
    "        can_merge = self.overlap_in_tokens(x, y)\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "\n",
    "        can_merge = self.has_causal_phrase(x, y)\n",
    "        if can_merge:\n",
    "            return (True, False)\n",
    "        \n",
    "        return (False, False)\n",
    "    \n",
    "    \n",
    "\n",
    "    def can_merge_intersent(self, X, Y):\n",
    "        x = X.order[-1]\n",
    "        y = Y.order[+0] # Do Not Ask\n",
    "\n",
    "        can_merge = self.overlap_in_species(x, y)\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "\n",
    "        can_merge = self.overlap_in_tokens(x, y)\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "\n",
    "        can_merge = self.has_causal_phrase_and_pron(x, y)\n",
    "        if can_merge:\n",
    "            return (True, False)\n",
    "        \n",
    "        return (False, False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def intrasent_sequences(self, indices_1D):\n",
    "        power_set_indices = power_set(indices_1D, current=[], result=[])\n",
    "        sequences = []\n",
    "        for subset in power_set_indices:\n",
    "            if subset:\n",
    "                sequences.extend(list(permutations(subset)))\n",
    "        return sequences\n",
    "\n",
    "\n",
    "    \n",
    "    def intersent_sequences(self, indices_2D):\n",
    "        sequences = interleave(indices_2D, current=[], used=[], result=[])\n",
    "        return sequences\n",
    "    \n",
    "    \n",
    "    \n",
    "    def merge_intrasent(self, sent_events):\n",
    "        i = 0\n",
    "        while i + 1 < len(sent_events):\n",
    "            X = sent_events[i]\n",
    "            Y = sent_events[i+1]\n",
    "\n",
    "            can_merge, del_end = self.can_merge_intrasent(X, Y)\n",
    "            if not can_merge:\n",
    "                return None\n",
    "\n",
    "            X.attach_event(Y, del_end=del_end)\n",
    "            sent_events.pop()\n",
    "\n",
    "        return sent_events[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def merge_intersent(self, sents_events):\n",
    "        i = 0\n",
    "        while i + 1 < len(sent_events):\n",
    "            X = sent_events[i]\n",
    "            Y = sent_events[i+1]\n",
    "\n",
    "            can_merge, del_end = self.can_merge_intrasent(X, Y)\n",
    "            if not can_merge:\n",
    "                return None\n",
    "\n",
    "            X.attach_event(Y, del_end=del_end)\n",
    "            sent_events.pop()\n",
    "\n",
    "        return sent_events[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_events(self):\n",
    "        sents_events = self.find_events()\n",
    "        merged_sents_events = []\n",
    "        \n",
    "        for sent_events in sents_events.values():\n",
    "            merged_sents_events.append([])\n",
    "\n",
    "            # Each event can be represented with its index\n",
    "            # for simplicity.\n",
    "            event_indices = list(range(len(sent_events)))\n",
    "\n",
    "            # The order in which you merge an event can result\n",
    "            # in a different outcome. For example, E1 + E2 may\n",
    "            # look different to E2 + E1, and so on.\n",
    "\n",
    "            # Idea:\n",
    "            # We try all the possible ways you can merge, which is\n",
    "            # the same as trying all the different orders you can\n",
    "            # merge the events. For example, merging E1, E2, and E3\n",
    "            # can be done in 8 ways: E1, E2, E3, E1 + E2, E2 + E1,\n",
    "            # and so on.\n",
    "            \n",
    "            # These are all the possible orders (or sequences) in \n",
    "            # which you can merge the above events. We will try all\n",
    "            # possible sequences, adding those that work.\n",
    "            sequences = self.intrasent_sequences(event_indices)\n",
    "\n",
    "            for sequence in sequences:\n",
    "                events = [events[i] for i in sequence]\n",
    "                merged_event = self.merge_intrasent(events)\n",
    "                if merged_event:\n",
    "                    merged_sents_events[-1].append(merged_event)\n",
    "\n",
    "        # Idea:\n",
    "        # Now that we've found all the events that can be made from\n",
    "        # merging the events within a sentence, we can merge across\n",
    "        # sentences. To do this, we also need to try all different\n",
    "        # sequences of events. However, we shouldn't try and merge\n",
    "        # events from the same sentence again, which means that no\n",
    "        # two consecutive events in the sequence can be from the same\n",
    "        # sentence. There's also another issue: we can't use a simple\n",
    "        # 1D-index for each event as we're dealing with a 2D-list of events.\n",
    "        # Therefore, we'll use the index equivalent to its 1D-form, or\n",
    "        # something similar as we don't have a matrix.\n",
    "        # If this doesn't make sense, I wouldn't be surprised, I'm more\n",
    "        # of a pictures person.\n",
    "        index_to_event = {}\n",
    "        i = 0\n",
    "        for sent_events in merged_sents_events.values():\n",
    "            for event in sent_events:\n",
    "                index_to_event[i] = event\n",
    "                i += 1\n",
    "\n",
    "        event_indices = list(range(i))\n",
    "        sequences = self.intersent_sequences(event_indices)\n",
    "\n",
    "        merged_events = []\n",
    "        \n",
    "        for sequence in sequences:\n",
    "            events = [index_to_event[i] for i in sequence]\n",
    "            merged_event = self.merge_intersent(events)\n",
    "            if merged_event:\n",
    "                merged_events.append(merged_event)\n",
    "        \n",
    "        return merged_events\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update(self):\n",
    "        self.events = self.load_events()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
