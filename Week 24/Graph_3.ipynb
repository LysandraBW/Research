{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3461b7-e257-4943-8101-e65febed4148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeln\\anaconda3\\envs\\3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import string\n",
    "import spacy\n",
    "import textacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from taxonerd import TaxoNERD\n",
    "from fastcoref import spacy_component\n",
    "from itertools import permutations, combinations\n",
    "from spacy.matcher import Matcher, DependencyMatcher, PhraseMatcher\n",
    "%run \"./Main.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77dc3b05-e304-48e6-8ca8-fd6ed4b79eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE_LEVEL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60242dd1-4d52-4a92-8676-5e40db025fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOUNS = [\n",
    "    \"NOUN\", \n",
    "    \"PROPN\", \n",
    "    \"PRON\"\n",
    "]\n",
    "\n",
    "DO_NOT_SWAP_PHRASES = [\n",
    "    \"resulted in\",\n",
    "    \"results in\"\n",
    "]\n",
    "\n",
    "PUSH_INCLUDE_LABELS = [\n",
    "    Unit.LIST,\n",
    "    Unit.ITEM,\n",
    "    Unit.P_PHRASE,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81fbd7b-d3df-4f29-af76-f0930c703995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_set(pool, current=None, result=None):\n",
    "    if current not in result:\n",
    "        result.append(current)\n",
    "\n",
    "    for item in pool:\n",
    "        if item not in current:\n",
    "            power_set(\n",
    "                pool,\n",
    "                current={*current, item},\n",
    "                result=result\n",
    "            )\n",
    "\n",
    "    return result\n",
    "\n",
    "def interleave(pool, current=None, used=None, skip=None, result=None):\n",
    "    if current and current not in result:\n",
    "        result.append(current)\n",
    "    \n",
    "    for i in range(len(pool)):\n",
    "        if i == skip:\n",
    "            continue\n",
    "        \n",
    "        for j in range(len(pool[i])):\n",
    "            if (i, j) not in used:\n",
    "                interleave(\n",
    "                    pool,\n",
    "                    current=[*current, pool[i][j]],\n",
    "                    used=[*used, (i, j)],\n",
    "                    skip=i,\n",
    "                    result=result\n",
    "                )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3290547e-68c6-40d4-b92d-eb478ac2e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, main, is_action=False, tokens=None):\n",
    "        self.main = main\n",
    "        self.tokens = tokens or []\n",
    "        self.is_action = is_action\n",
    "        self.other_tokens = []\n",
    "    \n",
    "    \n",
    "    def start(self):\n",
    "        i = math.inf\n",
    "        for token in self.tokens:\n",
    "            i = min(i, token.i)\n",
    "        return i\n",
    "    \n",
    "    \n",
    "    def end(self):\n",
    "        i = -math.inf\n",
    "        for token in self.tokens:\n",
    "            i = max(i, token.i)\n",
    "        return i\n",
    "    \n",
    "    \n",
    "    def get_tokens(self):\n",
    "        return self.tokens\n",
    "    \n",
    "    \n",
    "    def get_all_tokens(self):\n",
    "        return [*self.tokens, *self.other_tokens]\n",
    "    \n",
    "    \n",
    "    def get_traits(self):\n",
    "        tokens = set([*self.tokens, *self.other_tokens])\n",
    "        tokens = tokens & set(self.main.trait.tokens)\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    def get_species(self):\n",
    "        tokens = set([*self.tokens, *self.other_tokens])\n",
    "        tokens = tokens & set(self.main.species.tokens)\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    def copy(self):\n",
    "        node = Node(self.main)\n",
    "        node.tokens = [*self.tokens]\n",
    "        node.is_action = self.is_action\n",
    "        node.other_tokens = [*self.other_tokens]\n",
    "        return node\n",
    "    \n",
    "    \n",
    "    def __str__(self, nouns_only=False):\n",
    "        tokens = [*self.tokens, *self.other_tokens]\n",
    "        tokens = list(set(tokens))\n",
    "        if nouns_only:\n",
    "            tokens = [t for t in tokens if t.pos_ in NOUNS]\n",
    "        tokens = sorted(tokens, key=lambda token: token.i)\n",
    "        return f\"{','.join([t.text for t in tokens])}\"\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token_ref(main, tokens):\n",
    "        i = 0\n",
    "        size = len(tokens)\n",
    "        while i < size:\n",
    "            token = tokens[i]\n",
    "            if token.pos_ == \"PRON\":\n",
    "                tokens = [*main.coref_map.get(token, [token])]\n",
    "                break\n",
    "            i += 1\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token_unit(main, tokens):\n",
    "        i = 0\n",
    "        size = len(tokens)\n",
    "        unit_map_items = main.units.unit_map.items()\n",
    "        while i < size:\n",
    "            token = tokens[i]\n",
    "            for unit_bound, unit in unit_map_items:\n",
    "                token_in_unit = unit_bound[0] <= token.i <= unit_bound[1]\n",
    "                unit_is_valid = unit.label_has([Unit.ITEM])\n",
    "                if token_in_unit and unit_is_valid:\n",
    "                    tokens = [*main.sp_doc[unit_bound[0]:unit_bound[1]+1]]\n",
    "                    break\n",
    "            i += 1\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token_entity(main, tokens):\n",
    "        i = 0\n",
    "        size = len(tokens)\n",
    "        while i < size:\n",
    "            token = tokens[i]\n",
    "            if token in main.entity_map:\n",
    "                tokens.extend([*main.entity_map[token]])\n",
    "            i += 1\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token_noun_chunk(main, tokens):\n",
    "        i = 0\n",
    "        size = len(tokens)\n",
    "        while i < size:\n",
    "            token = tokens[i]\n",
    "            if token in main.noun_chunk_map:\n",
    "                tokens.extend([*main.noun_chunk_map[token]])\n",
    "            i += 1\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def extend_token(main, token):\n",
    "        tokens = [token]        \n",
    "        tokens = Node.extend_token_ref(main, tokens)\n",
    "        tokens = Node.extend_token_unit(main, tokens)\n",
    "        tokens = Node.extend_token_entity(main, tokens)\n",
    "        tokens = Node.extend_token_noun_chunk(main, tokens)\n",
    "        tokens = list(set(tokens))\n",
    "        tokens = sorted(tokens, key=lambda token: token.i)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77ab4234-8632-4116-905b-f075f6787380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.order = []\n",
    "    \n",
    "    \n",
    "    def start(self):\n",
    "        i = math.inf\n",
    "        for item in self.order:\n",
    "            i = min(i, item.start())\n",
    "        return i\n",
    "    \n",
    "    \n",
    "    def end(self):\n",
    "        i = -math.inf\n",
    "        for item in self.order:\n",
    "            i = max(i, item.end())\n",
    "        return i\n",
    "    \n",
    "    \n",
    "    def sent_i(self):\n",
    "        start = self.start()\n",
    "        sents = self.main.sp_doc.sents\n",
    "        for i, sent in enumerate(sents):\n",
    "            if sent.start <= start < sent.end:\n",
    "                return i\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "    def get_tokens(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_tokens())\n",
    "        return ret\n",
    "    \n",
    "    \n",
    "    def get_all_tokens(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_all_tokens())\n",
    "        return ret\n",
    "    \n",
    "    \n",
    "    def get_traits(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_traits())\n",
    "        return ret\n",
    "\n",
    "    \n",
    "    def get_species(self):\n",
    "        ret = []\n",
    "        for item in self.order:\n",
    "            ret.extend(item.get_species())\n",
    "        return ret\n",
    "    \n",
    "    \n",
    "    def attach_event(self, event, del_end=False):\n",
    "        if del_end:\n",
    "            del_node = self.order.pop()\n",
    "            for token in del_node.other_tokens:\n",
    "                if token.pos_ in [\"ADJ\"]:\n",
    "                    event.order[0].tokens.append(token)\n",
    "        self.order.extend(event.order)\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def copy(self):\n",
    "        event = Event(self.main)\n",
    "        event.order = []\n",
    "        for item in self.order:\n",
    "            event.order.append(item.copy())\n",
    "        return event\n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        ret = \"\"\n",
    "\n",
    "        i = 0\n",
    "        size = len(self.order)\n",
    "        while i < size:\n",
    "            ret += f\"({self.order[i]})\"\n",
    "            if i != size - 1:\n",
    "                ret += \"->\"\n",
    "            i += 1\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f935ba34-fa22-45ff-89e1-3eac1ca5cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventManager:\n",
    "    def __init__(self, main):\n",
    "        self.main = main\n",
    "        self.events = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fix_triple_tokens(self, s_node, v_node, o_node, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"fix_triple_tokens\")\n",
    "        \n",
    "        # Subject Tokens Transferred to Object\n",
    "        s_transfer_tokens = []\n",
    "        for token in s_node.tokens:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                s_transfer_tokens.append(token)\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Subject Tokens Transferred to Object: {s_transfer_tokens}\")\n",
    "        \n",
    "        s_node.tokens = [t for t in s_node.tokens if t not in s_transfer_tokens]\n",
    "        o_node.tokens.extend(s_transfer_tokens)\n",
    "        \n",
    "        # Object Tokens Transferred to Verb\n",
    "        o_transfer_tokens = []\n",
    "        for token in o_node.tokens:\n",
    "            if token in main.cause.tokens or token in main.change.tokens:\n",
    "                o_transfer_tokens.append(token)\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Object Tokens Transferred to Verb: {s_transfer_tokens}\")\n",
    "        \n",
    "        o_node.tokens = [t for t in o_node.tokens if t not in o_transfer_tokens]\n",
    "        v_node.tokens.extend(o_transfer_tokens)\n",
    "\n",
    "        return (s_node, v_node, o_node)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fix_triple_direction(self, s_node, v_node, o_node, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"fix_triple_direction\")\n",
    "        \n",
    "        v_tokens = sorted(v_node.tokens, key=lambda t: t.i)\n",
    "        \n",
    "        # Do Not Swap Phrases\n",
    "        # There are some phrases that would result in a swapping\n",
    "        # of the subject and object node, but shouldn't. So,\n",
    "        # we flag these phrases to prevent an incorrect swap.\n",
    "        v_text = \" \".join([token.lower_ for token in v_tokens])\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Verb Text: {v_text}\")\n",
    "        \n",
    "        for phrase in DO_NOT_SWAP_PHRASES:\n",
    "            if phrase in v_text:\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tVerb Text Contains: '{phrase}'\")\n",
    "                return (s_node, v_node, o_node)\n",
    "\n",
    "        # Swap Subject and Object\n",
    "        # If there is an AUX token before the verb (\"was caused\")\n",
    "        # or an ADP token after the verb (\"caused by\"), we swap\n",
    "        # the subject and object.\n",
    "        v_tokens_l = v_tokens[+0]\n",
    "        aux = v_tokens_l.nbor(-1) and v_tokens_l.nbor(-1).pos_ == \"AUX\"\n",
    "\n",
    "        v_tokens_r = v_tokens[-1]\n",
    "        adp = v_tokens_r.nbor(+1) and v_tokens_r.nbor(+1).pos_ == \"ADP\"\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"AUX: {aux}\")\n",
    "            print(f\"ADP: {aux}\")\n",
    "            print(f\"Swapped: {aux or adp}\")\n",
    "            \n",
    "        if aux or adp:\n",
    "            return (o_node, v_node, s_node)\n",
    "        return (s_node, v_node, o_node)\n",
    "            \n",
    "    \n",
    "    \n",
    "    def fix_triple_other_tokens(self, s_node, v_node, o_node, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"fix_triple_other_tokens\")\n",
    "        \n",
    "        s_node.other_tokens = flatten([Node.extend_token(main, t) for t in s_node.tokens])\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Subject Node Other Tokens: {s_node.other_tokens}\")\n",
    "        \n",
    "        o_node.other_tokens = flatten([Node.extend_token(main, t) for t in o_node.tokens])\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Object Node Other Tokens: {o_node.other_tokens}\")\n",
    "        \n",
    "        return (s_node, v_node, o_node)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def push_bound_l(self, bound, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"push_bound_l\")\n",
    "        \n",
    "        return self.push_bound(\n",
    "            bound, \n",
    "            reverse=True,\n",
    "            update_bound_fnc=lambda bound, unit: min(bound, unit.l),\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def push_bound_r(self, bound, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"push_bound_r\")\n",
    "        \n",
    "        return self.push_bound(\n",
    "            bound, \n",
    "            reverse=False,\n",
    "            update_bound_fnc=lambda bound, unit: max(bound, unit.r),\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def push_bound(self, bound, reverse=False, update_bound_fnc=None, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"push_bound\")\n",
    "        \n",
    "        units = self.main.units.units_at_i(bound)\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Units: {units}\")\n",
    "        \n",
    "        if not units:\n",
    "            if verbose and VERBOSE_LEVEL >= 1:\n",
    "                print(f\"No Units, Returned: {bound}\")\n",
    "            \n",
    "            return bound\n",
    "\n",
    "        start = False\n",
    "        unit_map_values = list(main.units.unit_map.values())\n",
    "        unit_map_values = sorted(unit_map_values, key=lambda u: (u.l, u.r), reverse=reverse)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(unit_map_values):\n",
    "            unit = unit_map_values[i]\n",
    "            \n",
    "            # Units must be in the same sent\n",
    "            # as the bound (token position).\n",
    "            if verbose and VERBOSE_LEVEL >= 1:\n",
    "                print(f\"Checking Unit: {unit}\")\n",
    "                print(f\"*Unit Start: {unit.sent_start()}\")\n",
    "                print(f\"*Token Unit Start: {units[0].sent_start()}\")\n",
    "                \n",
    "            if units[0].sent_start() != unit.sent_start():\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # The bound (token position) must\n",
    "            # be in the unit.\n",
    "            if unit not in units:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 1:\n",
    "                print(f\"Starting Unit: {unit}\")\n",
    "            \n",
    "            bound = update_bound_fnc(bound, unit)\n",
    "            \n",
    "            j = i + 1\n",
    "            while j < len(unit_map_values) and unit_map_values[j] not in units:\n",
    "                if verbose and VERBOSE_LEVEL >= 2:\n",
    "                    print(f\"\\tUnit: {unit_map_values[j]}\")\n",
    "                    print(f\"\\t*Unit Start: {unit_map_values[j].sent_start()}\")\n",
    "                    print(f\"\\t*Token Unit Start: {units[0].sent_start()}\")\n",
    "\n",
    "                if units[0].sent_start() != unit_map_values[j].sent_start():\n",
    "                    j += 1\n",
    "                    continue\n",
    "                \n",
    "                # If we see punctuation we break, we don't want to\n",
    "                # overstep any boundaries.\n",
    "                if unit_map_values[j].lower() in string.punctuation:\n",
    "                    if verbose and VERBOSE_LEVEL >= 2:\n",
    "                        print(f\"\\t\\tBreak, Punctuation Found\")\n",
    "                    break\n",
    "\n",
    "                if unit_map_values[j].label_has(PUSH_INCLUDE_LABELS):\n",
    "                    bound = update_bound_fnc(bound, unit_map_values[j])\n",
    "                    if verbose and VERBOSE_LEVEL >= 2:\n",
    "                        print(f\"\\t\\tBreak, Unit Found: {unit_map_values[j]}\")\n",
    "                    break\n",
    "                \n",
    "                j += 1\n",
    "            \n",
    "            break\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Returned: {bound}\")\n",
    "        \n",
    "        return bound\n",
    "    \n",
    "    \n",
    "\n",
    "    def find_verb_tokens(self, tokens, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"find_verb_tokens\")\n",
    "        \n",
    "        tokens = sorted(tokens, key=lambda t: t.i)\n",
    "        verb_tokens = []\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            token = tokens[i]\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tToken ({token.pos_}): {token}\")\n",
    "                print(f\"\\tNext Token ({token.nbor() and token.nbor().pos_}): {token.nbor()}\")\n",
    "\n",
    "            if token.pos_ != \"VERB\":\n",
    "                if token.pos_ != \"AUX\" or token.nbor().pos_ != \"VERB\":\n",
    "                    if verbose and VERBOSE_LEVEL >= 3:\n",
    "                        print(f\"\\t\\tNot a Verb, Continue\")\n",
    "                    \n",
    "                    i += 1\n",
    "                    continue\n",
    "            \n",
    "            verb_tokens.append(token)\n",
    "            \n",
    "            i += 1\n",
    "            while i < len(tokens) and tokens[i].pos_ in [\n",
    "                \"VERB\", \n",
    "                \"ADV\", \n",
    "                \"AUX\", \n",
    "                \"ADP\"\n",
    "            ]:\n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tAdd Verb Token: {tokens[i]}\")\n",
    "                \n",
    "                verb_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Returned: {verb_tokens}\")\n",
    "        \n",
    "        return verb_tokens\n",
    "        \n",
    "    \n",
    "    \n",
    "    def split_tokens(self, tokens, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"split_tokens\")\n",
    "        \n",
    "        # L Bound\n",
    "        l = tokens[0].i\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"l: {l}\")\n",
    "\n",
    "        # M Bound\n",
    "        # This is determined by the start and end verbs.\n",
    "        verb_tokens = self.find_verb_tokens(tokens, verbose=verbose)\n",
    "        v_l = verb_tokens[+0].i # Do Not Ask\n",
    "        v_r = verb_tokens[-1].i\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"v_l: {v_l}\")\n",
    "            print(f\"v_r: {v_r}\")\n",
    "\n",
    "        # R Bound\n",
    "        # We look for the first noun after the last verb.\n",
    "        i = tokens.index(verb_tokens[-1]) + 1\n",
    "        while i < len(tokens) and tokens[i].pos_ not in NOUNS:\n",
    "            i += 1\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"i: {i}\")\n",
    "        \n",
    "        no_noun_r = i <= 0 or i >= len(tokens)\n",
    "        r = v_r if no_noun_r else tokens[i].i\n",
    "            \n",
    "        no_noun_l = not set([token.pos_ for token in self.main.sp_doc[l:v_l]]) & set(NOUNS)\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"No Noun R: {no_noun_r}\")\n",
    "            print(f\"No Noun L: {no_noun_l}\")\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Returned: {((l, v_l, v_r, v_r), (no_noun_l, no_noun_r))}\")\n",
    "        return ((l, v_l, v_r, v_r), (no_noun_l, no_noun_r))\n",
    "\n",
    "                \n",
    "        # if no_noun_r and no_noun_l:\n",
    "        #     split = (-1, -1, -1, -1)\n",
    "        # elif no_noun_r:\n",
    "        #     split = (l, v_l, v_r, v_r)\n",
    "        # else: \n",
    "        #     r = tokens[i].i\n",
    "        #     if no_noun_l:\n",
    "        #         split = (v_l, v_l, v_r, r)\n",
    "        #     else:\n",
    "        #         split = (l, v_l, v_r, r)\n",
    "\n",
    "        # if verbose and VERBOSE_LEVEL >= 1:\n",
    "        #     print(f\"Returned: {split}\")\n",
    "        \n",
    "        # return split\n",
    "    \n",
    "    \n",
    "    \n",
    "    def convert_tokens_no_verb(self, tokens, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"convert_tokens_no_verb\")\n",
    "        \n",
    "        speech = [token.pos_ for token in tokens]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Speech: {speech}\")\n",
    "        \n",
    "        # ADP and SCONJ tokens sometimes indicate\n",
    "        # a cause-and-effect relationship, which is\n",
    "        # important.\n",
    "        if speech[0] in [\"ADP\", \"SCONJ\"]:\n",
    "            event = Event(self.main)\n",
    "            event.order = [Node(self.main, tokens=tokens)]\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 1:\n",
    "                print(f\"Returned: {event}\")\n",
    "            \n",
    "            return [event]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Returned: {None}\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    \n",
    "    \n",
    "    def convert_tokens_skewed_verb(self, tokens, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"convert_tokens_skewed_verb\")\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "        \n",
    "        # Verb Node\n",
    "        v_node = Node(self.main, is_action=True)\n",
    "        v_node.tokens = self.find_verb_tokens(tokens)\n",
    "\n",
    "        # Object Node\n",
    "        o_node = Node(self.main)\n",
    "        o_node.tokens = [token for token in tokens if token not in v_node.tokens]\n",
    "\n",
    "        # Check for Species, Traits\n",
    "        if not o_node.get_species() and not o_node.get_traits():\n",
    "            if verbose and VERBOSE_LEVEL >= 1:\n",
    "                print(f\"No Species or Traits, Returned []\")\n",
    "            return []\n",
    "\n",
    "        event = Event(self.main)\n",
    "        event.order = [v_node, o_node]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Returned: {event}\")\n",
    "        \n",
    "        return [event]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def convert_tokens_multiple_verbs(self, tokens, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"convert_tokens_multiple_verb\")\n",
    "        \n",
    "        tokens = [token for token in tokens if token.text not in string.punctuation]\n",
    "\n",
    "        # Number of Verbs\n",
    "        speech = [token.pos_ for token in tokens]\n",
    "        number_verbs = speech.count(\"VERB\")\n",
    "        \n",
    "        token_units = self.main.units.units_at_i(tokens[0].i)\n",
    "\n",
    "        # Example:\n",
    "        # The X increased and the Y decreased.\n",
    "        # We'd split the list into items and convert\n",
    "        # \"The X increased\" and \"the Y decreased\".\n",
    "        unit_lists = [unit for unit in token_units if unit.label_has([Unit.LIST])]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Unit Lists: {[l.__str__() for l in unit_lists]}\")\n",
    "        \n",
    "        if len(unit_lists) == 1:\n",
    "            unit_list = unit_lists[0]\n",
    "\n",
    "            # Check if Equal Distribution\n",
    "            # of Verbs\n",
    "            number_verbs_in_items = []\n",
    "            for child in unit_list.children:\n",
    "                if not child.label_has([Unit.ITEM]):\n",
    "                    continue\n",
    "                item_speech = [token.pos_ for token in child.span()]\n",
    "                number_verbs_in_item = item_speech.count(\"VERB\")\n",
    "                if number_verbs_in_item != 1:\n",
    "                    number_verbs_in_items = []\n",
    "                    break\n",
    "                number_verbs_in_items.append(number_verbs_in_item)\n",
    "\n",
    "            # Indirectly contains the number of \n",
    "            # items with 1 verb.\n",
    "            number_verbs_in_items = [n for n in number_verbs_in_items if n]\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tNumber Verbs: {number_verbs}\")\n",
    "                print(f\"\\tNumber Verbs in Items: {number_verbs_in_items}\")\n",
    "                \n",
    "            # Split by Item\n",
    "            if len(number_verbs_in_items) == number_verbs:\n",
    "                events = []\n",
    "                for child in unit_list.children:\n",
    "                    if not child.label_has([Unit.ITEM]):\n",
    "                        continue\n",
    "\n",
    "                    if verbose and VERBOSE_LEVEL >= 2:\n",
    "                        print(f\"\\tItem: {child}\")\n",
    "                        print(f\"\\tItem Tokens: {list(child.span())}\")\n",
    "                        \n",
    "                    item_events = self.convert_tokens(list(child.span()), verbose=verbose)\n",
    "                    if item_events is not None:\n",
    "                        events.extend(item_events)\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Returned:\")\n",
    "                    for event in events:\n",
    "                        print(f\"\\t{event}\")\n",
    "                \n",
    "                return events\n",
    "        # Skip\n",
    "        # This is overfitting the problem, so I'll leave it here for now.\n",
    "        # I may come back to it later, but I'll lump everything together\n",
    "        # for now.\n",
    "        # Example:\n",
    "        # The X and Y increased and decreased, respectively.\n",
    "        # We'd attach the X with increased and the Y with decreased.\n",
    "        # I will work on this later, I need to fix the lists and\n",
    "        # figure out other things.\n",
    "        # elif len(unit_lists) == 2:\n",
    "        #     list_a = unit_lists[0]\n",
    "        #     list_b = unit_lists[1]\n",
    "\n",
    "        #     list_a_items = [child for child in list_a.children if child.label_has([Unit.ITEM])]\n",
    "        #     list_b_items = [child for child in list_b.children if child.label_has([Unit.ITEM])]\n",
    "            \n",
    "        #     if len(list_a_items) == len(list_b_items):\n",
    "        #         list_a_tokens = list(list_a.span())\n",
    "        #         list_a_speech = [token.pos_ for token in list_a_tokens]\n",
    "        #         list_a_number_verbs = list_a_speech.count(\"VERB\")\n",
    "                \n",
    "        #         list_b_tokens = list(list_b.span())\n",
    "        #         list_b_speech = [token.pos_ for token in list_b_tokens]\n",
    "        #         list_b_number_verbs = list_b_speech.count(\"VERB\")\n",
    "\n",
    "        #         noun_list = None\n",
    "        #         verb_list = None\n",
    "                \n",
    "        #         if list_a_number_verbs == number_verbs and not list_b_number_verbs:\n",
    "        #             noun_list = list_b\n",
    "        #             verb_list = list_a\n",
    "        #         else:\n",
    "        #             noun_list = list_a\n",
    "        #             verb_list = list_b\n",
    "\n",
    "        #         list_a_items = sorted(list_a_items, key=lambda u: u.l)\n",
    "        #         list_b_items = sorted(list_b_items, key=lambda u: u.l)\n",
    "\n",
    "        #         events = []\n",
    "        #         for i in range(len(list_a_items)):\n",
    "        #             # Here, I'd have connect each item together,\n",
    "        #             # but again, missing the point, focusing too\n",
    "        #             # much on the details.\n",
    "        #             pass\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Returned: {None}\")\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    def convert_tokens(self, tokens, split=None, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >=1:\n",
    "            print(f\"convert_tokens\")\n",
    "\n",
    "        tokens = sorted(tokens, key=lambda t: t.i)\n",
    "        tokens = [token for token in tokens if token.text not in string.punctuation]\n",
    "        \n",
    "        if verbose and VERBOSE_LEVEL >=1:\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "        \n",
    "        if not tokens:\n",
    "            if verbose and VERBOSE_LEVEL >=1:\n",
    "                print(f\"No Tokens, Returned []\")\n",
    "            return []\n",
    "        \n",
    "        # Check Verbs\n",
    "        speech = [t.pos_ for t in tokens]\n",
    "        number_verbs = speech.count(\"VERB\")\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Speech: {speech}\")\n",
    "            print(f\"Number Verbs: {number_verbs}\")\n",
    "        \n",
    "        if number_verbs == 0:\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tNo Verbs\")\n",
    "            \n",
    "            events = self.convert_tokens_no_verb(tokens, verbose=verbose)\n",
    "            if events is not None:\n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Returned:\")\n",
    "                    for event in events:\n",
    "                        print(f\"\\t{event}\")\n",
    "                \n",
    "                return events\n",
    "        \n",
    "        elif number_verbs > 1:\n",
    "            \n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tMultiple Verbs\")\n",
    "                \n",
    "            events = self.convert_tokens_multiple_verbs(tokens, verbose=verbose)\n",
    "            if events is not None:    \n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Returned:\")\n",
    "                    for event in events:\n",
    "                        print(f\"\\t{event}\")\n",
    "                \n",
    "                return events\n",
    "        \n",
    "        if len(tokens) <= 2:\n",
    "            return []\n",
    "\n",
    "        # Check Split\n",
    "        (l, v_l, v_r, r), (no_noun_l, no_noun_r) = split or self.split_tokens(tokens, verbose=verbose)\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Split: ({l}, {v_l}, {v_r}, {r})\")\n",
    "        \n",
    "        if no_noun_l and no_noun_r:\n",
    "            if verbose and VERBOSE_LEVEL >= 1:\n",
    "                print(f\"No Split, Returned []\")\n",
    "            \n",
    "            return []\n",
    "        elif no_noun_l:\n",
    "            if verbose and VERBOSE_LEVEL >= 1:\n",
    "                print(f\"Skewed Right\")\n",
    "\n",
    "            r = self.push_bound_r(r, verbose=verbose)\n",
    "            events = self.convert_tokens_skewed_verb(self.main.sp_doc[l:r+1], verbose=verbose)\n",
    "            \n",
    "            if events is not None:\n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Returned:\")\n",
    "                    for event in events:\n",
    "                        print(f\"\\t{event}\")\n",
    "                \n",
    "                return events\n",
    "        elif no_noun_r:\n",
    "            if verbose and VERBOSE_LEVEL >= 1:\n",
    "                print(f\"Skewed Left\")\n",
    "\n",
    "            l = self.push_bound_l(l, verbose=verbose)\n",
    "            events = self.convert_tokens_skewed_verb(self.main.sp_doc[l:r+1], verbose=verbose)\n",
    "            \n",
    "            if events is not None:\n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Returned:\")\n",
    "                    for event in events:\n",
    "                        print(f\"\\t{event}\")\n",
    "                \n",
    "                return events\n",
    "\n",
    "        # Main\n",
    "        l = self.push_bound_l(l, verbose=verbose) # Do Not Ask\n",
    "        r = self.push_bound_r(r, verbose=verbose)\n",
    "        \n",
    "        v_node = Node(self.main, is_action=True)\n",
    "        v_node.tokens = [self.main.sp_doc[i] for i in range(v_l, v_r+1)]\n",
    "        \n",
    "        s_node = Node(self.main)\n",
    "        s_node.tokens = [self.main.sp_doc[i] for i in range(l, v_l)]\n",
    "            \n",
    "        o_node = Node(self.main)\n",
    "        o_node.tokens = [self.main.sp_doc[i] for i in range(v_r+1, r+1)]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Subject: {s_node}\")\n",
    "            print(f\"Verb: {v_node}\")\n",
    "            print(f\"Object: {o_node}\")\n",
    "\n",
    "        s_node, v_node, o_node = self.fix_triple_direction(s_node, v_node, o_node, verbose=verbose)\n",
    "        s_node, v_node, o_node = self.fix_triple_tokens(s_node, v_node, o_node, verbose=verbose)\n",
    "        s_node, v_node, o_node = self.fix_triple_other_tokens(s_node, v_node, o_node, verbose=verbose)\n",
    "\n",
    "        s_node_valid = s_node.get_species() or s_node.get_traits()\n",
    "        o_node_valid = o_node.get_species() or o_node.get_traits()\n",
    "        if not s_node_valid and not o_node_valid:\n",
    "            return []\n",
    "        \n",
    "        event = Event(self.main)\n",
    "        event.order = [s_node, v_node, o_node]\n",
    "\n",
    "        events = [event]\n",
    "        events.extend(self.convert_tokens(s_node.tokens, verbose=verbose))\n",
    "        events.extend(self.convert_tokens(o_node.tokens, verbose=verbose))\n",
    "        events = [event for event in events if event]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Returned:\")\n",
    "            for event in events:\n",
    "                print(f\"\\t{event}\")\n",
    "        \n",
    "        return events\n",
    "\n",
    "\n",
    "    \n",
    "    def convert_unit(self, unit, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"convert_unit\")\n",
    "        \n",
    "        tokens = [*self.main.sp_doc[unit.l:unit.r+1]]\n",
    "        \n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "        \n",
    "        return self.convert_tokens(tokens)\n",
    "    \n",
    "    \n",
    "\n",
    "    def convert_svo_triple(self, triple, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"convert_svo_triple\")\n",
    "        \n",
    "        tokens = [*triple.subject, *triple.object]\n",
    "        tokens = sorted(tokens, key=lambda t: t.i)\n",
    "    \n",
    "        l = tokens[+0].i\n",
    "        v_l = triple.verb[+0].i\n",
    "        v_r = triple.verb[-1].i\n",
    "        r = tokens[-1].i\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Split: ({l}, {v_l}, {v_r}, {r})\")\n",
    "        \n",
    "        tokens = [*self.main.sp_doc[l:r+1]]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "        \n",
    "        return self.convert_tokens(tokens, split=(l, v_l, v_r, r))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def distinct_events(self, events, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"distinct_events\")\n",
    "        \n",
    "        event_bounds_mapped = {}\n",
    "        for event in events:\n",
    "            event_bound = (event.start(), event.end())\n",
    "            if event_bound in event_bounds_mapped:\n",
    "                if len(event.order) > len(event_bounds_mapped[event_bound].order):\n",
    "                    event_bounds_mapped[event_bound] = event\n",
    "            else:\n",
    "                event_bounds_mapped[event_bound] = event\n",
    "        \n",
    "        event_bounds = event_bounds_mapped.keys()   \n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Event Bounds: {event_bounds}\")\n",
    "        \n",
    "        distinct_event_bounds = distinct_bounds(event_bounds, larger=True)\n",
    "        distinct_events = [event_bounds_mapped[b] for b in distinct_event_bounds]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Distinct Event Bounds: {distinct_event_bounds}\")\n",
    "        \n",
    "        return distinct_events\n",
    "    \n",
    "\n",
    "    \n",
    "    def find_events(self, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"find_events\")\n",
    "        \n",
    "        sents = list(self.main.sp_doc.sents)\n",
    "        sents_events = {sent.start: [] for sent in sents}\n",
    "\n",
    "         # Units\n",
    "        for unit_tokens in self.main.units.aggregate_units():\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"Finding Events in Tokens: {unit_tokens}\")\n",
    "            \n",
    "            events = self.convert_tokens(unit_tokens, verbose=verbose)\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                if events:\n",
    "                    print(f\"\\tEvents:\")\n",
    "                    for event in events:\n",
    "                        print(f\"\\t\\tEvent: {event}\")\n",
    "                else:\n",
    "                    print(f\"\\tNo Events\")\n",
    "            \n",
    "            sent_start = unit_tokens[0].sent.start\n",
    "            sents_events[sent_start].extend(events)\n",
    "        \n",
    "        # Triples\n",
    "        for triple in textacy.extract.subject_verb_object_triples(self.main.sp_doc):\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"Finding Events in Triple: {triple}\")\n",
    "            \n",
    "            events = self.convert_svo_triple(triple, verbose=verbose)\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                if events:\n",
    "                    print(f\"\\tEvents:\")\n",
    "                    for event in events:\n",
    "                        print(f\"\\t\\tEvent: {event}\")\n",
    "                else:\n",
    "                    print(f\"\\tNo Events\")\n",
    "            \n",
    "            sent_start = triple.verb[0].sent.start\n",
    "            sents_events[sent_start].extend(events)\n",
    "            \n",
    "        sents_events = {k: self.distinct_events(v) for k, v in sents_events.items()}\n",
    "        return sents_events\n",
    "    \n",
    "    \n",
    "    \n",
    "    def overlap_in_species(self, x, y, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"overlap_in_species\")\n",
    "        \n",
    "        sp_X = [self.main.species.span_at_token(species) for species in x.get_species()]\n",
    "        sp_Y = [self.main.species.span_at_token(species) for species in y.get_species()]\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"sp_X: {sp_X}\")\n",
    "            print(f\"sp_Y: {sp_Y}\")\n",
    "\n",
    "        for sp_x in sp_X:\n",
    "            if self.main.species.find_same_species(sp_Y, sp_x, verbose=verbose):\n",
    "                if verbose and VERBOSE_LEVEL >= 1:\n",
    "                    print(f\"Returned True\")\n",
    "                \n",
    "                return True\n",
    "        \n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Returned False\")\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    \n",
    "\n",
    "    def overlap_in_traits(self, x, y, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"overlap_in_traits\")\n",
    "        \n",
    "        tr_X = set([trait.lemma_.lower() for trait in x.get_traits()])\n",
    "        tr_Y = set([trait.lemma_.lower() for trait in y.get_traits()])\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"tr_X: {tr_X}\")\n",
    "            print(f\"tr_Y: {tr_Y}\")\n",
    "            print(f\"Returned {bool(tr_X & tr_Y)}\")\n",
    "        \n",
    "        return bool(tr_X & tr_Y)\n",
    "        \n",
    "\n",
    "    \n",
    "    def overlap_in_tokens(self, x, y, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"overlap_in_tokens\")\n",
    "        \n",
    "        x_nouns = set(self.get_non_species_nouns(x.other_tokens))\n",
    "        y_nouns = set(self.get_non_species_nouns(y.other_tokens))\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"x_nouns: {x_nouns}\")\n",
    "            print(f\"y_nouns: {y_nouns}\")\n",
    "            print(f\"Returned {bool(x_nouns & y_nouns)}\")\n",
    "        \n",
    "        return bool(x_nouns & y_nouns)\n",
    "\n",
    "\n",
    "    \n",
    "    def get_non_species_nouns(self, tokens):\n",
    "        nouns = []\n",
    "        \n",
    "        for token in tokens:            \n",
    "            if token in self.main.species.tokens:\n",
    "                continue\n",
    "\n",
    "            if token.pos_ not in [\"PROPN\", \"NOUN\"]:\n",
    "                continue\n",
    "            \n",
    "            nouns.append(token.lower_)\n",
    "            nouns.append(token.lemma_.lower())\n",
    "        \n",
    "        return nouns\n",
    "    \n",
    "    \n",
    "    \n",
    "    def has_cause_phrase(self, x, y, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"has_cause_phrase\")\n",
    "        \n",
    "        speech = [token.pos_ for token in x.get_tokens()]\n",
    "        cause = speech[0] in [\"ADP\", \"SCONJ\"]\n",
    "        order = x.start() < y.start()\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Speech: {speech}\")\n",
    "            print(f\"Cause ({bool(cause)}): {cause}\")\n",
    "            print(f\"Order ({bool(order)}): {order}\")\n",
    "            print(f\"Returned: {cause and order}\")\n",
    "        \n",
    "        return cause and order\n",
    "    \n",
    "    \n",
    "    \n",
    "    def has_effect_phrase(self, x, y, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"has_effect_phrase\")\n",
    "\n",
    "        speech = [token.pos_ for token in y.get_tokens()]\n",
    "        lowers = [token.lower_ for token in y.get_tokens()]\n",
    "        \n",
    "        cause = \"PRON\" in speech or \"SCONJ\" in speech\n",
    "        cause = cause or set([\"therefore\", \"cause\", \"result\", \"so\"]).intersection(lowers) # TODO: Create Long List\n",
    "        \n",
    "        order = x.start() < y.start()\n",
    "        space = y.start() - x.end() - 1\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Speech: {speech}\")\n",
    "            print(f\"Cause ({bool(cause)}): {cause}\")\n",
    "            print(f\"Order ({bool(order)}): {order}\")\n",
    "            print(f\"Space ({bool(0 <= space <= 1)}): {space}\")\n",
    "            print(f\"Returned: {(cause and order) or 0 <= space <= 1}\")\n",
    "\n",
    "        return (cause and order) or 0 <= space <= 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    def has_cause_phrase_and_pron(self, n, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"has_cause_phrase_and_pron\")\n",
    "        \n",
    "        speech = set([token.pos_ for token in n.tokens])\n",
    "        cause_and_pron = (\"ADP\" in speech or \"SCONJ\" in speech) and \"PRON\" in speech\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Speech: {speech}\")\n",
    "            print(f\"Cause and Pronoun: {cause_and_pron}\")\n",
    "            print(f\"Returned: {cause_and_pron}\")\n",
    "    \n",
    "        return cause_and_pron\n",
    "    \n",
    "    \n",
    "    \n",
    "    def can_merge_intrasent(self, X, Y, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"can_merge_intrasent\")\n",
    "            print(f\"*X: {X}\")\n",
    "            print(f\"*Y: {Y}\")\n",
    "        \n",
    "        x = X.order[-1]\n",
    "        y = Y.order[+0] # Do Not Ask\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"x: {x}\")\n",
    "            print(f\"y: {y}\")\n",
    "        \n",
    "        can_merge = self.overlap_in_species(x, y, verbose=verbose)\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "\n",
    "        can_merge = self.overlap_in_tokens(x, y, verbose=verbose)\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "\n",
    "        # Example:\n",
    "        # \"By the ..., X did Y.\"\n",
    "        can_merge = len(X.order) == 1 and self.has_cause_phrase(x, y, verbose=verbose)\n",
    "        if can_merge:\n",
    "            return (True, False)\n",
    "\n",
    "        # Example:\n",
    "        # \"..., therefore X did Y\"\n",
    "        can_merge = len(Y.order) == 3 and self.has_effect_phrase(x, y, verbose=verbose)\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "\n",
    "        # Example:\n",
    "        # \"..., therefore X increased\"\n",
    "        can_merge = len(Y.order) == 2 and self.has_effect_phrase(x, Y, verbose=verbose)\n",
    "        if can_merge:\n",
    "            return (True, False)\n",
    "        \n",
    "        return (False, False)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def can_merge_intersent(self, X, Y, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"can_merge_intersent\")\n",
    "            print(f\"*X: {X}\")\n",
    "            print(f\"*Y: {Y}\")\n",
    "        \n",
    "        x = X.order[-1]\n",
    "        y = Y.order[+0] # Do Not Ask\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"x: {x}\")\n",
    "            print(f\"y: {y}\")\n",
    "        \n",
    "        can_merge = self.overlap_in_species(x, y, verbose=verbose)\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "\n",
    "        can_merge = self.overlap_in_tokens(x, y, verbose=verbose)\n",
    "        if can_merge:\n",
    "            return (True, True)\n",
    "\n",
    "        can_merge = self.has_cause_phrase_and_pron(y, verbose=verbose)\n",
    "        if can_merge:\n",
    "            return (True, False)\n",
    "        \n",
    "        return (False, False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def intrasent_sequences(self, indices_1D):\n",
    "        power_set_indices = power_set(indices_1D, current=[], result=[])\n",
    "        sequences = []\n",
    "        for subset in power_set_indices:\n",
    "            if subset:\n",
    "                sequences.extend(list(permutations(subset)))\n",
    "        return sequences\n",
    "\n",
    "\n",
    "    \n",
    "    def intersent_sequences(self, indices_2D):\n",
    "        sequences = interleave(indices_2D, current=[], used=[], result=[])\n",
    "        return sequences\n",
    "    \n",
    "    \n",
    "    \n",
    "    def merge_intrasent(self, sent_events, verbose=False):\n",
    "        i = 0\n",
    "        while i + 1 < len(sent_events):\n",
    "            X = sent_events[i]\n",
    "            Y = sent_events[i+1]\n",
    "            \n",
    "            can_merge, del_end = self.can_merge_intrasent(X, Y, verbose=verbose)            \n",
    "            if not can_merge:\n",
    "                return None\n",
    "\n",
    "            X.attach_event(Y, del_end=del_end)\n",
    "            sent_events.pop()\n",
    "\n",
    "        if len(sent_events[0].order) <= 2:\n",
    "            return None\n",
    "        return sent_events[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def merge_intersent(self, events, verbose=False):\n",
    "        i = 0\n",
    "        while i + 1 < len(events):\n",
    "            X = events[i]\n",
    "            Y = events[i+1]\n",
    "\n",
    "            can_merge, del_end = self.can_merge_intersent(X, Y, verbose=verbose)\n",
    "            if not can_merge:\n",
    "                return None\n",
    "\n",
    "            X.attach_event(Y, del_end=del_end)\n",
    "            events.pop()\n",
    "\n",
    "        return events[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_events(self, verbose=False):\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(\"load_events\")\n",
    "        \n",
    "        sents_events = self.find_events(verbose=verbose)\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 1:\n",
    "            print(f\"Events Found:\")\n",
    "            for events in sents_events.values():\n",
    "                for event in events:\n",
    "                    print(f\"*{event}\")\n",
    "        \n",
    "        merged_sents_events = []\n",
    "        \n",
    "        for sent_events in sents_events.values():\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tMerging Sentence Events ({len(sent_events)})\")\n",
    "            \n",
    "            merged_sents_events.append([])\n",
    "\n",
    "            # Each event can be represented with its index\n",
    "            # for simplicity.\n",
    "            event_indices = list(range(len(sent_events)))\n",
    "            \n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tEvent Indices (INTRA): {event_indices}\")\n",
    "\n",
    "            # The order in which you merge an event can result\n",
    "            # in a different outcome. For example, E1 + E2 may\n",
    "            # look different to E2 + E1, and so on.\n",
    "\n",
    "            # Idea:\n",
    "            # We try all the possible ways you can merge, which is\n",
    "            # the same as trying all the different orders you can\n",
    "            # merge the events. For example, merging E1, E2, and E3\n",
    "            # can be done in 8 ways: E1, E2, E3, E1 + E2, E2 + E1,\n",
    "            # and so on.\n",
    "            \n",
    "            # These are all the possible orders (or sequences) in \n",
    "            # which you can merge the above events. We will try all\n",
    "            # possible sequences, adding those that work.\n",
    "            sequences = self.intrasent_sequences(event_indices)\n",
    "\n",
    "            if verbose and VERBOSE_LEVEL >= 2:\n",
    "                print(f\"\\tMerge Sequences: {sequences}\")\n",
    "            \n",
    "            for sequence in sequences:\n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tSequence: {sequence}\")\n",
    "                \n",
    "                events = [sent_events[i].copy() for i in sequence]\n",
    "                merged_event = self.merge_intrasent(events, verbose=verbose)\n",
    "\n",
    "                if verbose and VERBOSE_LEVEL >= 3:\n",
    "                    print(f\"\\t\\tMerged Event: {merged_event}\")\n",
    "                \n",
    "                if merged_event:\n",
    "                    merged_sents_events[-1].append(merged_event)\n",
    "        \n",
    "        # Idea:\n",
    "        # Now that we've found all the events that can be made from\n",
    "        # merging the events within a sentence, we can merge across\n",
    "        # sentences. To do this, we also need to try all different\n",
    "        # sequences of events. However, we shouldn't try and merge\n",
    "        # events from the same sentence again, which means that no\n",
    "        # two consecutive events in the sequence can be from the same\n",
    "        # sentence. There's also another issue: we can't use a simple\n",
    "        # 1D-index for each event as we're dealing with a 2D-list of events.\n",
    "        # Therefore, we'll use the index equivalent to its 1D-form, or\n",
    "        # something similar as we don't have a matrix.\n",
    "        # If this doesn't make sense, I wouldn't be surprised, I'm more\n",
    "        # of a pictures person.\n",
    "        i = 0\n",
    "        event_indices = []\n",
    "        index_to_event = {}\n",
    "        for sent_events in merged_sents_events:\n",
    "            event_indices.append([])\n",
    "            for event in sent_events:\n",
    "                index_to_event[i] = event\n",
    "                event_indices[-1].append(i)\n",
    "                i += 1\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 2:\n",
    "            print(f\"\\tEvent Indices (INTER): {event_indices}\")\n",
    "            \n",
    "        merged_events = []\n",
    "        sequences = self.intersent_sequences(event_indices)\n",
    "\n",
    "        if verbose and VERBOSE_LEVEL >= 2:\n",
    "            print(f\"\\tMerge Sequences: {sequences}\")\n",
    "            \n",
    "        for sequence in sequences:\n",
    "            if verbose and VERBOSE_LEVEL >= 3:\n",
    "                print(f\"\\t\\tSequence: {sequence}\")\n",
    "            \n",
    "            events = [index_to_event[i].copy() for i in sequence]\n",
    "            merged_event = self.merge_intersent(events, verbose=verbose)\n",
    "            \n",
    "            if verbose and VERBOSE_LEVEL >= 3:\n",
    "                print(f\"\\t\\tMerged Event: {merged_event}\")\n",
    "            \n",
    "            if merged_event:\n",
    "                merged_events.append(merged_event)\n",
    "        \n",
    "        return merged_events\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update(self, verbose=False):\n",
    "        self.events = self.load_events(verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "037079d2-3ece-40a1-923e-84936e2b44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main = Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "495eb192-04e6-4dd0-baae-5bfdf02803e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"The dog caused the cat to cry.\"\n",
    "# main.update_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "77ea920a-0913-4875-8405-46e6a09cbf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_events\n",
      "find_events\n",
      "Finding Events in Tokens: [The, dog, caused, the, cat]\n",
      "convert_tokens\n",
      "Tokens: [The, dog, caused, the, cat]\n",
      "Speech: ['DET', 'NOUN', 'VERB', 'DET', 'NOUN']\n",
      "Number Verbs: 1\n",
      "split_tokens\n",
      "l: 0\n",
      "find_verb_tokens\n",
      "Tokens: [The, dog, caused, the, cat]\n",
      "\tToken (DET): The\n",
      "\tNext Token (NOUN): dog\n",
      "\t\tNot a Verb, Continue\n",
      "\tToken (NOUN): dog\n",
      "\tNext Token (VERB): caused\n",
      "\t\tNot a Verb, Continue\n",
      "\tToken (VERB): caused\n",
      "\tNext Token (DET): the\n",
      "\tToken (DET): the\n",
      "\tNext Token (NOUN): cat\n",
      "\t\tNot a Verb, Continue\n",
      "\tToken (NOUN): cat\n",
      "\tNext Token (PART): to\n",
      "\t\tNot a Verb, Continue\n",
      "Returned: [caused]\n",
      "v_l: 2\n",
      "v_r: 2\n",
      "i: 4\n",
      "No Noun R: False\n",
      "No Noun L: False\n",
      "Returned: ((0, 2, 2, 2), (False, False))\n",
      "Split: (0, 2, 2, 2)\n",
      "push_bound_l\n",
      "push_bound\n",
      "Units: [<__main__.Unit object at 0x0000025C30943640>]\n",
      "Checking Unit: .\n",
      "*Unit Start: 0\n",
      "*Token Unit Start: 0\n",
      "Checking Unit: to cry\n",
      "*Unit Start: 0\n",
      "*Token Unit Start: 0\n",
      "Checking Unit: The dog caused the cat\n",
      "*Unit Start: 0\n",
      "*Token Unit Start: 0\n",
      "Starting Unit: The dog caused the cat\n",
      "Returned: 0\n",
      "push_bound_r\n",
      "push_bound\n",
      "Units: [<__main__.Unit object at 0x0000025C30943640>]\n",
      "Checking Unit: The dog caused the cat\n",
      "*Unit Start: 0\n",
      "*Token Unit Start: 0\n",
      "Starting Unit: The dog caused the cat\n",
      "\tUnit: to cry\n",
      "\t*Unit Start: 0\n",
      "\t*Token Unit Start: 0\n",
      "\t\tBreak, Unit Found: to cry\n",
      "Returned: 6\n",
      "Subject: The,dog\n",
      "Verb: caused\n",
      "Object: the,cat,to,cry\n",
      "fix_triple_direction\n",
      "Verb Text: caused\n",
      "AUX: False\n",
      "ADP: False\n",
      "Swapped: False\n",
      "fix_triple_tokens\n",
      "Subject Tokens Transferred to Object: []\n",
      "Object Tokens Transferred to Verb: []\n",
      "fix_triple_other_tokens\n",
      "Subject Node Other Tokens: [The, dog, The, dog]\n",
      "Object Node Other Tokens: [the, cat, the, cat, to, cry]\n",
      "convert_tokens\n",
      "Tokens: [The, dog]\n",
      "Speech: ['DET', 'NOUN']\n",
      "Number Verbs: 0\n",
      "\tNo Verbs\n",
      "convert_tokens_no_verb\n",
      "Speech: ['DET', 'NOUN']\n",
      "Returned: None\n",
      "Returned:\n",
      "convert_tokens\n",
      "Tokens: [the, cat, to, cry]\n",
      "Speech: ['DET', 'NOUN', 'PART', 'VERB']\n",
      "Number Verbs: 1\n",
      "split_tokens\n",
      "l: 3\n",
      "find_verb_tokens\n",
      "Tokens: [the, cat, to, cry]\n",
      "\tToken (DET): the\n",
      "\tNext Token (NOUN): cat\n",
      "\t\tNot a Verb, Continue\n",
      "\tToken (NOUN): cat\n",
      "\tNext Token (PART): to\n",
      "\t\tNot a Verb, Continue\n",
      "\tToken (PART): to\n",
      "\tNext Token (VERB): cry\n",
      "\t\tNot a Verb, Continue\n",
      "\tToken (VERB): cry\n",
      "\tNext Token (PUNCT): .\n",
      "Returned: [cry]\n",
      "v_l: 6\n",
      "v_r: 6\n",
      "i: 4\n",
      "No Noun R: True\n",
      "No Noun L: False\n",
      "Returned: ((3, 6, 6, 6), (False, True))\n",
      "Split: (3, 6, 6, 6)\n",
      "Skewed Left\n",
      "push_bound_l\n",
      "push_bound\n",
      "Units: [<__main__.Unit object at 0x0000025C30943640>]\n",
      "Checking Unit: .\n",
      "*Unit Start: 0\n",
      "*Token Unit Start: 0\n",
      "Checking Unit: to cry\n",
      "*Unit Start: 0\n",
      "*Token Unit Start: 0\n",
      "Checking Unit: The dog caused the cat\n",
      "*Unit Start: 0\n",
      "*Token Unit Start: 0\n",
      "Starting Unit: The dog caused the cat\n",
      "Returned: 0\n",
      "convert_tokens_skewed_verb\n",
      "Tokens: The dog caused the cat to cry\n",
      "Returned: (caused,cry)->(The,dog,the,cat,to)\n",
      "Returned:\n",
      "\t(caused,cry)->(The,dog,the,cat,to)\n",
      "Returned:\n",
      "\t(The,dog)->(caused)->(the,cat,to,cry)\n",
      "\t(caused,cry)->(The,dog,the,cat,to)\n",
      "\tEvents:\n",
      "\t\tEvent: (The,dog)->(caused)->(the,cat,to,cry)\n",
      "\t\tEvent: (caused,cry)->(The,dog,the,cat,to)\n",
      "Finding Events in Tokens: [to, cry]\n",
      "convert_tokens\n",
      "Tokens: [to, cry]\n",
      "Speech: ['PART', 'VERB']\n",
      "Number Verbs: 1\n",
      "\tNo Events\n",
      "Finding Events in Tokens: [.]\n",
      "convert_tokens\n",
      "Tokens: []\n",
      "No Tokens, Returned []\n",
      "\tNo Events\n",
      "Before Distinct-ifying:\n",
      "(The,dog)->(caused)->(the,cat,to,cry)\n",
      "(caused,cry)->(The,dog,the,cat,to)\n",
      "After Distinct-ifying:\n",
      "(The,dog)->(caused)->(the,cat,to,cry)\n",
      "Events Found:\n",
      "*(The,dog)->(caused)->(the,cat,to,cry)\n",
      "\tMerging Sentence Events (1)\n",
      "\tEvent Indices (INTRA): [0]\n",
      "\tMerge Sequences: [(0,)]\n",
      "\t\tSequence: (0,)\n",
      "\t\tMerged Event: (The,dog)->(caused)->(the,cat,to,cry)\n",
      "\tEvent Indices (INTER): [[0]]\n",
      "\tMerge Sequences: [[0]]\n",
      "\t\tSequence: [0]\n",
      "\t\tMerged Event: (The,dog)->(caused)->(the,cat,to,cry)\n",
      "(The,dog)->(caused)->(the,cat,to,cry)\n"
     ]
    }
   ],
   "source": [
    "event_manager = EventManager(main)\n",
    "event_manager.update(verbose=True)\n",
    "\n",
    "for event in event_manager.events:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0872ebd-93c0-4e1a-9e29-765c78514494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
